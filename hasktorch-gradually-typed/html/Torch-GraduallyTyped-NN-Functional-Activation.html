<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Torch.GraduallyTyped.NN.Functional.Activation</title><link href="linuwial.css" rel="stylesheet" type="text/css" title="Linuwial" /><link rel="stylesheet" type="text/css" href="quick-jump.css" /><link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400i,700" /><script src="haddock-bundle.min.js" async="async" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { processClass: "mathjax", ignoreClass: ".*" } });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body><div id="package-header"><span class="caption">hasktorch-gradually-typed-0.2.0.0: experimental project for hasktorch</span><ul class="links" id="page-menu"><li><a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html">Source</a></li><li><a href="../../share/doc/index.html">Contents</a></li><li><a href="../../share/doc/doc-index.html">Index</a></li></ul></div><div id="content"><div id="module-header"><table class="info"><tr><th>Safe Haskell</th><td>Safe-Inferred</td></tr><tr><th>Language</th><td>Haskell2010</td></tr></table><p class="caption">Torch.GraduallyTyped.NN.Functional.Activation</p></div><div id="synopsis"><details id="syn"><summary>Synopsis</summary><ul class="details-toggle" data-details-id="syn"><li class="src short"><a href="#v:threshold">threshold</a> :: <span class="keyword">forall</span> threshold value gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> threshold, <a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> value, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m) =&gt; threshold -&gt; value -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:relu">relu</a> :: <span class="keyword">forall</span> gradient layout device dataType shape. <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</li><li class="src short"><a href="#v:gelu">gelu</a> :: <span class="keyword">forall</span> gradient layout device dataType shape. <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</li><li class="src short"><a href="#v:geluNew">geluNew</a> :: <span class="keyword">forall</span> gradient layout device dataType shape m. <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m =&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:hardtanh">hardtanh</a> :: <span class="keyword">forall</span> minValue maxValue gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> minValue, <a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> maxValue, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m) =&gt; minValue -&gt; maxValue -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:hardswish">hardswish</a> :: <span class="keyword">forall</span> gradient layout device dataType shape. <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</li><li class="src short"><a href="#v:elu">elu</a> :: <span class="keyword">forall</span> alpha gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> alpha, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m) =&gt; alpha -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:selu">selu</a> :: <span class="keyword">forall</span> gradient layout device dataType shape m. <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m =&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:celu">celu</a> :: <span class="keyword">forall</span> alpha gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> alpha, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m) =&gt; alpha -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:leakyRelu">leakyRelu</a> :: <span class="keyword">forall</span> negativeSlope gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> negativeSlope, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m) =&gt; negativeSlope -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li><li class="src short"><a href="#v:prelu">prelu</a> :: <span class="keyword">forall</span> gradient' gradient layout device dataType shape m. <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m =&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient' layout device dataType shape -&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape -&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</li></ul></details></div><div id="interface"><h1>Documentation</h1><div class="doc"><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>import Torch.GraduallyTyped.Prelude.List (SList (..))
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>import Torch.GraduallyTyped
</code></strong></pre></div><div class="top"><p class="src"><a id="v:threshold" class="def">threshold</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#threshold" class="link">Source</a> <a href="#v:threshold" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> threshold value gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> threshold, <a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> value, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m)</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; threshold</td><td class="doc"><p>threshold</p></td></tr><tr><td class="src">-&gt; value</td><td class="doc"><p>value</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Thresholds each element of the input Tensor.</p></div></div><div class="top"><p class="src"><a id="v:relu" class="def">relu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#relu" class="link">Source</a> <a href="#v:relu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> gradient layout device dataType shape. <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the rectified linear unit function element-wise, that is,
 <span class="mathjax">\[
 \text{ReLU}(x) = max(0, x).
 \]</span></p></div></div><div class="top"><p class="src"><a id="v:gelu" class="def">gelu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#gelu" class="link">Source</a> <a href="#v:gelu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> gradient layout device dataType shape. <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the gaussian error linear unit function element-wise.</p></div></div><div class="top"><p class="src"><a id="v:geluNew" class="def">geluNew</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew" class="link">Source</a> <a href="#v:geluNew" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> gradient layout device dataType shape m. <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the gaussian error linear unit function element-wise.</p><p>This is the implementation of the GELU activation function from
 Google's BERT repo (and coincidentally also from OpenAI's GPT).
 See also <a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a>.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>t &lt;- sFull (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNil)) 0.5
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>t' &lt;- geluNew t
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>fromTensor @Float t'
</code></strong>0.345714
</pre></div></div><div class="top"><p class="src"><a id="v:hardtanh" class="def">hardtanh</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#hardtanh" class="link">Source</a> <a href="#v:hardtanh" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> minValue maxValue gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> minValue, <a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> maxValue, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m)</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; minValue</td><td class="doc"><p>minimum value</p></td></tr><tr><td class="src">-&gt; maxValue</td><td class="doc"><p>maximum value</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the HardTanh function element-wise.</p></div></div><div class="top"><p class="src"><a id="v:hardswish" class="def">hardswish</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#hardswish" class="link">Source</a> <a href="#v:hardswish" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> gradient layout device dataType shape. <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the hardswish function element-wise.</p></div></div><div class="top"><p class="src"><a id="v:elu" class="def">elu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#elu" class="link">Source</a> <a href="#v:elu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> alpha gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> alpha, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m)</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; alpha</td><td class="doc"><p>alpha value for ELU formulation</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the exponential linear unit function element-wise, with alpha input,
 <span class="mathjax">\[
 \text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1)).
 \]</span></p></div></div><div class="top"><p class="src"><a id="v:selu" class="def">selu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#selu" class="link">Source</a> <a href="#v:selu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> gradient layout device dataType shape m. <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the scaled exponential linear unit function element-wise, that is,
 <span class="mathjax">\[
 \text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)),
 \]</span>
 with <span class="mathjax">\(\alpha = 1.6732632423543772848170429916717\)</span>
 and <span class="mathjax">\(\text{scale}=1.0507009873554804934193349852946\)</span>.</p></div></div><div class="top"><p class="src"><a id="v:celu" class="def">celu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#celu" class="link">Source</a> <a href="#v:celu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> alpha gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> alpha, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m)</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; alpha</td><td class="doc"><p>alpha</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the continuously differentiable exponential linear unit function element-wise, that is,
 <span class="mathjax">\[
 \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1)).
 \]</span></p></div></div><div class="top"><p class="src"><a id="v:leakyRelu" class="def">leakyRelu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#leakyRelu" class="link">Source</a> <a href="#v:leakyRelu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> negativeSlope gradient layout device dataType shape m. (<a href="../../hasktorch/html/Torch-Scalar.html#t:Scalar" title="Torch.Scalar">Scalar</a> negativeSlope, <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m)</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; negativeSlope</td><td class="doc"><p>negative slope</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the element-wise function:
 <span class="mathjax">\[
 \text{LeakyReLU}(x) = \max(0,x) + \text{negativeSlope} * \min(0,x),
 \]</span>
 the the angle of the negative slope can be controlled.
 A typical value for it is 0.01.</p></div></div><div class="top"><p class="src"><a id="v:prelu" class="def">prelu</a> <a href="src/Torch.GraduallyTyped.NN.Functional.Activation.html#prelu" class="link">Source</a> <a href="#v:prelu" class="selflink">#</a></p><div class="subs arguments"><p class="caption">Arguments</p><table><tr><td class="src">:: <span class="keyword">forall</span> gradient' gradient layout device dataType shape m. <a href="file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/Control-Monad-Catch.html#t:MonadThrow" title="Control.Monad.Catch">MonadThrow</a> m</td><td class="doc empty">&nbsp;</td></tr><tr><td class="src">=&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient' layout device dataType shape</td><td class="doc"><p>weight (typically learnable)</p></td></tr><tr><td class="src">-&gt; <a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape</td><td class="doc"><p>input</p></td></tr><tr><td class="src">-&gt; m (<a href="Torch-GraduallyTyped-Tensor-Type.html#t:Tensor" title="Torch.GraduallyTyped.Tensor.Type">Tensor</a> gradient layout device dataType shape)</td><td class="doc"><p>output</p></td></tr></table></div><div class="doc"><p>Applies the parameterized rectified linear unit function element-wise, that is,
 <span class="mathjax">\[
 \text{PReLU}(x) = max(0, x) + \text{weight} * min(0, x).
 \]</span>
 The weight parameter is typically learnable.</p></div></div></div></div><div id="footer"><p>Produced by <a href="http://www.haskell.org/haddock/">Haddock</a> version 2.26.0</p></div></body></html>