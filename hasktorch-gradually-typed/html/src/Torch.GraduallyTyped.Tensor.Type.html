<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE AllowAmbiguousTypes #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE ConstraintKinds #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE DeriveGeneric #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE DerivingStrategies #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE FunctionalDependencies #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE InstanceSigs #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE KindSignatures #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE LambdaCase #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# LANGUAGE PatternSynonyms #-}</span><span>
</span><span id="line-15"></span><span class="hs-pragma">{-# LANGUAGE PolyKinds #-}</span><span>
</span><span id="line-16"></span><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><span id="line-17"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-18"></span><span class="hs-pragma">{-# LANGUAGE RoleAnnotations #-}</span><span>
</span><span id="line-19"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-20"></span><span class="hs-pragma">{-# LANGUAGE StandaloneKindSignatures #-}</span><span>
</span><span id="line-21"></span><span class="hs-pragma">{-# LANGUAGE TemplateHaskell #-}</span><span>
</span><span id="line-22"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-23"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-24"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-25"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-26"></span><span class="hs-pragma">{-# LANGUAGE UndecidableSuperClasses #-}</span><span>
</span><span id="line-27"></span><span class="hs-pragma">{-# LANGUAGE ViewPatterns #-}</span><span>
</span><span id="line-28"></span><span>
</span><span id="line-29"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.GraduallyTyped.Tensor.Type</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-30"></span><span>
</span><span id="line-31"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Applicative</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">empty</span></span><span class="hs-special">)</span><span>
</span><span id="line-32"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Category</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-operator">(&gt;&gt;&gt;)</span></span><span class="hs-special">)</span><span>
</span><span id="line-33"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Exception</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Exception</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-34"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">forM</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">forM_</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">when</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">(&lt;=&lt;)</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">(&gt;=&gt;)</span></span><span class="hs-special">)</span><span>
</span><span id="line-35"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad.Catch</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">MonadThrow</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-36"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Bifunctor</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">bimap</span></span><span class="hs-special">)</span><span>
</span><span id="line-37"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Coerce</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">coerce</span></span><span class="hs-special">)</span><span>
</span><span id="line-38"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Foldable</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">traverse_</span></span><span class="hs-special">)</span><span>
</span><span id="line-39"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Functor</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-operator">(&lt;&amp;&gt;)</span></span><span class="hs-special">)</span><span>
</span><span id="line-40"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Int</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Int16</span></span><span class="hs-special">)</span><span>
</span><span id="line-41"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.List.NonEmpty</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">NonEmpty</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-operator">(:|)</span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">nonEmpty</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">unzip</span></span><span class="hs-special">)</span><span>
</span><span id="line-42"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">maybeToList</span></span><span class="hs-special">)</span><span>
</span><span id="line-43"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Proxy</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Proxy</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-44"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier">Data.Singletons</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier">SingI</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier">sing</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier">fromSing</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-45"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Typeable</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Typeable</span></span><span class="hs-special">)</span><span>
</span><span id="line-46"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier">Data.Vector</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">V</span></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier">uncons</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-47"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier">Data.Vector.Generic.Sized.Internal</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">SVI</span></span><span>
</span><span id="line-48"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier">Data.Vector.Sized</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">SV</span></span><span>
</span><span id="line-49"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Ptr</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">Word8</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">castPtr</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">fromBool</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">peekElemOff</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">pokeElemOff</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">withForeignPtr</span></span><span class="hs-special">)</span><span>
</span><span id="line-50"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign.ForeignPtr</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">ForeignPtr</span></span><span class="hs-special">)</span><span>
</span><span id="line-51"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.Generics</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-52"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">KnownNat</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">KnownSymbol</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">Nat</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">Symbol</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">natVal</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">symbolVal</span></span><span class="hs-special">)</span><span>
</span><span id="line-53"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">System.IO.Unsafe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">unsafePerformIO</span></span><span class="hs-special">)</span><span>
</span><span id="line-54"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html"><span class="hs-identifier">Torch.GraduallyTyped.DType</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier">DType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier">DataType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier">SDataType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-55"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html"><span class="hs-identifier">Torch.GraduallyTyped.Device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier">DeviceType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier">SDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDeviceType"><span class="hs-identifier">SDeviceType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-56"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Internal.TensorOptions.html"><span class="hs-identifier">Torch.GraduallyTyped.Internal.TensorOptions</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Internal.TensorOptions.html#tensorOptions"><span class="hs-identifier">tensorOptions</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-57"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Internal.Vector.html"><span class="hs-identifier">Torch.GraduallyTyped.Internal.Vector</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">V</span></span><span>
</span><span id="line-58"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html"><span class="hs-identifier">Torch.GraduallyTyped.Layout</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier">Layout</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier">LayoutType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier">SLayout</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayoutType"><span class="hs-identifier">SLayoutType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-59"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html"><span class="hs-identifier">Torch.GraduallyTyped.Prelude</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier">Catch</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier">forgetIsChecked</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#ifM"><span class="hs-identifier">ifM</span></a></span><span class="hs-special">,</span><span> </span><span class="hs-keyword">pattern</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#%3A%7C%3A"><span class="hs-operator">(:|:)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-60"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.List.html"><span class="hs-identifier">Torch.GraduallyTyped.Prelude.List</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier">SList</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-61"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html"><span class="hs-identifier">Torch.GraduallyTyped.RequiresGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier">Gradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier">RequiresGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier">SGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SRequiresGradient"><span class="hs-identifier">SRequiresGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-62"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html"><span class="hs-identifier">Torch.GraduallyTyped.Shape.Class</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#InsertDimF"><span class="hs-identifier">InsertDimF</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#ReplaceDimF"><span class="hs-identifier">ReplaceDimF</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-63"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html"><span class="hs-identifier">Torch.GraduallyTyped.Shape.Type</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#By"><span class="hs-identifier">By</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#ByIndex"><span class="hs-identifier">ByIndex</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier">Name</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier">SDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SName"><span class="hs-identifier">SName</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier">SShape</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SSize"><span class="hs-identifier">SSize</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier">Shape</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier">Size</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-64"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html"><span class="hs-identifier">Torch.GraduallyTyped.Unify</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-keyword">type</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator">(&lt;+&gt;)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-65"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Torch.HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="hs-keyword">pattern</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-operator">(:.)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-66"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast0</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast1</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast2</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast4</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-67"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Class</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-68"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.GC</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">unsafeThrowableIO</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-69"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Native</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-70"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Type.Context</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-71"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Type.Extra</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-72"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Type.Tensor</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-73"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Type.TensorOptions</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-74"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Type</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Tensor</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">TensorList</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">TensorOptions</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-75"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Unmanaged.Type.Tensor</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">Unmanaged</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">tensor_data_ptr</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-76"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Torch.Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Unsafe</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-77"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">unzip</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">unzip3</span></span><span class="hs-special">)</span><span>
</span><span id="line-78"></span><span>
</span><span id="line-79"></span><span class="hs-comment">-- $setup</span><span>
</span><span id="line-80"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch.GraduallyTyped.Prelude.List (SList (..))</span><span>
</span><span id="line-81"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch.GraduallyTyped</span><span>
</span><span id="line-82"></span><span>
</span><span id="line-83"></span><span class="hs-comment">-- | A gradually typed tensor.</span><span>
</span><span id="line-84"></span><span class="hs-comment">--</span><span>
</span><span id="line-85"></span><span class="hs-comment">-- @</span><span>
</span><span id="line-86"></span><span class="hs-comment">--                          &#9484;&#9472;&#9658; Compute device, e.g. `'Device 'CPU`</span><span>
</span><span id="line-87"></span><span class="hs-comment">--                          &#9474;</span><span>
</span><span id="line-88"></span><span class="hs-comment">--                          &#9474;               &#9484;&#9472;&#9658; List of dimensions, e.g. `'Shape '[ 'Dim 'UncheckedName ('Size 8), 'Dim 'UncheckedName ('Size 1) ]`</span><span>
</span><span id="line-89"></span><span class="hs-comment">--                          &#9474;               &#9474;</span><span>
</span><span id="line-90"></span><span class="hs-comment">-- Tensor gradient layout device dataType shape</span><span>
</span><span id="line-91"></span><span class="hs-comment">--           &#9474;       &#9474;              &#9474;</span><span>
</span><span id="line-92"></span><span class="hs-comment">--           &#9474;       &#9474;              &#9492;&#9472;&#9658; Data type, e.g. `'DataType 'Float`</span><span>
</span><span id="line-93"></span><span class="hs-comment">--           &#9474;       &#9474;</span><span>
</span><span id="line-94"></span><span class="hs-comment">--           &#9474;       &#9492;&#9472;&#9658; Memory layout, e.g. `'Layout 'Dense`</span><span>
</span><span id="line-95"></span><span class="hs-comment">--           &#9474;</span><span>
</span><span id="line-96"></span><span class="hs-comment">--           &#9492;&#9472;&#9658; Whether or not the tensor requires a gradient, e.g. `'Gradient 'WithGradient` for one that does</span><span>
</span><span id="line-97"></span><span class="hs-comment">-- @</span><span>
</span><span id="line-98"></span><span class="hs-keyword">newtype</span><span>
</span><span id="line-99"></span><span>  </span><span id="Tensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-var">Tensor</span></a></span></span><span>
</span><span id="line-100"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682466"><span class="annot"><a href="#local-6989586621679682466"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier hs-type">RequiresGradient</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-101"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682465"><span class="annot"><a href="#local-6989586621679682465"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier hs-type">LayoutType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-102"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682464"><span class="annot"><a href="#local-6989586621679682464"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier hs-type">DeviceType</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-103"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682463"><span class="annot"><a href="#local-6989586621679682463"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-104"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682462"><span class="annot"><a href="#local-6989586621679682462"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-105"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-106"></span><span>  </span><span class="hs-comment">-- | Unsafe constructor for tensors.</span><span>
</span><span id="line-107"></span><span>  </span><span class="hs-comment">-- Do not call this constructor directly,</span><span>
</span><span id="line-108"></span><span>  </span><span class="hs-comment">-- use smart constructors like 'ones' or 'randn' instead.</span><span>
</span><span id="line-109"></span><span>  </span><span id="UnsafeTensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-var">UnsafeTensor</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-110"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683456"><span class="annot"><a href="#local-6989586621679683456"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683455"><span class="annot"><a href="#local-6989586621679683455"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683454"><span class="annot"><a href="#local-6989586621679683454"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683453"><span class="annot"><a href="#local-6989586621679683453"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683452"><span class="annot"><a href="#local-6989586621679683452"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-111"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-112"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683456"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683455"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683454"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683453"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683452"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-113"></span><span>
</span><span id="line-114"></span><span class="hs-keyword">type</span><span> </span><span class="hs-keyword">role</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><span class="hs-identifier">nominal</span></span><span> </span><span class="annot"><span class="hs-identifier">nominal</span></span><span> </span><span class="annot"><span class="hs-identifier">nominal</span></span><span> </span><span class="annot"><span class="hs-identifier">nominal</span></span><span> </span><span class="annot"><span class="hs-identifier">nominal</span></span><span>
</span><span id="line-115"></span><span>
</span><span id="line-116"></span><span id="local-6989586621679682456"><span id="local-6989586621679682457"><span id="local-6989586621679682458"><span id="local-6989586621679682459"><span id="local-6989586621679682460"><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682451"><span id="local-6989586621679682454"><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682460"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682459"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682458"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682457"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682456"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-117"></span><span>  </span><span id="local-6989586621679682449"><span class="annot"><span class="annottext">show :: Tensor gradient layout device dataType shape -&gt; String
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">show</span></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-type">UnsafeTensor</span></a></span><span> </span><span id="local-6989586621679682447"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682447"><span class="hs-identifier hs-var">t</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; Tensor
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">Torch.Tensor.Unsafe</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682447"><span class="hs-identifier hs-var">t</span></a></span><span class="hs-special">)</span></span></span></span></span></span><span>
</span><span id="line-118"></span><span>
</span><span id="line-119"></span><span id="local-6989586621679682445"><span id="local-6989586621679682446"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-120"></span><span>  </span><span id="TensorSpec"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorSpec"><span class="hs-identifier hs-var">TensorSpec</span></a></span></span><span>
</span><span id="line-121"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682444"><span class="annot"><a href="#local-6989586621679682444"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier hs-type">RequiresGradient</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-122"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682443"><span class="annot"><a href="#local-6989586621679682443"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier hs-type">LayoutType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-123"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682442"><span class="annot"><a href="#local-6989586621679682442"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier hs-type">DeviceType</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-124"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682441"><span class="annot"><a href="#local-6989586621679682441"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-125"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679682440"><span class="annot"><a href="#local-6989586621679682440"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-126"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-127"></span><span>  </span><span id="TensorSpec"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorSpec"><span class="hs-identifier hs-var">TensorSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-128"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682438"><span class="annot"><a href="#local-6989586621679682438"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682437"><span class="annot"><a href="#local-6989586621679682437"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682436"><span class="annot"><a href="#local-6989586621679682436"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682435"><span class="annot"><a href="#local-6989586621679682435"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682434"><span class="annot"><a href="#local-6989586621679682434"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-129"></span><span>    </span><span class="hs-special">{</span><span> </span><span id="tsGradient"><span class="annot"><span class="annottext">TensorSpec gradient layout device dataType shape
-&gt; SGradient gradient
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tsGradient"><span class="hs-identifier hs-var hs-var">tsGradient</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682438"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-130"></span><span>      </span><span id="tsLayout"><span class="annot"><span class="annottext">TensorSpec gradient layout device dataType shape -&gt; SLayout layout
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tsLayout"><span class="hs-identifier hs-var hs-var">tsLayout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682437"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-131"></span><span>      </span><span id="tsDevice"><span class="annot"><span class="annottext">TensorSpec gradient layout device dataType shape -&gt; SDevice device
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tsDevice"><span class="hs-identifier hs-var hs-var">tsDevice</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682436"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-132"></span><span>      </span><span id="tsDataType"><span class="annot"><span class="annottext">TensorSpec gradient layout device dataType shape
-&gt; SDataType dataType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tsDataType"><span class="hs-identifier hs-var hs-var">tsDataType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-type">SDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682435"><span class="hs-identifier hs-type">dataType</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-133"></span><span>      </span><span id="tsShape"><span class="annot"><span class="annottext">TensorSpec gradient layout device dataType shape -&gt; SShape shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tsShape"><span class="hs-identifier hs-var hs-var">tsShape</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-type">SShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682434"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-134"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-135"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorSpec"><span class="hs-identifier hs-type">TensorSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682438"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682437"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682436"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682435"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682434"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-136"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682423"><span id="local-6989586621679682425"><span id="local-6989586621679682427"><span class="annot"><span class="annottext">Int -&gt; TensorSpec gradient layout device dataType shape -&gt; ShowS
[TensorSpec gradient layout device dataType shape] -&gt; ShowS
TensorSpec gradient layout device dataType shape -&gt; String
(Int -&gt; TensorSpec gradient layout device dataType shape -&gt; ShowS)
-&gt; (TensorSpec gradient layout device dataType shape -&gt; String)
-&gt; ([TensorSpec gradient layout device dataType shape] -&gt; ShowS)
-&gt; Show (TensorSpec gradient layout device dataType shape)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Int -&gt; TensorSpec gradient layout device dataType shape -&gt; ShowS
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
[TensorSpec gradient layout device dataType shape] -&gt; ShowS
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
TensorSpec gradient layout device dataType shape -&gt; String
showList :: [TensorSpec gradient layout device dataType shape] -&gt; ShowS
$cshowList :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
[TensorSpec gradient layout device dataType shape] -&gt; ShowS
show :: TensorSpec gradient layout device dataType shape -&gt; String
$cshow :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
TensorSpec gradient layout device dataType shape -&gt; String
showsPrec :: Int -&gt; TensorSpec gradient layout device dataType shape -&gt; ShowS
$cshowsPrec :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Int -&gt; TensorSpec gradient layout device dataType shape -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TensorSpec gradient layout device dataType shape
 -&gt; Rep (TensorSpec gradient layout device dataType shape) x)
-&gt; (forall x.
    Rep (TensorSpec gradient layout device dataType shape) x
    -&gt; TensorSpec gradient layout device dataType shape)
-&gt; Generic (TensorSpec gradient layout device dataType shape)
forall x.
Rep (TensorSpec gradient layout device dataType shape) x
-&gt; TensorSpec gradient layout device dataType shape
forall x.
TensorSpec gradient layout device dataType shape
-&gt; Rep (TensorSpec gradient layout device dataType shape) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) x.
Rep (TensorSpec gradient layout device dataType shape) x
-&gt; TensorSpec gradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) x.
TensorSpec gradient layout device dataType shape
-&gt; Rep (TensorSpec gradient layout device dataType shape) x
$cto :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) x.
Rep (TensorSpec gradient layout device dataType shape) x
-&gt; TensorSpec gradient layout device dataType shape
$cfrom :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) x.
TensorSpec gradient layout device dataType shape
-&gt; Rep (TensorSpec gradient layout device dataType shape) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-137"></span><span>
</span><span id="line-138"></span><span class="hs-comment">-- | Alias for an untyped tensor without gradients.</span><span>
</span><span id="line-139"></span><span class="hs-keyword">type</span><span> </span><span id="UncheckedTensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#UncheckedTensor"><span class="hs-identifier hs-var">UncheckedTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#UncheckedGradient"><span class="hs-identifier hs-type">UncheckedGradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#UncheckedLayout"><span class="hs-identifier hs-type">UncheckedLayout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#UncheckedDevice"><span class="hs-identifier hs-type">UncheckedDevice</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#UncheckedDataType"><span class="hs-identifier hs-type">UncheckedDataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedShape"><span class="hs-identifier hs-type">UncheckedShape</span></a></span><span>
</span><span id="line-140"></span><span>
</span><span id="line-141"></span><span class="hs-comment">-- | Alias for an untyped tensor with gradients.</span><span>
</span><span id="line-142"></span><span class="hs-keyword">type</span><span> </span><span id="UncheckedParameter"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#UncheckedParameter"><span class="hs-identifier hs-var">UncheckedParameter</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#UncheckedLayout"><span class="hs-identifier hs-type">UncheckedLayout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#UncheckedDevice"><span class="hs-identifier hs-type">UncheckedDevice</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#UncheckedDataType"><span class="hs-identifier hs-type">UncheckedDataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedShape"><span class="hs-identifier hs-type">UncheckedShape</span></a></span><span>
</span><span id="line-143"></span><span>
</span><span id="line-144"></span><span class="hs-comment">-- | Alias for a tensor on CPU memory without gradients.</span><span>
</span><span id="line-145"></span><span class="hs-keyword">type</span><span> </span><span id="CPUTensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#CPUTensor"><span class="hs-identifier hs-var">CPUTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-type">Dense</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-type">CPU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-146"></span><span>
</span><span id="line-147"></span><span class="hs-comment">-- | Alias for a tensor on CPU memory with gradients.</span><span>
</span><span id="line-148"></span><span class="hs-keyword">type</span><span> </span><span id="CPUParameter"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#CPUParameter"><span class="hs-identifier hs-var">CPUParameter</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-type">Dense</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-type">CPU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-149"></span><span>
</span><span id="line-150"></span><span class="hs-comment">-- | Alias for a sparse tensor on CPU memory without gradients.</span><span>
</span><span id="line-151"></span><span class="hs-keyword">type</span><span> </span><span id="SparseCPUTensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SparseCPUTensor"><span class="hs-identifier hs-var">SparseCPUTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-type">Sparse</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-type">CPU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-152"></span><span>
</span><span id="line-153"></span><span class="hs-comment">-- | Alias for a sparse tensor on CPU memory with gradients.</span><span>
</span><span id="line-154"></span><span class="hs-keyword">type</span><span> </span><span id="SparseCPUParameter"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SparseCPUParameter"><span class="hs-identifier hs-var">SparseCPUParameter</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-type">Sparse</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-type">CPU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-155"></span><span>
</span><span id="line-156"></span><span class="hs-comment">-- | Alias for a tensor on CUDA memory without gradients.</span><span>
</span><span id="line-157"></span><span class="hs-keyword">type</span><span> </span><span id="CUDATensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#CUDATensor"><span class="hs-identifier hs-var">CUDATensor</span></a></span></span><span> </span><span id="local-6989586621679682412"><span class="annot"><a href="#local-6989586621679682412"><span class="hs-identifier hs-type">deviceId</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-type">Dense</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682412"><span class="hs-identifier hs-type">deviceId</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-158"></span><span>
</span><span id="line-159"></span><span class="hs-comment">-- | Alias for a tensor on CUDA memory with gradients.</span><span>
</span><span id="line-160"></span><span class="hs-keyword">type</span><span> </span><span id="CUDAParameter"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#CUDAParameter"><span class="hs-identifier hs-var">CUDAParameter</span></a></span></span><span> </span><span id="local-6989586621679682410"><span class="annot"><a href="#local-6989586621679682410"><span class="hs-identifier hs-type">deviceId</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-type">Dense</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682410"><span class="hs-identifier hs-type">deviceId</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-161"></span><span>
</span><span id="line-162"></span><span class="hs-comment">-- | Alias for a sparse tensor on CUDA memory without gradients.</span><span>
</span><span id="line-163"></span><span class="hs-keyword">type</span><span> </span><span id="SparseCUDATensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SparseCUDATensor"><span class="hs-identifier hs-var">SparseCUDATensor</span></a></span></span><span> </span><span id="local-6989586621679682408"><span class="annot"><a href="#local-6989586621679682408"><span class="hs-identifier hs-type">deviceId</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-type">Sparse</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682408"><span class="hs-identifier hs-type">deviceId</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-164"></span><span>
</span><span id="line-165"></span><span class="hs-comment">-- | Alias for a sparse tensor on CUDA memory with gradients.</span><span>
</span><span id="line-166"></span><span class="hs-keyword">type</span><span> </span><span id="SparseCUDAParameter"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SparseCUDAParameter"><span class="hs-identifier hs-var">SparseCUDAParameter</span></a></span></span><span> </span><span id="local-6989586621679682406"><span class="annot"><a href="#local-6989586621679682406"><span class="hs-identifier hs-type">deviceId</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-type">Sparse</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682406"><span class="hs-identifier hs-type">deviceId</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-167"></span><span>
</span><span id="line-168"></span><span id="local-6989586621679682401"><span id="local-6989586621679682402"><span id="local-6989586621679682403"><span id="local-6989586621679682404"><span id="local-6989586621679682405"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Num</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682405"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682404"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682403"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682402"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682401"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-169"></span><span>  </span><span id="local-6989586621679682392"><span class="annot"><span class="annottext">+ :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><span class="hs-operator hs-var hs-var hs-var hs-var">(+)</span></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((Tensor gradient layout device dataType shape
  -&gt; IO (Tensor gradient layout device dataType shape))
 -&gt; Tensor gradient layout device dataType shape
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.add_tt</span></a></span><span>
</span><span id="line-170"></span><span>  </span><span id="local-6989586621679682388"><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="hs-special">)</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((Tensor gradient layout device dataType shape
  -&gt; IO (Tensor gradient layout device dataType shape))
 -&gt; Tensor gradient layout device dataType shape
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.sub_tt</span></a></span><span>
</span><span id="line-171"></span><span>  </span><span id="local-6989586621679682386"><span class="annot"><span class="annottext">* :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><span class="hs-operator hs-var hs-var hs-var hs-var">(*)</span></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((Tensor gradient layout device dataType shape
  -&gt; IO (Tensor gradient layout device dataType shape))
 -&gt; Tensor gradient layout device dataType shape
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.mul_tt</span></a></span><span>
</span><span id="line-172"></span><span>  </span><span id="local-6989586621679682383"><span class="annot"><span class="annottext">negate :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">negate</span></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.neg_t</span></a></span><span>
</span><span id="line-173"></span><span>  </span><span id="local-6989586621679682381"><span class="annot"><span class="annottext">abs :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">abs</span></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.abs_t</span></a></span><span>
</span><span id="line-174"></span><span>  </span><span id="local-6989586621679682378"><span class="annot"><span class="annottext">signum :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">signum</span></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.sign_t</span></a></span><span>
</span><span id="line-175"></span><span>  </span><span id="local-6989586621679682375"><span class="annot"><span class="annottext">fromInteger :: Integer -&gt; Tensor gradient layout device dataType shape
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">fromInteger</span></span></span><span> </span><span id="local-6989586621679682374"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679682374"><span class="hs-identifier hs-var">_a</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
forall a. HasCallStack =&gt; a
</span><span class="hs-identifier hs-var">undefined</span></span></span></span></span></span></span><span>
</span><span id="line-176"></span><span>
</span><span id="line-177"></span><span id="local-6989586621679682368"><span id="local-6989586621679682369"><span id="local-6989586621679682370"><span id="local-6989586621679682371"><span id="local-6989586621679682372"><span class="hs-keyword">instance</span><span>
</span><span id="line-178"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span>
</span><span id="line-179"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682372"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682371"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682370"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682369"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682368"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-180"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-181"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-182"></span><span>  </span><span id="local-6989586621679682364"><span class="annot"><span class="annottext">cast :: Tensor gradient layout device dataType shape
-&gt; (ForeignPtr Tensor -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var hs-var hs-var hs-var">cast</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-type">UnsafeTensor</span></a></span><span> </span><span id="local-6989586621679682362"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682362"><span class="hs-identifier hs-var">atenTensor</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679682361"><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO r
</span><a href="#local-6989586621679682361"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO r
</span><a href="#local-6989586621679682361"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682362"><span class="hs-identifier hs-var">atenTensor</span></a></span><span>
</span><span id="line-183"></span><span>  </span><span id="local-6989586621679682360"><span class="annot"><span class="annottext">uncast :: ForeignPtr Tensor
-&gt; (Tensor gradient layout device dataType shape -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var hs-var hs-var hs-var">uncast</span></a></span></span><span> </span><span id="local-6989586621679682358"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682358"><span class="hs-identifier hs-var">atenTensor</span></a></span></span><span> </span><span id="local-6989586621679682357"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; IO r
</span><a href="#local-6989586621679682357"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; IO r
</span><a href="#local-6989586621679682357"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape -&gt; IO r)
-&gt; Tensor gradient layout device dataType shape -&gt; IO r
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-var">UnsafeTensor</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682358"><span class="hs-identifier hs-var">atenTensor</span></a></span></span></span></span></span></span><span>
</span><span id="line-184"></span><span>
</span><span id="line-185"></span><span id="local-6989586621679682352"><span id="local-6989586621679682353"><span id="local-6989586621679682354"><span id="local-6989586621679682355"><span id="local-6989586621679682356"><span class="hs-keyword">instance</span><span>
</span><span id="line-186"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span>
</span><span id="line-187"></span><span>    </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682356"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682355"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682354"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682353"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682352"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-188"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorList</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-189"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-190"></span><span>  </span><span id="local-6989586621679682349"><span class="annot"><span class="annottext">cast :: [Tensor gradient layout device dataType shape]
-&gt; (ForeignPtr TensorList -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682349"><span class="hs-identifier hs-var hs-var hs-var hs-var">cast</span></a></span></span><span> </span><span id="local-6989586621679682348"><span class="annot"><span class="annottext">[Tensor gradient layout device dataType shape]
</span><a href="#local-6989586621679682348"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span id="local-6989586621679682347"><span class="annot"><span class="annottext">ForeignPtr TensorList -&gt; IO r
</span><a href="#local-6989586621679682347"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-191"></span><span>    </span><span id="local-6989586621679682346"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682346"><span class="hs-identifier hs-var">ptrList</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; IO (ForeignPtr Tensor))
-&gt; [Tensor gradient layout device dataType shape]
-&gt; IO [ForeignPtr Tensor]
forall (t :: * -&gt; *) (m :: * -&gt; *) a b.
(Traversable t, Monad m) =&gt;
(a -&gt; m b) -&gt; t a -&gt; m (t b)
</span><span class="hs-identifier hs-var">mapM</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span id="local-6989586621679682344"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682344"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; (ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; IO (ForeignPtr Tensor)
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682344"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Tensor gradient layout device dataType shape]
</span><a href="#local-6989586621679682348"><span class="hs-identifier hs-var">xs</span></a></span><span>
</span><span id="line-192"></span><span>    </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; (ForeignPtr TensorList -&gt; IO r) -&gt; IO r
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682346"><span class="hs-identifier hs-var">ptrList</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList -&gt; IO r
</span><a href="#local-6989586621679682347"><span class="hs-identifier hs-var">f</span></a></span><span>
</span><span id="line-193"></span><span>  </span><span id="local-6989586621679682343"><span class="annot"><span class="annottext">uncast :: ForeignPtr TensorList
-&gt; ([Tensor gradient layout device dataType shape] -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682343"><span class="hs-identifier hs-var hs-var hs-var hs-var">uncast</span></a></span></span><span> </span><span id="local-6989586621679682342"><span class="annot"><span class="annottext">ForeignPtr TensorList
</span><a href="#local-6989586621679682342"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span id="local-6989586621679682341"><span class="annot"><span class="annottext">[Tensor gradient layout device dataType shape] -&gt; IO r
</span><a href="#local-6989586621679682341"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList -&gt; ([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList
</span><a href="#local-6989586621679682342"><span class="hs-identifier hs-var">xs</span></a></span><span> </span><span class="annot"><span class="annottext">(([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r)
-&gt; ([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679682340"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682340"><span class="hs-identifier hs-var">ptrList</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-194"></span><span>    </span><span id="local-6989586621679682339"><span class="annot"><span class="annottext">[Tensor gradient layout device dataType shape]
</span><a href="#local-6989586621679682339"><span class="hs-identifier hs-var">tensorList</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; [ForeignPtr Tensor]
-&gt; IO [Tensor gradient layout device dataType shape]
forall (t :: * -&gt; *) (m :: * -&gt; *) a b.
(Traversable t, Monad m) =&gt;
(a -&gt; m b) -&gt; t a -&gt; m (t b)
</span><span class="hs-identifier hs-var">mapM</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span class="hs-special">(</span><span id="local-6989586621679682338"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682338"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682338"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682340"><span class="hs-identifier hs-var">ptrList</span></a></span><span>
</span><span id="line-195"></span><span>    </span><span class="annot"><span class="annottext">[Tensor gradient layout device dataType shape] -&gt; IO r
</span><a href="#local-6989586621679682341"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">[Tensor gradient layout device dataType shape]
</span><a href="#local-6989586621679682339"><span class="hs-identifier hs-var">tensorList</span></a></span></span></span></span></span></span><span>
</span><span id="line-196"></span><span>
</span><span id="line-197"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-198"></span><span>  </span><span id="local-6989586621679682335"><span class="annot"><span class="annottext">cast :: HList '[] -&gt; ([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682335"><span class="hs-identifier hs-var hs-var hs-var hs-var">cast</span></a></span></span><span> </span><span class="annot"><span class="annottext">HList '[]
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">HNil</span></a></span><span> </span><span id="local-6989586621679682333"><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; IO r
</span><a href="#local-6989586621679682333"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; IO r
</span><a href="#local-6989586621679682333"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span>
</span><span id="line-199"></span><span>  </span><span id="local-6989586621679682332"><span class="annot"><span class="annottext">uncast :: [ForeignPtr Tensor] -&gt; (HList '[] -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682332"><span class="hs-identifier hs-var hs-var hs-var hs-var">uncast</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span id="local-6989586621679682331"><span class="annot"><span class="annottext">HList '[] -&gt; IO r
</span><a href="#local-6989586621679682331"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList '[] -&gt; IO r
</span><a href="#local-6989586621679682331"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
forall k. HList '[]
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">HNil</span></a></span><span>
</span><span id="line-200"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">HList '[] -&gt; IO r
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">String -&gt; IO r
forall (m :: * -&gt; *) a. MonadFail m =&gt; String -&gt; m a
</span><span class="hs-identifier hs-var">fail</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The list of tensors has more elements than expected. This means that the runtime length of the list exceeded its compile-time length.&quot;</span></span><span>
</span><span id="line-201"></span><span>
</span><span id="line-202"></span><span id="local-6989586621679682325"><span id="local-6989586621679682326"><span id="local-6989586621679682327"><span id="local-6989586621679682328"><span id="local-6989586621679682329"><span id="local-6989586621679682330"><span class="hs-keyword">instance</span><span>
</span><span id="line-203"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682330"><span class="hs-identifier hs-type">tensors</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-204"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-205"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682329"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682328"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682327"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682326"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682325"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="annot"><a href="#local-6989586621679682330"><span class="hs-identifier hs-type">tensors</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-206"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-207"></span><span>  </span><span id="local-6989586621679682322"><span class="annot"><span class="annottext">cast :: HList (Tensor gradient layout device dataType shape : tensors)
-&gt; ([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682322"><span class="hs-identifier hs-var hs-var hs-var hs-var">cast</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HCons</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682320"><span class="annot"><a href="#local-6989586621679682320"><span class="hs-identifier hs-var">tensor</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679682319"><span class="annot"><a href="#local-6989586621679682319"><span class="hs-identifier hs-var">tensors</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679682318"><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; IO r
</span><a href="#local-6989586621679682318"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-208"></span><span>    </span><span id="local-6989586621679682317"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682317"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; (ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; IO (ForeignPtr Tensor)
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682320"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span>
</span><span id="line-209"></span><span>    </span><span id="local-6989586621679682316"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682316"><span class="hs-identifier hs-var">ptrList</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">HList tensors
-&gt; ([ForeignPtr Tensor] -&gt; IO [ForeignPtr Tensor])
-&gt; IO [ForeignPtr Tensor]
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">HList tensors
</span><a href="#local-6989586621679682319"><span class="hs-identifier hs-var">tensors</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; IO [ForeignPtr Tensor]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span>
</span><span id="line-210"></span><span>    </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; IO r
</span><a href="#local-6989586621679682318"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682317"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; [ForeignPtr Tensor] -&gt; [ForeignPtr Tensor]
forall a. a -&gt; [a] -&gt; [a]
</span><span class="hs-glyph hs-var">:</span></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682316"><span class="hs-identifier hs-var">ptrList</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-211"></span><span>  </span><span id="local-6989586621679682315"><span class="annot"><span class="annottext">uncast :: [ForeignPtr Tensor]
-&gt; (HList (Tensor gradient layout device dataType shape : tensors)
    -&gt; IO r)
-&gt; IO r
</span><a href="#local-6989586621679682315"><span class="hs-identifier hs-var hs-var hs-var hs-var">uncast</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">HList (Tensor gradient layout device dataType shape : tensors)
-&gt; IO r
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">String -&gt; IO r
forall (m :: * -&gt; *) a. MonadFail m =&gt; String -&gt; m a
</span><span class="hs-identifier hs-var">fail</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The list of tensors ended prematurely. This means that the runtime length of the list was smaller than its compile-time length.&quot;</span></span><span>
</span><span id="line-212"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682314"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682314"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679682313"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682313"><span class="hs-identifier hs-var">ptrList</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679682312"><span class="annot"><span class="annottext">HList (Tensor gradient layout device dataType shape : tensors)
-&gt; IO r
</span><a href="#local-6989586621679682312"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-213"></span><span>    </span><span id="local-6989586621679682311"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682311"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679682314"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span>
</span><span id="line-214"></span><span>    </span><span id="local-6989586621679682310"><span class="annot"><span class="annottext">HList tensors
</span><a href="#local-6989586621679682310"><span class="hs-identifier hs-var">tensors</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
-&gt; (HList tensors -&gt; IO (HList tensors)) -&gt; IO (HList tensors)
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682313"><span class="hs-identifier hs-var">ptrList</span></a></span><span> </span><span class="annot"><span class="annottext">HList tensors -&gt; IO (HList tensors)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span>
</span><span id="line-215"></span><span>    </span><span class="annot"><span class="annottext">HList (Tensor gradient layout device dataType shape : tensors)
-&gt; IO r
</span><a href="#local-6989586621679682312"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682311"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; HList tensors
-&gt; HList (Tensor gradient layout device dataType shape : tensors)
forall x (xs :: [*]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="../../../../hasktorch/html/src"><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList tensors
</span><a href="#local-6989586621679682310"><span class="hs-identifier hs-var">tensors</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-216"></span><span>
</span><span id="line-217"></span><span id="local-6989586621679682309"><span class="hs-keyword">instance</span><span>
</span><span id="line-218"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682309"><span class="hs-identifier hs-type">l</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-219"></span><span>  </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682309"><span class="hs-identifier hs-type">l</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorList</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-220"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-221"></span><span>  </span><span id="local-6989586621679682306"><span class="annot"><span class="annottext">cast :: HList l -&gt; (ForeignPtr TensorList -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682306"><span class="hs-identifier hs-var hs-var hs-var hs-var">cast</span></a></span></span><span> </span><span id="local-6989586621679682305"><span class="annot"><span class="annottext">HList l
</span><a href="#local-6989586621679682305"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span id="local-6989586621679682304"><span class="annot"><span class="annottext">ForeignPtr TensorList -&gt; IO r
</span><a href="#local-6989586621679682304"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-222"></span><span>    </span><span id="local-6989586621679682303"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682303"><span class="hs-identifier hs-var">ts</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">HList l
-&gt; ([ForeignPtr Tensor] -&gt; IO [ForeignPtr Tensor])
-&gt; IO [ForeignPtr Tensor]
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">HList l
</span><a href="#local-6989586621679682305"><span class="hs-identifier hs-var">xs</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; IO [ForeignPtr Tensor]
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-223"></span><span>    </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; (ForeignPtr TensorList -&gt; IO r) -&gt; IO r
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682303"><span class="hs-identifier hs-var">ts</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList -&gt; IO r
</span><a href="#local-6989586621679682304"><span class="hs-identifier hs-var">f</span></a></span><span>
</span><span id="line-224"></span><span>  </span><span id="local-6989586621679682302"><span class="annot"><span class="annottext">uncast :: ForeignPtr TensorList -&gt; (HList l -&gt; IO r) -&gt; IO r
</span><a href="#local-6989586621679682302"><span class="hs-identifier hs-var hs-var hs-var hs-var">uncast</span></a></span></span><span> </span><span id="local-6989586621679682301"><span class="annot"><span class="annottext">ForeignPtr TensorList
</span><a href="#local-6989586621679682301"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span id="local-6989586621679682300"><span class="annot"><span class="annottext">HList l -&gt; IO r
</span><a href="#local-6989586621679682300"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList -&gt; ([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList
</span><a href="#local-6989586621679682301"><span class="hs-identifier hs-var">xs</span></a></span><span> </span><span class="annot"><span class="annottext">(([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r)
-&gt; ([ForeignPtr Tensor] -&gt; IO r) -&gt; IO r
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span class="hs-special">(</span><span id="local-6989586621679682299"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682299"><span class="hs-identifier hs-var">ptrList</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-225"></span><span>    </span><span id="local-6989586621679682298"><span class="annot"><span class="annottext">HList l
</span><a href="#local-6989586621679682298"><span class="hs-identifier hs-var">ts</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor] -&gt; (HList l -&gt; IO (HList l)) -&gt; IO (HList l)
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">uncast</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679682299"><span class="hs-identifier hs-var">ptrList</span></a></span><span> </span><span class="annot"><span class="annottext">HList l -&gt; IO (HList l)
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682309"><span class="hs-identifier hs-type">l</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-226"></span><span>    </span><span class="annot"><span class="annottext">HList l -&gt; IO r
</span><a href="#local-6989586621679682300"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">HList l
</span><a href="#local-6989586621679682298"><span class="hs-identifier hs-var">ts</span></a></span></span><span>
</span><span id="line-227"></span><span>
</span><span id="line-228"></span><span class="hs-comment">-- | Takes a tensor that may or may not require gradient computations</span><span>
</span><span id="line-229"></span><span class="hs-comment">-- and returns a copy that does not require them.</span><span>
</span><span id="line-230"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#withoutGradient"><span class="hs-identifier hs-type">withoutGradient</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-231"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682296"><span class="annot"><a href="#local-6989586621679682296"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682295"><span class="annot"><a href="#local-6989586621679682295"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682294"><span class="annot"><a href="#local-6989586621679682294"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682293"><span class="annot"><a href="#local-6989586621679682293"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682292"><span class="annot"><a href="#local-6989586621679682292"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-232"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-233"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682296"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682295"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682294"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682293"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682292"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-234"></span><span>  </span><span class="hs-comment">-- | copy of the input tensor without gradient computations turned off.</span><span>
</span><span id="line-235"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682295"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682294"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682293"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682292"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-236"></span><span id="withoutGradient"><span class="annot"><span class="annottext">withoutGradient :: Tensor gradient layout device dataType shape
-&gt; IO
     (Tensor ('Gradient 'WithoutGradient) layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withoutGradient"><span class="hs-identifier hs-var hs-var">withoutGradient</span></a></span></span><span> </span><span id="local-6989586621679682291"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682291"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Bool
-&gt; IO
     (Tensor ('Gradient 'WithoutGradient) layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_set_requires_grad_b</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682291"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-237"></span><span>
</span><span id="line-238"></span><span class="hs-comment">-- | Takes a tensor that does not requires gradient computations</span><span>
</span><span id="line-239"></span><span class="hs-comment">-- and returns a copy that requires them.</span><span>
</span><span id="line-240"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#withGradient"><span class="hs-identifier hs-type">withGradient</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-241"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682288"><span class="annot"><a href="#local-6989586621679682288"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682287"><span class="annot"><a href="#local-6989586621679682287"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682286"><span class="annot"><a href="#local-6989586621679682286"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682285"><span class="annot"><a href="#local-6989586621679682285"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682284"><span class="annot"><a href="#local-6989586621679682284"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-242"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-243"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682288"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682287"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682286"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682285"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682284"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-244"></span><span>  </span><span class="hs-comment">-- | copy of the input tensor with gradient computations turned on.</span><span>
</span><span id="line-245"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682287"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682286"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682285"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682284"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-246"></span><span id="withGradient"><span class="annot"><span class="annottext">withGradient :: Tensor gradient layout device dataType shape
-&gt; IO
     (Tensor ('Gradient 'WithGradient) layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withGradient"><span class="hs-identifier hs-var hs-var">withGradient</span></a></span></span><span> </span><span id="local-6989586621679682283"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682283"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Bool
-&gt; IO
     (Tensor ('Gradient 'WithGradient) layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_set_requires_grad_b</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682283"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span>
</span><span id="line-247"></span><span>
</span><span id="line-248"></span><span class="hs-comment">-- | Turn gradient computations off or on for a tensor.</span><span>
</span><span id="line-249"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetGradient"><span class="hs-identifier hs-type">sSetGradient</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-250"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682281"><span class="annot"><a href="#local-6989586621679682281"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682280"><span class="annot"><a href="#local-6989586621679682280"><span class="hs-identifier hs-type">gradient'</span></a></span></span><span> </span><span id="local-6989586621679682279"><span class="annot"><a href="#local-6989586621679682279"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682278"><span class="annot"><a href="#local-6989586621679682278"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682277"><span class="annot"><a href="#local-6989586621679682277"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682276"><span class="annot"><a href="#local-6989586621679682276"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-251"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682281"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-252"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682280"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682279"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682278"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682277"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682276"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-253"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682281"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682279"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682278"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682277"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682276"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-254"></span><span id="sSetGradient"><span class="annot"><span class="annottext">sSetGradient :: SGradient gradient
-&gt; Tensor gradient' layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetGradient"><span class="hs-identifier hs-var hs-var">sSetGradient</span></a></span></span><span> </span><span id="local-6989586621679682275"><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679682275"><span class="hs-identifier hs-var">gradient</span></a></span></span><span> </span><span id="local-6989586621679682274"><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679682274"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-255"></span><span>  </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">IsChecked RequiresGradient -&gt; RequiresGradient
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Sing gradient -&gt; Demote (Gradient RequiresGradient)
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">Sing gradient
SGradient gradient
</span><a href="#local-6989586621679682275"><span class="hs-identifier hs-var">gradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-256"></span><span>    </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-var">WithoutGradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient' layout device dataType shape
-&gt; Bool
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_set_requires_grad_b</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679682274"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-257"></span><span>    </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-var">WithGradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient' layout device dataType shape
-&gt; Bool
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; CBool -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_set_requires_grad_b</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679682274"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span>
</span><span id="line-258"></span><span>
</span><span id="line-259"></span><span class="hs-keyword">class</span><span> </span><span id="SGetGradient"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-var">SGetGradient</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679683386"><span class="annot"><a href="#local-6989586621679683386"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier hs-type">RequiresGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-260"></span><span>  </span><span class="hs-comment">-- | Returns the gradually typed information for whether or not gradient computations for the tensor are turned on.</span><span>
</span><span id="line-261"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-262"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' gradient = sOnes $ TensorSpec gradient (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-263"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SGradient SWithGradient</span><span>
</span><span id="line-264"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetGradient t</span><span>
</span><span id="line-265"></span><span>  </span><span class="hs-comment">-- SGradient SWithGradient</span><span>
</span><span id="line-266"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedGradient WithoutGradient</span><span>
</span><span id="line-267"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetGradient t</span><span>
</span><span id="line-268"></span><span>  </span><span class="hs-comment">-- SUncheckedGradient WithoutGradient</span><span>
</span><span id="line-269"></span><span>  </span><span id="sGetGradient"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetGradient"><span class="hs-identifier hs-type">sGetGradient</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-270"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683380"><span class="annot"><a href="#local-6989586621679683380"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683379"><span class="annot"><a href="#local-6989586621679683379"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683378"><span class="annot"><a href="#local-6989586621679683378"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683377"><span class="annot"><a href="#local-6989586621679683377"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-271"></span><span>    </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-272"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683386"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683380"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683379"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683378"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683377"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-273"></span><span>    </span><span class="hs-comment">-- | information about whether or not gradient computations are required</span><span>
</span><span id="line-274"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683386"><span class="hs-identifier hs-type">gradient</span></a></span><span>
</span><span id="line-275"></span><span>
</span><span id="line-276"></span><span>  </span><span class="hs-comment">-- | Returns the untyped memory layout of the input tensor.</span><span>
</span><span id="line-277"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-278"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' gradient = sOnes $ TensorSpec gradient (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-279"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SGradient SWithGradient</span><span>
</span><span id="line-280"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getRequiresGradient t</span><span>
</span><span id="line-281"></span><span>  </span><span class="hs-comment">-- WithGradient</span><span>
</span><span id="line-282"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedGradient WithoutGradient</span><span>
</span><span id="line-283"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getRequiresGradient t</span><span>
</span><span id="line-284"></span><span>  </span><span class="hs-comment">-- WithoutGradient</span><span>
</span><span id="line-285"></span><span>  </span><span id="getRequiresGradient"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getRequiresGradient"><span class="hs-identifier hs-type">getRequiresGradient</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-286"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683375"><span class="annot"><a href="#local-6989586621679683375"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683374"><span class="annot"><a href="#local-6989586621679683374"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683373"><span class="annot"><a href="#local-6989586621679683373"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683372"><span class="annot"><a href="#local-6989586621679683372"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-287"></span><span>    </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-288"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683386"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683375"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683374"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683373"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683372"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-289"></span><span>    </span><span class="hs-comment">-- | information about whether or not gradient computations are required</span><span>
</span><span id="line-290"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier hs-type">RequiresGradient</span></a></span><span>
</span><span id="line-291"></span><span>  </span><span id="local-6989586621679682271"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getRequiresGradient"><span class="hs-identifier hs-var hs-var">getRequiresGradient</span></a></span><span> </span><span id="local-6989586621679682270"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682270"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked RequiresGradient -&gt; RequiresGradient
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked RequiresGradient -&gt; RequiresGradient)
-&gt; (SGradient gradient -&gt; IsChecked RequiresGradient)
-&gt; SGradient gradient
-&gt; RequiresGradient
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SGradient gradient -&gt; IsChecked RequiresGradient
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SGradient gradient -&gt; RequiresGradient)
-&gt; SGradient gradient -&gt; RequiresGradient
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SGradient gradient
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetGradient gradient =&gt;
Tensor gradient layout device dataType shape -&gt; SGradient gradient
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetGradient"><span class="hs-identifier hs-var">sGetGradient</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682270"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-292"></span><span>
</span><span id="line-293"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682267"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#UncheckedGradient"><span class="hs-identifier hs-type">UncheckedGradient</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-294"></span><span>  </span><span id="local-6989586621679682265"><span class="annot"><span class="annottext">sGetGradient :: Tensor 'UncheckedGradient layout device dataType shape
-&gt; SGradient 'UncheckedGradient
</span><a href="#local-6989586621679682265"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetGradient</span></a></span></span><span> </span><span id="local-6989586621679682264"><span class="annot"><span class="annottext">Tensor 'UncheckedGradient layout device dataType shape
</span><a href="#local-6989586621679682264"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-295"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor 'UncheckedGradient layout device dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_requires_grad</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor 'UncheckedGradient layout device dataType shape
</span><a href="#local-6989586621679682264"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">RequiresGradient -&gt; SGradient 'UncheckedGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SUncheckedGradient"><span class="hs-identifier hs-var">SUncheckedGradient</span></a></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-var">WithGradient</span></a></span><span>
</span><span id="line-296"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">RequiresGradient -&gt; SGradient 'UncheckedGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SUncheckedGradient"><span class="hs-identifier hs-var">SUncheckedGradient</span></a></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-var">WithoutGradient</span></a></span><span>
</span><span id="line-297"></span><span>
</span><span id="line-298"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682259"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-299"></span><span>  </span><span id="local-6989586621679682258"><span class="annot"><span class="annottext">sGetGradient :: Tensor ('Gradient 'WithGradient) layout device dataType shape
-&gt; SGradient ('Gradient 'WithGradient)
</span><a href="#local-6989586621679682258"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetGradient</span></a></span></span><span> </span><span id="local-6989586621679682257"><span class="annot"><span class="annottext">Tensor ('Gradient 'WithGradient) layout device dataType shape
</span><a href="#local-6989586621679682257"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-300"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor ('Gradient 'WithGradient) layout device dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_requires_grad</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor ('Gradient 'WithGradient) layout device dataType shape
</span><a href="#local-6989586621679682257"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
-&gt; SGradient ('Gradient 'WithGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithGradient"><span class="hs-identifier hs-var">SWithGradient</span></a></span><span>
</span><span id="line-301"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-302"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SGradient ('Gradient 'WithGradient)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SGradient ('Gradient 'WithGradient))
-&gt; String -&gt; SGradient ('Gradient 'WithGradient)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-303"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should require gradient computations but doesn't. &quot;</span></span><span>
</span><span id="line-304"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-305"></span><span>
</span><span id="line-306"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682250"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-307"></span><span>  </span><span id="local-6989586621679682249"><span class="annot"><span class="annottext">sGetGradient :: Tensor ('Gradient 'WithoutGradient) layout device dataType shape
-&gt; SGradient ('Gradient 'WithoutGradient)
</span><a href="#local-6989586621679682249"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetGradient</span></a></span></span><span> </span><span id="local-6989586621679682248"><span class="annot"><span class="annottext">Tensor ('Gradient 'WithoutGradient) layout device dataType shape
</span><a href="#local-6989586621679682248"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-308"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor ('Gradient 'WithoutGradient) layout device dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_requires_grad</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor ('Gradient 'WithoutGradient) layout device dataType shape
</span><a href="#local-6989586621679682248"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-309"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SGradient ('Gradient 'WithoutGradient)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SGradient ('Gradient 'WithoutGradient))
-&gt; String -&gt; SGradient ('Gradient 'WithoutGradient)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-310"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should not require gradient computations but does. &quot;</span></span><span>
</span><span id="line-311"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-312"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
-&gt; SGradient ('Gradient 'WithoutGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithoutGradient"><span class="hs-identifier hs-var">SWithoutGradient</span></a></span><span>
</span><span id="line-313"></span><span>
</span><span id="line-314"></span><span class="hs-keyword">data</span><span> </span><span id="GradientError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#GradientError"><span class="hs-identifier hs-var">GradientError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="GradientError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#GradientError"><span class="hs-identifier hs-var">GradientError</span></a></span></span><span> </span><span class="hs-special">{</span><span id="geExpected"><span class="annot"><span class="annottext">GradientError -&gt; RequiresGradient
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#geExpected"><span class="hs-identifier hs-var hs-var">geExpected</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier hs-type">RequiresGradient</span></a></span><span class="hs-special">,</span><span> </span><span id="geActual"><span class="annot"><span class="annottext">GradientError -&gt; RequiresGradient
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#geActual"><span class="hs-identifier hs-var hs-var">geActual</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#RequiresGradient"><span class="hs-identifier hs-type">RequiresGradient</span></a></span><span class="hs-special">}</span><span>
</span><span id="line-315"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682238"><span id="local-6989586621679682240"><span id="local-6989586621679682242"><span class="annot"><span class="annottext">Int -&gt; GradientError -&gt; ShowS
[GradientError] -&gt; ShowS
GradientError -&gt; String
(Int -&gt; GradientError -&gt; ShowS)
-&gt; (GradientError -&gt; String)
-&gt; ([GradientError] -&gt; ShowS)
-&gt; Show GradientError
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [GradientError] -&gt; ShowS
$cshowList :: [GradientError] -&gt; ShowS
show :: GradientError -&gt; String
$cshow :: GradientError -&gt; String
showsPrec :: Int -&gt; GradientError -&gt; ShowS
$cshowsPrec :: Int -&gt; GradientError -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Typeable</span></span><span class="hs-special">)</span><span>
</span><span id="line-316"></span><span>
</span><span id="line-317"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682232"><span id="local-6989586621679682234"><span class="annot"><span class="hs-identifier hs-type">Exception</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#GradientError"><span class="hs-identifier hs-type">GradientError</span></a></span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-318"></span><span>  </span><span id="local-6989586621679682229"><span class="annot"><span class="annottext">displayException :: GradientError -&gt; String
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">displayException</span></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#GradientError"><span class="hs-identifier hs-type">GradientError</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679682226"><span id="local-6989586621679682227"><span class="annot"><span class="annottext">RequiresGradient
geActual :: RequiresGradient
geExpected :: RequiresGradient
geActual :: GradientError -&gt; RequiresGradient
geExpected :: GradientError -&gt; RequiresGradient
</span><a href="#local-6989586621679682226"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-319"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor's information about whether or not gradient computations are required reads `&quot;</span></span><span>
</span><span id="line-320"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">RequiresGradient -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="#local-6989586621679682226"><span class="hs-identifier hs-var">geActual</span></a></span><span>
</span><span id="line-321"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;` instead of `&quot;</span></span><span>
</span><span id="line-322"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">RequiresGradient -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="#local-6989586621679682227"><span class="hs-identifier hs-var">geExpected</span></a></span><span>
</span><span id="line-323"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;`.&quot;</span></span><span>
</span><span id="line-324"></span><span>
</span><span id="line-325"></span><span class="hs-comment">-- | Checks whether or not gradient computations are required for a tensor</span><span>
</span><span id="line-326"></span><span class="hs-comment">-- and returns a statically annotated copy of it wrapped in a 'MonadThrow' 'm'.</span><span>
</span><span id="line-327"></span><span class="hs-comment">--</span><span>
</span><span id="line-328"></span><span class="hs-comment">-- For instance, if 'm' is 'Maybe', then the result will be wrapped in 'Just'</span><span>
</span><span id="line-329"></span><span class="hs-comment">-- if and only if gradients are computed for the tensor according to the argument @gradient@.</span><span>
</span><span id="line-330"></span><span class="hs-comment">-- If gradients are expected but none are computed, then the result will be 'Nothing'.</span><span>
</span><span id="line-331"></span><span class="hs-comment">-- If gradients are not expected but are computed, then the result will be 'Nothing' as well.</span><span>
</span><span id="line-332"></span><span class="hs-comment">--</span><span>
</span><span id="line-333"></span><span class="hs-comment">-- In the REPL, 'm' will default to 'IO':</span><span>
</span><span id="line-334"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes $ TensorSpec (SUncheckedGradient WithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-335"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedGradient (SGradient SWithGradient) t</span><span>
</span><span id="line-336"></span><span class="hs-comment">-- &gt;&gt;&gt; :type t'</span><span>
</span><span id="line-337"></span><span class="hs-comment">-- t'</span><span>
</span><span id="line-338"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-339"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-340"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-341"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-342"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-343"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-344"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-345"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-346"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedGradient (SGradient SWithoutGradient) t</span><span>
</span><span id="line-347"></span><span class="hs-comment">-- *** Exception: GradientError {geExpected = WithoutGradient, geActual = WithGradient}</span><span>
</span><span id="line-348"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-type">sCheckedGradient</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-349"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683315"><span class="annot"><a href="#local-6989586621679683315"><span class="hs-identifier hs-type">gradient'</span></a></span></span><span> </span><span id="local-6989586621679683316"><span class="annot"><a href="#local-6989586621679683316"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679683317"><span class="annot"><a href="#local-6989586621679683317"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683312"><span class="annot"><a href="#local-6989586621679683312"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683311"><span class="annot"><a href="#local-6989586621679683311"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683310"><span class="annot"><a href="#local-6989586621679683310"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683309"><span class="annot"><a href="#local-6989586621679683309"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-350"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683317"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679683316"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679683317"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683315"><span class="hs-identifier hs-type">gradient'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-351"></span><span>  </span><span class="hs-comment">-- | layout</span><span>
</span><span id="line-352"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683315"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-353"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-354"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683317"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683312"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683311"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683310"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683309"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-355"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-356"></span><span>  </span><span class="annot"><a href="#local-6989586621679683316"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683315"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683312"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683311"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683310"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683309"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-357"></span><span id="sCheckedGradient"><span class="annot"><span class="annottext">sCheckedGradient :: SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-var hs-var">sCheckedGradient</span></a></span></span><span> </span><span id="local-6989586621679682224"><span class="annot"><span class="annottext">SGradient gradient'
</span><a href="#local-6989586621679682224"><span class="hs-identifier hs-var">gradient'</span></a></span></span><span> </span><span id="local-6989586621679682223"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682223"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-358"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679682222"><span class="annot"><span class="annottext">actualGradient :: RequiresGradient
</span><a href="#local-6989586621679682222"><span class="hs-identifier hs-var hs-var">actualGradient</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked RequiresGradient -&gt; RequiresGradient
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked RequiresGradient -&gt; RequiresGradient)
-&gt; (SGradient gradient -&gt; IsChecked RequiresGradient)
-&gt; SGradient gradient
-&gt; RequiresGradient
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SGradient gradient -&gt; IsChecked RequiresGradient
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SGradient gradient -&gt; RequiresGradient)
-&gt; SGradient gradient -&gt; RequiresGradient
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SGradient gradient
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetGradient gradient =&gt;
Tensor gradient layout device dataType shape -&gt; SGradient gradient
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetGradient"><span class="hs-identifier hs-var">sGetGradient</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682223"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-359"></span><span>      </span><span id="local-6989586621679682221"><span class="annot"><span class="annottext">expectedGradient :: RequiresGradient
</span><a href="#local-6989586621679682221"><span class="hs-identifier hs-var hs-var">expectedGradient</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked RequiresGradient -&gt; RequiresGradient
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked RequiresGradient -&gt; RequiresGradient)
-&gt; (SGradient gradient' -&gt; IsChecked RequiresGradient)
-&gt; SGradient gradient'
-&gt; RequiresGradient
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SGradient gradient' -&gt; IsChecked RequiresGradient
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SGradient gradient' -&gt; RequiresGradient)
-&gt; SGradient gradient' -&gt; RequiresGradient
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SGradient gradient'
</span><a href="#local-6989586621679682224"><span class="hs-identifier hs-var">gradient'</span></a></span><span>
</span><span id="line-360"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="hs-keyword">if</span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="#local-6989586621679682222"><span class="hs-identifier hs-var">actualGradient</span></a></span><span> </span><span class="annot"><span class="annottext">RequiresGradient -&gt; RequiresGradient -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="#local-6989586621679682221"><span class="hs-identifier hs-var">expectedGradient</span></a></span><span>
</span><span id="line-361"></span><span>        </span><span class="hs-keyword">then</span><span> </span><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient' layout device dataType shape
 -&gt; m (Tensor gradient' layout device dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient' layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient' layout device dataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; m (Tensor gradient' layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682223"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-362"></span><span>        </span><span class="hs-keyword">else</span><span> </span><span class="annot"><span class="annottext">GradientError -&gt; m (Tensor gradient' layout device dataType shape)
forall (m :: * -&gt; *) e a. (MonadThrow m, Exception e) =&gt; e -&gt; m a
</span><span class="hs-identifier hs-var">throwM</span></span><span> </span><span class="annot"><span class="annottext">(GradientError
 -&gt; m (Tensor gradient' layout device dataType shape))
-&gt; GradientError
-&gt; m (Tensor gradient' layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">RequiresGradient -&gt; RequiresGradient -&gt; GradientError
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#GradientError"><span class="hs-identifier hs-var">GradientError</span></a></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="#local-6989586621679682221"><span class="hs-identifier hs-var">expectedGradient</span></a></span><span> </span><span class="annot"><span class="annottext">RequiresGradient
</span><a href="#local-6989586621679682222"><span class="hs-identifier hs-var">actualGradient</span></a></span><span>
</span><span id="line-363"></span><span>
</span><span id="line-364"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedGradient"><span class="hs-identifier hs-type">checkedGradient</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-365"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682218"><span class="annot"><a href="#local-6989586621679682218"><span class="hs-identifier hs-type">gradient'</span></a></span></span><span> </span><span id="local-6989586621679682217"><span class="annot"><a href="#local-6989586621679682217"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682216"><span class="annot"><a href="#local-6989586621679682216"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682215"><span class="annot"><a href="#local-6989586621679682215"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682214"><span class="annot"><a href="#local-6989586621679682214"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682213"><span class="annot"><a href="#local-6989586621679682213"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682212"><span class="annot"><a href="#local-6989586621679682212"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-366"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682218"><span class="hs-identifier hs-type">gradient'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682216"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682217"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682216"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682218"><span class="hs-identifier hs-type">gradient'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-367"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-368"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682216"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682215"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682214"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682213"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682212"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-369"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-370"></span><span>  </span><span class="annot"><a href="#local-6989586621679682217"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682218"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682215"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682214"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682213"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682212"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-371"></span><span id="checkedGradient"><span class="annot"><span class="annottext">checkedGradient :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedGradient"><span class="hs-identifier hs-var hs-var">checkedGradient</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
forall (gradient' :: Gradient RequiresGradient) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetGradient gradient, MonadThrow m,
 Catch (gradient &lt;+&gt; gradient')) =&gt;
SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-var">sCheckedGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI gradient' =&gt; Sing gradient'
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679682218"><span class="hs-identifier hs-type">gradient'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-372"></span><span>
</span><span id="line-373"></span><span class="hs-comment">-- | Returns the input tensor but with 'UncheckedGradeint' as gradient type annotation.</span><span>
</span><span id="line-374"></span><span class="hs-comment">-- Any static information about whether or not the gradient computation is required for the tensor is lost.</span><span>
</span><span id="line-375"></span><span class="hs-comment">-- However, the tensor's underlying data structure is not changed.</span><span>
</span><span id="line-376"></span><span class="hs-comment">--</span><span>
</span><span id="line-377"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-378"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedGradient t</span><span>
</span><span id="line-379"></span><span class="hs-comment">-- uncheckedGradient t</span><span>
</span><span id="line-380"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-381"></span><span class="hs-comment">--        'UncheckedGradient</span><span>
</span><span id="line-382"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-383"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-384"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-385"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-386"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-387"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-388"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedGradient"><span class="hs-identifier hs-type">uncheckedGradient</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-389"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682210"><span class="annot"><a href="#local-6989586621679682210"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682209"><span class="annot"><a href="#local-6989586621679682209"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682208"><span class="annot"><a href="#local-6989586621679682208"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682207"><span class="annot"><a href="#local-6989586621679682207"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682206"><span class="annot"><a href="#local-6989586621679682206"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-390"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-391"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682210"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682209"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682208"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682207"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682206"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-392"></span><span>  </span><span class="hs-comment">-- | tensor without checked layout</span><span>
</span><span id="line-393"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#UncheckedGradient"><span class="hs-identifier hs-type">UncheckedGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682209"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682208"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682207"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682206"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-394"></span><span id="uncheckedGradient"><span class="annot"><span class="annottext">uncheckedGradient :: Tensor gradient layout device dataType shape
-&gt; Tensor 'UncheckedGradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedGradient"><span class="hs-identifier hs-var hs-var">uncheckedGradient</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor 'UncheckedGradient layout device dataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span>
</span><span id="line-395"></span><span>
</span><span id="line-396"></span><span class="hs-comment">-- | Returns a dense copy of the tensor.</span><span>
</span><span id="line-397"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#toDense"><span class="hs-identifier hs-type">toDense</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-398"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682204"><span class="annot"><a href="#local-6989586621679682204"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682203"><span class="annot"><a href="#local-6989586621679682203"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682202"><span class="annot"><a href="#local-6989586621679682202"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682201"><span class="annot"><a href="#local-6989586621679682201"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682200"><span class="annot"><a href="#local-6989586621679682200"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682199"><span class="annot"><a href="#local-6989586621679682199"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-399"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682204"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-400"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-401"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682203"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682202"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682201"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682200"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682199"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-402"></span><span>  </span><span class="hs-comment">-- | dense copy</span><span>
</span><span id="line-403"></span><span>  </span><span class="annot"><a href="#local-6989586621679682204"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682203"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-type">Dense</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682201"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682200"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682199"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-404"></span><span id="toDense"><span class="annot"><span class="annottext">toDense :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient ('Layout 'Dense) device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#toDense"><span class="hs-identifier hs-var hs-var">toDense</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient ('Layout 'Dense) device dataType shape)
-&gt; m (Tensor gradient ('Layout 'Dense) device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient ('Layout 'Dense) device dataType shape)
 -&gt; m (Tensor gradient ('Layout 'Dense) device dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient ('Layout 'Dense) device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient ('Layout 'Dense) device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient ('Layout 'Dense) device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_dense</span></a></span><span>
</span><span id="line-405"></span><span>
</span><span id="line-406"></span><span class="hs-comment">-- | Returns a sparse copy of the tensor.</span><span>
</span><span id="line-407"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#toSparse"><span class="hs-identifier hs-type">toSparse</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-408"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682196"><span class="annot"><a href="#local-6989586621679682196"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682195"><span class="annot"><a href="#local-6989586621679682195"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682194"><span class="annot"><a href="#local-6989586621679682194"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682193"><span class="annot"><a href="#local-6989586621679682193"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682192"><span class="annot"><a href="#local-6989586621679682192"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682191"><span class="annot"><a href="#local-6989586621679682191"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-409"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682196"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-410"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-411"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682195"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682194"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682193"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682192"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682191"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-412"></span><span>  </span><span class="hs-comment">-- | sparse copy</span><span>
</span><span id="line-413"></span><span>  </span><span class="annot"><a href="#local-6989586621679682196"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682195"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-type">Sparse</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682193"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682192"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682191"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-414"></span><span id="toSparse"><span class="annot"><span class="annottext">toSparse :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient ('Layout 'Sparse) device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#toSparse"><span class="hs-identifier hs-var hs-var">toSparse</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient ('Layout 'Sparse) device dataType shape)
-&gt; m (Tensor gradient ('Layout 'Sparse) device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient ('Layout 'Sparse) device dataType shape)
 -&gt; m (Tensor gradient ('Layout 'Sparse) device dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient ('Layout 'Sparse) device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient ('Layout 'Sparse) device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient ('Layout 'Sparse) device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_sparse</span></a></span><span>
</span><span id="line-415"></span><span>
</span><span id="line-416"></span><span class="hs-comment">-- | Set the memory layout of a tensor to a given layout.</span><span>
</span><span id="line-417"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetLayout"><span class="hs-identifier hs-type">sSetLayout</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-418"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682188"><span class="annot"><a href="#local-6989586621679682188"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682187"><span class="annot"><a href="#local-6989586621679682187"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682186"><span class="annot"><a href="#local-6989586621679682186"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682185"><span class="annot"><a href="#local-6989586621679682185"><span class="hs-identifier hs-type">layout'</span></a></span></span><span> </span><span id="local-6989586621679682184"><span class="annot"><a href="#local-6989586621679682184"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682183"><span class="annot"><a href="#local-6989586621679682183"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682182"><span class="annot"><a href="#local-6989586621679682182"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-419"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682188"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-420"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682186"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-421"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682187"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682185"><span class="hs-identifier hs-type">layout'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682184"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682183"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682182"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-422"></span><span>  </span><span class="annot"><a href="#local-6989586621679682188"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682187"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682186"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682184"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682183"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682182"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-423"></span><span id="sSetLayout"><span class="annot"><span class="annottext">sSetLayout :: SLayout layout
-&gt; Tensor gradient layout' device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetLayout"><span class="hs-identifier hs-var hs-var">sSetLayout</span></a></span></span><span> </span><span id="local-6989586621679682181"><span class="annot"><span class="annottext">SLayout layout
</span><a href="#local-6989586621679682181"><span class="hs-identifier hs-var">layout</span></a></span></span><span> </span><span id="local-6989586621679682180"><span class="annot"><span class="annottext">Tensor gradient layout' device dataType shape
</span><a href="#local-6989586621679682180"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-424"></span><span>  </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">IsChecked LayoutType -&gt; LayoutType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Sing layout -&gt; Demote (Layout LayoutType)
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">Sing layout
SLayout layout
</span><a href="#local-6989586621679682181"><span class="hs-identifier hs-var">layout</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-425"></span><span>    </span><span class="annot"><span class="annottext">LayoutType
</span><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-var">Dense</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; (Tensor gradient layout' device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout' device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout' device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_dense</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout' device dataType shape
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout' device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout' device dataType shape
</span><a href="#local-6989586621679682180"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-426"></span><span>    </span><span class="annot"><span class="annottext">LayoutType
</span><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-var">Sparse</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; (Tensor gradient layout' device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout' device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout' device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_sparse</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout' device dataType shape
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout' device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout' device dataType shape
</span><a href="#local-6989586621679682180"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-427"></span><span>
</span><span id="line-428"></span><span class="hs-keyword">class</span><span> </span><span id="SGetLayout"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetLayout"><span class="hs-identifier hs-var">SGetLayout</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679683275"><span class="annot"><a href="#local-6989586621679683275"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier hs-type">LayoutType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-429"></span><span>  </span><span class="hs-comment">-- | Returns the gradually typed memory layout of the input tensor.</span><span>
</span><span id="line-430"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-431"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' layout = sOnes $ TensorSpec (SGradient SWithGradient) layout (SDevice SCPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-432"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SLayout SDense</span><span>
</span><span id="line-433"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetLayout t</span><span>
</span><span id="line-434"></span><span>  </span><span class="hs-comment">-- SLayout SDense</span><span>
</span><span id="line-435"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedLayout Dense</span><span>
</span><span id="line-436"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetLayout t</span><span>
</span><span id="line-437"></span><span>  </span><span class="hs-comment">-- SUncheckedLayout Dense</span><span>
</span><span id="line-438"></span><span>  </span><span id="sGetLayout"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetLayout"><span class="hs-identifier hs-type">sGetLayout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-439"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683270"><span class="annot"><a href="#local-6989586621679683270"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683269"><span class="annot"><a href="#local-6989586621679683269"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683268"><span class="annot"><a href="#local-6989586621679683268"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683267"><span class="annot"><a href="#local-6989586621679683267"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-440"></span><span>    </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-441"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683270"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683275"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683269"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683268"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683267"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-442"></span><span>    </span><span class="hs-comment">-- | memory layout</span><span>
</span><span id="line-443"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683275"><span class="hs-identifier hs-type">layout</span></a></span><span>
</span><span id="line-444"></span><span>
</span><span id="line-445"></span><span>  </span><span class="hs-comment">-- | Returns the untyped memory layout of the input tensor.</span><span>
</span><span id="line-446"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-447"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' layout = sOnes $ TensorSpec (SGradient SWithGradient) layout (SDevice SCPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-448"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SLayout SDense</span><span>
</span><span id="line-449"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getLayoutType t</span><span>
</span><span id="line-450"></span><span>  </span><span class="hs-comment">-- Dense</span><span>
</span><span id="line-451"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedLayout Dense</span><span>
</span><span id="line-452"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getLayoutType t</span><span>
</span><span id="line-453"></span><span>  </span><span class="hs-comment">-- Dense</span><span>
</span><span id="line-454"></span><span>  </span><span id="getLayoutType"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getLayoutType"><span class="hs-identifier hs-type">getLayoutType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-455"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683265"><span class="annot"><a href="#local-6989586621679683265"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683264"><span class="annot"><a href="#local-6989586621679683264"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683263"><span class="annot"><a href="#local-6989586621679683263"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683262"><span class="annot"><a href="#local-6989586621679683262"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-456"></span><span>    </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-457"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683265"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683275"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683264"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683263"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683262"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-458"></span><span>    </span><span class="hs-comment">-- | memory layout</span><span>
</span><span id="line-459"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier hs-type">LayoutType</span></a></span><span>
</span><span id="line-460"></span><span>  </span><span id="local-6989586621679682177"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getLayoutType"><span class="hs-identifier hs-var hs-var">getLayoutType</span></a></span><span> </span><span id="local-6989586621679682176"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682176"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked LayoutType -&gt; LayoutType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked LayoutType -&gt; LayoutType)
-&gt; (SLayout layout -&gt; IsChecked LayoutType)
-&gt; SLayout layout
-&gt; LayoutType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SLayout layout -&gt; IsChecked LayoutType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SLayout layout -&gt; LayoutType) -&gt; SLayout layout -&gt; LayoutType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SLayout layout
forall (layout :: Layout LayoutType)
       (gradient :: Gradient RequiresGradient)
       (device :: Device (DeviceType Nat)) (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetLayout layout =&gt;
Tensor gradient layout device dataType shape -&gt; SLayout layout
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetLayout"><span class="hs-identifier hs-var">sGetLayout</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682176"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-461"></span><span>
</span><span id="line-462"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682173"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetLayout"><span class="hs-identifier hs-type">SGetLayout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#UncheckedLayout"><span class="hs-identifier hs-type">UncheckedLayout</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-463"></span><span>  </span><span id="local-6989586621679682171"><span class="annot"><span class="annottext">sGetLayout :: Tensor gradient 'UncheckedLayout device dataType shape
-&gt; SLayout 'UncheckedLayout
</span><a href="#local-6989586621679682171"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetLayout</span></a></span></span><span> </span><span id="local-6989586621679682170"><span class="annot"><span class="annottext">Tensor gradient 'UncheckedLayout device dataType shape
</span><a href="#local-6989586621679682170"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-464"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient 'UncheckedLayout device dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_sparse</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient 'UncheckedLayout device dataType shape
</span><a href="#local-6989586621679682170"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">LayoutType -&gt; SLayout 'UncheckedLayout
</span><a href="Torch.GraduallyTyped.Layout.html#SUncheckedLayout"><span class="hs-identifier hs-var">SUncheckedLayout</span></a></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-var">Sparse</span></a></span><span>
</span><span id="line-465"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">LayoutType -&gt; SLayout 'UncheckedLayout
</span><a href="Torch.GraduallyTyped.Layout.html#SUncheckedLayout"><span class="hs-identifier hs-var">SUncheckedLayout</span></a></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-var">Dense</span></a></span><span>
</span><span id="line-466"></span><span>
</span><span id="line-467"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682165"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetLayout"><span class="hs-identifier hs-type">SGetLayout</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Sparse"><span class="hs-identifier hs-type">Sparse</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-468"></span><span>  </span><span id="local-6989586621679682164"><span class="annot"><span class="annottext">sGetLayout :: Tensor gradient ('Layout 'Sparse) device dataType shape
-&gt; SLayout ('Layout 'Sparse)
</span><a href="#local-6989586621679682164"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetLayout</span></a></span></span><span> </span><span id="local-6989586621679682163"><span class="annot"><span class="annottext">Tensor gradient ('Layout 'Sparse) device dataType shape
</span><a href="#local-6989586621679682163"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-469"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient ('Layout 'Sparse) device dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_sparse</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient ('Layout 'Sparse) device dataType shape
</span><a href="#local-6989586621679682163"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SLayoutType 'Sparse -&gt; SLayout ('Layout 'Sparse)
forall (layoutType :: LayoutType).
SLayoutType layoutType -&gt; SLayout ('Layout layoutType)
</span><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-var">SLayout</span></a></span><span> </span><span class="annot"><span class="annottext">SLayoutType 'Sparse
</span><a href="Torch.GraduallyTyped.Layout.html#SSparse"><span class="hs-identifier hs-var">SSparse</span></a></span><span>
</span><span id="line-470"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-471"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SLayout ('Layout 'Sparse)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SLayout ('Layout 'Sparse))
-&gt; String -&gt; SLayout ('Layout 'Sparse)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-472"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should be sparse but isn't. &quot;</span></span><span>
</span><span id="line-473"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-474"></span><span>
</span><span id="line-475"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682158"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetLayout"><span class="hs-identifier hs-type">SGetLayout</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Layout"><span class="hs-identifier hs-type">Layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#Dense"><span class="hs-identifier hs-type">Dense</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-476"></span><span>  </span><span id="local-6989586621679682157"><span class="annot"><span class="annottext">sGetLayout :: Tensor gradient ('Layout 'Dense) device dataType shape
-&gt; SLayout ('Layout 'Dense)
</span><a href="#local-6989586621679682157"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetLayout</span></a></span></span><span> </span><span id="local-6989586621679682156"><span class="annot"><span class="annottext">Tensor gradient ('Layout 'Dense) device dataType shape
</span><a href="#local-6989586621679682156"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-477"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient ('Layout 'Dense) device dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_sparse</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient ('Layout 'Dense) device dataType shape
</span><a href="#local-6989586621679682156"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-478"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SLayout ('Layout 'Dense)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SLayout ('Layout 'Dense))
-&gt; String -&gt; SLayout ('Layout 'Dense)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-479"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should be dense but isn't. &quot;</span></span><span>
</span><span id="line-480"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-481"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SLayoutType 'Dense -&gt; SLayout ('Layout 'Dense)
forall (layoutType :: LayoutType).
SLayoutType layoutType -&gt; SLayout ('Layout layoutType)
</span><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-var">SLayout</span></a></span><span> </span><span class="annot"><span class="annottext">SLayoutType 'Dense
</span><a href="Torch.GraduallyTyped.Layout.html#SDense"><span class="hs-identifier hs-var">SDense</span></a></span><span>
</span><span id="line-482"></span><span>
</span><span id="line-483"></span><span class="hs-keyword">data</span><span> </span><span id="LayoutError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#LayoutError"><span class="hs-identifier hs-var">LayoutError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="LayoutError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#LayoutError"><span class="hs-identifier hs-var">LayoutError</span></a></span></span><span> </span><span class="hs-special">{</span><span id="leExpected"><span class="annot"><span class="annottext">LayoutError -&gt; LayoutType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#leExpected"><span class="hs-identifier hs-var hs-var">leExpected</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier hs-type">LayoutType</span></a></span><span class="hs-special">,</span><span> </span><span id="leActual"><span class="annot"><span class="annottext">LayoutError -&gt; LayoutType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#leActual"><span class="hs-identifier hs-var hs-var">leActual</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#LayoutType"><span class="hs-identifier hs-type">LayoutType</span></a></span><span class="hs-special">}</span><span>
</span><span id="line-484"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682146"><span id="local-6989586621679682148"><span id="local-6989586621679682150"><span class="annot"><span class="annottext">Int -&gt; LayoutError -&gt; ShowS
[LayoutError] -&gt; ShowS
LayoutError -&gt; String
(Int -&gt; LayoutError -&gt; ShowS)
-&gt; (LayoutError -&gt; String)
-&gt; ([LayoutError] -&gt; ShowS)
-&gt; Show LayoutError
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [LayoutError] -&gt; ShowS
$cshowList :: [LayoutError] -&gt; ShowS
show :: LayoutError -&gt; String
$cshow :: LayoutError -&gt; String
showsPrec :: Int -&gt; LayoutError -&gt; ShowS
$cshowsPrec :: Int -&gt; LayoutError -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Typeable</span></span><span class="hs-special">)</span><span>
</span><span id="line-485"></span><span>
</span><span id="line-486"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682140"><span id="local-6989586621679682142"><span class="annot"><span class="hs-identifier hs-type">Exception</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#LayoutError"><span class="hs-identifier hs-type">LayoutError</span></a></span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-487"></span><span>  </span><span id="local-6989586621679682138"><span class="annot"><span class="annottext">displayException :: LayoutError -&gt; String
</span><a href="#local-6989586621679682138"><span class="hs-identifier hs-var hs-var hs-var hs-var">displayException</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#LayoutError"><span class="hs-identifier hs-type">LayoutError</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679682136"><span id="local-6989586621679682137"><span class="annot"><span class="annottext">LayoutType
leActual :: LayoutType
leExpected :: LayoutType
leActual :: LayoutError -&gt; LayoutType
leExpected :: LayoutError -&gt; LayoutType
</span><a href="#local-6989586621679682136"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-488"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor does not have the memory layout `&quot;</span></span><span>
</span><span id="line-489"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayoutType -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="#local-6989586621679682137"><span class="hs-identifier hs-var">leExpected</span></a></span><span>
</span><span id="line-490"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;` but `&quot;</span></span><span>
</span><span id="line-491"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayoutType -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="#local-6989586621679682136"><span class="hs-identifier hs-var">leActual</span></a></span><span>
</span><span id="line-492"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;`.&quot;</span></span><span>
</span><span id="line-493"></span><span>
</span><span id="line-494"></span><span class="hs-comment">-- | Checks whether or not the input tensor has the memory layout 'layout'</span><span>
</span><span id="line-495"></span><span class="hs-comment">-- and returns a statically annotated copy of it wrapped in a 'MonadThrow' 'm'.</span><span>
</span><span id="line-496"></span><span class="hs-comment">--</span><span>
</span><span id="line-497"></span><span class="hs-comment">-- For instance, if 'm' is 'Maybe', then the result will be wrapped in 'Just' if and only if the tensor has indeed the memory layout 'layout'.</span><span>
</span><span id="line-498"></span><span class="hs-comment">-- If it does not have it, then the result will be 'Nothing'.</span><span>
</span><span id="line-499"></span><span class="hs-comment">--</span><span>
</span><span id="line-500"></span><span class="hs-comment">-- In the REPL, 'm' will default to 'IO':</span><span>
</span><span id="line-501"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SUncheckedLayout Dense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-502"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedLayout (SLayout SDense) t</span><span>
</span><span id="line-503"></span><span class="hs-comment">-- &gt;&gt;&gt; :type t'</span><span>
</span><span id="line-504"></span><span class="hs-comment">-- t'</span><span>
</span><span id="line-505"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-506"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-507"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-508"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-509"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-510"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-511"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-512"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-513"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedLayout (SLayout SSparse) t</span><span>
</span><span id="line-514"></span><span class="hs-comment">-- *** Exception: LayoutError {leExpected = Sparse, leActual = Dense}</span><span>
</span><span id="line-515"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedLayout"><span class="hs-identifier hs-type">sCheckedLayout</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-516"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683218"><span class="annot"><a href="#local-6989586621679683218"><span class="hs-identifier hs-type">layout'</span></a></span></span><span> </span><span id="local-6989586621679683219"><span class="annot"><a href="#local-6989586621679683219"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679683217"><span class="annot"><a href="#local-6989586621679683217"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683220"><span class="annot"><a href="#local-6989586621679683220"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683216"><span class="annot"><a href="#local-6989586621679683216"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683215"><span class="annot"><a href="#local-6989586621679683215"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683214"><span class="annot"><a href="#local-6989586621679683214"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-517"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetLayout"><span class="hs-identifier hs-type">SGetLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683220"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679683219"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679683220"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683218"><span class="hs-identifier hs-type">layout'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-518"></span><span>  </span><span class="hs-comment">-- | layout</span><span>
</span><span id="line-519"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683218"><span class="hs-identifier hs-type">layout'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-520"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-521"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683217"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683220"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683216"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683215"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683214"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-522"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-523"></span><span>  </span><span class="annot"><a href="#local-6989586621679683219"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683217"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683218"><span class="hs-identifier hs-type">layout'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683216"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683215"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683214"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-524"></span><span id="sCheckedLayout"><span class="annot"><span class="annottext">sCheckedLayout :: SLayout layout'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedLayout"><span class="hs-identifier hs-var hs-var">sCheckedLayout</span></a></span></span><span> </span><span id="local-6989586621679682134"><span class="annot"><span class="annottext">SLayout layout'
</span><a href="#local-6989586621679682134"><span class="hs-identifier hs-var">layout'</span></a></span></span><span> </span><span id="local-6989586621679682133"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682133"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-525"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679682132"><span class="annot"><span class="annottext">actualLayout :: LayoutType
</span><a href="#local-6989586621679682132"><span class="hs-identifier hs-var hs-var">actualLayout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked LayoutType -&gt; LayoutType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked LayoutType -&gt; LayoutType)
-&gt; (SLayout layout -&gt; IsChecked LayoutType)
-&gt; SLayout layout
-&gt; LayoutType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SLayout layout -&gt; IsChecked LayoutType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SLayout layout -&gt; LayoutType) -&gt; SLayout layout -&gt; LayoutType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SLayout layout
forall (layout :: Layout LayoutType)
       (gradient :: Gradient RequiresGradient)
       (device :: Device (DeviceType Nat)) (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetLayout layout =&gt;
Tensor gradient layout device dataType shape -&gt; SLayout layout
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetLayout"><span class="hs-identifier hs-var">sGetLayout</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682133"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-526"></span><span>      </span><span id="local-6989586621679682131"><span class="annot"><span class="annottext">expectedLayout :: LayoutType
</span><a href="#local-6989586621679682131"><span class="hs-identifier hs-var hs-var">expectedLayout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked LayoutType -&gt; LayoutType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked LayoutType -&gt; LayoutType)
-&gt; (SLayout layout' -&gt; IsChecked LayoutType)
-&gt; SLayout layout'
-&gt; LayoutType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SLayout layout' -&gt; IsChecked LayoutType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SLayout layout' -&gt; LayoutType) -&gt; SLayout layout' -&gt; LayoutType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SLayout layout'
</span><a href="#local-6989586621679682134"><span class="hs-identifier hs-var">layout'</span></a></span><span>
</span><span id="line-527"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="hs-keyword">if</span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="#local-6989586621679682132"><span class="hs-identifier hs-var">actualLayout</span></a></span><span> </span><span class="annot"><span class="annottext">LayoutType -&gt; LayoutType -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="#local-6989586621679682131"><span class="hs-identifier hs-var">expectedLayout</span></a></span><span>
</span><span id="line-528"></span><span>        </span><span class="hs-keyword">then</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout' device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout' device dataType shape
 -&gt; m (Tensor gradient layout' device dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout' device dataType shape)
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout' device dataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; m (Tensor gradient layout' device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682133"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-529"></span><span>        </span><span class="hs-keyword">else</span><span> </span><span class="annot"><span class="annottext">LayoutError -&gt; m (Tensor gradient layout' device dataType shape)
forall (m :: * -&gt; *) e a. (MonadThrow m, Exception e) =&gt; e -&gt; m a
</span><span class="hs-identifier hs-var">throwM</span></span><span> </span><span class="annot"><span class="annottext">(LayoutError -&gt; m (Tensor gradient layout' device dataType shape))
-&gt; LayoutError -&gt; m (Tensor gradient layout' device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">LayoutType -&gt; LayoutType -&gt; LayoutError
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#LayoutError"><span class="hs-identifier hs-var">LayoutError</span></a></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="#local-6989586621679682131"><span class="hs-identifier hs-var">expectedLayout</span></a></span><span> </span><span class="annot"><span class="annottext">LayoutType
</span><a href="#local-6989586621679682132"><span class="hs-identifier hs-var">actualLayout</span></a></span><span>
</span><span id="line-530"></span><span>
</span><span id="line-531"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedLayout"><span class="hs-identifier hs-type">checkedLayout</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-532"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682129"><span class="annot"><a href="#local-6989586621679682129"><span class="hs-identifier hs-type">layout'</span></a></span></span><span> </span><span id="local-6989586621679682128"><span class="annot"><a href="#local-6989586621679682128"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682127"><span class="annot"><a href="#local-6989586621679682127"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682126"><span class="annot"><a href="#local-6989586621679682126"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682125"><span class="annot"><a href="#local-6989586621679682125"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682124"><span class="annot"><a href="#local-6989586621679682124"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682123"><span class="annot"><a href="#local-6989586621679682123"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-533"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682129"><span class="hs-identifier hs-type">layout'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetLayout"><span class="hs-identifier hs-type">SGetLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682126"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682128"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682126"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682129"><span class="hs-identifier hs-type">layout'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-534"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-535"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682127"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682126"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682125"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682124"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682123"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-536"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-537"></span><span>  </span><span class="annot"><a href="#local-6989586621679682128"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682127"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682129"><span class="hs-identifier hs-type">layout'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682125"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682124"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682123"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-538"></span><span id="checkedLayout"><span class="annot"><span class="annottext">checkedLayout :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedLayout"><span class="hs-identifier hs-var hs-var">checkedLayout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SLayout layout'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
forall (layout' :: Layout LayoutType) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetLayout layout, MonadThrow m, Catch (layout &lt;+&gt; layout')) =&gt;
SLayout layout'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout' device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedLayout"><span class="hs-identifier hs-var">sCheckedLayout</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI layout' =&gt; Sing layout'
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679682129"><span class="hs-identifier hs-type">layout'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-539"></span><span>
</span><span id="line-540"></span><span class="hs-comment">-- | Returns the input tensor but with 'UncheckedLayout' as memory layout type annotation.</span><span>
</span><span id="line-541"></span><span class="hs-comment">-- Any static information about the tensor's memory layout is thus erased.</span><span>
</span><span id="line-542"></span><span class="hs-comment">-- However, the tensor's underlying data structure is not changed.</span><span>
</span><span id="line-543"></span><span class="hs-comment">--</span><span>
</span><span id="line-544"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-545"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedLayout t</span><span>
</span><span id="line-546"></span><span class="hs-comment">-- uncheckedLayout t</span><span>
</span><span id="line-547"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-548"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-549"></span><span class="hs-comment">--        'UncheckedLayout</span><span>
</span><span id="line-550"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-551"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-552"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-553"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-554"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-555"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedLayout"><span class="hs-identifier hs-type">uncheckedLayout</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-556"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682121"><span class="annot"><a href="#local-6989586621679682121"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682120"><span class="annot"><a href="#local-6989586621679682120"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682119"><span class="annot"><a href="#local-6989586621679682119"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682118"><span class="annot"><a href="#local-6989586621679682118"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682117"><span class="annot"><a href="#local-6989586621679682117"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-557"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-558"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682121"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682120"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682119"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682118"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682117"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-559"></span><span>  </span><span class="hs-comment">-- | tensor without checked layout</span><span>
</span><span id="line-560"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682121"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#UncheckedLayout"><span class="hs-identifier hs-type">UncheckedLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682119"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682118"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682117"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-561"></span><span id="uncheckedLayout"><span class="annot"><span class="annottext">uncheckedLayout :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient 'UncheckedLayout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedLayout"><span class="hs-identifier hs-var hs-var">uncheckedLayout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient 'UncheckedLayout device dataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span>
</span><span id="line-562"></span><span>
</span><span id="line-563"></span><span class="hs-comment">-- | Returns a copy of the tensor in CPU memory.</span><span>
</span><span id="line-564"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#cpu"><span class="hs-identifier hs-type">cpu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-565"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682115"><span class="annot"><a href="#local-6989586621679682115"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682114"><span class="annot"><a href="#local-6989586621679682114"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682113"><span class="annot"><a href="#local-6989586621679682113"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682112"><span class="annot"><a href="#local-6989586621679682112"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682111"><span class="annot"><a href="#local-6989586621679682111"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682110"><span class="annot"><a href="#local-6989586621679682110"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-566"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682115"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-567"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-568"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682114"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682113"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682112"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682111"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682110"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-569"></span><span>  </span><span class="hs-comment">-- | copy in CPU memory</span><span>
</span><span id="line-570"></span><span>  </span><span class="annot"><a href="#local-6989586621679682115"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682114"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682113"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-type">CPU</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682111"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682110"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-571"></span><span id="cpu"><span class="annot"><span class="annottext">cpu :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout ('Device 'CPU) dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#cpu"><span class="hs-identifier hs-var hs-var">cpu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout ('Device 'CPU) dataType shape)
-&gt; m (Tensor gradient layout ('Device 'CPU) dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout ('Device 'CPU) dataType shape)
 -&gt; m (Tensor gradient layout ('Device 'CPU) dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout ('Device 'CPU) dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout ('Device 'CPU) dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout ('Device 'CPU) dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_cpu</span></a></span><span>
</span><span id="line-572"></span><span>
</span><span id="line-573"></span><span class="hs-comment">-- | Returns a copy of the tensor in CUDA memory.</span><span>
</span><span id="line-574"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#cuda"><span class="hs-identifier hs-type">cuda</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-575"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682107"><span class="annot"><a href="#local-6989586621679682107"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682106"><span class="annot"><a href="#local-6989586621679682106"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682105"><span class="annot"><a href="#local-6989586621679682105"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682104"><span class="annot"><a href="#local-6989586621679682104"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682103"><span class="annot"><a href="#local-6989586621679682103"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682102"><span class="annot"><a href="#local-6989586621679682102"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-576"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682107"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-577"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-578"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682106"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682105"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682104"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682103"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682102"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-579"></span><span>  </span><span class="hs-comment">-- | copy in CUDA memory</span><span>
</span><span id="line-580"></span><span>  </span><span class="annot"><a href="#local-6989586621679682107"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682106"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682105"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682103"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682102"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-581"></span><span id="cuda"><span class="annot"><span class="annottext">cuda :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#cuda"><span class="hs-identifier hs-var hs-var">cuda</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)
-&gt; m (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)
 -&gt; m (Tensor gradient layout ('Device ('CUDA 0)) dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout ('Device ('CUDA 0)) dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_cuda</span></a></span><span>
</span><span id="line-582"></span><span>
</span><span id="line-583"></span><span class="hs-comment">-- | Reallocates a tensor on the specified device.</span><span>
</span><span id="line-584"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetDevice"><span class="hs-identifier hs-type">sSetDevice</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-585"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682758"><span class="annot"><a href="#local-6989586621679682758"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682756"><span class="annot"><a href="#local-6989586621679682756"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682755"><span class="annot"><a href="#local-6989586621679682755"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682757"><span class="annot"><a href="#local-6989586621679682757"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682754"><span class="annot"><a href="#local-6989586621679682754"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679682753"><span class="annot"><a href="#local-6989586621679682753"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682752"><span class="annot"><a href="#local-6989586621679682752"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-586"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682758"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-587"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682757"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-588"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682756"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682755"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682754"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682753"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682752"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-589"></span><span>  </span><span class="annot"><a href="#local-6989586621679682758"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682756"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682755"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682757"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682753"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682752"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-590"></span><span id="sSetDevice"><span class="annot"><span class="annottext">sSetDevice :: SDevice device
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetDevice"><span class="hs-identifier hs-var hs-var">sSetDevice</span></a></span></span><span> </span><span id="local-6989586621679682099"><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679682099"><span class="hs-identifier hs-var">device</span></a></span></span><span> </span><span id="local-6989586621679682098"><span class="annot"><span class="annottext">Tensor gradient layout device' dataType shape
</span><a href="#local-6989586621679682098"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-591"></span><span>  </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">IsChecked (DeviceType Int16) -&gt; DeviceType Int16
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Sing device -&gt; Demote (Device (DeviceType Nat))
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">Sing device
SDevice device
</span><a href="#local-6989586621679682099"><span class="hs-identifier hs-var">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-592"></span><span>    </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-var">CPU</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-593"></span><span>      </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; (Tensor gradient layout device' dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device' dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_cpu</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device' dataType shape
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device' dataType shape
</span><a href="#local-6989586621679682098"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-594"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><span class="annottext">Int16
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-595"></span><span>      </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; (Tensor gradient layout device' dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device' dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_cuda</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device' dataType shape
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device' dataType shape
</span><a href="#local-6989586621679682098"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-596"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span id="local-6989586621679682097"><span class="annot"><span class="annottext">Int16
</span><a href="#local-6989586621679682097"><span class="hs-identifier hs-var">idx</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-597"></span><span>      </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-598"></span><span>        </span><span id="local-6989586621679682096"><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679682096"><span class="hs-identifier hs-var">opts</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorOptions</span></a></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr TensorOptions))
-&gt; Tensor gradient layout device' dataType shape
-&gt; IO (ForeignPtr TensorOptions)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr TensorOptions)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_options</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device' dataType shape
</span><a href="#local-6989586621679682098"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-599"></span><span>        </span><span id="local-6989586621679682094"><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679682094"><span class="hs-identifier hs-var">opts'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorOptions</span></a></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr TensorOptions
 -&gt; Int16 -&gt; IO (ForeignPtr TensorOptions))
-&gt; ForeignPtr TensorOptions
-&gt; Int16
-&gt; IO (ForeignPtr TensorOptions)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorOptions -&gt; Int16 -&gt; IO (ForeignPtr TensorOptions)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensorOptions_device_index_s</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679682096"><span class="hs-identifier hs-var">opts</span></a></span><span> </span><span class="annot"><span class="annottext">Int16
</span><a href="#local-6989586621679682097"><span class="hs-identifier hs-var">idx</span></a></span><span>
</span><span id="line-600"></span><span>        </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; ForeignPtr TensorOptions
 -&gt; CBool
 -&gt; CBool
 -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device' dataType shape
-&gt; ForeignPtr TensorOptions
-&gt; Bool
-&gt; Bool
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 x2 cx2 x3 cx3 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable x3 cx3,
 Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; cx3 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; x3 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast4</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr TensorOptions
-&gt; CBool
-&gt; CBool
-&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_obb</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device' dataType shape
</span><a href="#local-6989586621679682098"><span class="hs-identifier hs-var">input</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679682094"><span class="hs-identifier hs-var">opts'</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679682091"><span class="hs-identifier hs-var">nonBlocking</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679682090"><span class="hs-identifier hs-var">copy</span></a></span><span>
</span><span id="line-601"></span><span>      </span><span class="hs-keyword">where</span><span>
</span><span id="line-602"></span><span>        </span><span id="local-6989586621679682091"><span class="annot"><span class="annottext">nonBlocking :: Bool
</span><a href="#local-6989586621679682091"><span class="hs-identifier hs-var hs-var">nonBlocking</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-603"></span><span>        </span><span id="local-6989586621679682090"><span class="annot"><span class="annottext">copy :: Bool
</span><a href="#local-6989586621679682090"><span class="hs-identifier hs-var hs-var">copy</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-604"></span><span>
</span><span id="line-605"></span><span class="hs-keyword">class</span><span> </span><span id="SGetDevice"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDevice"><span class="hs-identifier hs-var">SGetDevice</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679683173"><span class="annot"><a href="#local-6989586621679683173"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier hs-type">DeviceType</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-606"></span><span>  </span><span class="hs-comment">-- | Returns the gradually typed compute device of the input tensor.</span><span>
</span><span id="line-607"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-608"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; ones' device = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) device (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-609"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones' $ SDevice SCPU</span><span>
</span><span id="line-610"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetDevice t</span><span>
</span><span id="line-611"></span><span>  </span><span class="hs-comment">-- SDevice SCPU</span><span>
</span><span id="line-612"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones' $ SUncheckedDevice CPU</span><span>
</span><span id="line-613"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetDevice t</span><span>
</span><span id="line-614"></span><span>  </span><span class="hs-comment">-- SUncheckedDevice CPU</span><span>
</span><span id="line-615"></span><span>  </span><span id="sGetDevice"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDevice"><span class="hs-identifier hs-type">sGetDevice</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-616"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683169"><span class="annot"><a href="#local-6989586621679683169"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683168"><span class="annot"><a href="#local-6989586621679683168"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683167"><span class="annot"><a href="#local-6989586621679683167"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683166"><span class="annot"><a href="#local-6989586621679683166"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-617"></span><span>    </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-618"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683169"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683168"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683173"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683167"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683166"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-619"></span><span>    </span><span class="hs-comment">-- | compute device of the input tensor</span><span>
</span><span id="line-620"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683173"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-621"></span><span>
</span><span id="line-622"></span><span>  </span><span class="hs-comment">-- | Returns the untyped compute device of the input tensor.</span><span>
</span><span id="line-623"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-624"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; ones' device = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) device (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-625"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones' $ SDevice SCPU</span><span>
</span><span id="line-626"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDeviceType t</span><span>
</span><span id="line-627"></span><span>  </span><span class="hs-comment">-- CPU</span><span>
</span><span id="line-628"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones' $ SUncheckedDevice CPU</span><span>
</span><span id="line-629"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDeviceType t</span><span>
</span><span id="line-630"></span><span>  </span><span class="hs-comment">-- CPU</span><span>
</span><span id="line-631"></span><span>  </span><span id="getDeviceType"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getDeviceType"><span class="hs-identifier hs-type">getDeviceType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-632"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683164"><span class="annot"><a href="#local-6989586621679683164"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683163"><span class="annot"><a href="#local-6989586621679683163"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683162"><span class="annot"><a href="#local-6989586621679683162"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683161"><span class="annot"><a href="#local-6989586621679683161"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-633"></span><span>    </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-634"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683164"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683163"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683173"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683162"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683161"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-635"></span><span>    </span><span class="hs-comment">-- | compute device of the input tensor</span><span>
</span><span id="line-636"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier hs-type">DeviceType</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int16</span></span><span>
</span><span id="line-637"></span><span>  </span><span id="local-6989586621679682087"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getDeviceType"><span class="hs-identifier hs-var hs-var">getDeviceType</span></a></span><span> </span><span id="local-6989586621679682086"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682086"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked (DeviceType Int16) -&gt; DeviceType Int16
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked (DeviceType Int16) -&gt; DeviceType Int16)
-&gt; (SDevice device -&gt; IsChecked (DeviceType Int16))
-&gt; SDevice device
-&gt; DeviceType Int16
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SDevice device -&gt; IsChecked (DeviceType Int16)
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SDevice device -&gt; DeviceType Int16)
-&gt; SDevice device -&gt; DeviceType Int16
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SDevice device
forall (device :: Device (DeviceType Nat))
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetDevice device =&gt;
Tensor gradient layout device dataType shape -&gt; SDevice device
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDevice"><span class="hs-identifier hs-var">sGetDevice</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682086"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-638"></span><span>
</span><span id="line-639"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682083"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDevice"><span class="hs-identifier hs-type">SGetDevice</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#UncheckedDevice"><span class="hs-identifier hs-type">UncheckedDevice</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-640"></span><span>  </span><span id="local-6989586621679682081"><span class="annot"><span class="annottext">sGetDevice :: Tensor gradient layout 'UncheckedDevice dataType shape
-&gt; SDevice 'UncheckedDevice
</span><a href="#local-6989586621679682081"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDevice</span></a></span></span><span> </span><span id="local-6989586621679682080"><span class="annot"><span class="annottext">Tensor gradient layout 'UncheckedDevice dataType shape
</span><a href="#local-6989586621679682080"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-641"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IO CBool -&gt; IO Bool
forall a ca. Castable a ca =&gt; IO ca -&gt; IO a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast0</span></a></span><span> </span><span class="annot"><span class="annottext">IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hasCUDA</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient layout 'UncheckedDevice dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_cuda</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout 'UncheckedDevice dataType shape
</span><a href="#local-6989586621679682080"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-642"></span><span>      </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">IO Int -&gt; Int
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO Int64)
-&gt; Tensor gradient layout 'UncheckedDevice dataType shape -&gt; IO Int
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO Int64
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_get_device</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout 'UncheckedDevice dataType shape
</span><a href="#local-6989586621679682080"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-643"></span><span>        </span><span id="local-6989586621679682075"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679682075"><span class="hs-identifier hs-var">deviceIndex</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">DeviceType Int16 -&gt; SDevice 'UncheckedDevice
</span><a href="Torch.GraduallyTyped.Device.html#SUncheckedDevice"><span class="hs-identifier hs-var">SUncheckedDevice</span></a></span><span> </span><span class="annot"><span class="annottext">(DeviceType Int16 -&gt; SDevice 'UncheckedDevice)
-&gt; (Int -&gt; DeviceType Int16) -&gt; Int -&gt; SDevice 'UncheckedDevice
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int16 -&gt; DeviceType Int16
forall deviceId. deviceId -&gt; DeviceType deviceId
</span><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-var">CUDA</span></a></span><span> </span><span class="annot"><span class="annottext">(Int16 -&gt; DeviceType Int16)
-&gt; (Int -&gt; Int16) -&gt; Int -&gt; DeviceType Int16
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int16
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; SDevice 'UncheckedDevice)
-&gt; Int -&gt; SDevice 'UncheckedDevice
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679682075"><span class="hs-identifier hs-var">deviceIndex</span></a></span><span>
</span><span id="line-644"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">DeviceType Int16 -&gt; SDevice 'UncheckedDevice
</span><a href="Torch.GraduallyTyped.Device.html#SUncheckedDevice"><span class="hs-identifier hs-var">SUncheckedDevice</span></a></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
forall deviceId. DeviceType deviceId
</span><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-var">CPU</span></a></span><span>
</span><span id="line-645"></span><span>
</span><span id="line-646"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682071"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDevice"><span class="hs-identifier hs-type">SGetDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CPU"><span class="hs-identifier hs-type">CPU</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-647"></span><span>  </span><span id="local-6989586621679682070"><span class="annot"><span class="annottext">sGetDevice :: Tensor gradient layout ('Device 'CPU) dataType shape
-&gt; SDevice ('Device 'CPU)
</span><a href="#local-6989586621679682070"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDevice</span></a></span></span><span> </span><span id="local-6989586621679682069"><span class="annot"><span class="annottext">Tensor gradient layout ('Device 'CPU) dataType shape
</span><a href="#local-6989586621679682069"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-648"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IO CBool -&gt; IO Bool
forall a ca. Castable a ca =&gt; IO ca -&gt; IO a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast0</span></a></span><span> </span><span class="annot"><span class="annottext">IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hasCUDA</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient layout ('Device 'CPU) dataType shape -&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_cuda</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout ('Device 'CPU) dataType shape
</span><a href="#local-6989586621679682069"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-649"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SDevice ('Device 'CPU)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SDevice ('Device 'CPU))
-&gt; String -&gt; SDevice ('Device 'CPU)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-650"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should be on CPU but is on CUDA. &quot;</span></span><span>
</span><span id="line-651"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-652"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SDeviceType 'CPU -&gt; SDevice ('Device 'CPU)
forall (deviceType :: DeviceType Nat).
SDeviceType deviceType -&gt; SDevice ('Device deviceType)
</span><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-var">SDevice</span></a></span><span> </span><span class="annot"><span class="annottext">SDeviceType 'CPU
</span><a href="Torch.GraduallyTyped.Device.html#SCPU"><span class="hs-identifier hs-var">SCPU</span></a></span><span>
</span><span id="line-653"></span><span>
</span><span id="line-654"></span><span id="local-6989586621679682066"><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682063"><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679682066"><span class="hs-identifier hs-type">deviceIndex</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDevice"><span class="hs-identifier hs-type">SGetDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#Device"><span class="hs-identifier hs-type">Device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#CUDA"><span class="hs-identifier hs-type">CUDA</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682066"><span class="hs-identifier hs-type">deviceIndex</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-655"></span><span>  </span><span id="local-6989586621679682062"><span class="annot"><span class="annottext">sGetDevice :: Tensor gradient layout ('Device ('CUDA deviceIndex)) dataType shape
-&gt; SDevice ('Device ('CUDA deviceIndex))
</span><a href="#local-6989586621679682062"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDevice</span></a></span></span><span> </span><span id="local-6989586621679682061"><span class="annot"><span class="annottext">Tensor gradient layout ('Device ('CUDA deviceIndex)) dataType shape
</span><a href="#local-6989586621679682061"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-656"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IO CBool -&gt; IO Bool
forall a ca. Castable a ca =&gt; IO ca -&gt; IO a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast0</span></a></span><span> </span><span class="annot"><span class="annottext">IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hasCUDA</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor
     gradient layout ('Device ('CUDA deviceIndex)) dataType shape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_cuda</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout ('Device ('CUDA deviceIndex)) dataType shape
</span><a href="#local-6989586621679682061"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-657"></span><span>      </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">IO Int -&gt; Int
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO Int64)
-&gt; Tensor
     gradient layout ('Device ('CUDA deviceIndex)) dataType shape
-&gt; IO Int
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO Int64
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_get_device</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout ('Device ('CUDA deviceIndex)) dataType shape
</span><a href="#local-6989586621679682061"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-658"></span><span>        </span><span id="local-6989586621679682060"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679682060"><span class="hs-identifier hs-var">deviceIndex</span></a></span></span><span>
</span><span id="line-659"></span><span>          </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679682060"><span class="hs-identifier hs-var">deviceIndex</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Int
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Proxy deviceIndex -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; *).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Proxy deviceIndex
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679682066"><span class="hs-identifier hs-type">deviceIndex</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">SDeviceType ('CUDA deviceIndex)
-&gt; SDevice ('Device ('CUDA deviceIndex))
forall (deviceType :: DeviceType Nat).
SDeviceType deviceType -&gt; SDevice ('Device deviceType)
</span><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-var">SDevice</span></a></span><span> </span><span class="annot"><span class="annottext">SDeviceType ('CUDA deviceIndex)
forall (deviceId :: Nat).
KnownNat deviceId =&gt;
SDeviceType ('CUDA deviceId)
</span><a href="Torch.GraduallyTyped.Device.html#SCUDA"><span class="hs-identifier hs-var">SCUDA</span></a></span><span>
</span><span id="line-660"></span><span>          </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-661"></span><span>            </span><span class="annot"><span class="annottext">String -&gt; SDevice ('Device ('CUDA deviceIndex))
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SDevice ('Device ('CUDA deviceIndex)))
-&gt; String -&gt; SDevice ('Device ('CUDA deviceIndex))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-662"></span><span>              </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should be on CUDA device &quot;</span></span><span>
</span><span id="line-663"></span><span>                </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Proxy deviceIndex -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; *).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Proxy deviceIndex
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679682066"><span class="hs-identifier hs-type">deviceIndex</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-664"></span><span>                </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot; but is on device &quot;</span></span><span>
</span><span id="line-665"></span><span>                </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679682060"><span class="hs-identifier hs-var">deviceIndex</span></a></span><span>
</span><span id="line-666"></span><span>                </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;. &quot;</span></span><span>
</span><span id="line-667"></span><span>                </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-668"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-669"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SDevice ('Device ('CUDA deviceIndex))
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SDevice ('Device ('CUDA deviceIndex)))
-&gt; String -&gt; SDevice ('Device ('CUDA deviceIndex))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-670"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should be on CUDA but is on CPU. &quot;</span></span><span>
</span><span id="line-671"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span></span><span>
</span><span id="line-672"></span><span>
</span><span id="line-673"></span><span class="hs-keyword">data</span><span> </span><span id="DeviceError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DeviceError"><span class="hs-identifier hs-var">DeviceError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="DeviceError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DeviceError"><span class="hs-identifier hs-var">DeviceError</span></a></span></span><span> </span><span class="hs-special">{</span><span id="deExpected"><span class="annot"><span class="annottext">DeviceError -&gt; DeviceType Int16
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#deExpected"><span class="hs-identifier hs-var hs-var">deExpected</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier hs-type">DeviceType</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int16</span></span><span class="hs-special">,</span><span> </span><span id="deActual"><span class="annot"><span class="annottext">DeviceError -&gt; DeviceType Int16
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#deActual"><span class="hs-identifier hs-var hs-var">deActual</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#DeviceType"><span class="hs-identifier hs-type">DeviceType</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int16</span></span><span class="hs-special">}</span><span>
</span><span id="line-674"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682049"><span id="local-6989586621679682051"><span id="local-6989586621679682053"><span class="annot"><span class="annottext">Int -&gt; DeviceError -&gt; ShowS
[DeviceError] -&gt; ShowS
DeviceError -&gt; String
(Int -&gt; DeviceError -&gt; ShowS)
-&gt; (DeviceError -&gt; String)
-&gt; ([DeviceError] -&gt; ShowS)
-&gt; Show DeviceError
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [DeviceError] -&gt; ShowS
$cshowList :: [DeviceError] -&gt; ShowS
show :: DeviceError -&gt; String
$cshow :: DeviceError -&gt; String
showsPrec :: Int -&gt; DeviceError -&gt; ShowS
$cshowsPrec :: Int -&gt; DeviceError -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Typeable</span></span><span class="hs-special">)</span><span>
</span><span id="line-675"></span><span>
</span><span id="line-676"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679682043"><span id="local-6989586621679682045"><span class="annot"><span class="hs-identifier hs-type">Exception</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DeviceError"><span class="hs-identifier hs-type">DeviceError</span></a></span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-677"></span><span>  </span><span id="local-6989586621679682041"><span class="annot"><span class="annottext">displayException :: DeviceError -&gt; String
</span><a href="#local-6989586621679682041"><span class="hs-identifier hs-var hs-var hs-var hs-var">displayException</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DeviceError"><span class="hs-identifier hs-type">DeviceError</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679682039"><span id="local-6989586621679682040"><span class="annot"><span class="annottext">DeviceType Int16
deActual :: DeviceType Int16
deExpected :: DeviceType Int16
deActual :: DeviceError -&gt; DeviceType Int16
deExpected :: DeviceError -&gt; DeviceType Int16
</span><a href="#local-6989586621679682039"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-678"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor is not in the memory of the device `&quot;</span></span><span>
</span><span id="line-679"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16 -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="#local-6989586621679682040"><span class="hs-identifier hs-var">deExpected</span></a></span><span>
</span><span id="line-680"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;` but `&quot;</span></span><span>
</span><span id="line-681"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16 -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="#local-6989586621679682039"><span class="hs-identifier hs-var">deActual</span></a></span><span>
</span><span id="line-682"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;`.&quot;</span></span><span>
</span><span id="line-683"></span><span>
</span><span id="line-684"></span><span class="hs-comment">-- | Checks whether or not the input tensor is in the memory of 'device'</span><span>
</span><span id="line-685"></span><span class="hs-comment">-- and returns a statically annotated copy of it wrapped in a 'MonadThrow' 'm'.</span><span>
</span><span id="line-686"></span><span class="hs-comment">--</span><span>
</span><span id="line-687"></span><span class="hs-comment">-- For instance, if 'm' is 'Maybe', then the result will be wrapped in 'Just' if and only if the tensor is indeed on 'device'.</span><span>
</span><span id="line-688"></span><span class="hs-comment">-- If it is not, then the result will be 'Nothing'.</span><span>
</span><span id="line-689"></span><span class="hs-comment">--</span><span>
</span><span id="line-690"></span><span class="hs-comment">-- In the REPL, 'm' will default to 'IO':</span><span>
</span><span id="line-691"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SUncheckedDevice CPU) (SDataType SFloat) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-692"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedDevice (SDevice SCPU) t</span><span>
</span><span id="line-693"></span><span class="hs-comment">-- &gt;&gt;&gt; :type t'</span><span>
</span><span id="line-694"></span><span class="hs-comment">-- t'</span><span>
</span><span id="line-695"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-696"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-697"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-698"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-699"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-700"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-701"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-702"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-703"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedDevice (SDevice (SCUDA @0)) t</span><span>
</span><span id="line-704"></span><span class="hs-comment">-- *** Exception: DeviceError {deExpected = CUDA 0, deActual = CPU}</span><span>
</span><span id="line-705"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedDevice"><span class="hs-identifier hs-type">sCheckedDevice</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-706"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683104"><span class="annot"><a href="#local-6989586621679683104"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679683105"><span class="annot"><a href="#local-6989586621679683105"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679683103"><span class="annot"><a href="#local-6989586621679683103"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683102"><span class="annot"><a href="#local-6989586621679683102"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683106"><span class="annot"><a href="#local-6989586621679683106"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683101"><span class="annot"><a href="#local-6989586621679683101"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679683100"><span class="annot"><a href="#local-6989586621679683100"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-707"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDevice"><span class="hs-identifier hs-type">SGetDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683106"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679683105"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679683106"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683104"><span class="hs-identifier hs-type">device'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-708"></span><span>  </span><span class="hs-comment">-- | device</span><span>
</span><span id="line-709"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683104"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-710"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-711"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683103"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683102"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683106"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683101"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683100"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-712"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-713"></span><span>  </span><span class="annot"><a href="#local-6989586621679683105"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683103"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683102"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683104"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683101"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683100"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-714"></span><span id="sCheckedDevice"><span class="annot"><span class="annottext">sCheckedDevice :: SDevice device'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedDevice"><span class="hs-identifier hs-var hs-var">sCheckedDevice</span></a></span></span><span> </span><span id="local-6989586621679682037"><span class="annot"><span class="annottext">SDevice device'
</span><a href="#local-6989586621679682037"><span class="hs-identifier hs-var">device'</span></a></span></span><span> </span><span id="local-6989586621679682036"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682036"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-715"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679682035"><span class="annot"><span class="annottext">actualDevice :: DeviceType Int16
</span><a href="#local-6989586621679682035"><span class="hs-identifier hs-var hs-var">actualDevice</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked (DeviceType Int16) -&gt; DeviceType Int16
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked (DeviceType Int16) -&gt; DeviceType Int16)
-&gt; (SDevice device -&gt; IsChecked (DeviceType Int16))
-&gt; SDevice device
-&gt; DeviceType Int16
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SDevice device -&gt; IsChecked (DeviceType Int16)
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SDevice device -&gt; DeviceType Int16)
-&gt; SDevice device -&gt; DeviceType Int16
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SDevice device
forall (device :: Device (DeviceType Nat))
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetDevice device =&gt;
Tensor gradient layout device dataType shape -&gt; SDevice device
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDevice"><span class="hs-identifier hs-var">sGetDevice</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682036"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-716"></span><span>      </span><span id="local-6989586621679682034"><span class="annot"><span class="annottext">expectedDevice :: DeviceType Int16
</span><a href="#local-6989586621679682034"><span class="hs-identifier hs-var hs-var">expectedDevice</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked (DeviceType Int16) -&gt; DeviceType Int16
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked (DeviceType Int16) -&gt; DeviceType Int16)
-&gt; (SDevice device' -&gt; IsChecked (DeviceType Int16))
-&gt; SDevice device'
-&gt; DeviceType Int16
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SDevice device' -&gt; IsChecked (DeviceType Int16)
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SDevice device' -&gt; DeviceType Int16)
-&gt; SDevice device' -&gt; DeviceType Int16
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SDevice device'
</span><a href="#local-6989586621679682037"><span class="hs-identifier hs-var">device'</span></a></span><span>
</span><span id="line-717"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="hs-keyword">if</span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="#local-6989586621679682035"><span class="hs-identifier hs-var">actualDevice</span></a></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16 -&gt; DeviceType Int16 -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="#local-6989586621679682034"><span class="hs-identifier hs-var">expectedDevice</span></a></span><span>
</span><span id="line-718"></span><span>        </span><span class="hs-keyword">then</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device' dataType shape
 -&gt; m (Tensor gradient layout device' dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout device' dataType shape)
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device' dataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; m (Tensor gradient layout device' dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682036"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-719"></span><span>        </span><span class="hs-keyword">else</span><span> </span><span class="annot"><span class="annottext">DeviceError -&gt; m (Tensor gradient layout device' dataType shape)
forall (m :: * -&gt; *) e a. (MonadThrow m, Exception e) =&gt; e -&gt; m a
</span><span class="hs-identifier hs-var">throwM</span></span><span> </span><span class="annot"><span class="annottext">(DeviceError -&gt; m (Tensor gradient layout device' dataType shape))
-&gt; DeviceError -&gt; m (Tensor gradient layout device' dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16 -&gt; DeviceType Int16 -&gt; DeviceError
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#DeviceError"><span class="hs-identifier hs-var">DeviceError</span></a></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="#local-6989586621679682034"><span class="hs-identifier hs-var">expectedDevice</span></a></span><span> </span><span class="annot"><span class="annottext">DeviceType Int16
</span><a href="#local-6989586621679682035"><span class="hs-identifier hs-var">actualDevice</span></a></span><span>
</span><span id="line-720"></span><span>
</span><span id="line-721"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedDevice"><span class="hs-identifier hs-type">checkedDevice</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-722"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682032"><span class="annot"><a href="#local-6989586621679682032"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679682031"><span class="annot"><a href="#local-6989586621679682031"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682030"><span class="annot"><a href="#local-6989586621679682030"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682029"><span class="annot"><a href="#local-6989586621679682029"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682028"><span class="annot"><a href="#local-6989586621679682028"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682027"><span class="annot"><a href="#local-6989586621679682027"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682026"><span class="annot"><a href="#local-6989586621679682026"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-723"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682032"><span class="hs-identifier hs-type">device'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDevice"><span class="hs-identifier hs-type">SGetDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682028"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682031"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682028"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682032"><span class="hs-identifier hs-type">device'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-724"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-725"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682030"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682029"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682028"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682027"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682026"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-726"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-727"></span><span>  </span><span class="annot"><a href="#local-6989586621679682031"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682030"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682029"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682032"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682027"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682026"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-728"></span><span id="checkedDevice"><span class="annot"><span class="annottext">checkedDevice :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedDevice"><span class="hs-identifier hs-var hs-var">checkedDevice</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SDevice device'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
forall (device' :: Device (DeviceType Nat)) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetDevice device, MonadThrow m, Catch (device &lt;+&gt; device')) =&gt;
SDevice device'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device' dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedDevice"><span class="hs-identifier hs-var">sCheckedDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI device' =&gt; Sing device'
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679682032"><span class="hs-identifier hs-type">device'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-729"></span><span>
</span><span id="line-730"></span><span class="hs-comment">-- | Returns the input tensor but with 'UncheckedDevice' as device type annotation.</span><span>
</span><span id="line-731"></span><span class="hs-comment">-- Any static information about the tensor's device is thus erased.</span><span>
</span><span id="line-732"></span><span class="hs-comment">-- However, the tensor's underlying data structure is not changed.</span><span>
</span><span id="line-733"></span><span class="hs-comment">--</span><span>
</span><span id="line-734"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-735"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedDevice t</span><span>
</span><span id="line-736"></span><span class="hs-comment">-- uncheckedDevice t</span><span>
</span><span id="line-737"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-738"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-739"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-740"></span><span class="hs-comment">--        'UncheckedDevice</span><span>
</span><span id="line-741"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-742"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-743"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-744"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-745"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedDevice"><span class="hs-identifier hs-type">uncheckedDevice</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-746"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682024"><span class="annot"><a href="#local-6989586621679682024"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682023"><span class="annot"><a href="#local-6989586621679682023"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682022"><span class="annot"><a href="#local-6989586621679682022"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682021"><span class="annot"><a href="#local-6989586621679682021"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682020"><span class="annot"><a href="#local-6989586621679682020"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-747"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-748"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682024"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682023"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682022"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682021"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682020"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-749"></span><span>  </span><span class="hs-comment">-- | tensor without checked device</span><span>
</span><span id="line-750"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682024"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682023"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#UncheckedDevice"><span class="hs-identifier hs-type">UncheckedDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682021"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682020"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-751"></span><span id="uncheckedDevice"><span class="annot"><span class="annottext">uncheckedDevice :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout 'UncheckedDevice dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedDevice"><span class="hs-identifier hs-var hs-var">uncheckedDevice</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout 'UncheckedDevice dataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span>
</span><span id="line-752"></span><span>
</span><span id="line-753"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to 'Bool'.</span><span>
</span><span id="line-754"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#bool"><span class="hs-identifier hs-type">bool</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-755"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682018"><span class="annot"><a href="#local-6989586621679682018"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682017"><span class="annot"><a href="#local-6989586621679682017"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682016"><span class="annot"><a href="#local-6989586621679682016"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682015"><span class="annot"><a href="#local-6989586621679682015"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682014"><span class="annot"><a href="#local-6989586621679682014"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682013"><span class="annot"><a href="#local-6989586621679682013"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-756"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682018"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-757"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-758"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682017"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682016"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682015"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682014"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682013"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-759"></span><span>  </span><span class="hs-comment">-- | 'Bool' copy</span><span>
</span><span id="line-760"></span><span>  </span><span class="annot"><a href="#local-6989586621679682018"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682016"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682015"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Bool"><span class="hs-identifier hs-type">Bool</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682013"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-761"></span><span id="bool"><span class="annot"><span class="annottext">bool :: Tensor gradient layout device dataType shape
-&gt; m (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#bool"><span class="hs-identifier hs-var hs-var">bool</span></a></span></span><span> </span><span id="local-6989586621679682012"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682012"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  (Tensor
     ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
 -&gt; m (Tensor
         ('Gradient 'WithoutGradient)
         layout
         device
         ('DataType 'Bool)
         shape))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682012"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Bool"><span class="hs-identifier hs-var">Bool</span></a></span><span>
</span><span id="line-762"></span><span>
</span><span id="line-763"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to 'UInt8'.</span><span>
</span><span id="line-764"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#byte"><span class="hs-identifier hs-type">byte</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-765"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682009"><span class="annot"><a href="#local-6989586621679682009"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682008"><span class="annot"><a href="#local-6989586621679682008"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682007"><span class="annot"><a href="#local-6989586621679682007"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682006"><span class="annot"><a href="#local-6989586621679682006"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682005"><span class="annot"><a href="#local-6989586621679682005"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682004"><span class="annot"><a href="#local-6989586621679682004"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-766"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682009"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-767"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-768"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682008"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682007"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682006"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682005"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682004"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-769"></span><span>  </span><span class="hs-comment">-- | 'UInt8' copy</span><span>
</span><span id="line-770"></span><span>  </span><span class="annot"><a href="#local-6989586621679682009"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682007"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682006"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#UInt8"><span class="hs-identifier hs-type">UInt8</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679682004"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-771"></span><span id="byte"><span class="annot"><span class="annottext">byte :: Tensor gradient layout device dataType shape
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'UInt8)
        shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#byte"><span class="hs-identifier hs-var hs-var">byte</span></a></span></span><span> </span><span id="local-6989586621679682003"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682003"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  (Tensor
     ('Gradient 'WithoutGradient)
     layout
     device
     ('DataType 'UInt8)
     shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'UInt8)
        shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      ('Gradient 'WithoutGradient)
      layout
      device
      ('DataType 'UInt8)
      shape)
 -&gt; m (Tensor
         ('Gradient 'WithoutGradient)
         layout
         device
         ('DataType 'UInt8)
         shape))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'UInt8)
        shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'UInt8)
        shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'UInt8)
        shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679682003"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#UInt8"><span class="hs-identifier hs-var">UInt8</span></a></span><span>
</span><span id="line-772"></span><span>
</span><span id="line-773"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to 'Int8'.</span><span>
</span><span id="line-774"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#char"><span class="hs-identifier hs-type">char</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-775"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682001"><span class="annot"><a href="#local-6989586621679682001"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682000"><span class="annot"><a href="#local-6989586621679682000"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681999"><span class="annot"><a href="#local-6989586621679681999"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681998"><span class="annot"><a href="#local-6989586621679681998"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681997"><span class="annot"><a href="#local-6989586621679681997"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681996"><span class="annot"><a href="#local-6989586621679681996"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-776"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682001"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-777"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-778"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682000"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681999"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681998"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681997"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681996"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-779"></span><span>  </span><span class="hs-comment">-- | 'Int8' copy</span><span>
</span><span id="line-780"></span><span>  </span><span class="annot"><a href="#local-6989586621679682001"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681999"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681998"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Int8"><span class="hs-identifier hs-type">Int8</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681996"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-781"></span><span id="char"><span class="annot"><span class="annottext">char :: Tensor gradient layout device dataType shape
-&gt; m (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#char"><span class="hs-identifier hs-var hs-var">char</span></a></span></span><span> </span><span id="local-6989586621679681995"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681995"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  (Tensor
     ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
 -&gt; m (Tensor
         ('Gradient 'WithoutGradient)
         layout
         device
         ('DataType 'Int8)
         shape))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681995"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Int8"><span class="hs-identifier hs-var">Int8</span></a></span><span>
</span><span id="line-782"></span><span>
</span><span id="line-783"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to 'Int16'.</span><span>
</span><span id="line-784"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#short"><span class="hs-identifier hs-type">short</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-785"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681993"><span class="annot"><a href="#local-6989586621679681993"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681992"><span class="annot"><a href="#local-6989586621679681992"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681991"><span class="annot"><a href="#local-6989586621679681991"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681990"><span class="annot"><a href="#local-6989586621679681990"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681989"><span class="annot"><a href="#local-6989586621679681989"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681988"><span class="annot"><a href="#local-6989586621679681988"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-786"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681993"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-787"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-788"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681992"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681991"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681990"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681989"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681988"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-789"></span><span>  </span><span class="hs-comment">-- | 'Int16' copy</span><span>
</span><span id="line-790"></span><span>  </span><span class="annot"><a href="#local-6989586621679681993"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681991"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681990"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Int16"><span class="hs-identifier hs-type">Int16</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681988"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-791"></span><span id="short"><span class="annot"><span class="annottext">short :: Tensor gradient layout device dataType shape
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int16)
        shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#short"><span class="hs-identifier hs-var hs-var">short</span></a></span></span><span> </span><span id="local-6989586621679681987"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681987"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  (Tensor
     ('Gradient 'WithoutGradient)
     layout
     device
     ('DataType 'Int16)
     shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int16)
        shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      ('Gradient 'WithoutGradient)
      layout
      device
      ('DataType 'Int16)
      shape)
 -&gt; m (Tensor
         ('Gradient 'WithoutGradient)
         layout
         device
         ('DataType 'Int16)
         shape))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int16)
        shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int16)
        shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int16)
        shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681987"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Int16"><span class="hs-identifier hs-var">Int16</span></a></span><span>
</span><span id="line-792"></span><span>
</span><span id="line-793"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to 'Int32'.</span><span>
</span><span id="line-794"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#int"><span class="hs-identifier hs-type">int</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-795"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681985"><span class="annot"><a href="#local-6989586621679681985"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681984"><span class="annot"><a href="#local-6989586621679681984"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681983"><span class="annot"><a href="#local-6989586621679681983"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681982"><span class="annot"><a href="#local-6989586621679681982"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681981"><span class="annot"><a href="#local-6989586621679681981"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681980"><span class="annot"><a href="#local-6989586621679681980"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-796"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681985"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-797"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-798"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681984"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681983"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681982"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681981"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681980"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-799"></span><span>  </span><span class="hs-comment">-- | 'Int32' copy</span><span>
</span><span id="line-800"></span><span>  </span><span class="annot"><a href="#local-6989586621679681985"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681983"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681982"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Int32"><span class="hs-identifier hs-type">Int32</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681980"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-801"></span><span id="int"><span class="annot"><span class="annottext">int :: Tensor gradient layout device dataType shape
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int32)
        shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#int"><span class="hs-identifier hs-var hs-var">int</span></a></span></span><span> </span><span id="local-6989586621679681979"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681979"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  (Tensor
     ('Gradient 'WithoutGradient)
     layout
     device
     ('DataType 'Int32)
     shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int32)
        shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      ('Gradient 'WithoutGradient)
      layout
      device
      ('DataType 'Int32)
      shape)
 -&gt; m (Tensor
         ('Gradient 'WithoutGradient)
         layout
         device
         ('DataType 'Int32)
         shape))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int32)
        shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int32)
        shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int32)
        shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681979"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Int32"><span class="hs-identifier hs-var">Int32</span></a></span><span>
</span><span id="line-802"></span><span>
</span><span id="line-803"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to 'Int64'.</span><span>
</span><span id="line-804"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#long"><span class="hs-identifier hs-type">long</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-805"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681977"><span class="annot"><a href="#local-6989586621679681977"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681976"><span class="annot"><a href="#local-6989586621679681976"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681975"><span class="annot"><a href="#local-6989586621679681975"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681974"><span class="annot"><a href="#local-6989586621679681974"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681973"><span class="annot"><a href="#local-6989586621679681973"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681972"><span class="annot"><a href="#local-6989586621679681972"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-806"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681977"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-807"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-808"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681976"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681975"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681974"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681973"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681972"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-809"></span><span>  </span><span class="hs-comment">-- | 'Int64' copy</span><span>
</span><span id="line-810"></span><span>  </span><span class="annot"><a href="#local-6989586621679681977"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681975"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681974"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Int64"><span class="hs-identifier hs-type">Int64</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681972"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-811"></span><span id="long"><span class="annot"><span class="annottext">long :: Tensor gradient layout device dataType shape
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int64)
        shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#long"><span class="hs-identifier hs-var hs-var">long</span></a></span></span><span> </span><span id="local-6989586621679681971"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681971"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  (Tensor
     ('Gradient 'WithoutGradient)
     layout
     device
     ('DataType 'Int64)
     shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int64)
        shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      ('Gradient 'WithoutGradient)
      layout
      device
      ('DataType 'Int64)
      shape)
 -&gt; m (Tensor
         ('Gradient 'WithoutGradient)
         layout
         device
         ('DataType 'Int64)
         shape))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int64)
        shape)
-&gt; m (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int64)
        shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        layout
        device
        ('DataType 'Int64)
        shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681971"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Int64"><span class="hs-identifier hs-var">Int64</span></a></span><span>
</span><span id="line-812"></span><span>
</span><span id="line-813"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to the 16-bit floating point format 'Half'.</span><span>
</span><span id="line-814"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#half"><span class="hs-identifier hs-type">half</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-815"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681969"><span class="annot"><a href="#local-6989586621679681969"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681968"><span class="annot"><a href="#local-6989586621679681968"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681967"><span class="annot"><a href="#local-6989586621679681967"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681966"><span class="annot"><a href="#local-6989586621679681966"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681965"><span class="annot"><a href="#local-6989586621679681965"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681964"><span class="annot"><a href="#local-6989586621679681964"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-816"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681969"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-817"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-818"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681968"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681967"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681966"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681965"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681964"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-819"></span><span>  </span><span class="hs-comment">-- | 'Half' copy</span><span>
</span><span id="line-820"></span><span>  </span><span class="annot"><a href="#local-6989586621679681969"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681968"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681967"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681966"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Half"><span class="hs-identifier hs-type">Half</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681964"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-821"></span><span id="half"><span class="annot"><span class="annottext">half :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device ('DataType 'Half) shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#half"><span class="hs-identifier hs-var hs-var">half</span></a></span></span><span> </span><span id="local-6989586621679681963"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681963"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device ('DataType 'Half) shape)
-&gt; m (Tensor gradient layout device ('DataType 'Half) shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device ('DataType 'Half) shape)
 -&gt; m (Tensor gradient layout device ('DataType 'Half) shape))
-&gt; IO (Tensor gradient layout device ('DataType 'Half) shape)
-&gt; m (Tensor gradient layout device ('DataType 'Half) shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO (Tensor gradient layout device ('DataType 'Half) shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681963"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Half"><span class="hs-identifier hs-var">Half</span></a></span><span>
</span><span id="line-822"></span><span>
</span><span id="line-823"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to the 32-bit floating point format 'Float'.</span><span>
</span><span id="line-824"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#float"><span class="hs-identifier hs-type">float</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-825"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681961"><span class="annot"><a href="#local-6989586621679681961"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681960"><span class="annot"><a href="#local-6989586621679681960"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681959"><span class="annot"><a href="#local-6989586621679681959"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681958"><span class="annot"><a href="#local-6989586621679681958"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681957"><span class="annot"><a href="#local-6989586621679681957"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681956"><span class="annot"><a href="#local-6989586621679681956"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-826"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681961"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-827"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-828"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681960"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681959"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681958"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681957"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681956"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-829"></span><span>  </span><span class="hs-comment">-- | 'Float' copy</span><span>
</span><span id="line-830"></span><span>  </span><span class="annot"><a href="#local-6989586621679681961"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681960"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681959"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681958"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Float"><span class="hs-identifier hs-type">Float</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681956"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-831"></span><span id="float"><span class="annot"><span class="annottext">float :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device ('DataType 'Float) shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#float"><span class="hs-identifier hs-var hs-var">float</span></a></span></span><span> </span><span id="local-6989586621679681955"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681955"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device ('DataType 'Float) shape)
-&gt; m (Tensor gradient layout device ('DataType 'Float) shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device ('DataType 'Float) shape)
 -&gt; m (Tensor gradient layout device ('DataType 'Float) shape))
-&gt; IO (Tensor gradient layout device ('DataType 'Float) shape)
-&gt; m (Tensor gradient layout device ('DataType 'Float) shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO (Tensor gradient layout device ('DataType 'Float) shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681955"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Float"><span class="hs-identifier hs-var">Float</span></a></span><span>
</span><span id="line-832"></span><span>
</span><span id="line-833"></span><span class="hs-comment">-- | Returns a copy of the tensor converted to the 32-bit floating point format 'Double'.</span><span>
</span><span id="line-834"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#double"><span class="hs-identifier hs-type">double</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-835"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681953"><span class="annot"><a href="#local-6989586621679681953"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681952"><span class="annot"><a href="#local-6989586621679681952"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681951"><span class="annot"><a href="#local-6989586621679681951"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681950"><span class="annot"><a href="#local-6989586621679681950"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681949"><span class="annot"><a href="#local-6989586621679681949"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681948"><span class="annot"><a href="#local-6989586621679681948"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-836"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681953"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-837"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-838"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681952"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681951"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681950"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681949"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681948"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-839"></span><span>  </span><span class="hs-comment">-- | 'Double' copy</span><span>
</span><span id="line-840"></span><span>  </span><span class="annot"><a href="#local-6989586621679681953"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681952"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681951"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681950"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Double"><span class="hs-identifier hs-type">Double</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681948"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-841"></span><span id="double"><span class="annot"><span class="annottext">double :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device ('DataType 'Double) shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#double"><span class="hs-identifier hs-var hs-var">double</span></a></span></span><span> </span><span id="local-6989586621679681947"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681947"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device ('DataType 'Double) shape)
-&gt; m (Tensor gradient layout device ('DataType 'Double) shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device ('DataType 'Double) shape)
 -&gt; m (Tensor gradient layout device ('DataType 'Double) shape))
-&gt; IO (Tensor gradient layout device ('DataType 'Double) shape)
-&gt; m (Tensor gradient layout device ('DataType 'Double) shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; DType
-&gt; IO (Tensor gradient layout device ('DataType 'Double) shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ScalarType -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_toType_s</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681947"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="Torch.GraduallyTyped.DType.html#Double"><span class="hs-identifier hs-var">Double</span></a></span><span>
</span><span id="line-842"></span><span>
</span><span id="line-843"></span><span class="hs-comment">-- | Set the data type of a tensor to the specified data type.</span><span>
</span><span id="line-844"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetDataType"><span class="hs-identifier hs-type">sSetDataType</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-845"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681945"><span class="annot"><a href="#local-6989586621679681945"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681944"><span class="annot"><a href="#local-6989586621679681944"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681943"><span class="annot"><a href="#local-6989586621679681943"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681942"><span class="annot"><a href="#local-6989586621679681942"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681941"><span class="annot"><a href="#local-6989586621679681941"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681940"><span class="annot"><a href="#local-6989586621679681940"><span class="hs-identifier hs-type">dataType'</span></a></span></span><span> </span><span id="local-6989586621679681939"><span class="annot"><a href="#local-6989586621679681939"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-846"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681945"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-847"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-type">SDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681941"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-848"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681944"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681943"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681942"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681940"><span class="hs-identifier hs-type">dataType'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681939"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-849"></span><span>  </span><span class="annot"><a href="#local-6989586621679681945"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681944"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681943"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681942"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681941"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681939"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-850"></span><span id="sSetDataType"><span class="annot"><span class="annottext">sSetDataType :: SDataType dataType
-&gt; Tensor gradient layout device dataType' shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetDataType"><span class="hs-identifier hs-var hs-var">sSetDataType</span></a></span></span><span> </span><span id="local-6989586621679681938"><span class="annot"><span class="annottext">SDataType dataType
</span><a href="#local-6989586621679681938"><span class="hs-identifier hs-var">dataType</span></a></span></span><span> </span><span id="local-6989586621679681937"><span class="annot"><span class="annottext">Tensor gradient layout device dataType' shape
</span><a href="#local-6989586621679681937"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-851"></span><span>  </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">IsChecked DType -&gt; DType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Sing dataType -&gt; Demote (DataType DType)
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">Sing dataType
SDataType dataType
</span><a href="#local-6989586621679681938"><span class="hs-identifier hs-var">dataType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-852"></span><span>    </span><span id="local-6989586621679681936"><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681936"><span class="hs-identifier hs-var">dType</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-853"></span><span>      </span><span id="local-6989586621679681935"><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679681935"><span class="hs-identifier hs-var">opts</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorOptions</span></a></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr TensorOptions))
-&gt; Tensor gradient layout device dataType' shape
-&gt; IO (ForeignPtr TensorOptions)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr TensorOptions)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_options</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType' shape
</span><a href="#local-6989586621679681937"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-854"></span><span>      </span><span id="local-6989586621679681934"><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679681934"><span class="hs-identifier hs-var">opts'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorOptions</span></a></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr TensorOptions
 -&gt; ScalarType -&gt; IO (ForeignPtr TensorOptions))
-&gt; ForeignPtr TensorOptions
-&gt; DType
-&gt; IO (ForeignPtr TensorOptions)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorOptions
-&gt; ScalarType -&gt; IO (ForeignPtr TensorOptions)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensorOptions_dtype_s</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679681935"><span class="hs-identifier hs-var">opts</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681936"><span class="hs-identifier hs-var">dType</span></a></span><span>
</span><span id="line-855"></span><span>      </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; ForeignPtr TensorOptions
 -&gt; CBool
 -&gt; CBool
 -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType' shape
-&gt; ForeignPtr TensorOptions
-&gt; Bool
-&gt; Bool
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 x2 cx2 x3 cx3 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable x3 cx3,
 Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; cx3 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; x3 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast4</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr TensorOptions
-&gt; CBool
-&gt; CBool
-&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_obb</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType' shape
</span><a href="#local-6989586621679681937"><span class="hs-identifier hs-var">input</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorOptions
</span><a href="#local-6989586621679681934"><span class="hs-identifier hs-var">opts'</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681932"><span class="hs-identifier hs-var">nonBlocking</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681931"><span class="hs-identifier hs-var">copy</span></a></span><span>
</span><span id="line-856"></span><span>      </span><span class="hs-keyword">where</span><span>
</span><span id="line-857"></span><span>        </span><span id="local-6989586621679681932"><span class="annot"><span class="annottext">nonBlocking :: Bool
</span><a href="#local-6989586621679681932"><span class="hs-identifier hs-var hs-var">nonBlocking</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-858"></span><span>        </span><span id="local-6989586621679681931"><span class="annot"><span class="annottext">copy :: Bool
</span><a href="#local-6989586621679681931"><span class="hs-identifier hs-var hs-var">copy</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-859"></span><span>
</span><span id="line-860"></span><span class="hs-keyword">class</span><span> </span><span id="SGetDataType"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDataType"><span class="hs-identifier hs-var">SGetDataType</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679683019"><span class="annot"><a href="#local-6989586621679683019"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-861"></span><span>  </span><span class="hs-comment">-- | Returns the gradually typed compute data type of the input tensor.</span><span>
</span><span id="line-862"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-863"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' dataType = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) dataType (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-864"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SDataType SFloat</span><span>
</span><span id="line-865"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetDataType t</span><span>
</span><span id="line-866"></span><span>  </span><span class="hs-comment">-- SDataType SFloat</span><span>
</span><span id="line-867"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedDataType Float</span><span>
</span><span id="line-868"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetDataType t</span><span>
</span><span id="line-869"></span><span>  </span><span class="hs-comment">-- SUncheckedDataType Float</span><span>
</span><span id="line-870"></span><span>  </span><span id="sGetDataType"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDataType"><span class="hs-identifier hs-type">sGetDataType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-871"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683016"><span class="annot"><a href="#local-6989586621679683016"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683015"><span class="annot"><a href="#local-6989586621679683015"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683014"><span class="annot"><a href="#local-6989586621679683014"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683013"><span class="annot"><a href="#local-6989586621679683013"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-872"></span><span>    </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-873"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683016"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683015"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683014"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683019"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683013"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-874"></span><span>    </span><span class="hs-comment">-- | data type of the input tensor</span><span>
</span><span id="line-875"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-type">SDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683019"><span class="hs-identifier hs-type">dataType</span></a></span><span>
</span><span id="line-876"></span><span>
</span><span id="line-877"></span><span>  </span><span class="hs-comment">-- | Returns the untyped compute data type of the input tensor.</span><span>
</span><span id="line-878"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-879"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' dataType = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) dataType (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-880"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SDataType SFloat</span><span>
</span><span id="line-881"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDType t</span><span>
</span><span id="line-882"></span><span>  </span><span class="hs-comment">-- Float</span><span>
</span><span id="line-883"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedDataType Float</span><span>
</span><span id="line-884"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDType t</span><span>
</span><span id="line-885"></span><span>  </span><span class="hs-comment">-- Float</span><span>
</span><span id="line-886"></span><span>  </span><span id="getDType"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getDType"><span class="hs-identifier hs-type">getDType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-887"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679683011"><span class="annot"><a href="#local-6989586621679683011"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679683010"><span class="annot"><a href="#local-6989586621679683010"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679683009"><span class="annot"><a href="#local-6989586621679683009"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679683008"><span class="annot"><a href="#local-6989586621679683008"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-888"></span><span>    </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-889"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683011"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683010"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683009"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683019"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679683008"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-890"></span><span>    </span><span class="hs-comment">-- | data type of the input tensor</span><span>
</span><span id="line-891"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span>
</span><span id="line-892"></span><span>  </span><span id="local-6989586621679681928"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getDType"><span class="hs-identifier hs-var hs-var">getDType</span></a></span><span> </span><span id="local-6989586621679681927"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681927"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked DType -&gt; DType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked DType -&gt; DType)
-&gt; (SDataType dataType -&gt; IsChecked DType)
-&gt; SDataType dataType
-&gt; DType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SDataType dataType -&gt; IsChecked DType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SDataType dataType -&gt; DType) -&gt; SDataType dataType -&gt; DType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SDataType dataType
forall (dataType :: DataType DType)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetDataType dataType =&gt;
Tensor gradient layout device dataType shape -&gt; SDataType dataType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDataType"><span class="hs-identifier hs-var">sGetDataType</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681927"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-893"></span><span>
</span><span id="line-894"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681924"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDataType"><span class="hs-identifier hs-type">SGetDataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#UncheckedDataType"><span class="hs-identifier hs-type">UncheckedDataType</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-895"></span><span>  </span><span id="local-6989586621679681922"><span class="annot"><span class="annottext">sGetDataType :: Tensor gradient layout device 'UncheckedDataType shape
-&gt; SDataType 'UncheckedDataType
</span><a href="#local-6989586621679681922"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDataType</span></a></span></span><span> </span><span id="local-6989586621679681921"><span class="annot"><span class="annottext">Tensor gradient layout device 'UncheckedDataType shape
</span><a href="#local-6989586621679681921"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">DType -&gt; SDataType 'UncheckedDataType
</span><a href="Torch.GraduallyTyped.DType.html#SUncheckedDataType"><span class="hs-identifier hs-var">SUncheckedDataType</span></a></span><span> </span><span class="annot"><span class="annottext">(DType -&gt; SDataType 'UncheckedDataType)
-&gt; (IO DType -&gt; DType) -&gt; IO DType -&gt; SDataType 'UncheckedDataType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">IO DType -&gt; DType
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO DType -&gt; SDataType 'UncheckedDataType)
-&gt; IO DType -&gt; SDataType 'UncheckedDataType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO ScalarType)
-&gt; Tensor gradient layout device 'UncheckedDataType shape
-&gt; IO DType
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO ScalarType
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_scalar_type</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device 'UncheckedDataType shape
</span><a href="#local-6989586621679681921"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-896"></span><span>
</span><span id="line-897"></span><span id="local-6989586621679681918"><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681915"><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681918"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDataType"><span class="hs-identifier hs-type">SGetDataType</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681918"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-898"></span><span>  </span><span id="local-6989586621679681914"><span class="annot"><span class="annottext">sGetDataType :: Tensor gradient layout device ('DataType dType) shape
-&gt; SDataType ('DataType dType)
</span><a href="#local-6989586621679681914"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDataType</span></a></span></span><span> </span><span id="local-6989586621679681913"><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) shape
</span><a href="#local-6989586621679681913"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-899"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">IO DType -&gt; DType
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO ScalarType)
-&gt; Tensor gradient layout device ('DataType dType) shape
-&gt; IO DType
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO ScalarType
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_scalar_type</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) shape
</span><a href="#local-6989586621679681913"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">DType -&gt; DType -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">Sing dType -&gt; Demote DType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI dType =&gt; Sing dType
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681918"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SDType dType -&gt; SDataType ('DataType dType)
forall (dType :: DType).
SDType dType -&gt; SDataType ('DataType dType)
</span><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-var">SDataType</span></a></span><span> </span><span class="annot"><span class="annottext">(SDType dType -&gt; SDataType ('DataType dType))
-&gt; SDType dType -&gt; SDataType ('DataType dType)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SingI dType =&gt; Sing dType
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681918"><span class="hs-identifier hs-type">dType</span></a></span><span>
</span><span id="line-900"></span><span>    </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-901"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; SDataType ('DataType dType)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; SDataType ('DataType dType))
-&gt; String -&gt; SDataType ('DataType dType)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-902"></span><span>        </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor should have data type &quot;</span></span><span>
</span><span id="line-903"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">DType -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Sing dType -&gt; Demote DType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(Sing dType -&gt; Demote DType) -&gt; Sing dType -&gt; Demote DType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SingI dType =&gt; Sing dType
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681918"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-904"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot; but hasn't. &quot;</span></span><span>
</span><span id="line-905"></span><span>          </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span></span><span>
</span><span id="line-906"></span><span>
</span><span id="line-907"></span><span class="hs-keyword">data</span><span> </span><span id="DataTypeError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DataTypeError"><span class="hs-identifier hs-var">DataTypeError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="DataTypeError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DataTypeError"><span class="hs-identifier hs-var">DataTypeError</span></a></span></span><span> </span><span class="hs-special">{</span><span id="dtExpected"><span class="annot"><span class="annottext">DataTypeError -&gt; DType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dtExpected"><span class="hs-identifier hs-var hs-var">dtExpected</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span class="hs-special">,</span><span> </span><span id="dtActual"><span class="annot"><span class="annottext">DataTypeError -&gt; DType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dtActual"><span class="hs-identifier hs-var hs-var">dtActual</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span class="hs-special">}</span><span>
</span><span id="line-908"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681903"><span id="local-6989586621679681905"><span id="local-6989586621679681907"><span class="annot"><span class="annottext">Int -&gt; DataTypeError -&gt; ShowS
[DataTypeError] -&gt; ShowS
DataTypeError -&gt; String
(Int -&gt; DataTypeError -&gt; ShowS)
-&gt; (DataTypeError -&gt; String)
-&gt; ([DataTypeError] -&gt; ShowS)
-&gt; Show DataTypeError
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [DataTypeError] -&gt; ShowS
$cshowList :: [DataTypeError] -&gt; ShowS
show :: DataTypeError -&gt; String
$cshow :: DataTypeError -&gt; String
showsPrec :: Int -&gt; DataTypeError -&gt; ShowS
$cshowsPrec :: Int -&gt; DataTypeError -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Typeable</span></span><span class="hs-special">)</span><span>
</span><span id="line-909"></span><span>
</span><span id="line-910"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681897"><span id="local-6989586621679681899"><span class="annot"><span class="hs-identifier hs-type">Exception</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DataTypeError"><span class="hs-identifier hs-type">DataTypeError</span></a></span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-911"></span><span>  </span><span id="local-6989586621679681895"><span class="annot"><span class="annottext">displayException :: DataTypeError -&gt; String
</span><a href="#local-6989586621679681895"><span class="hs-identifier hs-var hs-var hs-var hs-var">displayException</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DataTypeError"><span class="hs-identifier hs-type">DataTypeError</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679681893"><span id="local-6989586621679681894"><span class="annot"><span class="annottext">DType
dtActual :: DType
dtExpected :: DType
dtActual :: DataTypeError -&gt; DType
dtExpected :: DataTypeError -&gt; DType
</span><a href="#local-6989586621679681893"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-912"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor does not have the data type `&quot;</span></span><span>
</span><span id="line-913"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">DType -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681894"><span class="hs-identifier hs-var">dtExpected</span></a></span><span>
</span><span id="line-914"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;` but `&quot;</span></span><span>
</span><span id="line-915"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">DType -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681893"><span class="hs-identifier hs-var">dtActual</span></a></span><span>
</span><span id="line-916"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;`.&quot;</span></span><span>
</span><span id="line-917"></span><span>
</span><span id="line-918"></span><span class="hs-comment">-- | Checks whether or not the input tensor has the data type 'dataType'</span><span>
</span><span id="line-919"></span><span class="hs-comment">-- and returns a statically annotated copy of it wrapped in a 'MonadThrow' 'm'.</span><span>
</span><span id="line-920"></span><span class="hs-comment">--</span><span>
</span><span id="line-921"></span><span class="hs-comment">-- For instance, if 'm' is 'Maybe', then the result will be wrapped in 'Just' if and only if the tensor has indeed the data type 'dataType'.</span><span>
</span><span id="line-922"></span><span class="hs-comment">-- If it does not have it, then the result will be 'Nothing'.</span><span>
</span><span id="line-923"></span><span class="hs-comment">--</span><span>
</span><span id="line-924"></span><span class="hs-comment">-- In the REPL, 'm' will default to 'IO':</span><span>
</span><span id="line-925"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SUncheckedDataType Float) (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil)</span><span>
</span><span id="line-926"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- checkedDataType @('DataType 'Float) t</span><span>
</span><span id="line-927"></span><span class="hs-comment">-- &gt;&gt;&gt; :type t'</span><span>
</span><span id="line-928"></span><span class="hs-comment">-- t'</span><span>
</span><span id="line-929"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-930"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-931"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-932"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-933"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-934"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-935"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-936"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-937"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- checkedDataType @('DataType 'Double) t</span><span>
</span><span id="line-938"></span><span class="hs-comment">-- *** Exception: DataTypeError {dtExpected = Double, dtActual = Float}</span><span>
</span><span id="line-939"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedDataType"><span class="hs-identifier hs-type">sCheckedDataType</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-940"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682971"><span class="annot"><a href="#local-6989586621679682971"><span class="hs-identifier hs-type">dataType'</span></a></span></span><span> </span><span id="local-6989586621679682972"><span class="annot"><a href="#local-6989586621679682972"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682970"><span class="annot"><a href="#local-6989586621679682970"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682969"><span class="annot"><a href="#local-6989586621679682969"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682968"><span class="annot"><a href="#local-6989586621679682968"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682973"><span class="annot"><a href="#local-6989586621679682973"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682967"><span class="annot"><a href="#local-6989586621679682967"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-941"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDataType"><span class="hs-identifier hs-type">SGetDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682973"><span class="hs-identifier hs-type">dataType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682972"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682973"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682971"><span class="hs-identifier hs-type">dataType'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-942"></span><span>  </span><span class="hs-comment">-- | data type</span><span>
</span><span id="line-943"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-type">SDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682971"><span class="hs-identifier hs-type">dataType'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-944"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-945"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682970"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682969"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682968"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682973"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682967"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-946"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-947"></span><span>  </span><span class="annot"><a href="#local-6989586621679682972"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682970"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682969"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682968"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682971"><span class="hs-identifier hs-type">dataType'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682967"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-948"></span><span id="sCheckedDataType"><span class="annot"><span class="annottext">sCheckedDataType :: SDataType dataType'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType' shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedDataType"><span class="hs-identifier hs-var hs-var">sCheckedDataType</span></a></span></span><span> </span><span id="local-6989586621679681891"><span class="annot"><span class="annottext">SDataType dataType'
</span><a href="#local-6989586621679681891"><span class="hs-identifier hs-var">dataType'</span></a></span></span><span> </span><span id="local-6989586621679681890"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681890"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-949"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679681889"><span class="annot"><span class="annottext">actualDataType :: DType
</span><a href="#local-6989586621679681889"><span class="hs-identifier hs-var hs-var">actualDataType</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked DType -&gt; DType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked DType -&gt; DType)
-&gt; (SDataType dataType -&gt; IsChecked DType)
-&gt; SDataType dataType
-&gt; DType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SDataType dataType -&gt; IsChecked DType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SDataType dataType -&gt; DType) -&gt; SDataType dataType -&gt; DType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SDataType dataType
forall (dataType :: DataType DType)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGetDataType dataType =&gt;
Tensor gradient layout device dataType shape -&gt; SDataType dataType
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDataType"><span class="hs-identifier hs-var">sGetDataType</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681890"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-950"></span><span>      </span><span id="local-6989586621679681888"><span class="annot"><span class="annottext">expectedDataType :: DType
</span><a href="#local-6989586621679681888"><span class="hs-identifier hs-var hs-var">expectedDataType</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IsChecked DType -&gt; DType
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked DType -&gt; DType)
-&gt; (SDataType dataType' -&gt; IsChecked DType)
-&gt; SDataType dataType'
-&gt; DType
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SDataType dataType' -&gt; IsChecked DType
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SDataType dataType' -&gt; DType) -&gt; SDataType dataType' -&gt; DType
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SDataType dataType'
</span><a href="#local-6989586621679681891"><span class="hs-identifier hs-var">dataType'</span></a></span><span>
</span><span id="line-951"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="hs-keyword">if</span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681889"><span class="hs-identifier hs-var">actualDataType</span></a></span><span> </span><span class="annot"><span class="annottext">DType -&gt; DType -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681888"><span class="hs-identifier hs-var">expectedDataType</span></a></span><span>
</span><span id="line-952"></span><span>        </span><span class="hs-keyword">then</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType' shape
-&gt; m (Tensor gradient layout device dataType' shape)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType' shape
 -&gt; m (Tensor gradient layout device dataType' shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout device dataType' shape)
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType' shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType' shape
</span><span class="hs-identifier hs-var">coerce</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; m (Tensor gradient layout device dataType' shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType' shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681890"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-953"></span><span>        </span><span class="hs-keyword">else</span><span> </span><span class="annot"><span class="annottext">DataTypeError -&gt; m (Tensor gradient layout device dataType' shape)
forall (m :: * -&gt; *) e a. (MonadThrow m, Exception e) =&gt; e -&gt; m a
</span><span class="hs-identifier hs-var">throwM</span></span><span> </span><span class="annot"><span class="annottext">(DataTypeError
 -&gt; m (Tensor gradient layout device dataType' shape))
-&gt; DataTypeError
-&gt; m (Tensor gradient layout device dataType' shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">DType -&gt; DType -&gt; DataTypeError
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#DataTypeError"><span class="hs-identifier hs-var">DataTypeError</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681888"><span class="hs-identifier hs-var">expectedDataType</span></a></span><span> </span><span class="annot"><span class="annottext">DType
</span><a href="#local-6989586621679681889"><span class="hs-identifier hs-var">actualDataType</span></a></span><span>
</span><span id="line-954"></span><span>
</span><span id="line-955"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedDataType"><span class="hs-identifier hs-type">checkedDataType</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-956"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681886"><span class="annot"><a href="#local-6989586621679681886"><span class="hs-identifier hs-type">dataType'</span></a></span></span><span> </span><span id="local-6989586621679681885"><span class="annot"><a href="#local-6989586621679681885"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681884"><span class="annot"><a href="#local-6989586621679681884"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681883"><span class="annot"><a href="#local-6989586621679681883"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681882"><span class="annot"><a href="#local-6989586621679681882"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681881"><span class="annot"><a href="#local-6989586621679681881"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681880"><span class="annot"><a href="#local-6989586621679681880"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-957"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681886"><span class="hs-identifier hs-type">dataType'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDataType"><span class="hs-identifier hs-type">SGetDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681881"><span class="hs-identifier hs-type">dataType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681885"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681881"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681886"><span class="hs-identifier hs-type">dataType'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-958"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-959"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681884"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681883"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681882"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681881"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681880"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-960"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-961"></span><span>  </span><span class="annot"><a href="#local-6989586621679681885"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681884"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681883"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681882"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681886"><span class="hs-identifier hs-type">dataType'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681880"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-962"></span><span id="checkedDataType"><span class="annot"><span class="annottext">checkedDataType :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType' shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedDataType"><span class="hs-identifier hs-var hs-var">checkedDataType</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SDataType dataType'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType' shape)
forall (dataType' :: DataType DType) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetDataType dataType, MonadThrow m,
 Catch (dataType &lt;+&gt; dataType')) =&gt;
SDataType dataType'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType' shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedDataType"><span class="hs-identifier hs-var">sCheckedDataType</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI dataType' =&gt; Sing dataType'
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681886"><span class="hs-identifier hs-type">dataType'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-963"></span><span>
</span><span id="line-964"></span><span class="hs-comment">-- | Returns the input tensor but with 'UncheckedDataType' as data-type type annotation.</span><span>
</span><span id="line-965"></span><span class="hs-comment">-- Any static information about the tensor's data type is thus erased.</span><span>
</span><span id="line-966"></span><span class="hs-comment">-- However, the tensor's underlying data structure is not changed.</span><span>
</span><span id="line-967"></span><span class="hs-comment">--</span><span>
</span><span id="line-968"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-969"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedDataType t</span><span>
</span><span id="line-970"></span><span class="hs-comment">-- uncheckedDataType t</span><span>
</span><span id="line-971"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-972"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-973"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-974"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-975"></span><span class="hs-comment">--        'UncheckedDataType</span><span>
</span><span id="line-976"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-977"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-978"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-979"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedDataType"><span class="hs-identifier hs-type">uncheckedDataType</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-980"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681878"><span class="annot"><a href="#local-6989586621679681878"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681877"><span class="annot"><a href="#local-6989586621679681877"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681876"><span class="annot"><a href="#local-6989586621679681876"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681875"><span class="annot"><a href="#local-6989586621679681875"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681874"><span class="annot"><a href="#local-6989586621679681874"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-981"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-982"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681878"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681877"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681876"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681875"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681874"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-983"></span><span>  </span><span class="hs-comment">-- | tensor without checked data type</span><span>
</span><span id="line-984"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681878"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681877"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681876"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#UncheckedDataType"><span class="hs-identifier hs-type">UncheckedDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681874"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-985"></span><span id="uncheckedDataType"><span class="annot"><span class="annottext">uncheckedDataType :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device 'UncheckedDataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedDataType"><span class="hs-identifier hs-var hs-var">uncheckedDataType</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device 'UncheckedDataType shape
</span><span class="hs-identifier hs-var">coerce</span></span><span>
</span><span id="line-986"></span><span>
</span><span id="line-987"></span><span class="hs-keyword">class</span><span> </span><span id="SGetShape"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-var">SGetShape</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682957"><span class="annot"><a href="#local-6989586621679682957"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-988"></span><span>  </span><span class="hs-comment">-- | Returns the gradually typed shape of the input tensor.</span><span>
</span><span id="line-989"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-990"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' = sOnes . TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)</span><span>
</span><span id="line-991"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil</span><span>
</span><span id="line-992"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetShape t</span><span>
</span><span id="line-993"></span><span>  </span><span class="hs-comment">-- SShape (SCons (SDim {sDimName = SName, sDimSize = SSize}) (SCons (SDim {sDimName = SName, sDimSize = SSize}) SNil))</span><span>
</span><span id="line-994"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SUncheckedShape $ [Dim &quot;batch&quot; 32, Dim &quot;feature&quot; 8]</span><span>
</span><span id="line-995"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetShape t</span><span>
</span><span id="line-996"></span><span>  </span><span class="hs-comment">-- SUncheckedShape [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 8}]</span><span>
</span><span id="line-997"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SUncheckedName &quot;batch&quot; :&amp;: SUncheckedSize 32 :|: SUncheckedName &quot;feature&quot; :&amp;: SSize @32 :|: SNil</span><span>
</span><span id="line-998"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetShape t</span><span>
</span><span id="line-999"></span><span>  </span><span class="hs-comment">-- SShape (SCons (SDim {sDimName = SUncheckedName &quot;batch&quot;, sDimSize = SUncheckedSize 32}) (SCons (SDim {sDimName = SUncheckedName &quot;feature&quot;, sDimSize = SSize}) SNil))</span><span>
</span><span id="line-1000"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @&quot;batch&quot; :&amp;: SUncheckedSize 32 :|: SName @&quot;feature&quot; :&amp;: SUncheckedSize 8 :|: SNil</span><span>
</span><span id="line-1001"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sGetShape t</span><span>
</span><span id="line-1002"></span><span>  </span><span class="hs-comment">-- SShape (SCons (SDim {sDimName = SName, sDimSize = SUncheckedSize 32}) (SCons (SDim {sDimName = SName, sDimSize = SUncheckedSize 8}) SNil))</span><span>
</span><span id="line-1003"></span><span>  </span><span id="sGetShape"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetShape"><span class="hs-identifier hs-type">sGetShape</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1004"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682946"><span class="annot"><a href="#local-6989586621679682946"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682945"><span class="annot"><a href="#local-6989586621679682945"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682944"><span class="annot"><a href="#local-6989586621679682944"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682943"><span class="annot"><a href="#local-6989586621679682943"><span class="hs-identifier hs-type">dataType</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1005"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682946"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682945"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682944"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682943"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682957"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1006"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-type">SShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682957"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-1007"></span><span>
</span><span id="line-1008"></span><span>  </span><span class="hs-comment">-- | Returns the untyped shape of the input tensor.</span><span>
</span><span id="line-1009"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-1010"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; sOnes' = sOnes . TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)</span><span>
</span><span id="line-1011"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil</span><span>
</span><span id="line-1012"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDims t</span><span>
</span><span id="line-1013"></span><span>  </span><span class="hs-comment">-- [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 8}]</span><span>
</span><span id="line-1014"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SUncheckedShape $ [Dim &quot;batch&quot; 32, Dim &quot;feature&quot; 8]</span><span>
</span><span id="line-1015"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDims t</span><span>
</span><span id="line-1016"></span><span>  </span><span class="hs-comment">-- [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 8}]</span><span>
</span><span id="line-1017"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SUncheckedName &quot;batch&quot; :&amp;: SUncheckedSize 32 :|: SUncheckedName &quot;feature&quot; :&amp;: SSize @32 :|: SNil</span><span>
</span><span id="line-1018"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDims t</span><span>
</span><span id="line-1019"></span><span>  </span><span class="hs-comment">-- [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 32}]</span><span>
</span><span id="line-1020"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @&quot;batch&quot; :&amp;: SUncheckedSize 32 :|: SName @&quot;feature&quot; :&amp;: SUncheckedSize 8 :|: SNil</span><span>
</span><span id="line-1021"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; getDims t</span><span>
</span><span id="line-1022"></span><span>  </span><span class="hs-comment">-- [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 8}]</span><span>
</span><span id="line-1023"></span><span>  </span><span id="getDims"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getDims"><span class="hs-identifier hs-type">getDims</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1024"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682941"><span class="annot"><a href="#local-6989586621679682941"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682940"><span class="annot"><a href="#local-6989586621679682940"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682939"><span class="annot"><a href="#local-6989586621679682939"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682938"><span class="annot"><a href="#local-6989586621679682938"><span class="hs-identifier hs-type">dataType</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1025"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682941"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682940"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682939"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682938"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682957"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1026"></span><span>    </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Integer</span></span><span class="hs-special">]</span><span>
</span><span id="line-1027"></span><span>  </span><span id="local-6989586621679681871"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#getDims"><span class="hs-identifier hs-var hs-var">getDims</span></a></span><span> </span><span id="local-6989586621679681870"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681870"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Dim (IsChecked String) (IsChecked Integer) -&gt; Dim String Integer)
-&gt; [Dim (IsChecked String) (IsChecked Integer)]
-&gt; [Dim String Integer]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-identifier hs-var">fmap</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(IsChecked String -&gt; String)
-&gt; (IsChecked Integer -&gt; Integer)
-&gt; Dim (IsChecked String) (IsChecked Integer)
-&gt; Dim String Integer
forall (p :: * -&gt; * -&gt; *) a b c d.
Bifunctor p =&gt;
(a -&gt; b) -&gt; (c -&gt; d) -&gt; p a c -&gt; p b d
</span><span class="hs-identifier hs-var">bimap</span></span><span> </span><span class="annot"><span class="annottext">IsChecked String -&gt; String
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">IsChecked Integer -&gt; Integer
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">([Dim (IsChecked String) (IsChecked Integer)]
 -&gt; [Dim String Integer])
-&gt; (SShape shape -&gt; [Dim (IsChecked String) (IsChecked Integer)])
-&gt; SShape shape
-&gt; [Dim String Integer]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">IsChecked [Dim (IsChecked String) (IsChecked Integer)]
-&gt; [Dim (IsChecked String) (IsChecked Integer)]
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked [Dim (IsChecked String) (IsChecked Integer)]
 -&gt; [Dim (IsChecked String) (IsChecked Integer)])
-&gt; (SShape shape
    -&gt; IsChecked [Dim (IsChecked String) (IsChecked Integer)])
-&gt; SShape shape
-&gt; [Dim (IsChecked String) (IsChecked Integer)]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">SShape shape
-&gt; IsChecked [Dim (IsChecked String) (IsChecked Integer)]
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span> </span><span class="annot"><span class="annottext">(SShape shape -&gt; [Dim String Integer])
-&gt; SShape shape -&gt; [Dim String Integer]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SShape shape
forall (shape :: Shape [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType).
SGetShape shape =&gt;
Tensor gradient layout device dataType shape -&gt; SShape shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetShape"><span class="hs-identifier hs-var">sGetShape</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681870"><span class="hs-identifier hs-var">tensor</span></a></span></span><span>
</span><span id="line-1028"></span><span>
</span><span id="line-1029"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681867"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-type">SGetShape</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedShape"><span class="hs-identifier hs-type">UncheckedShape</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1030"></span><span>  </span><span id="local-6989586621679681865"><span class="annot"><span class="annottext">sGetShape :: Tensor gradient layout device dataType 'UncheckedShape
-&gt; SShape 'UncheckedShape
</span><a href="#local-6989586621679681865"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetShape</span></a></span></span><span> </span><span id="local-6989586621679681864"><span class="annot"><span class="annottext">Tensor gradient layout device dataType 'UncheckedShape
</span><a href="#local-6989586621679681864"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; SShape 'UncheckedShape
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SUncheckedShape"><span class="hs-identifier hs-var">SUncheckedShape</span></a></span><span> </span><span class="annot"><span class="annottext">([Dim String Integer] -&gt; SShape 'UncheckedShape)
-&gt; (IO [Dim String Integer] -&gt; [Dim String Integer])
-&gt; IO [Dim String Integer]
-&gt; SShape 'UncheckedShape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">IO [Dim String Integer] -&gt; [Dim String Integer]
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO [Dim String Integer] -&gt; SShape 'UncheckedShape)
-&gt; IO [Dim String Integer] -&gt; SShape 'UncheckedShape
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1031"></span><span>    </span><span id="local-6989586621679681862"><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681862"><span class="hs-identifier hs-var">sizes</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr IntArray))
-&gt; Tensor gradient layout device dataType 'UncheckedShape
-&gt; IO [Integer]
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr IntArray)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_sizes</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType 'UncheckedShape
</span><a href="#local-6989586621679681864"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-1032"></span><span>    </span><span class="annot"><span class="annottext">IO Bool
-&gt; IO [Dim String Integer]
-&gt; IO [Dim String Integer]
-&gt; IO [Dim String Integer]
forall (m :: * -&gt; *) a. Monad m =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a
</span><a href="Torch.GraduallyTyped.Prelude.html#ifM"><span class="hs-identifier hs-var">ifM</span></a></span><span>
</span><span id="line-1033"></span><span>      </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient layout device dataType 'UncheckedShape
-&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_has_names</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType 'UncheckedShape
</span><a href="#local-6989586621679681864"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1034"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1035"></span><span>          </span><span id="local-6989586621679681859"><span class="annot"><span class="annottext">[String]
</span><a href="#local-6989586621679681859"><span class="hs-identifier hs-var">names</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr DimnameList))
-&gt; Tensor gradient layout device dataType 'UncheckedShape
-&gt; IO [String]
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr DimnameList)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_names</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType 'UncheckedShape
</span><a href="#local-6989586621679681864"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-1036"></span><span>          </span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; IO [Dim String Integer]
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">([Dim String Integer] -&gt; IO [Dim String Integer])
-&gt; [Dim String Integer] -&gt; IO [Dim String Integer]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; Integer -&gt; Dim String Integer)
-&gt; [String] -&gt; [Integer] -&gt; [Dim String Integer]
forall a b c. (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]
</span><span class="hs-identifier hs-var">zipWith</span></span><span> </span><span class="annot"><span class="annottext">String -&gt; Integer -&gt; Dim String Integer
forall name size. name -&gt; size -&gt; Dim name size
</span><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-var">Dim</span></a></span><span> </span><span class="annot"><span class="annottext">[String]
</span><a href="#local-6989586621679681859"><span class="hs-identifier hs-var">names</span></a></span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681862"><span class="hs-identifier hs-var">sizes</span></a></span><span>
</span><span id="line-1037"></span><span>      </span><span class="hs-special">)</span><span>
</span><span id="line-1038"></span><span>      </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; IO [Dim String Integer]
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">([Dim String Integer] -&gt; IO [Dim String Integer])
-&gt; [Dim String Integer] -&gt; IO [Dim String Integer]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">String -&gt; Integer -&gt; Dim String Integer
forall name size. name -&gt; size -&gt; Dim name size
</span><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-var">Dim</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;*&quot;</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Dim String Integer)
-&gt; [Integer] -&gt; [Dim String Integer]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681862"><span class="hs-identifier hs-var">sizes</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1039"></span><span>
</span><span id="line-1040"></span><span id="local-6989586621679681855"><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681852"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681855"><span class="hs-identifier hs-type">dims</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-type">SGetShape</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681855"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1041"></span><span>  </span><span id="local-6989586621679681851"><span class="annot"><span class="annottext">sGetShape :: Tensor gradient layout device dataType ('Shape dims)
-&gt; SShape ('Shape dims)
</span><a href="#local-6989586621679681851"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetShape</span></a></span></span><span> </span><span id="local-6989586621679681850"><span class="annot"><span class="annottext">Tensor gradient layout device dataType ('Shape dims)
</span><a href="#local-6989586621679681850"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1042"></span><span>    </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679681849"><span class="annot"><span class="annottext">sizes :: [Integer]
</span><a href="#local-6989586621679681849"><span class="hs-identifier hs-var hs-var">sizes</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1043"></span><span>          </span><span class="annot"><span class="annottext">IO [Integer] -&gt; [Integer]
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO [Integer] -&gt; [Integer]) -&gt; IO [Integer] -&gt; [Integer]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1044"></span><span>            </span><span class="annot"><span class="annottext">IO Bool -&gt; IO [Integer] -&gt; IO [Integer] -&gt; IO [Integer]
forall (m :: * -&gt; *) a. Monad m =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a
</span><a href="Torch.GraduallyTyped.Prelude.html#ifM"><span class="hs-identifier hs-var">ifM</span></a></span><span>
</span><span id="line-1045"></span><span>              </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Bool) -&gt; IO Int -&gt; IO Bool
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO Int64)
-&gt; Tensor gradient layout device dataType ('Shape dims) -&gt; IO Int
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO Int64
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_dim</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType ('Shape dims)
</span><a href="#local-6989586621679681850"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1046"></span><span>              </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr IntArray))
-&gt; Tensor gradient layout device dataType ('Shape dims)
-&gt; IO [Integer]
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr IntArray)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_sizes</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType ('Shape dims)
</span><a href="#local-6989586621679681850"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1047"></span><span>              </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Integer] -&gt; IO [Integer]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-1048"></span><span>        </span><span id="local-6989586621679681846"><span class="annot"><span class="annottext">names :: [String]
</span><a href="#local-6989586621679681846"><span class="hs-identifier hs-var hs-var">names</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1049"></span><span>          </span><span class="annot"><span class="annottext">IO [String] -&gt; [String]
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO [String] -&gt; [String]) -&gt; IO [String] -&gt; [String]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1050"></span><span>            </span><span class="annot"><span class="annottext">IO Bool -&gt; IO [String] -&gt; IO [String] -&gt; IO [String]
forall (m :: * -&gt; *) a. Monad m =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a
</span><a href="Torch.GraduallyTyped.Prelude.html#ifM"><span class="hs-identifier hs-var">ifM</span></a></span><span>
</span><span id="line-1051"></span><span>              </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient layout device dataType ('Shape dims) -&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_has_names</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType ('Shape dims)
</span><a href="#local-6989586621679681850"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1052"></span><span>              </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr DimnameList))
-&gt; Tensor gradient layout device dataType ('Shape dims)
-&gt; IO [String]
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr DimnameList)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_names</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType ('Shape dims)
</span><a href="#local-6989586621679681850"><span class="hs-identifier hs-var">tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1053"></span><span>              </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[String] -&gt; IO [String]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">([String] -&gt; IO [String]) -&gt; [String] -&gt; IO [String]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; String) -&gt; [Integer] -&gt; [String]
forall a b. (a -&gt; b) -&gt; [a] -&gt; [b]
</span><span class="hs-identifier hs-var">map</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">String -&gt; Integer -&gt; String
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;*&quot;</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681849"><span class="hs-identifier hs-var">sizes</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1054"></span><span>     </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">SList dims -&gt; SShape ('Shape dims)
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SList dims -&gt; SShape ('Shape dims)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-var">SShape</span></a></span><span> </span><span class="annot"><span class="annottext">(SList dims -&gt; SShape ('Shape dims))
-&gt; SList dims -&gt; SShape ('Shape dims)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[String] -&gt; [Integer] -&gt; SList dims
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SGetDims dims =&gt;
[String] -&gt; [Integer] -&gt; SList dims
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDims"><span class="hs-identifier hs-var">sGetDims</span></a></span><span> </span><span class="annot"><span class="annottext">[String]
</span><a href="#local-6989586621679681846"><span class="hs-identifier hs-var">names</span></a></span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681849"><span class="hs-identifier hs-var">sizes</span></a></span></span><span>
</span><span id="line-1055"></span><span>
</span><span id="line-1056"></span><span class="hs-keyword">class</span><span> </span><span id="SGetDims"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-var">SGetDims</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682899"><span class="annot"><a href="#local-6989586621679682899"><span class="hs-identifier hs-type">dims</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1057"></span><span>  </span><span id="sGetDims"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDims"><span class="hs-identifier hs-type">sGetDims</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Integer</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682899"><span class="hs-identifier hs-type">dims</span></a></span><span>
</span><span id="line-1058"></span><span>
</span><span id="line-1059"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#dimsError"><span class="hs-identifier hs-type">dimsError</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682891"><span class="annot"><a href="#local-6989586621679682891"><span class="hs-identifier hs-type">a</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="#local-6989586621679682891"><span class="hs-identifier hs-type">a</span></a></span><span>
</span><span id="line-1060"></span><span id="dimsError"><span class="annot"><span class="annottext">dimsError :: a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimsError"><span class="hs-identifier hs-var hs-var">dimsError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">String -&gt; a
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; a) -&gt; String -&gt; a
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The numbers of compile- and runtime dimensions are not the same. &quot;</span></span><span> </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-1061"></span><span>
</span><span id="line-1062"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameError"><span class="hs-identifier hs-type">dimNameError</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682870"><span class="annot"><a href="#local-6989586621679682870"><span class="hs-identifier hs-type">a</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682870"><span class="hs-identifier hs-type">a</span></a></span><span>
</span><span id="line-1063"></span><span id="dimNameError"><span class="annot"><span class="annottext">dimNameError :: String -&gt; String -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameError"><span class="hs-identifier hs-var hs-var">dimNameError</span></a></span></span><span> </span><span id="local-6989586621679681840"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681840"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681839"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681839"><span class="hs-identifier hs-var">name'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1064"></span><span>  </span><span class="annot"><span class="annottext">String -&gt; a
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; a) -&gt; String -&gt; a
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1065"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The compile- and runtime dimension names are not the same, '&quot;</span></span><span>
</span><span id="line-1066"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681840"><span class="hs-identifier hs-var">name</span></a></span><span>
</span><span id="line-1067"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;' != '&quot;</span></span><span>
</span><span id="line-1068"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681839"><span class="hs-identifier hs-var">name'</span></a></span><span>
</span><span id="line-1069"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;'. &quot;</span></span><span>
</span><span id="line-1070"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-1071"></span><span>
</span><span id="line-1072"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#dimSizeError"><span class="hs-identifier hs-type">dimSizeError</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682866"><span class="annot"><a href="#local-6989586621679682866"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679682865"><span class="annot"><a href="#local-6989586621679682865"><span class="hs-identifier hs-type">b</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="annot"><a href="#local-6989586621679682866"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682866"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682866"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682865"><span class="hs-identifier hs-type">b</span></a></span><span>
</span><span id="line-1073"></span><span id="dimSizeError"><span class="annot"><span class="annottext">dimSizeError :: a -&gt; a -&gt; b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimSizeError"><span class="hs-identifier hs-var hs-var">dimSizeError</span></a></span></span><span> </span><span id="local-6989586621679681837"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681837"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span id="local-6989586621679681836"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681836"><span class="hs-identifier hs-var">size'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1074"></span><span>  </span><span class="annot"><span class="annottext">String -&gt; b
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; b) -&gt; String -&gt; b
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1075"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The compile- and runtime dimension sizes are not the same, '&quot;</span></span><span>
</span><span id="line-1076"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681837"><span class="hs-identifier hs-var">size</span></a></span><span>
</span><span id="line-1077"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;' != '&quot;</span></span><span>
</span><span id="line-1078"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681836"><span class="hs-identifier hs-var">size'</span></a></span><span>
</span><span id="line-1079"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;'. &quot;</span></span><span>
</span><span id="line-1080"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-1081"></span><span>
</span><span id="line-1082"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameSizeError"><span class="hs-identifier hs-type">dimNameSizeError</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682862"><span class="annot"><a href="#local-6989586621679682862"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679682861"><span class="annot"><a href="#local-6989586621679682861"><span class="hs-identifier hs-type">b</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="annot"><a href="#local-6989586621679682862"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682862"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682862"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682861"><span class="hs-identifier hs-type">b</span></a></span><span>
</span><span id="line-1083"></span><span id="dimNameSizeError"><span class="annot"><span class="annottext">dimNameSizeError :: String -&gt; String -&gt; a -&gt; a -&gt; b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameSizeError"><span class="hs-identifier hs-var hs-var">dimNameSizeError</span></a></span></span><span> </span><span id="local-6989586621679681834"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681834"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681833"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681833"><span class="hs-identifier hs-var">name'</span></a></span></span><span> </span><span id="local-6989586621679681832"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681832"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span id="local-6989586621679681831"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681831"><span class="hs-identifier hs-var">size'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1084"></span><span>  </span><span class="annot"><span class="annottext">String -&gt; b
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; b) -&gt; String -&gt; b
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1085"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The compile- and runtime dimension names and sizes are not the same, '&quot;</span></span><span>
</span><span id="line-1086"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681834"><span class="hs-identifier hs-var">name</span></a></span><span>
</span><span id="line-1087"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;' != '&quot;</span></span><span>
</span><span id="line-1088"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681833"><span class="hs-identifier hs-var">name'</span></a></span><span>
</span><span id="line-1089"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;' and '&quot;</span></span><span>
</span><span id="line-1090"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681832"><span class="hs-identifier hs-var">size</span></a></span><span>
</span><span id="line-1091"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;' != '&quot;</span></span><span>
</span><span id="line-1092"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681831"><span class="hs-identifier hs-var">size'</span></a></span><span>
</span><span id="line-1093"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;'. &quot;</span></span><span>
</span><span id="line-1094"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var">gitHubErrorMsg</span></a></span><span>
</span><span id="line-1095"></span><span>
</span><span id="line-1096"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1097"></span><span>  </span><span id="local-6989586621679681828"><span class="annot"><span class="annottext">sGetDims :: [String] -&gt; [Integer] -&gt; SList '[]
</span><a href="#local-6989586621679681828"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDims</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SList '[]
forall a. SList '[]
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">SNil</span></a></span><span>
</span><span id="line-1098"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDims"><span class="hs-identifier hs-var">sGetDims</span></a></span><span> </span><span class="annot"><span class="annottext">[String]
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SList '[]
forall a. a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimsError"><span class="hs-identifier hs-var">dimsError</span></a></span><span>
</span><span id="line-1099"></span><span>
</span><span id="line-1100"></span><span id="local-6989586621679681825"><span id="local-6989586621679681826"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDim"><span class="hs-identifier hs-type">SGetDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681826"><span class="hs-identifier hs-type">dim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681825"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681826"><span class="hs-identifier hs-type">dim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span class="annot"><a href="#local-6989586621679681825"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1101"></span><span>  </span><span id="local-6989586621679681823"><span class="annot"><span class="annottext">sGetDims :: [String] -&gt; [Integer] -&gt; SList (dim : dims)
</span><a href="#local-6989586621679681823"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDims</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681822"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681822"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681821"><span class="annot"><span class="annottext">[String]
</span><a href="#local-6989586621679681821"><span class="hs-identifier hs-var">names</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681820"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681820"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681819"><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681819"><span class="hs-identifier hs-var">sizes</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">String -&gt; Integer -&gt; SDim dim
forall (dim :: Dim (Name Symbol) (Size Nat)).
SGetDim dim =&gt;
String -&gt; Integer -&gt; SDim dim
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDim"><span class="hs-identifier hs-var">sGetDim</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681822"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681820"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Sing dim -&gt; SList dims -&gt; SList (dim : dims)
forall a1 (a2 :: a1) (as :: [a1]).
Sing a2 -&gt; SList as -&gt; SList (a2 : as)
</span><a href="Torch.GraduallyTyped.Prelude.html#%3A%7C%3A"><span class="hs-operator hs-var">:|:</span></a></span><span> </span><span class="annot"><span class="annottext">[String] -&gt; [Integer] -&gt; SList dims
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SGetDims dims =&gt;
[String] -&gt; [Integer] -&gt; SList dims
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDims"><span class="hs-identifier hs-var">sGetDims</span></a></span><span> </span><span class="annot"><span class="annottext">[String]
</span><a href="#local-6989586621679681821"><span class="hs-identifier hs-var">names</span></a></span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><a href="#local-6989586621679681819"><span class="hs-identifier hs-var">sizes</span></a></span><span>
</span><span id="line-1102"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDims"><span class="hs-identifier hs-var">sGetDims</span></a></span><span> </span><span class="annot"><span class="annottext">[String]
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">[Integer]
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SList (dim : dims)
forall a. a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimsError"><span class="hs-identifier hs-var">dimsError</span></a></span></span></span><span>
</span><span id="line-1103"></span><span>
</span><span id="line-1104"></span><span class="hs-keyword">class</span><span> </span><span id="SGetDim"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDim"><span class="hs-identifier hs-var">SGetDim</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682886"><span class="annot"><a href="#local-6989586621679682886"><span class="hs-identifier hs-type">dim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1105"></span><span>  </span><span id="sGetDim"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetDim"><span class="hs-identifier hs-type">sGetDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Integer</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-type">SDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682886"><span class="hs-identifier hs-type">dim</span></a></span><span>
</span><span id="line-1106"></span><span>
</span><span id="line-1107"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDim"><span class="hs-identifier hs-type">SGetDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedName"><span class="hs-identifier hs-type">UncheckedName</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedSize"><span class="hs-identifier hs-type">UncheckedSize</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1108"></span><span>  </span><span id="local-6989586621679681815"><span class="annot"><span class="annottext">sGetDim :: String -&gt; Integer -&gt; SDim ('Dim 'UncheckedName 'UncheckedSize)
</span><a href="#local-6989586621679681815"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDim</span></a></span></span><span> </span><span id="local-6989586621679681814"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681814"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681813"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681813"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SName 'UncheckedName
-&gt; SSize 'UncheckedSize
-&gt; SDim ('Dim 'UncheckedName 'UncheckedSize)
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-var">SDim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">String -&gt; SName 'UncheckedName
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SUncheckedName"><span class="hs-identifier hs-var">SUncheckedName</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681814"><span class="hs-identifier hs-var">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; SSize 'UncheckedSize
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SUncheckedSize"><span class="hs-identifier hs-var">SUncheckedSize</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681813"><span class="hs-identifier hs-var">size</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1109"></span><span>
</span><span id="line-1110"></span><span id="local-6989586621679681809"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownSymbol</span></span><span> </span><span class="annot"><a href="#local-6989586621679681809"><span class="hs-identifier hs-type">name</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDim"><span class="hs-identifier hs-type">SGetDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681809"><span class="hs-identifier hs-type">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedSize"><span class="hs-identifier hs-type">UncheckedSize</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1111"></span><span>  </span><span id="local-6989586621679681807"><span class="annot"><span class="annottext">sGetDim :: String -&gt; Integer -&gt; SDim ('Dim ('Name name) 'UncheckedSize)
</span><a href="#local-6989586621679681807"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDim</span></a></span></span><span> </span><span id="local-6989586621679681806"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681806"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681805"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681805"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Proxy name -&gt; String
forall (n :: Symbol) (proxy :: Symbol -&gt; *).
KnownSymbol n =&gt;
proxy n -&gt; String
</span><span class="hs-identifier hs-var">symbolVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy name -&gt; String) -&gt; Proxy name -&gt; String
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy name
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681809"><span class="hs-identifier hs-type">name</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-1112"></span><span>    </span><span id="local-6989586621679681804"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681804"><span class="hs-identifier hs-var">name'</span></a></span></span><span>
</span><span id="line-1113"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681806"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String -&gt; String -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681804"><span class="hs-identifier hs-var">name'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">SName ('Name name)
-&gt; SSize 'UncheckedSize -&gt; SDim ('Dim ('Name name) 'UncheckedSize)
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-var">SDim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">KnownSymbol name =&gt; SName ('Name name)
forall (name :: Symbol). KnownSymbol name =&gt; SName ('Name name)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SName"><span class="hs-identifier hs-var">SName</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681809"><span class="hs-identifier hs-type">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; SSize 'UncheckedSize
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SUncheckedSize"><span class="hs-identifier hs-var">SUncheckedSize</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681805"><span class="hs-identifier hs-var">size</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1114"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">String -&gt; String -&gt; SDim ('Dim ('Name name) 'UncheckedSize)
forall a. String -&gt; String -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameError"><span class="hs-identifier hs-var">dimNameError</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681806"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681804"><span class="hs-identifier hs-var">name'</span></a></span></span><span>
</span><span id="line-1115"></span><span>
</span><span id="line-1116"></span><span id="local-6989586621679681802"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679681802"><span class="hs-identifier hs-type">size</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDim"><span class="hs-identifier hs-type">SGetDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedName"><span class="hs-identifier hs-type">UncheckedName</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681802"><span class="hs-identifier hs-type">size</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1117"></span><span>  </span><span id="local-6989586621679681800"><span class="annot"><span class="annottext">sGetDim :: String -&gt; Integer -&gt; SDim ('Dim 'UncheckedName ('Size size))
</span><a href="#local-6989586621679681800"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDim</span></a></span></span><span> </span><span id="local-6989586621679681799"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681799"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681798"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681798"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Proxy size -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; *).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy size -&gt; Integer) -&gt; Proxy size -&gt; Integer
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy size
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681802"><span class="hs-identifier hs-type">size</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-1118"></span><span>    </span><span id="local-6989586621679681797"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681797"><span class="hs-identifier hs-var">size'</span></a></span></span><span>
</span><span id="line-1119"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681798"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Integer -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681797"><span class="hs-identifier hs-var">size'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">SName 'UncheckedName
-&gt; SSize ('Size size) -&gt; SDim ('Dim 'UncheckedName ('Size size))
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-var">SDim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">String -&gt; SName 'UncheckedName
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SUncheckedName"><span class="hs-identifier hs-var">SUncheckedName</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681799"><span class="hs-identifier hs-var">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">KnownNat size =&gt; SSize ('Size size)
forall (size :: Nat). KnownNat size =&gt; SSize ('Size size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SSize"><span class="hs-identifier hs-var">SSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681802"><span class="hs-identifier hs-type">size</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1120"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Integer -&gt; SDim ('Dim 'UncheckedName ('Size size))
forall a b. Show a =&gt; a -&gt; a -&gt; b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimSizeError"><span class="hs-identifier hs-var">dimSizeError</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681798"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681797"><span class="hs-identifier hs-var">size'</span></a></span></span><span>
</span><span id="line-1121"></span><span>
</span><span id="line-1122"></span><span id="local-6989586621679681794"><span id="local-6989586621679681795"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">KnownSymbol</span></span><span> </span><span class="annot"><a href="#local-6989586621679681795"><span class="hs-identifier hs-type">name</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679681794"><span class="hs-identifier hs-type">size</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDim"><span class="hs-identifier hs-type">SGetDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681795"><span class="hs-identifier hs-type">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681794"><span class="hs-identifier hs-type">size</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1123"></span><span>  </span><span id="local-6989586621679681792"><span class="annot"><span class="annottext">sGetDim :: String -&gt; Integer -&gt; SDim ('Dim ('Name name) ('Size size))
</span><a href="#local-6989586621679681792"><span class="hs-identifier hs-var hs-var hs-var hs-var">sGetDim</span></a></span></span><span> </span><span id="local-6989586621679681791"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681791"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681790"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681790"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Proxy name -&gt; String
forall (n :: Symbol) (proxy :: Symbol -&gt; *).
KnownSymbol n =&gt;
proxy n -&gt; String
</span><span class="hs-identifier hs-var">symbolVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy name -&gt; String) -&gt; Proxy name -&gt; String
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy name
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681795"><span class="hs-identifier hs-type">name</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Proxy size -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; *).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy size -&gt; Integer) -&gt; Proxy size -&gt; Integer
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy size
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681794"><span class="hs-identifier hs-type">size</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-1124"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679681789"><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681789"><span class="hs-identifier hs-var">name'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681788"><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681788"><span class="hs-identifier hs-var">size'</span></a></span></span><span class="hs-special">)</span><span>
</span><span id="line-1125"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681791"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String -&gt; String -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681789"><span class="hs-identifier hs-var">name'</span></a></span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681790"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Integer -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681788"><span class="hs-identifier hs-var">size'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">SName ('Name name)
-&gt; SSize ('Size size) -&gt; SDim ('Dim ('Name name) ('Size size))
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-var">SDim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">KnownSymbol name =&gt; SName ('Name name)
forall (name :: Symbol). KnownSymbol name =&gt; SName ('Name name)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SName"><span class="hs-identifier hs-var">SName</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681795"><span class="hs-identifier hs-type">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">KnownNat size =&gt; SSize ('Size size)
forall (size :: Nat). KnownNat size =&gt; SSize ('Size size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SSize"><span class="hs-identifier hs-var">SSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681794"><span class="hs-identifier hs-type">size</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1126"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681791"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String -&gt; String -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">/=</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681789"><span class="hs-identifier hs-var">name'</span></a></span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681790"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Integer -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681788"><span class="hs-identifier hs-var">size'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">String -&gt; String -&gt; SDim ('Dim ('Name name) ('Size size))
forall a. String -&gt; String -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameError"><span class="hs-identifier hs-var">dimNameError</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681791"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681789"><span class="hs-identifier hs-var">name'</span></a></span><span>
</span><span id="line-1127"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681791"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String -&gt; String -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681789"><span class="hs-identifier hs-var">name'</span></a></span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681790"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Integer -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">/=</span></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681788"><span class="hs-identifier hs-var">size'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Integer -&gt; SDim ('Dim ('Name name) ('Size size))
forall a b. Show a =&gt; a -&gt; a -&gt; b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimSizeError"><span class="hs-identifier hs-var">dimSizeError</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681790"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681788"><span class="hs-identifier hs-var">size'</span></a></span><span>
</span><span id="line-1128"></span><span>      </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">String
-&gt; String
-&gt; Integer
-&gt; Integer
-&gt; SDim ('Dim ('Name name) ('Size size))
forall a b. Show a =&gt; String -&gt; String -&gt; a -&gt; a -&gt; b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dimNameSizeError"><span class="hs-identifier hs-var">dimNameSizeError</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681791"><span class="hs-identifier hs-var">name</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><a href="#local-6989586621679681789"><span class="hs-identifier hs-var">name'</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681790"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">Integer
</span><a href="#local-6989586621679681788"><span class="hs-identifier hs-var">size'</span></a></span></span></span><span>
</span><span id="line-1129"></span><span>
</span><span id="line-1130"></span><span class="hs-keyword">data</span><span> </span><span id="ShapeError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#ShapeError"><span class="hs-identifier hs-var">ShapeError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="ShapeError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#ShapeError"><span class="hs-identifier hs-var">ShapeError</span></a></span></span><span> </span><span class="hs-special">{</span><span id="seExpected"><span class="annot"><span class="annottext">ShapeError -&gt; [Dim String Integer]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#seExpected"><span class="hs-identifier hs-var hs-var">seExpected</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Integer</span></span><span class="hs-special">]</span><span class="hs-special">,</span><span> </span><span id="seActual"><span class="annot"><span class="annottext">ShapeError -&gt; [Dim String Integer]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#seActual"><span class="hs-identifier hs-var hs-var">seActual</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Integer</span></span><span class="hs-special">]</span><span class="hs-special">}</span><span>
</span><span id="line-1131"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681778"><span id="local-6989586621679681780"><span id="local-6989586621679681782"><span class="annot"><span class="annottext">Int -&gt; ShapeError -&gt; ShowS
[ShapeError] -&gt; ShowS
ShapeError -&gt; String
(Int -&gt; ShapeError -&gt; ShowS)
-&gt; (ShapeError -&gt; String)
-&gt; ([ShapeError] -&gt; ShowS)
-&gt; Show ShapeError
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [ShapeError] -&gt; ShowS
$cshowList :: [ShapeError] -&gt; ShowS
show :: ShapeError -&gt; String
$cshow :: ShapeError -&gt; String
showsPrec :: Int -&gt; ShapeError -&gt; ShowS
$cshowsPrec :: Int -&gt; ShapeError -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-1132"></span><span>
</span><span id="line-1133"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681772"><span id="local-6989586621679681774"><span class="annot"><span class="hs-identifier hs-type">Exception</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#ShapeError"><span class="hs-identifier hs-type">ShapeError</span></a></span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1134"></span><span>  </span><span id="local-6989586621679681770"><span class="annot"><span class="annottext">displayException :: ShapeError -&gt; String
</span><a href="#local-6989586621679681770"><span class="hs-identifier hs-var hs-var hs-var hs-var">displayException</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#ShapeError"><span class="hs-identifier hs-type">ShapeError</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679681768"><span id="local-6989586621679681769"><span class="annot"><span class="annottext">[Dim String Integer]
seActual :: [Dim String Integer]
seExpected :: [Dim String Integer]
seActual :: ShapeError -&gt; [Dim String Integer]
seExpected :: ShapeError -&gt; [Dim String Integer]
</span><a href="#local-6989586621679681768"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1135"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;The tensor does not have the shape `&quot;</span></span><span>
</span><span id="line-1136"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer]
</span><a href="#local-6989586621679681769"><span class="hs-identifier hs-var">seExpected</span></a></span><span>
</span><span id="line-1137"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;` but `&quot;</span></span><span>
</span><span id="line-1138"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer]
</span><a href="#local-6989586621679681768"><span class="hs-identifier hs-var">seActual</span></a></span><span>
</span><span id="line-1139"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;`.&quot;</span></span><span>
</span><span id="line-1140"></span><span>
</span><span id="line-1141"></span><span class="hs-comment">-- | Checks whether or not the input tensor has the shape 'shape'</span><span>
</span><span id="line-1142"></span><span class="hs-comment">-- and returns a statically annotated copy of it wrapped in a 'MonadThrow' 'm'.</span><span>
</span><span id="line-1143"></span><span class="hs-comment">--</span><span>
</span><span id="line-1144"></span><span class="hs-comment">-- For instance, if 'm' is 'Maybe', then the result will be wrapped in 'Just' if and only if the tensor has indeed the shape 'shape'.</span><span>
</span><span id="line-1145"></span><span class="hs-comment">-- If it is not, then the result will be 'Nothing'.</span><span>
</span><span id="line-1146"></span><span class="hs-comment">--</span><span>
</span><span id="line-1147"></span><span class="hs-comment">-- In the REPL, 'm' will default to 'IO':</span><span>
</span><span id="line-1148"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SUncheckedShape [Dim &quot;batch&quot; 32, Dim &quot;feature&quot; 8])</span><span>
</span><span id="line-1149"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SSize @8 :|: SNil) t</span><span>
</span><span id="line-1150"></span><span class="hs-comment">-- &gt;&gt;&gt; :type t'</span><span>
</span><span id="line-1151"></span><span class="hs-comment">-- t'</span><span>
</span><span id="line-1152"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-1153"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-1154"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-1155"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-1156"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-1157"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-1158"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-1159"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-1160"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SUncheckedName &quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SUncheckedSize 8 :|: SNil) t</span><span>
</span><span id="line-1161"></span><span class="hs-comment">-- &gt;&gt;&gt; :type t'</span><span>
</span><span id="line-1162"></span><span class="hs-comment">-- t'</span><span>
</span><span id="line-1163"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-1164"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-1165"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-1166"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-1167"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-1168"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-1169"></span><span class="hs-comment">--           '[ 'Dim 'UncheckedName ('Size 32),</span><span>
</span><span id="line-1170"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) 'UncheckedSize])</span><span>
</span><span id="line-1171"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SName @&quot;batch&quot; :&amp;: SSize @32 :|: SName @&quot;feature&quot; :&amp;: SUncheckedSize 32 :|: SNil) t</span><span>
</span><span id="line-1172"></span><span class="hs-comment">-- *** Exception: ShapeError {seExpected = [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 32}], seActual = [Dim {dimName = &quot;batch&quot;, dimSize = 32},Dim {dimName = &quot;feature&quot;, dimSize = 8}]}</span><span>
</span><span id="line-1173"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-type">sCheckedShape</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1174"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682842"><span class="annot"><a href="#local-6989586621679682842"><span class="hs-identifier hs-type">shape'</span></a></span></span><span> </span><span id="local-6989586621679682843"><span class="annot"><a href="#local-6989586621679682843"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682841"><span class="annot"><a href="#local-6989586621679682841"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682840"><span class="annot"><a href="#local-6989586621679682840"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682839"><span class="annot"><a href="#local-6989586621679682839"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682838"><span class="annot"><a href="#local-6989586621679682838"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682844"><span class="annot"><a href="#local-6989586621679682844"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1175"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-type">SGetShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682844"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682843"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682844"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682842"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1176"></span><span>  </span><span class="hs-comment">-- | shape</span><span>
</span><span id="line-1177"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-type">SShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682842"><span class="hs-identifier hs-type">shape'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1178"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-1179"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682841"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682840"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682839"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682838"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682844"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1180"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-1181"></span><span>  </span><span class="annot"><a href="#local-6989586621679682843"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682841"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682840"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682839"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682838"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682842"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1182"></span><span id="sCheckedShape"><span class="annot"><span class="annottext">sCheckedShape :: SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-var hs-var">sCheckedShape</span></a></span></span><span> </span><span id="local-6989586621679681766"><span class="annot"><span class="annottext">SShape shape'
</span><a href="#local-6989586621679681766"><span class="hs-identifier hs-var">shape'</span></a></span></span><span> </span><span id="local-6989586621679681765"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681765"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1183"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679681764"><span class="annot"><span class="annottext">f :: Sing a -&gt; [Dim String Integer]
</span><a href="#local-6989586621679681764"><span class="hs-identifier hs-var hs-var">f</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Dim (IsChecked String) (IsChecked Integer) -&gt; Dim String Integer)
-&gt; [Dim (IsChecked String) (IsChecked Integer)]
-&gt; [Dim String Integer]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-identifier hs-var">fmap</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span id="local-6989586621679681763"><span class="annot"><span class="annottext">IsChecked String
</span><a href="#local-6989586621679681763"><span class="hs-identifier hs-var">name</span></a></span></span><span> </span><span id="local-6989586621679681762"><span class="annot"><span class="annottext">IsChecked Integer
</span><a href="#local-6989586621679681762"><span class="hs-identifier hs-var">size</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">String -&gt; Integer -&gt; Dim String Integer
forall name size. name -&gt; size -&gt; Dim name size
</span><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-var">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IsChecked String -&gt; String
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">IsChecked String
</span><a href="#local-6989586621679681763"><span class="hs-identifier hs-var">name</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">IsChecked Integer -&gt; Integer
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">IsChecked Integer
</span><a href="#local-6989586621679681762"><span class="hs-identifier hs-var">size</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">([Dim (IsChecked String) (IsChecked Integer)]
 -&gt; [Dim String Integer])
-&gt; (Sing a -&gt; [Dim (IsChecked String) (IsChecked Integer)])
-&gt; Sing a
-&gt; [Dim String Integer]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">IsChecked [Dim (IsChecked String) (IsChecked Integer)]
-&gt; [Dim (IsChecked String) (IsChecked Integer)]
forall a. IsChecked a -&gt; a
</span><a href="Torch.GraduallyTyped.Prelude.html#forgetIsChecked"><span class="hs-identifier hs-var">forgetIsChecked</span></a></span><span> </span><span class="annot"><span class="annottext">(IsChecked [Dim (IsChecked String) (IsChecked Integer)]
 -&gt; [Dim (IsChecked String) (IsChecked Integer)])
-&gt; (Sing a
    -&gt; IsChecked [Dim (IsChecked String) (IsChecked Integer)])
-&gt; Sing a
-&gt; [Dim (IsChecked String) (IsChecked Integer)]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Sing a -&gt; IsChecked [Dim (IsChecked String) (IsChecked Integer)]
forall k (a :: k). SingKind k =&gt; Sing a -&gt; Demote k
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">fromSing</span></a></span><span>
</span><span id="line-1184"></span><span>      </span><span id="local-6989586621679681761"><span class="annot"><span class="annottext">actualShape :: [Dim String Integer]
</span><a href="#local-6989586621679681761"><span class="hs-identifier hs-var hs-var">actualShape</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Sing shape -&gt; [Dim String Integer]
forall (a :: Shape [Dim (Name Symbol) (Size Nat)]).
Sing a -&gt; [Dim String Integer]
</span><a href="#local-6989586621679681764"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">(Sing shape -&gt; [Dim String Integer])
-&gt; Sing shape -&gt; [Dim String Integer]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; SShape shape
forall (shape :: Shape [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType).
SGetShape shape =&gt;
Tensor gradient layout device dataType shape -&gt; SShape shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sGetShape"><span class="hs-identifier hs-var">sGetShape</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681765"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-1185"></span><span>      </span><span id="local-6989586621679681760"><span class="annot"><span class="annottext">expectedShape :: [Dim String Integer]
</span><a href="#local-6989586621679681760"><span class="hs-identifier hs-var hs-var">expectedShape</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Sing shape' -&gt; [Dim String Integer]
forall (a :: Shape [Dim (Name Symbol) (Size Nat)]).
Sing a -&gt; [Dim String Integer]
</span><a href="#local-6989586621679681764"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">Sing shape'
SShape shape'
</span><a href="#local-6989586621679681766"><span class="hs-identifier hs-var">shape'</span></a></span><span>
</span><span id="line-1186"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="hs-keyword">if</span><span> </span><span class="annot"><span class="annottext">[Dim String Integer]
</span><a href="#local-6989586621679681761"><span class="hs-identifier hs-var">actualShape</span></a></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; [Dim String Integer] -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer]
</span><a href="#local-6989586621679681760"><span class="hs-identifier hs-var">expectedShape</span></a></span><span>
</span><span id="line-1187"></span><span>        </span><span class="hs-keyword">then</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape'
-&gt; m (Tensor gradient layout device dataType shape')
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape'
 -&gt; m (Tensor gradient layout device dataType shape'))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; Tensor gradient layout device dataType shape')
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape'
</span><span class="hs-identifier hs-var">coerce</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; m (Tensor gradient layout device dataType shape'))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681765"><span class="hs-identifier hs-var">tensor</span></a></span><span>
</span><span id="line-1188"></span><span>        </span><span class="hs-keyword">else</span><span> </span><span class="annot"><span class="annottext">ShapeError -&gt; m (Tensor gradient layout device dataType shape')
forall (m :: * -&gt; *) e a. (MonadThrow m, Exception e) =&gt; e -&gt; m a
</span><span class="hs-identifier hs-var">throwM</span></span><span> </span><span class="annot"><span class="annottext">(ShapeError -&gt; m (Tensor gradient layout device dataType shape'))
-&gt; ShapeError -&gt; m (Tensor gradient layout device dataType shape')
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer] -&gt; [Dim String Integer] -&gt; ShapeError
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#ShapeError"><span class="hs-identifier hs-var">ShapeError</span></a></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer]
</span><a href="#local-6989586621679681760"><span class="hs-identifier hs-var">expectedShape</span></a></span><span> </span><span class="annot"><span class="annottext">[Dim String Integer]
</span><a href="#local-6989586621679681761"><span class="hs-identifier hs-var">actualShape</span></a></span><span>
</span><span id="line-1189"></span><span>
</span><span id="line-1190"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedShape"><span class="hs-identifier hs-type">checkedShape</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1191"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681758"><span class="annot"><a href="#local-6989586621679681758"><span class="hs-identifier hs-type">shape'</span></a></span></span><span> </span><span id="local-6989586621679681757"><span class="annot"><a href="#local-6989586621679681757"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679681756"><span class="annot"><a href="#local-6989586621679681756"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681755"><span class="annot"><a href="#local-6989586621679681755"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681754"><span class="annot"><a href="#local-6989586621679681754"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681753"><span class="annot"><a href="#local-6989586621679681753"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681752"><span class="annot"><a href="#local-6989586621679681752"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1192"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681758"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-type">SGetShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681752"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681757"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681752"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681758"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1193"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-1194"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681756"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681755"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681754"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681753"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681752"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1195"></span><span>  </span><span class="hs-comment">-- | annotated output tensor wrapped in 'm'</span><span>
</span><span id="line-1196"></span><span>  </span><span class="annot"><a href="#local-6989586621679681757"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681756"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681755"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681754"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681753"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681758"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1197"></span><span id="checkedShape"><span class="annot"><span class="annottext">checkedShape :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkedShape"><span class="hs-identifier hs-var hs-var">checkedShape</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
forall (shape' :: Shape [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetShape shape, MonadThrow m, Catch (shape &lt;+&gt; shape')) =&gt;
SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-var">sCheckedShape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI shape' =&gt; Sing shape'
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681758"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1198"></span><span>
</span><span id="line-1199"></span><span class="hs-comment">-- | Returns the input tensor but with the selected dimension replaces with 'UncheckedDim' as dimension type annotation.</span><span>
</span><span id="line-1200"></span><span class="hs-comment">-- The static information about the selected tensor dimension is thus erased.</span><span>
</span><span id="line-1201"></span><span class="hs-comment">-- However, the tensor's underlying data structure is not changed.</span><span>
</span><span id="line-1202"></span><span class="hs-comment">--</span><span>
</span><span id="line-1203"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-1204"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedDim @('SelectDim ('ByName &quot;batch&quot;)) t</span><span>
</span><span id="line-1205"></span><span class="hs-comment">-- uncheckedDim @('SelectDim ('ByName &quot;batch&quot;)) t</span><span>
</span><span id="line-1206"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-1207"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-1208"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-1209"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-1210"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-1211"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-1212"></span><span class="hs-comment">--           '[ 'Dim 'UncheckedName 'UncheckedSize,</span><span>
</span><span id="line-1213"></span><span class="hs-comment">--              'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-1214"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-1215"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedDim @('SelectDim ('ByIndex 1)) t</span><span>
</span><span id="line-1216"></span><span class="hs-comment">-- uncheckedDim @('SelectDim ('ByIndex 1)) t</span><span>
</span><span id="line-1217"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-1218"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-1219"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-1220"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-1221"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-1222"></span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-1223"></span><span class="hs-comment">--           '[ 'Dim ('Name &quot;batch&quot;) ('Size 32),</span><span>
</span><span id="line-1224"></span><span class="hs-comment">--              'Dim 'UncheckedName 'UncheckedSize])</span><span>
</span><span id="line-1225"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedDim"><span class="hs-identifier hs-type">uncheckedDim</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1226"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681750"><span class="annot"><a href="#local-6989586621679681750"><span class="hs-identifier hs-type">selectDim</span></a></span></span><span> </span><span id="local-6989586621679681749"><span class="annot"><a href="#local-6989586621679681749"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681748"><span class="annot"><a href="#local-6989586621679681748"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681747"><span class="annot"><a href="#local-6989586621679681747"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681746"><span class="annot"><a href="#local-6989586621679681746"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681745"><span class="annot"><a href="#local-6989586621679681745"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1227"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-1228"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681749"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681748"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681747"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681746"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681745"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1229"></span><span>  </span><span class="hs-comment">-- | tensor with the selected dimensions unchecked</span><span>
</span><span id="line-1230"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681749"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681748"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681747"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681746"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#ReplaceDimF"><span class="hs-identifier hs-type">ReplaceDimF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681750"><span class="hs-identifier hs-type">selectDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681745"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedName"><span class="hs-identifier hs-type">UncheckedName</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedSize"><span class="hs-identifier hs-type">UncheckedSize</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1231"></span><span id="uncheckedDim"><span class="annot"><span class="annottext">uncheckedDim :: Tensor gradient layout device dataType shape
-&gt; Tensor
     gradient
     layout
     device
     dataType
     (ReplaceDimF selectDim shape ('Dim 'UncheckedName 'UncheckedSize))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedDim"><span class="hs-identifier hs-var hs-var">uncheckedDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor
     gradient
     layout
     device
     dataType
     (ReplaceDimF selectDim shape ('Dim 'UncheckedName 'UncheckedSize))
</span><span class="hs-identifier hs-var">coerce</span></span><span>
</span><span id="line-1232"></span><span>
</span><span id="line-1233"></span><span class="hs-comment">-- | Returns the input tensor but with 'UncheckedShape' as shape type annotation.</span><span>
</span><span id="line-1234"></span><span class="hs-comment">-- Any static information about the tensor's shape is thus erased.</span><span>
</span><span id="line-1235"></span><span class="hs-comment">-- However, the tensor's underlying data structure is not changed.</span><span>
</span><span id="line-1236"></span><span class="hs-comment">--</span><span>
</span><span id="line-1237"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name &quot;batch&quot;) ('Size 32), 'Dim ('Name &quot;feature&quot;) ('Size 8)])</span><span>
</span><span id="line-1238"></span><span class="hs-comment">-- &gt;&gt;&gt; :type uncheckedShape t</span><span>
</span><span id="line-1239"></span><span class="hs-comment">-- uncheckedShape t</span><span>
</span><span id="line-1240"></span><span class="hs-comment">--   :: Tensor</span><span>
</span><span id="line-1241"></span><span class="hs-comment">--        ('Gradient 'WithGradient)</span><span>
</span><span id="line-1242"></span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-1243"></span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-1244"></span><span class="hs-comment">--        ('DataType 'Float)</span><span>
</span><span id="line-1245"></span><span class="hs-comment">--        'UncheckedShape</span><span>
</span><span id="line-1246"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedShape"><span class="hs-identifier hs-type">uncheckedShape</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1247"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681743"><span class="annot"><a href="#local-6989586621679681743"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681742"><span class="annot"><a href="#local-6989586621679681742"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681741"><span class="annot"><a href="#local-6989586621679681741"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681740"><span class="annot"><a href="#local-6989586621679681740"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681739"><span class="annot"><a href="#local-6989586621679681739"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1248"></span><span>  </span><span class="hs-comment">-- | input tensor</span><span>
</span><span id="line-1249"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681743"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681742"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681741"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681740"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681739"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1250"></span><span>  </span><span class="hs-comment">-- | tensor without checked shape</span><span>
</span><span id="line-1251"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681743"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681742"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681741"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681740"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedShape"><span class="hs-identifier hs-type">UncheckedShape</span></a></span><span>
</span><span id="line-1252"></span><span id="uncheckedShape"><span class="annot"><span class="annottext">uncheckedShape :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType 'UncheckedShape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#uncheckedShape"><span class="hs-identifier hs-var hs-var">uncheckedShape</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType 'UncheckedShape
</span><span class="hs-identifier hs-var">coerce</span></span><span>
</span><span id="line-1253"></span><span>
</span><span id="line-1254"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-type">gitHubErrorMsg</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">String</span></span><span>
</span><span id="line-1255"></span><span id="gitHubErrorMsg"><span class="annot"><span class="annottext">gitHubErrorMsg :: String
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#gitHubErrorMsg"><span class="hs-identifier hs-var hs-var">gitHubErrorMsg</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;Please open a ticket on GitHub.&quot;</span></span><span>
</span><span id="line-1256"></span><span>
</span><span id="line-1257"></span><span id="local-6989586621679682805"><span id="local-6989586621679682806"><span id="local-6989586621679682807"><span id="local-6989586621679682808"><span id="local-6989586621679682809"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#isContiguous"><span class="hs-identifier hs-type">isContiguous</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1258"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682809"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682808"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682807"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682806"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682805"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1259"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span></span></span></span></span></span><span>
</span><span id="line-1260"></span><span id="isContiguous"><span class="annot"><span class="annottext">isContiguous :: Tensor gradient layout device dataType shape -&gt; Bool
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#isContiguous"><span class="hs-identifier hs-var hs-var">isContiguous</span></a></span></span><span> </span><span id="local-6989586621679681737"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681737"><span class="hs-identifier hs-var">t</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO Bool -&gt; Bool
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO Bool -&gt; Bool) -&gt; IO Bool -&gt; Bool
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO CBool)
-&gt; Tensor gradient layout device dataType shape -&gt; IO Bool
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO CBool
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_is_contiguous</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681737"><span class="hs-identifier hs-var">t</span></a></span><span>
</span><span id="line-1261"></span><span>
</span><span id="line-1262"></span><span id="local-6989586621679681731"><span id="local-6989586621679681732"><span id="local-6989586621679681733"><span id="local-6989586621679681734"><span id="local-6989586621679681735"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#contiguous"><span class="hs-identifier hs-type">contiguous</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1263"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681735"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681734"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681733"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681732"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681731"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1264"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681735"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681734"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681733"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681732"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681731"><span class="hs-identifier hs-type">shape</span></a></span></span></span></span></span></span><span>
</span><span id="line-1265"></span><span id="contiguous"><span class="annot"><span class="annottext">contiguous :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#contiguous"><span class="hs-identifier hs-var hs-var">contiguous</span></a></span></span><span> </span><span id="local-6989586621679681729"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681729"><span class="hs-identifier hs-var">t</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_contiguous</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681729"><span class="hs-identifier hs-var">t</span></a></span><span>
</span><span id="line-1266"></span><span>
</span><span id="line-1267"></span><span id="local-6989586621679682759"><span id="local-6989586621679682760"><span id="local-6989586621679682761"><span id="local-6989586621679682762"><span id="local-6989586621679682763"><span id="local-6989586621679682764"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#withTensor"><span class="hs-identifier hs-type">withTensor</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682764"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682763"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682762"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682761"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682760"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="#local-6989586621679682759"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="#local-6989586621679682759"><span class="hs-identifier hs-type">a</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-1268"></span><span id="withTensor"><span class="annot"><span class="annottext">withTensor :: Tensor gradient layout device dataType shape
-&gt; (Ptr () -&gt; IO a) -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withTensor"><span class="hs-identifier hs-var hs-var">withTensor</span></a></span></span><span> </span><span id="local-6989586621679681726"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681726"><span class="hs-identifier hs-var">t</span></a></span></span><span> </span><span id="local-6989586621679681725"><span class="annot"><span class="annottext">Ptr () -&gt; IO a
</span><a href="#local-6989586621679681725"><span class="hs-identifier hs-var">fn</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1269"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679681724"><span class="annot"><span class="annottext">contiguousTensor :: Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681724"><span class="hs-identifier hs-var hs-var">contiguousTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">if</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape -&gt; Bool
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape -&gt; Bool
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#isContiguous"><span class="hs-identifier hs-var">isContiguous</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681726"><span class="hs-identifier hs-var">t</span></a></span><span> </span><span class="hs-keyword">then</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681726"><span class="hs-identifier hs-var">t</span></a></span><span> </span><span class="hs-keyword">else</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#contiguous"><span class="hs-identifier hs-var">contiguous</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681726"><span class="hs-identifier hs-var">t</span></a></span><span>
</span><span id="line-1270"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; (ForeignPtr Tensor -&gt; IO a) -&gt; IO a
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679681724"><span class="hs-identifier hs-var">contiguousTensor</span></a></span><span> </span><span class="annot"><span class="annottext">((ForeignPtr Tensor -&gt; IO a) -&gt; IO a)
-&gt; (ForeignPtr Tensor -&gt; IO a) -&gt; IO a
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679681723"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679681723"><span class="hs-identifier hs-var">ct</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; (Ptr Tensor -&gt; IO a) -&gt; IO a
forall a b. ForeignPtr a -&gt; (Ptr a -&gt; IO b) -&gt; IO b
</span><span class="hs-identifier hs-var">withForeignPtr</span></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679681723"><span class="hs-identifier hs-var">ct</span></a></span><span> </span><span class="annot"><span class="annottext">((Ptr Tensor -&gt; IO a) -&gt; IO a) -&gt; (Ptr Tensor -&gt; IO a) -&gt; IO a
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Ptr Tensor -&gt; IO (Ptr ())
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">Unmanaged.tensor_data_ptr</span></a></span><span> </span><span class="annot"><span class="annottext">(Ptr Tensor -&gt; IO (Ptr ()))
-&gt; (Ptr () -&gt; IO a) -&gt; Ptr Tensor -&gt; IO a
forall (m :: * -&gt; *) a b c.
Monad m =&gt;
(a -&gt; m b) -&gt; (b -&gt; m c) -&gt; a -&gt; m c
</span><span class="hs-operator hs-var">&gt;=&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; IO a
</span><a href="#local-6989586621679681725"><span class="hs-identifier hs-var">fn</span></a></span><span>
</span><span id="line-1271"></span><span>
</span><span id="line-1272"></span><span class="hs-keyword">class</span><span> </span><span id="TensorLikeRaw"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-var">TensorLikeRaw</span></a></span></span><span> </span><span id="local-6989586621679682796"><span class="annot"><a href="#local-6989586621679682796"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1273"></span><span>  </span><span class="hs-comment">-- | Guesses outer dim.</span><span>
</span><span id="line-1274"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-1275"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; guessDim @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]</span><span>
</span><span id="line-1276"></span><span>  </span><span class="hs-comment">-- Just 3</span><span>
</span><span id="line-1277"></span><span>  </span><span id="guessDim"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDim"><span class="hs-identifier hs-type">guessDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1278"></span><span>    </span><span class="hs-comment">-- | value</span><span>
</span><span id="line-1279"></span><span>    </span><span class="hs-comment">-- 'Nothing' if the data type wrapping 'a' is empty.</span><span>
</span><span id="line-1280"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="annot"><a href="#local-6989586621679682796"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1281"></span><span>    </span><span class="hs-comment">-- | dimension</span><span>
</span><span id="line-1282"></span><span>    </span><span class="hs-comment">-- 'Nothing' if 'a' is a scalar.</span><span>
</span><span id="line-1283"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span>
</span><span id="line-1284"></span><span>
</span><span id="line-1285"></span><span>  </span><span class="hs-comment">-- | Guesses inner dims.</span><span>
</span><span id="line-1286"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-1287"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; guessInnerDims @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]</span><span>
</span><span id="line-1288"></span><span>  </span><span class="hs-comment">-- [2]</span><span>
</span><span id="line-1289"></span><span>  </span><span id="local-6989586621679682794"><span id="guessInnerDims"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#guessInnerDims"><span class="hs-identifier hs-type">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1290"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682794"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1291"></span><span>    </span><span class="hs-comment">-- | value</span><span>
</span><span id="line-1292"></span><span>    </span><span class="hs-comment">-- 'Nothing' if the data type wrapping 'a' is empty.</span><span>
</span><span id="line-1293"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="annot"><a href="#local-6989586621679682796"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1294"></span><span>    </span><span class="hs-comment">-- | inner dimensions</span><span>
</span><span id="line-1295"></span><span>    </span><span class="annot"><a href="#local-6989586621679682794"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span></span><span>
</span><span id="line-1296"></span><span>
</span><span id="line-1297"></span><span>  </span><span class="hs-comment">-- | Reads a value from a tensor.</span><span>
</span><span id="line-1298"></span><span>  </span><span id="tensorPeekElemOff"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-type">tensorPeekElemOff</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1299"></span><span>    </span><span class="hs-comment">-- | pointer to tensor</span><span>
</span><span id="line-1300"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1301"></span><span>    </span><span class="hs-comment">-- | offset</span><span>
</span><span id="line-1302"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1303"></span><span>    </span><span class="hs-comment">-- | tensor dimensions</span><span>
</span><span id="line-1304"></span><span>    </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1305"></span><span>    </span><span class="hs-comment">-- | value</span><span>
</span><span id="line-1306"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="#local-6989586621679682796"><span class="hs-identifier hs-type">a</span></a></span><span>
</span><span id="line-1307"></span><span>
</span><span id="line-1308"></span><span>  </span><span class="hs-comment">-- | Writes a value to a tensor.</span><span>
</span><span id="line-1309"></span><span>  </span><span id="tensorPokeElemOff"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-type">tensorPokeElemOff</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1310"></span><span>    </span><span class="hs-comment">-- | pointer to tensor</span><span>
</span><span id="line-1311"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1312"></span><span>    </span><span class="hs-comment">-- | offset</span><span>
</span><span id="line-1313"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1314"></span><span>    </span><span class="hs-comment">-- | tensor dimensions</span><span>
</span><span id="line-1315"></span><span>    </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1316"></span><span>    </span><span class="hs-comment">-- | value</span><span>
</span><span id="line-1317"></span><span>    </span><span class="annot"><a href="#local-6989586621679682796"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1318"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-1319"></span><span>
</span><span id="line-1320"></span><span class="hs-comment">-- | Guesses dims: concatenates 'guessDim' with 'guessInnerDims'.</span><span>
</span><span id="line-1321"></span><span class="hs-comment">--</span><span>
</span><span id="line-1322"></span><span class="hs-comment">-- &gt;&gt;&gt; guessDims @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]</span><span>
</span><span id="line-1323"></span><span class="hs-comment">-- [3,2]</span><span>
</span><span id="line-1324"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-type">guessDims</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682789"><span class="annot"><a href="#local-6989586621679682789"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679682788"><span class="annot"><a href="#local-6989586621679682788"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682789"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682788"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="annot"><a href="#local-6989586621679682789"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682788"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span>
</span><span id="line-1325"></span><span id="guessDims"><span class="annot"><span class="annottext">guessDims :: Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var hs-var">guessDims</span></a></span></span><span> </span><span id="local-6989586621679681717"><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681717"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681716"><span class="hs-identifier hs-var">outerDim</span></a></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; [Int]
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">([Int] -&gt; [Int]) -&gt; m [Int] -&gt; m [Int]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessInnerDims"><span class="hs-identifier hs-var">guessInnerDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681717"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1326"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1327"></span><span>    </span><span id="local-6989586621679681716"><span class="annot"><span class="annottext">outerDim :: [Int]
</span><a href="#local-6989586621679681716"><span class="hs-identifier hs-var hs-var">outerDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; [Int]
forall a. Maybe a -&gt; [a]
</span><span class="hs-identifier hs-var">maybeToList</span></span><span> </span><span class="annot"><span class="annottext">(Maybe Int -&gt; [Int]) -&gt; Maybe Int -&gt; [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; Maybe Int
forall a. TensorLikeRaw a =&gt; Maybe a -&gt; Maybe Int
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDim"><span class="hs-identifier hs-var">guessDim</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681717"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1328"></span><span>
</span><span id="line-1329"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-type">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682701"><span class="annot"><a href="#local-6989586621679682701"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679682700"><span class="annot"><a href="#local-6989586621679682700"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679682699"><span class="annot"><a href="#local-6989586621679682699"><span class="hs-identifier hs-type">b</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682701"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682700"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="annot"><a href="#local-6989586621679682701"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682700"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682699"><span class="hs-identifier hs-type">b</span></a></span><span>
</span><span id="line-1330"></span><span id="unexpectedDimsError"><span class="annot"><span class="annottext">unexpectedDimsError :: [Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var hs-var">unexpectedDimsError</span></a></span></span><span> </span><span id="local-6989586621679681714"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681714"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681713"><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681713"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1331"></span><span>  </span><span id="local-6989586621679681712"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681712"><span class="hs-identifier hs-var">expected</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681713"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1332"></span><span>  </span><span class="annot"><span class="annottext">String -&gt; m b
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span> </span><span class="annot"><span class="annottext">(String -&gt; m b) -&gt; String -&gt; m b
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;Expected shape to be &quot;</span></span><span> </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681712"><span class="hs-identifier hs-var">expected</span></a></span><span> </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot; got: &quot;</span></span><span> </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681714"><span class="hs-identifier hs-var">dims'</span></a></span><span>
</span><span id="line-1333"></span><span>
</span><span id="line-1334"></span><span class="hs-keyword">class</span><span> </span><span id="TensorLike"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-var">TensorLike</span></a></span></span><span> </span><span id="local-6989586621679682780"><span class="annot"><a href="#local-6989586621679682780"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682779"><span class="annot"><a href="#local-6989586621679682779"><span class="hs-identifier hs-type">dType</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DType"><span class="hs-identifier hs-type">DType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679682778"><span class="annot"><a href="#local-6989586621679682778"><span class="hs-identifier hs-type">dims</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">|</span><span> </span><span class="annot"><a href="#local-6989586621679682780"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682778"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679682780"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682779"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1335"></span><span>  </span><span class="hs-comment">-- | Creates a tensor from a 'TensorLike' value.</span><span>
</span><span id="line-1336"></span><span>  </span><span class="hs-comment">--</span><span>
</span><span id="line-1337"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sToTensor (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) ([(1, 2), (3, 4), (5, 6)] :: [(Int, Int)])</span><span>
</span><span id="line-1338"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; t</span><span>
</span><span id="line-1339"></span><span>  </span><span class="hs-comment">-- Tensor Int64 [3,2] [[ 1,  2],</span><span>
</span><span id="line-1340"></span><span>  </span><span class="hs-comment">--                     [ 3,  4],</span><span>
</span><span id="line-1341"></span><span>  </span><span class="hs-comment">--                     [ 5,  6]]</span><span>
</span><span id="line-1342"></span><span>  </span><span class="hs-comment">-- &gt;&gt;&gt; :type t</span><span>
</span><span id="line-1343"></span><span>  </span><span class="hs-comment">-- t :: Tensor</span><span>
</span><span id="line-1344"></span><span>  </span><span class="hs-comment">--        ('Gradient 'WithoutGradient)</span><span>
</span><span id="line-1345"></span><span>  </span><span class="hs-comment">--        ('Layout 'Dense)</span><span>
</span><span id="line-1346"></span><span>  </span><span class="hs-comment">--        ('Device 'CPU)</span><span>
</span><span id="line-1347"></span><span>  </span><span class="hs-comment">--        ('DataType 'Int64)</span><span>
</span><span id="line-1348"></span><span>  </span><span class="hs-comment">--        ('Shape</span><span>
</span><span id="line-1349"></span><span>  </span><span class="hs-comment">--           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 2)])</span><span>
</span><span id="line-1350"></span><span>  </span><span id="sToTensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensor"><span class="hs-identifier hs-type">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1351"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682775"><span class="annot"><a href="#local-6989586621679682775"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682774"><span class="annot"><a href="#local-6989586621679682774"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682773"><span class="annot"><a href="#local-6989586621679682773"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682776"><span class="annot"><a href="#local-6989586621679682776"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1352"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682776"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1353"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682775"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1354"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682774"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1355"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682773"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1356"></span><span>    </span><span class="annot"><a href="#local-6989586621679682780"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1357"></span><span>    </span><span class="annot"><a href="#local-6989586621679682776"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682775"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682774"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682773"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682779"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682778"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1358"></span><span>
</span><span id="line-1359"></span><span>  </span><span class="hs-comment">-- | Creates a 'TensorLike' from a tensor.</span><span>
</span><span id="line-1360"></span><span>  </span><span id="fromTensor"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensor"><span class="hs-identifier hs-type">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1361"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682736"><span class="annot"><a href="#local-6989586621679682736"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682735"><span class="annot"><a href="#local-6989586621679682735"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682734"><span class="annot"><a href="#local-6989586621679682734"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1362"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682736"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682735"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682734"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682779"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682778"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1363"></span><span>    </span><span class="annot"><a href="#local-6989586621679682780"><span class="hs-identifier hs-type">a</span></a></span><span>
</span><span id="line-1364"></span><span>
</span><span id="line-1365"></span><span class="hs-comment">-- | Non-singleton version of 'sToTensor'.</span><span>
</span><span id="line-1366"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#toTensor"><span class="hs-identifier hs-type">toTensor</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1367"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681708"><span class="annot"><a href="#local-6989586621679681708"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681707"><span class="annot"><a href="#local-6989586621679681707"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681706"><span class="annot"><a href="#local-6989586621679681706"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681705"><span class="annot"><a href="#local-6989586621679681705"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679681704"><span class="annot"><a href="#local-6989586621679681704"><span class="hs-identifier hs-type">dType</span></a></span></span><span> </span><span id="local-6989586621679681703"><span class="annot"><a href="#local-6989586621679681703"><span class="hs-identifier hs-type">dims</span></a></span></span><span> </span><span id="local-6989586621679681702"><span class="annot"><a href="#local-6989586621679681702"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1368"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681705"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681704"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681703"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1369"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681708"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1370"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681707"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1371"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681706"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1372"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679681702"><span class="hs-identifier hs-type">m</span></a></span><span>
</span><span id="line-1373"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1374"></span><span>  </span><span class="annot"><a href="#local-6989586621679681705"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1375"></span><span>  </span><span class="annot"><a href="#local-6989586621679681702"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681708"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681707"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681706"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681704"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681703"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1376"></span><span id="toTensor"><span class="annot"><span class="annottext">toTensor :: a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#toTensor"><span class="hs-identifier hs-var hs-var">toTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
forall a (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(TensorLike a dType dims, MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensor"><span class="hs-identifier hs-var">sToTensor</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI gradient =&gt; Sing gradient
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681708"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI layout =&gt; Sing layout
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681707"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI device =&gt; Sing device
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681706"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1377"></span><span>
</span><span id="line-1378"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-type">sToTensorRaw</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1379"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682725"><span class="annot"><a href="#local-6989586621679682725"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682724"><span class="annot"><a href="#local-6989586621679682724"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682723"><span class="annot"><a href="#local-6989586621679682723"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682729"><span class="annot"><a href="#local-6989586621679682729"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679682728"><span class="annot"><a href="#local-6989586621679682728"><span class="hs-identifier hs-type">dType</span></a></span></span><span> </span><span id="local-6989586621679682727"><span class="annot"><a href="#local-6989586621679682727"><span class="hs-identifier hs-type">dims</span></a></span></span><span> </span><span id="local-6989586621679682726"><span class="annot"><a href="#local-6989586621679682726"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1380"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682729"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682728"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682727"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682729"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682728"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682726"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1381"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682725"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1382"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682724"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1383"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682723"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1384"></span><span>  </span><span class="annot"><a href="#local-6989586621679682729"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1385"></span><span>  </span><span class="annot"><a href="#local-6989586621679682726"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682725"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682724"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682723"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682728"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682727"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1386"></span><span id="sToTensorRaw"><span class="annot"><span class="annottext">sToTensorRaw :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var hs-var">sToTensorRaw</span></a></span></span><span> </span><span id="local-6989586621679681700"><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679681700"><span class="hs-identifier hs-var">gradient'</span></a></span></span><span> </span><span id="local-6989586621679681699"><span class="annot"><span class="annottext">SLayout layout
</span><a href="#local-6989586621679681699"><span class="hs-identifier hs-var">layout</span></a></span></span><span> </span><span id="local-6989586621679681698"><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679681698"><span class="hs-identifier hs-var">device</span></a></span></span><span> </span><span id="local-6989586621679681697"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681697"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1387"></span><span>  </span><span id="local-6989586621679681696"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681696"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe a -&gt; m [Int]) -&gt; Maybe a -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; Maybe a
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681697"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1388"></span><span>
</span><span id="line-1389"></span><span>  </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dims)
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device ('DataType dType) ('Shape dims)
 -&gt; m (Tensor
         gradient layout device ('DataType dType) ('Shape dims)))
-&gt; Tensor gradient layout device ('DataType dType) ('Shape dims)
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1390"></span><span>    </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device ('DataType dType) ('Shape dims))
-&gt; Tensor gradient layout device ('DataType dType) ('Shape dims)
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device ('DataType dType) ('Shape dims))
 -&gt; Tensor gradient layout device ('DataType dType) ('Shape dims))
-&gt; IO
     (Tensor gradient layout device ('DataType dType) ('Shape dims))
-&gt; Tensor gradient layout device ('DataType dType) ('Shape dims)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1391"></span><span>      </span><span id="local-6989586621679681695"><span class="annot"><span class="annottext">Tensor gradient layout Any ('DataType dType) ('Shape dims)
</span><a href="#local-6989586621679681695"><span class="hs-identifier hs-var">t</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; Tensor gradient layout Any ('DataType dType) ('Shape dims)
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-var">UnsafeTensor</span></a></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; Tensor gradient layout Any ('DataType dType) ('Shape dims))
-&gt; IO (ForeignPtr Tensor)
-&gt; IO (Tensor gradient layout Any ('DataType dType) ('Shape dims))
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr IntArray
 -&gt; ForeignPtr TensorOptions -&gt; IO (ForeignPtr Tensor))
-&gt; [Int] -&gt; TensorOptions -&gt; IO (ForeignPtr Tensor)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr IntArray
-&gt; ForeignPtr TensorOptions -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.empty_lo</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681696"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">TensorOptions
</span><a href="#local-6989586621679681693"><span class="hs-identifier hs-var">opts</span></a></span><span>
</span><span id="line-1392"></span><span>      </span><span class="annot"><span class="annottext">Tensor gradient layout Any ('DataType dType) ('Shape dims)
-&gt; (Ptr () -&gt; IO ()) -&gt; IO ()
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) a.
Tensor gradient layout device dataType shape
-&gt; (Ptr () -&gt; IO a) -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withTensor"><span class="hs-identifier hs-var">withTensor</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout Any ('DataType dType) ('Shape dims)
</span><a href="#local-6989586621679681695"><span class="hs-identifier hs-var">t</span></a></span><span> </span><span class="annot"><span class="annottext">((Ptr () -&gt; IO ()) -&gt; IO ()) -&gt; (Ptr () -&gt; IO ()) -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679681692"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681692"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1393"></span><span>        </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681692"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681696"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681697"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1394"></span><span>      </span><span class="annot"><span class="annottext">SDevice device
-&gt; Tensor gradient layout Any ('DataType dType) ('Shape dims)
-&gt; IO
     (Tensor gradient layout device ('DataType dType) ('Shape dims))
forall (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (device' :: Device (DeviceType Nat)) (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
MonadThrow m =&gt;
SDevice device
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetDevice"><span class="hs-identifier hs-var">sSetDevice</span></a></span><span> </span><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679681698"><span class="hs-identifier hs-var">device</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout Any ('DataType dType) ('Shape dims)
</span><a href="#local-6989586621679681695"><span class="hs-identifier hs-var">t</span></a></span><span>
</span><span id="line-1395"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1396"></span><span>    </span><span id="local-6989586621679681693"><span class="annot"><span class="annottext">opts :: TensorOptions
</span><a href="#local-6989586621679681693"><span class="hs-identifier hs-var hs-var">opts</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice ('Device 'CPU)
-&gt; SDataType ('DataType dType)
-&gt; TensorOptions
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType).
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; TensorOptions
</span><a href="Torch.GraduallyTyped.Internal.TensorOptions.html#tensorOptions"><span class="hs-identifier hs-var">tensorOptions</span></a></span><span> </span><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679681700"><span class="hs-identifier hs-var">gradient'</span></a></span><span> </span><span class="annot"><span class="annottext">SLayout layout
</span><a href="#local-6989586621679681699"><span class="hs-identifier hs-var">layout</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SDeviceType 'CPU -&gt; SDevice ('Device 'CPU)
forall (deviceType :: DeviceType Nat).
SDeviceType deviceType -&gt; SDevice ('Device deviceType)
</span><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-var">SDevice</span></a></span><span> </span><span class="annot"><span class="annottext">SDeviceType 'CPU
</span><a href="Torch.GraduallyTyped.Device.html#SCPU"><span class="hs-identifier hs-var">SCPU</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI ('DataType dType) =&gt; Sing ('DataType dType)
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682728"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1397"></span><span>
</span><span id="line-1398"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-type">fromTensorRaw</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1399"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682716"><span class="annot"><a href="#local-6989586621679682716"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682715"><span class="annot"><a href="#local-6989586621679682715"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682714"><span class="annot"><a href="#local-6989586621679682714"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682719"><span class="annot"><a href="#local-6989586621679682719"><span class="hs-identifier hs-type">a</span></a></span></span><span> </span><span id="local-6989586621679682718"><span class="annot"><a href="#local-6989586621679682718"><span class="hs-identifier hs-type">dType</span></a></span></span><span> </span><span id="local-6989586621679682717"><span class="annot"><a href="#local-6989586621679682717"><span class="hs-identifier hs-type">dims</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1400"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682719"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682718"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682717"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682719"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682717"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1401"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682716"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682715"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682714"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#DataType"><span class="hs-identifier hs-type">DataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682718"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682717"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1402"></span><span>  </span><span class="annot"><a href="#local-6989586621679682719"><span class="hs-identifier hs-type">a</span></a></span><span>
</span><span id="line-1403"></span><span id="fromTensorRaw"><span class="annot"><span class="annottext">fromTensorRaw :: Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var hs-var">fromTensorRaw</span></a></span></span><span> </span><span id="local-6989586621679681690"><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dims)
</span><a href="#local-6989586621679681690"><span class="hs-identifier hs-var">t</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1404"></span><span>  </span><span class="annot"><span class="annottext">IO a -&gt; a
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO a -&gt; a) -&gt; IO a -&gt; a
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-1405"></span><span>    </span><span class="annot"><span class="annottext">SDevice ('Device 'CPU)
-&gt; Tensor gradient layout device ('DataType dType) ('Shape dims)
-&gt; IO
     (Tensor
        gradient layout ('Device 'CPU) ('DataType dType) ('Shape dims))
forall (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (device' :: Device (DeviceType Nat)) (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
MonadThrow m =&gt;
SDevice device
-&gt; Tensor gradient layout device' dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetDevice"><span class="hs-identifier hs-var">sSetDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SDeviceType 'CPU -&gt; SDevice ('Device 'CPU)
forall (deviceType :: DeviceType Nat).
SDeviceType deviceType -&gt; SDevice ('Device deviceType)
</span><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-var">SDevice</span></a></span><span> </span><span class="annot"><span class="annottext">SDeviceType 'CPU
</span><a href="Torch.GraduallyTyped.Device.html#SCPU"><span class="hs-identifier hs-var">SCPU</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dims)
</span><a href="#local-6989586621679681690"><span class="hs-identifier hs-var">t</span></a></span><span>
</span><span id="line-1406"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Tensor
     gradient layout ('Device 'CPU) ('DataType dType) ('Shape dims))
-&gt; (Tensor
      gradient layout ('Device 'CPU) ('DataType dType) ('Shape dims)
    -&gt; IO a)
-&gt; IO a
forall (m :: * -&gt; *) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">(Tensor
   gradient layout ('Device 'CPU) ('DataType dType) ('Shape dims)
 -&gt; (Ptr () -&gt; IO a) -&gt; IO a)
-&gt; (Ptr () -&gt; IO a)
-&gt; Tensor
     gradient layout ('Device 'CPU) ('DataType dType) ('Shape dims)
-&gt; IO a
forall a b c. (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
</span><span class="hs-identifier hs-var">flip</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  gradient layout ('Device 'CPU) ('DataType dType) ('Shape dims)
-&gt; (Ptr () -&gt; IO a) -&gt; IO a
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) a.
Tensor gradient layout device dataType shape
-&gt; (Ptr () -&gt; IO a) -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withTensor"><span class="hs-identifier hs-var">withTensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span id="local-6989586621679681688"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681688"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681688"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="annot"><span class="annottext">([Int] -&gt; IO a) -&gt; [Int] -&gt; IO a
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Integer -&gt; Int
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Int)
-&gt; (Dim String Integer -&gt; Integer) -&gt; Dim String Integer -&gt; Int
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Dim String Integer -&gt; Integer
forall name size. Dim name size -&gt; size
</span><a href="Torch.GraduallyTyped.Shape.Type.html#dimSize"><span class="hs-identifier hs-var hs-var">dimSize</span></a></span><span> </span><span class="annot"><span class="annottext">(Dim String Integer -&gt; Int) -&gt; [Dim String Integer] -&gt; [Int]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dims)
-&gt; [Dim String Integer]
forall (shape :: Shape [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType).
SGetShape shape =&gt;
Tensor gradient layout device dataType shape
-&gt; [Dim String Integer]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#getDims"><span class="hs-identifier hs-var">getDims</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dims)
</span><a href="#local-6989586621679681690"><span class="hs-identifier hs-var">t</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1407"></span><span>
</span><span id="line-1408"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Bool"><span class="hs-identifier hs-type">Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1409"></span><span>  </span><span id="local-6989586621679681683"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Bool
-&gt; m (Tensor gradient layout device ('DataType 'Bool) ('Shape '[]))
</span><a href="#local-6989586621679681683"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Bool
-&gt; m (Tensor gradient layout device ('DataType 'Bool) ('Shape '[]))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1410"></span><span>  </span><span id="local-6989586621679681682"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType 'Bool) ('Shape '[])
-&gt; Bool
</span><a href="#local-6989586621679681682"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType 'Bool) ('Shape '[])
-&gt; Bool
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span><span>
</span><span id="line-1411"></span><span>
</span><span id="line-1412"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1413"></span><span>  </span><span id="local-6989586621679681676"><span class="annot"><span class="annottext">guessDim :: Maybe Bool -&gt; Maybe Int
</span><a href="#local-6989586621679681676"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; Maybe Bool -&gt; Maybe Int
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">Maybe Int
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1414"></span><span>
</span><span id="line-1415"></span><span>  </span><span id="local-6989586621679681675"><span class="annot"><span class="annottext">guessInnerDims :: Maybe Bool -&gt; m [Int]
</span><a href="#local-6989586621679681675"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">m [Int] -&gt; Maybe Bool -&gt; m [Int]
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">(m [Int] -&gt; Maybe Bool -&gt; m [Int])
-&gt; m [Int] -&gt; Maybe Bool -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
forall a. Monoid a =&gt; a
</span><span class="hs-identifier hs-var">mempty</span></span><span>
</span><span id="line-1416"></span><span>
</span><span id="line-1417"></span><span>  </span><span id="local-6989586621679681674"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO Bool
</span><a href="#local-6989586621679681674"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681673"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681673"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681672"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681672"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Word8 -&gt; Int -&gt; IO Word8
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; IO a
</span><span class="hs-identifier hs-var">peekElemOff</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-identifier hs-type">Word8</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Word8
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681673"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681672"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">IO Word8 -&gt; (Word8 -&gt; Bool) -&gt; IO Bool
forall (f :: * -&gt; *) a b. Functor f =&gt; f a -&gt; (a -&gt; b) -&gt; f b
</span><span class="hs-operator hs-var">&lt;&amp;&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Word8 -&gt; Word8 -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="annottext">Word8
</span><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-1418"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681671"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681671"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Bool -&gt; IO Bool
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681671"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe Bool
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1419"></span><span>
</span><span id="line-1420"></span><span>  </span><span id="local-6989586621679681670"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; Bool -&gt; IO ()
</span><a href="#local-6989586621679681670"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681669"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681669"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681668"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681668"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span id="local-6989586621679681667"><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681667"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Word8 -&gt; Int -&gt; Word8 -&gt; IO ()
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; a -&gt; IO ()
</span><span class="hs-identifier hs-var">pokeElemOff</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-identifier hs-type">Word8</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Word8
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681669"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681668"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Bool -&gt; Word8
forall a. Num a =&gt; Bool -&gt; a
</span><span class="hs-identifier hs-var">fromBool</span></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681667"><span class="hs-identifier hs-var">x</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1421"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681666"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681666"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681665"><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681665"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Bool -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681666"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe Bool -&gt; IO ()) -&gt; Maybe Bool -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Maybe Bool
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681665"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1422"></span><span>
</span><span id="line-1423"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Int64"><span class="hs-identifier hs-type">Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1424"></span><span>  </span><span id="local-6989586621679681662"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Int
-&gt; m (Tensor
        gradient layout device ('DataType 'Int64) ('Shape '[]))
</span><a href="#local-6989586621679681662"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Int
-&gt; m (Tensor
        gradient layout device ('DataType 'Int64) ('Shape '[]))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1425"></span><span>  </span><span id="local-6989586621679681661"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType 'Int64) ('Shape '[])
-&gt; Int
</span><a href="#local-6989586621679681661"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType 'Int64) ('Shape '[])
-&gt; Int
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span><span>
</span><span id="line-1426"></span><span>
</span><span id="line-1427"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1428"></span><span>  </span><span id="local-6989586621679681656"><span class="annot"><span class="annottext">guessDim :: Maybe Int -&gt; Maybe Int
</span><a href="#local-6989586621679681656"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; Maybe Int -&gt; Maybe Int
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">Maybe Int
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1429"></span><span>
</span><span id="line-1430"></span><span>  </span><span id="local-6989586621679681655"><span class="annot"><span class="annottext">guessInnerDims :: Maybe Int -&gt; m [Int]
</span><a href="#local-6989586621679681655"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">m [Int] -&gt; Maybe Int -&gt; m [Int]
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">(m [Int] -&gt; Maybe Int -&gt; m [Int])
-&gt; m [Int] -&gt; Maybe Int -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1431"></span><span>
</span><span id="line-1432"></span><span>  </span><span id="local-6989586621679681654"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO Int
</span><a href="#local-6989586621679681654"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681653"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681653"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681652"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681652"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Int -&gt; Int -&gt; IO Int
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; IO a
</span><span class="hs-identifier hs-var">peekElemOff</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Int
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681653"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681652"><span class="hs-identifier hs-var">offset</span></a></span><span>
</span><span id="line-1433"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681651"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681651"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Int -&gt; IO Int
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681651"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe Int
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1434"></span><span>
</span><span id="line-1435"></span><span>  </span><span id="local-6989586621679681650"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; Int -&gt; IO ()
</span><a href="#local-6989586621679681650"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681649"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681649"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681648"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681648"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span id="local-6989586621679681647"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681647"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Int -&gt; Int -&gt; Int -&gt; IO ()
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; a -&gt; IO ()
</span><span class="hs-identifier hs-var">pokeElemOff</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Int
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681649"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681648"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681647"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1436"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681646"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681646"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681645"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681645"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Int -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681646"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe Int -&gt; IO ()) -&gt; Maybe Int -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Maybe Int
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681645"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1437"></span><span>
</span><span id="line-1438"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Float"><span class="hs-identifier hs-type">Float</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1439"></span><span>  </span><span id="local-6989586621679681642"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Float
-&gt; m (Tensor
        gradient layout device ('DataType 'Float) ('Shape '[]))
</span><a href="#local-6989586621679681642"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Float
-&gt; m (Tensor
        gradient layout device ('DataType 'Float) ('Shape '[]))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1440"></span><span>  </span><span id="local-6989586621679681641"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType 'Float) ('Shape '[])
-&gt; Float
</span><a href="#local-6989586621679681641"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType 'Float) ('Shape '[])
-&gt; Float
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span><span>
</span><span id="line-1441"></span><span>
</span><span id="line-1442"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1443"></span><span>  </span><span id="local-6989586621679681636"><span class="annot"><span class="annottext">guessDim :: Maybe Float -&gt; Maybe Int
</span><a href="#local-6989586621679681636"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; Maybe Float -&gt; Maybe Int
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">Maybe Int
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1444"></span><span>
</span><span id="line-1445"></span><span>  </span><span id="local-6989586621679681635"><span class="annot"><span class="annottext">guessInnerDims :: Maybe Float -&gt; m [Int]
</span><a href="#local-6989586621679681635"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">m [Int] -&gt; Maybe Float -&gt; m [Int]
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">(m [Int] -&gt; Maybe Float -&gt; m [Int])
-&gt; m [Int] -&gt; Maybe Float -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1446"></span><span>
</span><span id="line-1447"></span><span>  </span><span id="local-6989586621679681634"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO Float
</span><a href="#local-6989586621679681634"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681633"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681633"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681632"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681632"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Float -&gt; Int -&gt; IO Float
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; IO a
</span><span class="hs-identifier hs-var">peekElemOff</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Float
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681633"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681632"><span class="hs-identifier hs-var">offset</span></a></span><span>
</span><span id="line-1448"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681631"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681631"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Float -&gt; IO Float
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681631"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe Float
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1449"></span><span>
</span><span id="line-1450"></span><span>  </span><span id="local-6989586621679681630"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; Float -&gt; IO ()
</span><a href="#local-6989586621679681630"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681629"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681629"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681628"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681628"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span id="local-6989586621679681627"><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679681627"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Float -&gt; Int -&gt; Float -&gt; IO ()
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; a -&gt; IO ()
</span><span class="hs-identifier hs-var">pokeElemOff</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Float
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681629"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681628"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679681627"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1451"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681626"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681626"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681625"><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679681625"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Float -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681626"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe Float -&gt; IO ()) -&gt; Maybe Float -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Maybe Float
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679681625"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1452"></span><span>
</span><span id="line-1453"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#Double"><span class="hs-identifier hs-type">Double</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1454"></span><span>  </span><span id="local-6989586621679681622"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Double
-&gt; m (Tensor
        gradient layout device ('DataType 'Double) ('Shape '[]))
</span><a href="#local-6989586621679681622"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Double
-&gt; m (Tensor
        gradient layout device ('DataType 'Double) ('Shape '[]))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1455"></span><span>  </span><span id="local-6989586621679681621"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType 'Double) ('Shape '[])
-&gt; Double
</span><a href="#local-6989586621679681621"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType 'Double) ('Shape '[])
-&gt; Double
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span><span>
</span><span id="line-1456"></span><span>
</span><span id="line-1457"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1458"></span><span>  </span><span id="local-6989586621679681616"><span class="annot"><span class="annottext">guessDim :: Maybe Double -&gt; Maybe Int
</span><a href="#local-6989586621679681616"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; Maybe Double -&gt; Maybe Int
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">Maybe Int
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1459"></span><span>
</span><span id="line-1460"></span><span>  </span><span id="local-6989586621679681615"><span class="annot"><span class="annottext">guessInnerDims :: Maybe Double -&gt; m [Int]
</span><a href="#local-6989586621679681615"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">m [Int] -&gt; Maybe Double -&gt; m [Int]
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">(m [Int] -&gt; Maybe Double -&gt; m [Int])
-&gt; m [Int] -&gt; Maybe Double -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1461"></span><span>
</span><span id="line-1462"></span><span>  </span><span id="local-6989586621679681614"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO Double
</span><a href="#local-6989586621679681614"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681613"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681613"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681612"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681612"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Double -&gt; Int -&gt; IO Double
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; IO a
</span><span class="hs-identifier hs-var">peekElemOff</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Double
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681613"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681612"><span class="hs-identifier hs-var">offset</span></a></span><span>
</span><span id="line-1463"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681611"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681611"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Double -&gt; IO Double
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681611"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe Double
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1464"></span><span>
</span><span id="line-1465"></span><span>  </span><span id="local-6989586621679681610"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; Double -&gt; IO ()
</span><a href="#local-6989586621679681610"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681609"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681609"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681608"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681608"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">[</span><span class="hs-special">]</span><span> </span><span id="local-6989586621679681607"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679681607"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Double -&gt; Int -&gt; Double -&gt; IO ()
forall a. Storable a =&gt; Ptr a -&gt; Int -&gt; a -&gt; IO ()
</span><span class="hs-identifier hs-var">pokeElemOff</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr Double
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681609"><span class="hs-identifier hs-var">ptr</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681608"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679681607"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1466"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681606"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681606"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681605"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679681605"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe Double -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681606"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe Double -&gt; IO ()) -&gt; Maybe Double -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Maybe Double
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679681605"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1467"></span><span>
</span><span id="line-1468"></span><span class="hs-keyword">data</span><span> </span><span id="DimMismatchError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DimMismatchError"><span class="hs-identifier hs-var">DimMismatchError</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="DimMismatchError"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DimMismatchError"><span class="hs-identifier hs-var">DimMismatchError</span></a></span></span><span> </span><span class="hs-special">{</span><span id="dmeFirst"><span class="annot"><span class="annottext">DimMismatchError -&gt; [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dmeFirst"><span class="hs-identifier hs-var hs-var">dmeFirst</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span class="hs-special">,</span><span> </span><span id="dmeOther"><span class="annot"><span class="annottext">DimMismatchError -&gt; [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#dmeOther"><span class="hs-identifier hs-var hs-var">dmeOther</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span class="hs-special">}</span><span>
</span><span id="line-1469"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681596"><span id="local-6989586621679681598"><span id="local-6989586621679681600"><span class="annot"><span class="annottext">Int -&gt; DimMismatchError -&gt; ShowS
[DimMismatchError] -&gt; ShowS
DimMismatchError -&gt; String
(Int -&gt; DimMismatchError -&gt; ShowS)
-&gt; (DimMismatchError -&gt; String)
-&gt; ([DimMismatchError] -&gt; ShowS)
-&gt; Show DimMismatchError
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [DimMismatchError] -&gt; ShowS
$cshowList :: [DimMismatchError] -&gt; ShowS
show :: DimMismatchError -&gt; String
$cshow :: DimMismatchError -&gt; String
showsPrec :: Int -&gt; DimMismatchError -&gt; ShowS
$cshowsPrec :: Int -&gt; DimMismatchError -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681592"><span id="local-6989586621679681594"><span class="annot"><span class="annottext">DimMismatchError -&gt; DimMismatchError -&gt; Bool
(DimMismatchError -&gt; DimMismatchError -&gt; Bool)
-&gt; (DimMismatchError -&gt; DimMismatchError -&gt; Bool)
-&gt; Eq DimMismatchError
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
/= :: DimMismatchError -&gt; DimMismatchError -&gt; Bool
$c/= :: DimMismatchError -&gt; DimMismatchError -&gt; Bool
== :: DimMismatchError -&gt; DimMismatchError -&gt; Bool
$c== :: DimMismatchError -&gt; DimMismatchError -&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-1470"></span><span>
</span><span id="line-1471"></span><span class="hs-keyword">instance</span><span> </span><span id="local-6989586621679681585"><span id="local-6989586621679681587"><span class="annot"><span class="hs-identifier hs-type">Exception</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DimMismatchError"><span class="hs-identifier hs-type">DimMismatchError</span></a></span></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1472"></span><span>  </span><span id="local-6989586621679681583"><span class="annot"><span class="annottext">displayException :: DimMismatchError -&gt; String
</span><a href="#local-6989586621679681583"><span class="hs-identifier hs-var hs-var hs-var hs-var">displayException</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#DimMismatchError"><span class="hs-identifier hs-type">DimMismatchError</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679681581"><span id="local-6989586621679681582"><span class="annot"><span class="annottext">[Int]
dmeOther :: [Int]
dmeFirst :: [Int]
dmeOther :: DimMismatchError -&gt; [Int]
dmeFirst :: DimMismatchError -&gt; [Int]
</span><a href="#local-6989586621679681581"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1473"></span><span>    </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;When converting to a tensor, all elements on the same dimension must have the same shape, &quot;</span></span><span>
</span><span id="line-1474"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;but the first element has shape &quot;</span></span><span>
</span><span id="line-1475"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681582"><span class="hs-identifier hs-var">dmeFirst</span></a></span><span>
</span><span id="line-1476"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot; while another element has shape &quot;</span></span><span>
</span><span id="line-1477"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681581"><span class="hs-identifier hs-var">dmeOther</span></a></span><span>
</span><span id="line-1478"></span><span>      </span><span class="annot"><span class="annottext">String -&gt; ShowS
forall a. Semigroup a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">&lt;&gt;</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;.&quot;</span></span><span>
</span><span id="line-1479"></span><span>
</span><span id="line-1480"></span><span id="local-6989586621679682647"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#checkDims"><span class="hs-identifier hs-type">checkDims</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679682647"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682647"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span></span><span>
</span><span id="line-1481"></span><span id="checkDims"><span class="annot"><span class="annottext">checkDims :: [Int] -&gt; [Int] -&gt; m ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkDims"><span class="hs-identifier hs-var hs-var">checkDims</span></a></span></span><span> </span><span id="local-6989586621679681579"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681579"><span class="hs-identifier hs-var">firstDims</span></a></span></span><span> </span><span id="local-6989586621679681578"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681578"><span class="hs-identifier hs-var">otherDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool -&gt; m () -&gt; m ()
forall (f :: * -&gt; *). Applicative f =&gt; Bool -&gt; f () -&gt; f ()
</span><span class="hs-identifier hs-var">when</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681579"><span class="hs-identifier hs-var">firstDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">/=</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681578"><span class="hs-identifier hs-var">otherDims</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(m () -&gt; m ()) -&gt; m () -&gt; m ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">DimMismatchError -&gt; m ()
forall (m :: * -&gt; *) e a. (MonadThrow m, Exception e) =&gt; e -&gt; m a
</span><span class="hs-identifier hs-var">throwM</span></span><span> </span><span class="annot"><span class="annottext">(DimMismatchError -&gt; m ()) -&gt; DimMismatchError -&gt; m ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; DimMismatchError
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#DimMismatchError"><span class="hs-identifier hs-var">DimMismatchError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681579"><span class="hs-identifier hs-var">firstDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681578"><span class="hs-identifier hs-var">otherDims</span></a></span><span>
</span><span id="line-1482"></span><span>
</span><span id="line-1483"></span><span id="local-6989586621679681572"><span id="local-6989586621679681573"><span id="local-6989586621679681574"><span id="local-6989586621679681575"><span id="local-6989586621679681576"><span id="local-6989586621679681577"><span class="hs-keyword">instance</span><span>
</span><span id="line-1484"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681577"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681576"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681575"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1485"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681574"><span class="hs-identifier hs-type">b</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681576"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681573"><span class="hs-identifier hs-type">dims'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1486"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681577"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1487"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681574"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1488"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681576"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1489"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681572"><span class="hs-identifier hs-type">dimsOut</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1490"></span><span>    </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681572"><span class="hs-identifier hs-type">dimsOut</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#InsertDimF"><span class="hs-identifier hs-type">InsertDimF</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier hs-type">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#ByIndex"><span class="hs-identifier hs-type">ByIndex</span></a></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681575"><span class="hs-identifier hs-type">dims</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681573"><span class="hs-identifier hs-type">dims'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1491"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1492"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681577"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681574"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681576"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681572"><span class="hs-identifier hs-type">dimsOut</span></a></span><span>
</span><span id="line-1493"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1494"></span><span>  </span><span id="local-6989586621679681569"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; (a, b)
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
</span><a href="#local-6989586621679681569"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; (a, b)
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1495"></span><span>  </span><span id="local-6989586621679681568"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; (a, b)
</span><a href="#local-6989586621679681568"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; (a, b)
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-1496"></span><span>
</span><span id="line-1497"></span><span id="local-6989586621679681566"><span id="local-6989586621679681567"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681567"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681566"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681567"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681566"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1498"></span><span>  </span><span id="local-6989586621679681561"><span class="annot"><span class="annottext">guessDim :: Maybe (a, b) -&gt; Maybe Int
</span><a href="#local-6989586621679681561"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; Maybe (a, b) -&gt; Maybe Int
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">(Maybe Int -&gt; Maybe (a, b) -&gt; Maybe Int)
-&gt; Maybe Int -&gt; Maybe (a, b) -&gt; Maybe Int
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Maybe Int
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">2</span></span><span>
</span><span id="line-1499"></span><span>
</span><span id="line-1500"></span><span>  </span><span id="local-6989586621679681560"><span class="annot"><span class="annottext">guessInnerDims :: Maybe (a, b) -&gt; m [Int]
</span><a href="#local-6989586621679681560"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Maybe (a, b) -&gt; (Maybe a, Maybe b)
forall (f :: * -&gt; *) a b. Functor f =&gt; f (a, b) -&gt; (f a, f b)
</span><span class="hs-identifier hs-var">unzip</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681559"><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681559"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681558"><span class="annot"><span class="annottext">Maybe b
</span><a href="#local-6989586621679681558"><span class="hs-identifier hs-var">y</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1501"></span><span>    </span><span id="local-6989586621679681557"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681557"><span class="hs-identifier hs-var">xDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681559"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1502"></span><span>    </span><span id="local-6989586621679681556"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681556"><span class="hs-identifier hs-var">yDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe b -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe b
</span><a href="#local-6989586621679681558"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-1503"></span><span>    </span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; m ()
forall (m :: * -&gt; *). MonadThrow m =&gt; [Int] -&gt; [Int] -&gt; m ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkDims"><span class="hs-identifier hs-var">checkDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681557"><span class="hs-identifier hs-var">xDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681556"><span class="hs-identifier hs-var">yDims</span></a></span><span>
</span><span id="line-1504"></span><span>    </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681557"><span class="hs-identifier hs-var">xDims</span></a></span><span>
</span><span id="line-1505"></span><span>
</span><span id="line-1506"></span><span>  </span><span id="local-6989586621679681555"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO (a, b)
</span><a href="#local-6989586621679681555"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681554"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681554"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681553"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681553"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681552"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681552"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1507"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">,</span><span class="hs-special">)</span><span>
</span><span id="line-1508"></span><span>      </span><span class="annot"><span class="annottext">(a -&gt; b -&gt; (a, b)) -&gt; IO a -&gt; IO (b -&gt; (a, b))
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681554"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681553"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681552"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1509"></span><span>      </span><span class="annot"><span class="annottext">IO (b -&gt; (a, b)) -&gt; IO b -&gt; IO (a, b)
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO b
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681554"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681553"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681551"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681552"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1510"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1511"></span><span>      </span><span id="local-6989586621679681551"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681551"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681552"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1512"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681549"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681549"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe (a, b) -&gt; IO (a, b)
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681567"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681566"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681549"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (a, b)
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1513"></span><span>
</span><span id="line-1514"></span><span>  </span><span id="local-6989586621679681548"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; (a, b) -&gt; IO ()
</span><a href="#local-6989586621679681548"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681547"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681547"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681546"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681546"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681545"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681545"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681544"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681544"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681543"><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679681543"><span class="hs-identifier hs-var">y</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1515"></span><span>    </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681547"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681546"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681545"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681544"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1516"></span><span>    </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; b -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681547"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681546"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681542"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681545"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679681543"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-1517"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1518"></span><span>      </span><span id="local-6989586621679681542"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681542"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681545"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1519"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681541"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681541"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681540"><span class="annot"><span class="annottext">(a, b)
</span><a href="#local-6989586621679681540"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe (a, b) -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681541"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe (a, b) -&gt; IO ()) -&gt; Maybe (a, b) -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(a, b) -&gt; Maybe (a, b)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(a, b)
</span><a href="#local-6989586621679681540"><span class="hs-identifier hs-var">x</span></a></span></span></span><span>
</span><span id="line-1520"></span><span>
</span><span id="line-1521"></span><span id="local-6989586621679681533"><span id="local-6989586621679681534"><span id="local-6989586621679681535"><span id="local-6989586621679681536"><span id="local-6989586621679681537"><span id="local-6989586621679681538"><span id="local-6989586621679681539"><span class="hs-keyword">instance</span><span>
</span><span id="line-1522"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681539"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681538"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681537"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1523"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681536"><span class="hs-identifier hs-type">b</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681538"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681535"><span class="hs-identifier hs-type">dims'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1524"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681534"><span class="hs-identifier hs-type">c</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681538"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681535"><span class="hs-identifier hs-type">dims'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1525"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681539"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1526"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681536"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1527"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681534"><span class="hs-identifier hs-type">c</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1528"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681538"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1529"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681533"><span class="hs-identifier hs-type">dimsOut</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1530"></span><span>    </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681533"><span class="hs-identifier hs-type">dimsOut</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#InsertDimF"><span class="hs-identifier hs-type">InsertDimF</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier hs-type">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#ByIndex"><span class="hs-identifier hs-type">ByIndex</span></a></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681537"><span class="hs-identifier hs-type">dims</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681535"><span class="hs-identifier hs-type">dims'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1531"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1532"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681539"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681536"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681534"><span class="hs-identifier hs-type">c</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681538"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681533"><span class="hs-identifier hs-type">dimsOut</span></a></span><span>
</span><span id="line-1533"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1534"></span><span>  </span><span id="local-6989586621679681530"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; (a, b, c)
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
</span><a href="#local-6989586621679681530"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; (a, b, c)
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1535"></span><span>  </span><span id="local-6989586621679681529"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; (a, b, c)
</span><a href="#local-6989586621679681529"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; (a, b, c)
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span></span></span></span></span></span></span></span><span>
</span><span id="line-1536"></span><span>
</span><span id="line-1537"></span><span id="local-6989586621679682617"><span id="local-6989586621679682618"><span id="local-6989586621679682619"><span id="local-6989586621679682620"><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#unzip3"><span class="hs-identifier hs-type">unzip3</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Functor</span></span><span> </span><span class="annot"><a href="#local-6989586621679682620"><span class="hs-identifier hs-type">f</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679682620"><span class="hs-identifier hs-type">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682619"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679682618"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679682617"><span class="hs-identifier hs-type">c</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679682620"><span class="hs-identifier hs-type">f</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682619"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679682620"><span class="hs-identifier hs-type">f</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682618"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679682620"><span class="hs-identifier hs-type">f</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682617"><span class="hs-identifier hs-type">c</span></a></span><span class="hs-special">)</span></span></span></span></span><span>
</span><span id="line-1538"></span><span id="unzip3"><span class="annot"><span class="annottext">unzip3 :: f (a, b, c) -&gt; (f a, f b, f c)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unzip3"><span class="hs-identifier hs-var hs-var">unzip3</span></a></span></span><span> </span><span id="local-6989586621679681527"><span class="annot"><span class="annottext">f (a, b, c)
</span><a href="#local-6989586621679681527"><span class="hs-identifier hs-var">xyz</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1539"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span class="hs-special">(</span><span id="local-6989586621679681526"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681526"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">b
</span><span class="hs-identifier">_</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">c
</span><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681526"><span class="hs-identifier hs-var">x</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((a, b, c) -&gt; a) -&gt; f (a, b, c) -&gt; f a
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">f (a, b, c)
</span><a href="#local-6989586621679681527"><span class="hs-identifier hs-var">xyz</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1540"></span><span>    </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span class="hs-special">(</span><span class="annot"><span class="annottext">a
</span><span class="hs-identifier">_</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681525"><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679681525"><span class="hs-identifier hs-var">y</span></a></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">c
</span><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679681525"><span class="hs-identifier hs-var">y</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((a, b, c) -&gt; b) -&gt; f (a, b, c) -&gt; f b
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">f (a, b, c)
</span><a href="#local-6989586621679681527"><span class="hs-identifier hs-var">xyz</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1541"></span><span>    </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span class="hs-special">(</span><span class="annot"><span class="annottext">a
</span><span class="hs-identifier">_</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">b
</span><span class="hs-identifier">_</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681524"><span class="annot"><span class="annottext">c
</span><a href="#local-6989586621679681524"><span class="hs-identifier hs-var">z</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">c
</span><a href="#local-6989586621679681524"><span class="hs-identifier hs-var">z</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((a, b, c) -&gt; c) -&gt; f (a, b, c) -&gt; f c
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">f (a, b, c)
</span><a href="#local-6989586621679681527"><span class="hs-identifier hs-var">xyz</span></a></span><span>
</span><span id="line-1542"></span><span>  </span><span class="hs-special">)</span><span>
</span><span id="line-1543"></span><span>
</span><span id="line-1544"></span><span id="local-6989586621679681521"><span id="local-6989586621679681522"><span id="local-6989586621679681523"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681523"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681522"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681521"><span class="hs-identifier hs-type">c</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681523"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681522"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681521"><span class="hs-identifier hs-type">c</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1545"></span><span>  </span><span id="local-6989586621679681516"><span class="annot"><span class="annottext">guessDim :: Maybe (a, b, c) -&gt; Maybe Int
</span><a href="#local-6989586621679681516"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe Int -&gt; Maybe (a, b, c) -&gt; Maybe Int
forall a b. a -&gt; b -&gt; a
</span><span class="hs-identifier hs-var">const</span></span><span> </span><span class="annot"><span class="annottext">(Maybe Int -&gt; Maybe (a, b, c) -&gt; Maybe Int)
-&gt; Maybe Int -&gt; Maybe (a, b, c) -&gt; Maybe Int
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Maybe Int
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">2</span></span><span>
</span><span id="line-1546"></span><span>
</span><span id="line-1547"></span><span>  </span><span id="local-6989586621679681515"><span class="annot"><span class="annottext">guessInnerDims :: Maybe (a, b, c) -&gt; m [Int]
</span><a href="#local-6989586621679681515"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Maybe (a, b, c) -&gt; (Maybe a, Maybe b, Maybe c)
forall (f :: * -&gt; *) a b c.
Functor f =&gt;
f (a, b, c) -&gt; (f a, f b, f c)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unzip3"><span class="hs-identifier hs-var">unzip3</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681514"><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681514"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681513"><span class="annot"><span class="annottext">Maybe b
</span><a href="#local-6989586621679681513"><span class="hs-identifier hs-var">y</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681512"><span class="annot"><span class="annottext">Maybe c
</span><a href="#local-6989586621679681512"><span class="hs-identifier hs-var">z</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1548"></span><span>    </span><span id="local-6989586621679681511"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681511"><span class="hs-identifier hs-var">xDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
</span><a href="#local-6989586621679681514"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1549"></span><span>    </span><span id="local-6989586621679681510"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681510"><span class="hs-identifier hs-var">yDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe b -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe b
</span><a href="#local-6989586621679681513"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-1550"></span><span>    </span><span id="local-6989586621679681509"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681509"><span class="hs-identifier hs-var">zDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe c -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe c
</span><a href="#local-6989586621679681512"><span class="hs-identifier hs-var">z</span></a></span><span>
</span><span id="line-1551"></span><span>    </span><span class="annot"><span class="annottext">([Int] -&gt; m ()) -&gt; [[Int]] -&gt; m ()
forall (t :: * -&gt; *) (f :: * -&gt; *) a b.
(Foldable t, Applicative f) =&gt;
(a -&gt; f b) -&gt; t a -&gt; f ()
</span><span class="hs-identifier hs-var">traverse_</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; m ()
forall (m :: * -&gt; *). MonadThrow m =&gt; [Int] -&gt; [Int] -&gt; m ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkDims"><span class="hs-identifier hs-var">checkDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681511"><span class="hs-identifier hs-var">xDims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681510"><span class="hs-identifier hs-var">yDims</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681509"><span class="hs-identifier hs-var">zDims</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-1552"></span><span>    </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681511"><span class="hs-identifier hs-var">xDims</span></a></span><span>
</span><span id="line-1553"></span><span>
</span><span id="line-1554"></span><span>  </span><span id="local-6989586621679681508"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO (a, b, c)
</span><a href="#local-6989586621679681508"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681507"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681507"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681506"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681506"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681505"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681505"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1555"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">,</span><span class="hs-special">,</span><span class="hs-special">)</span><span>
</span><span id="line-1556"></span><span>      </span><span class="annot"><span class="annottext">(a -&gt; b -&gt; c -&gt; (a, b, c)) -&gt; IO a -&gt; IO (b -&gt; c -&gt; (a, b, c))
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681507"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681506"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681505"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1557"></span><span>      </span><span class="annot"><span class="annottext">IO (b -&gt; c -&gt; (a, b, c)) -&gt; IO b -&gt; IO (c -&gt; (a, b, c))
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO b
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681507"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681506"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681504"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681505"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1558"></span><span>      </span><span class="annot"><span class="annottext">IO (c -&gt; (a, b, c)) -&gt; IO c -&gt; IO (a, b, c)
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO c
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681507"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681506"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681504"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681505"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1559"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1560"></span><span>      </span><span id="local-6989586621679681504"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681504"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681505"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1561"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681503"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681503"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe (a, b) -&gt; IO (a, b, c)
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679681523"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679681522"><span class="hs-identifier hs-type">b</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681503"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (a, b)
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1562"></span><span>
</span><span id="line-1563"></span><span>  </span><span id="local-6989586621679681502"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; (a, b, c) -&gt; IO ()
</span><a href="#local-6989586621679681502"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681501"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681501"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681500"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681500"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681499"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681499"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681498"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681498"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681497"><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679681497"><span class="hs-identifier hs-var">y</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681496"><span class="annot"><span class="annottext">c
</span><a href="#local-6989586621679681496"><span class="hs-identifier hs-var">z</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1564"></span><span>    </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681501"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681500"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681499"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681498"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1565"></span><span>    </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; b -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681501"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681500"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681495"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681499"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679681497"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-1566"></span><span>    </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; c -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681501"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681500"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681495"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681499"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">c
</span><a href="#local-6989586621679681496"><span class="hs-identifier hs-var">z</span></a></span><span>
</span><span id="line-1567"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1568"></span><span>      </span><span id="local-6989586621679681495"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681495"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681499"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1569"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681494"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681494"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681493"><span class="annot"><span class="annottext">(a, b, c)
</span><a href="#local-6989586621679681493"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe (a, b, c) -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681494"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe (a, b, c) -&gt; IO ()) -&gt; Maybe (a, b, c) -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(a, b, c) -&gt; Maybe (a, b, c)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(a, b, c)
</span><a href="#local-6989586621679681493"><span class="hs-identifier hs-var">x</span></a></span></span></span></span><span>
</span><span id="line-1570"></span><span>
</span><span id="line-1571"></span><span id="local-6989586621679681489"><span id="local-6989586621679681490"><span id="local-6989586621679681491"><span id="local-6989586621679681492"><span class="hs-keyword">instance</span><span>
</span><span id="line-1572"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681492"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681491"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681490"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1573"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681492"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1574"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681491"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1575"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681489"><span class="hs-identifier hs-type">dimsOut</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1576"></span><span>    </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681489"><span class="hs-identifier hs-type">dimsOut</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#InsertDimF"><span class="hs-identifier hs-type">InsertDimF</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier hs-type">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#ByIndex"><span class="hs-identifier hs-type">ByIndex</span></a></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681490"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedSize"><span class="hs-identifier hs-type">UncheckedSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1577"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1578"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679681492"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679681491"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681489"><span class="hs-identifier hs-type">dimsOut</span></a></span><span>
</span><span id="line-1579"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1580"></span><span>  </span><span id="local-6989586621679681486"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; [a]
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
</span><a href="#local-6989586621679681486"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; [a]
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1581"></span><span>  </span><span id="local-6989586621679681485"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; [a]
</span><a href="#local-6989586621679681485"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; [a]
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span></span></span></span></span><span>
</span><span id="line-1582"></span><span>
</span><span id="line-1583"></span><span id="local-6989586621679681484"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681484"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679681484"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-1584"></span><span>  </span><span id="local-6989586621679681479"><span class="annot"><span class="annottext">guessDim :: Maybe [a] -&gt; Maybe Int
</span><a href="#local-6989586621679681479"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int -&gt; Maybe Int
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Maybe Int) -&gt; (Maybe [a] -&gt; Int) -&gt; Maybe [a] -&gt; Maybe Int
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; ([a] -&gt; Int) -&gt; Maybe [a] -&gt; Int
forall b a. b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b
</span><span class="hs-identifier hs-var">maybe</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="annot"><span class="annottext">[a] -&gt; Int
forall (t :: * -&gt; *) a. Foldable t =&gt; t a -&gt; Int
</span><span class="hs-identifier hs-var">length</span></span><span>
</span><span id="line-1585"></span><span>
</span><span id="line-1586"></span><span>  </span><span id="local-6989586621679681476"><span class="annot"><span class="annottext">guessInnerDims :: Maybe [a] -&gt; m [Int]
</span><a href="#local-6989586621679681476"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1587"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Maybe [a] -&gt; ([a] -&gt; Maybe (NonEmpty a)) -&gt; Maybe (NonEmpty a)
forall (m :: * -&gt; *) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">[a] -&gt; Maybe (NonEmpty a)
forall a. [a] -&gt; Maybe (NonEmpty a)
</span><span class="hs-identifier hs-var">nonEmpty</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Maybe [a] -&gt; Maybe (NonEmpty a))
-&gt; (Maybe (NonEmpty a) -&gt; m [Int]) -&gt; Maybe [a] -&gt; m [Int]
forall k (cat :: k -&gt; k -&gt; *) (a :: k) (b :: k) (c :: k).
Category cat =&gt;
cat a b -&gt; cat b c -&gt; cat a c
</span><span class="hs-operator hs-var">&gt;&gt;&gt;</span></span><span> </span><span class="hs-glyph">\</span><span class="hs-glyph">case</span><span>
</span><span id="line-1588"></span><span>      </span><span class="annot"><span class="annottext">Maybe (NonEmpty a)
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681484"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1589"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681475"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681475"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="annot"><span class="hs-operator hs-type">:|</span></span><span> </span><span id="local-6989586621679681474"><span class="annot"><span class="annottext">[a]
</span><a href="#local-6989586621679681474"><span class="hs-identifier hs-var">xs</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1590"></span><span>        </span><span id="local-6989586621679681473"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681473"><span class="hs-identifier hs-var">xDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe a -&gt; m [Int]) -&gt; Maybe a -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; Maybe a
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681475"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1591"></span><span>        </span><span class="annot"><span class="annottext">(a -&gt; m ()) -&gt; [a] -&gt; m ()
forall (t :: * -&gt; *) (f :: * -&gt; *) a b.
(Foldable t, Applicative f) =&gt;
(a -&gt; f b) -&gt; t a -&gt; f ()
</span><span class="hs-identifier hs-var">traverse_</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; m ()
forall (m :: * -&gt; *). MonadThrow m =&gt; [Int] -&gt; [Int] -&gt; m ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkDims"><span class="hs-identifier hs-var">checkDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681473"><span class="hs-identifier hs-var">xDims</span></a></span><span> </span><span class="annot"><span class="annottext">([Int] -&gt; m ()) -&gt; (a -&gt; m [Int]) -&gt; a -&gt; m ()
forall (m :: * -&gt; *) b c a.
Monad m =&gt;
(b -&gt; m c) -&gt; (a -&gt; m b) -&gt; a -&gt; m c
</span><span class="hs-operator hs-var">&lt;=&lt;</span></span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe a -&gt; m [Int]) -&gt; (a -&gt; Maybe a) -&gt; a -&gt; m [Int]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; Maybe a
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[a]
</span><a href="#local-6989586621679681474"><span class="hs-identifier hs-var">xs</span></a></span><span>
</span><span id="line-1592"></span><span>        </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681473"><span class="hs-identifier hs-var">xDims</span></a></span><span>
</span><span id="line-1593"></span><span>
</span><span id="line-1594"></span><span>  </span><span id="local-6989586621679681472"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO [a]
</span><a href="#local-6989586621679681472"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681471"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681471"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681470"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681470"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681469"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681469"><span class="hs-identifier hs-var">d</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681468"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681468"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1595"></span><span>    </span><span class="annot"><span class="annottext">[Int] -&gt; (Int -&gt; IO a) -&gt; IO [a]
forall (t :: * -&gt; *) (m :: * -&gt; *) a b.
(Traversable t, Monad m) =&gt;
t a -&gt; (a -&gt; m b) -&gt; m (t b)
</span><span class="hs-identifier hs-var">forM</span></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">..</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681469"><span class="hs-identifier hs-var">d</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">((Int -&gt; IO a) -&gt; IO [a]) -&gt; (Int -&gt; IO a) -&gt; IO [a]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679681467"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681467"><span class="hs-identifier hs-var">i</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1596"></span><span>      </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681471"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681470"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681467"><span class="hs-identifier hs-var">i</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681466"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681468"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1597"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1598"></span><span>      </span><span id="local-6989586621679681466"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681466"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681468"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1599"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681465"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681465"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe [a] -&gt; IO [a]
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679681484"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681465"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe [a]
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1600"></span><span>
</span><span id="line-1601"></span><span>  </span><span id="local-6989586621679681464"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; [a] -&gt; IO ()
</span><a href="#local-6989586621679681464"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681463"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681463"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681462"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681462"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681461"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681461"><span class="hs-identifier hs-var">d</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681460"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681460"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679681459"><span class="annot"><span class="annottext">[a]
</span><a href="#local-6989586621679681459"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1602"></span><span>    </span><span class="annot"><span class="annottext">[(Int, a)] -&gt; ((Int, a) -&gt; IO ()) -&gt; IO ()
forall (t :: * -&gt; *) (m :: * -&gt; *) a b.
(Foldable t, Monad m) =&gt;
t a -&gt; (a -&gt; m b) -&gt; m ()
</span><span class="hs-identifier hs-var">forM_</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; [a] -&gt; [(Int, a)]
forall a b. [a] -&gt; [b] -&gt; [(a, b)]
</span><span class="hs-identifier hs-var">zip</span></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">..</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681461"><span class="hs-identifier hs-var">d</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">[a]
</span><a href="#local-6989586621679681459"><span class="hs-identifier hs-var">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(((Int, a) -&gt; IO ()) -&gt; IO ()) -&gt; ((Int, a) -&gt; IO ()) -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span class="hs-special">(</span><span id="local-6989586621679681458"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681458"><span class="hs-identifier hs-var">i</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681457"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681457"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1603"></span><span>      </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681463"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681462"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681458"><span class="hs-identifier hs-var">i</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681456"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681460"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681457"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1604"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1605"></span><span>      </span><span id="local-6989586621679681456"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681456"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681460"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1606"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681455"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681455"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681454"><span class="annot"><span class="annottext">[a]
</span><a href="#local-6989586621679681454"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe [a] -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681455"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe [a] -&gt; IO ()) -&gt; Maybe [a] -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[a] -&gt; Maybe [a]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[a]
</span><a href="#local-6989586621679681454"><span class="hs-identifier hs-var">x</span></a></span></span><span>
</span><span id="line-1607"></span><span>
</span><span id="line-1608"></span><span id="local-6989586621679681450"><span id="local-6989586621679681451"><span id="local-6989586621679681452"><span id="local-6989586621679681453"><span class="hs-keyword">instance</span><span>
</span><span id="line-1609"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681453"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681452"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681451"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1610"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681453"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1611"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681452"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1612"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681450"><span class="hs-identifier hs-type">dimsOut</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1613"></span><span>    </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681450"><span class="hs-identifier hs-type">dimsOut</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#InsertDimF"><span class="hs-identifier hs-type">InsertDimF</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier hs-type">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#ByIndex"><span class="hs-identifier hs-type">ByIndex</span></a></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681451"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#UncheckedSize"><span class="hs-identifier hs-type">UncheckedSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1614"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1615"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier hs-type">V.Vector</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681453"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681452"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681450"><span class="hs-identifier hs-type">dimsOut</span></a></span><span>
</span><span id="line-1616"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1617"></span><span>  </span><span id="local-6989586621679681447"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Vector a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
</span><a href="#local-6989586621679681447"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Vector a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1618"></span><span>  </span><span id="local-6989586621679681446"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; Vector a
</span><a href="#local-6989586621679681446"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; Vector a
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span></span></span></span></span><span>
</span><span id="line-1619"></span><span>
</span><span id="line-1620"></span><span id="local-6989586621679681445"><span class="hs-keyword">instance</span><span>
</span><span id="line-1621"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681445"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1622"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier hs-type">V.Vector</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681445"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1623"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1624"></span><span>  </span><span id="local-6989586621679681440"><span class="annot"><span class="annottext">guessDim :: Maybe (Vector a) -&gt; Maybe Int
</span><a href="#local-6989586621679681440"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int -&gt; Maybe Int
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Maybe Int)
-&gt; (Maybe (Vector a) -&gt; Int) -&gt; Maybe (Vector a) -&gt; Maybe Int
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; (Vector a -&gt; Int) -&gt; Maybe (Vector a) -&gt; Int
forall b a. b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b
</span><span class="hs-identifier hs-var">maybe</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="annot"><span class="annottext">Vector a -&gt; Int
forall (t :: * -&gt; *) a. Foldable t =&gt; t a -&gt; Int
</span><span class="hs-identifier hs-var">length</span></span><span>
</span><span id="line-1625"></span><span>
</span><span id="line-1626"></span><span>  </span><span id="local-6989586621679681439"><span class="annot"><span class="annottext">guessInnerDims :: Maybe (Vector a) -&gt; m [Int]
</span><a href="#local-6989586621679681439"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1627"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Maybe (Vector a)
-&gt; (Vector a -&gt; Maybe (a, Vector a)) -&gt; Maybe (a, Vector a)
forall (m :: * -&gt; *) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">Vector a -&gt; Maybe (a, Vector a)
forall a. Vector a -&gt; Maybe (a, Vector a)
</span><a href="Torch.GraduallyTyped.Internal.Vector.html#uncons"><span class="hs-identifier hs-var">V.uncons</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Maybe (Vector a) -&gt; Maybe (a, Vector a))
-&gt; (Maybe (a, Vector a) -&gt; m [Int]) -&gt; Maybe (Vector a) -&gt; m [Int]
forall k (cat :: k -&gt; k -&gt; *) (a :: k) (b :: k) (c :: k).
Category cat =&gt;
cat a b -&gt; cat b c -&gt; cat a c
</span><span class="hs-operator hs-var">&gt;&gt;&gt;</span></span><span> </span><span class="hs-glyph">\</span><span class="hs-glyph">case</span><span>
</span><span id="line-1628"></span><span>      </span><span class="annot"><span class="annottext">Maybe (a, Vector a)
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681445"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe a
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1629"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681437"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681437"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681436"><span class="annot"><span class="annottext">Vector a
</span><a href="#local-6989586621679681436"><span class="hs-identifier hs-var">xs</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1630"></span><span>        </span><span id="local-6989586621679681435"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681435"><span class="hs-identifier hs-var">xDims</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe a -&gt; m [Int]) -&gt; Maybe a -&gt; m [Int]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; Maybe a
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681437"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1631"></span><span>        </span><span class="annot"><span class="annottext">(a -&gt; m ()) -&gt; Vector a -&gt; m ()
forall (t :: * -&gt; *) (f :: * -&gt; *) a b.
(Foldable t, Applicative f) =&gt;
(a -&gt; f b) -&gt; t a -&gt; f ()
</span><span class="hs-identifier hs-var">traverse_</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; [Int] -&gt; m ()
forall (m :: * -&gt; *). MonadThrow m =&gt; [Int] -&gt; [Int] -&gt; m ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#checkDims"><span class="hs-identifier hs-var">checkDims</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681435"><span class="hs-identifier hs-var">xDims</span></a></span><span> </span><span class="annot"><span class="annottext">([Int] -&gt; m ()) -&gt; (a -&gt; m [Int]) -&gt; a -&gt; m ()
forall (m :: * -&gt; *) b c a.
Monad m =&gt;
(b -&gt; m c) -&gt; (a -&gt; m b) -&gt; a -&gt; m c
</span><span class="hs-operator hs-var">&lt;=&lt;</span></span><span> </span><span class="annot"><span class="annottext">Maybe a -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessDims"><span class="hs-identifier hs-var">guessDims</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe a -&gt; m [Int]) -&gt; (a -&gt; Maybe a) -&gt; a -&gt; m [Int]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">a -&gt; Maybe a
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Vector a
</span><a href="#local-6989586621679681436"><span class="hs-identifier hs-var">xs</span></a></span><span>
</span><span id="line-1632"></span><span>        </span><span class="annot"><span class="annottext">[Int] -&gt; m [Int]
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681435"><span class="hs-identifier hs-var">xDims</span></a></span><span>
</span><span id="line-1633"></span><span>
</span><span id="line-1634"></span><span>  </span><span id="local-6989586621679681434"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO (Vector a)
</span><a href="#local-6989586621679681434"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681433"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681433"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681432"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681432"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681431"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681431"><span class="hs-identifier hs-var">d</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681430"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681430"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1635"></span><span>    </span><span class="annot"><span class="annottext">Vector Int -&gt; (Int -&gt; IO a) -&gt; IO (Vector a)
forall (t :: * -&gt; *) (m :: * -&gt; *) a b.
(Traversable t, Monad m) =&gt;
t a -&gt; (a -&gt; m b) -&gt; m (t b)
</span><span class="hs-identifier hs-var">forM</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Vector Int
forall a. Enum a =&gt; a -&gt; a -&gt; Vector a
</span><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier hs-var">V.enumFromTo</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681431"><span class="hs-identifier hs-var">d</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">((Int -&gt; IO a) -&gt; IO (Vector a)) -&gt; (Int -&gt; IO a) -&gt; IO (Vector a)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679681428"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681428"><span class="hs-identifier hs-var">i</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1636"></span><span>      </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681433"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681432"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681428"><span class="hs-identifier hs-var">i</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681427"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681430"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1637"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1638"></span><span>      </span><span id="local-6989586621679681427"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681427"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681430"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1639"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681426"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681426"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe (Vector a) -&gt; IO (Vector a)
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier hs-type">V.Vector</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681445"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681426"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Vector a)
forall (f :: * -&gt; *) a. Alternative f =&gt; f a
</span><span class="hs-identifier hs-var">empty</span></span><span>
</span><span id="line-1640"></span><span>
</span><span id="line-1641"></span><span>  </span><span id="local-6989586621679681425"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; Vector a -&gt; IO ()
</span><a href="#local-6989586621679681425"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681424"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681424"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681423"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681423"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679681422"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681422"><span class="hs-identifier hs-var">d</span></a></span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">:</span></span><span> </span><span id="local-6989586621679681421"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681421"><span class="hs-identifier hs-var">innerDims</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679681420"><span class="annot"><span class="annottext">Vector a
</span><a href="#local-6989586621679681420"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-1642"></span><span>    </span><span class="annot"><span class="annottext">Vector (Int, a) -&gt; ((Int, a) -&gt; IO ()) -&gt; IO ()
forall (t :: * -&gt; *) (m :: * -&gt; *) a b.
(Foldable t, Monad m) =&gt;
t a -&gt; (a -&gt; m b) -&gt; m ()
</span><span class="hs-identifier hs-var">forM_</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Vector Int -&gt; Vector a -&gt; Vector (Int, a)
forall a b. Vector a -&gt; Vector b -&gt; Vector (a, b)
</span><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier hs-var">V.zip</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Vector Int
forall a. Enum a =&gt; a -&gt; a -&gt; Vector a
</span><a href="../file:///nix/store/h5r4cy0d8f6bvn55ksc78pqny0m60hsm-vector-lib-vector-0.12.3.1-haddock-doc/share/doc/vector/html/src"><span class="hs-identifier hs-var">V.enumFromTo</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681422"><span class="hs-identifier hs-var">d</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Vector a
</span><a href="#local-6989586621679681420"><span class="hs-identifier hs-var">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(((Int, a) -&gt; IO ()) -&gt; IO ()) -&gt; ((Int, a) -&gt; IO ()) -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span class="hs-special">(</span><span id="local-6989586621679681418"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681418"><span class="hs-identifier hs-var">i</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679681417"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681417"><span class="hs-identifier hs-var">x</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1643"></span><span>      </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681424"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681423"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681418"><span class="hs-identifier hs-var">i</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681416"><span class="hs-identifier hs-var">width</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681421"><span class="hs-identifier hs-var">innerDims</span></a></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679681417"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-1644"></span><span>    </span><span class="hs-keyword">where</span><span>
</span><span id="line-1645"></span><span>      </span><span id="local-6989586621679681416"><span class="annot"><span class="annottext">width :: Int
</span><a href="#local-6989586621679681416"><span class="hs-identifier hs-var hs-var">width</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681421"><span class="hs-identifier hs-var">innerDims</span></a></span><span>
</span><span id="line-1646"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679681415"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681415"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span id="local-6989586621679681414"><span class="annot"><span class="annottext">Vector a
</span><a href="#local-6989586621679681414"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Maybe (Vector a) -&gt; IO ()
forall a (m :: * -&gt; *) b.
(TensorLikeRaw a, MonadThrow m) =&gt;
[Int] -&gt; Maybe a -&gt; m b
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#unexpectedDimsError"><span class="hs-identifier hs-var">unexpectedDimsError</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681415"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe (Vector a) -&gt; IO ()) -&gt; Maybe (Vector a) -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Vector a -&gt; Maybe (Vector a)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Vector a
</span><a href="#local-6989586621679681414"><span class="hs-identifier hs-var">x</span></a></span></span><span>
</span><span id="line-1647"></span><span>
</span><span id="line-1648"></span><span id="local-6989586621679681409"><span id="local-6989586621679681410"><span id="local-6989586621679681411"><span id="local-6989586621679681412"><span id="local-6989586621679681413"><span class="hs-keyword">instance</span><span>
</span><span id="line-1649"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679681413"><span class="hs-identifier hs-type">n</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1650"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681412"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681411"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681410"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1651"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681412"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1652"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681411"><span class="hs-identifier hs-type">dType</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1653"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetDims"><span class="hs-identifier hs-type">SGetDims</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681409"><span class="hs-identifier hs-type">dimsOut</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1654"></span><span>    </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681409"><span class="hs-identifier hs-type">dimsOut</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Class.html#InsertDimF"><span class="hs-identifier hs-type">InsertDimF</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier hs-type">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#ByIndex"><span class="hs-identifier hs-type">ByIndex</span></a></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681410"><span class="hs-identifier hs-type">dims</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Dim"><span class="hs-identifier hs-type">Dim</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Name"><span class="hs-identifier hs-type">Name</span></a></span><span> </span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Size"><span class="hs-identifier hs-type">Size</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681413"><span class="hs-identifier hs-type">n</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-1655"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1656"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLike"><span class="hs-identifier hs-type">TensorLike</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-type">SV.Vector</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681413"><span class="hs-identifier hs-type">n</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681412"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679681411"><span class="hs-identifier hs-type">dType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681409"><span class="hs-identifier hs-type">dimsOut</span></a></span><span>
</span><span id="line-1657"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1658"></span><span>  </span><span id="local-6989586621679681406"><span class="annot"><span class="annottext">sToTensor :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Vector n a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
</span><a href="#local-6989586621679681406"><span class="hs-identifier hs-var hs-var hs-var hs-var">sToTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; Vector n a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dimsOut))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *).
(TensorLike a dType dims, TensorLikeRaw a, SingI dType,
 MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensorRaw"><span class="hs-identifier hs-var">sToTensorRaw</span></a></span><span>
</span><span id="line-1659"></span><span>  </span><span id="local-6989586621679681405"><span class="annot"><span class="annottext">fromTensor :: Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; Vector n a
</span><a href="#local-6989586621679681405"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromTensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device ('DataType dType) ('Shape dimsOut)
-&gt; Vector n a
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) a
       (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]).
(TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensorRaw"><span class="hs-identifier hs-var">fromTensorRaw</span></a></span></span></span></span></span></span><span>
</span><span id="line-1660"></span><span>
</span><span id="line-1661"></span><span id="local-6989586621679681403"><span id="local-6989586621679681404"><span class="hs-keyword">instance</span><span>
</span><span id="line-1662"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679681404"><span class="hs-identifier hs-type">n</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1663"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681403"><span class="hs-identifier hs-type">a</span></a></span><span>
</span><span id="line-1664"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1665"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#TensorLikeRaw"><span class="hs-identifier hs-type">TensorLikeRaw</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-type">SV.Vector</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681404"><span class="hs-identifier hs-type">n</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681403"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1666"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1667"></span><span>  </span><span id="local-6989586621679681398"><span class="annot"><span class="annottext">guessDim :: Maybe (Vector n a) -&gt; Maybe Int
</span><a href="#local-6989586621679681398"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int -&gt; Maybe Int
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Maybe Int)
-&gt; (Maybe (Vector n a) -&gt; Int) -&gt; Maybe (Vector n a) -&gt; Maybe Int
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; (Vector n a -&gt; Int) -&gt; Maybe (Vector n a) -&gt; Int
forall b a. b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b
</span><span class="hs-identifier hs-var">maybe</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="annot"><span class="annottext">Vector n a -&gt; Int
forall (t :: * -&gt; *) a. Foldable t =&gt; t a -&gt; Int
</span><span class="hs-identifier hs-var">length</span></span><span>
</span><span id="line-1668"></span><span>
</span><span id="line-1669"></span><span>  </span><span id="local-6989586621679681397"><span class="annot"><span class="annottext">guessInnerDims :: Maybe (Vector n a) -&gt; m [Int]
</span><a href="#local-6989586621679681397"><span class="hs-identifier hs-var hs-var hs-var hs-var">guessInnerDims</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Maybe (Vector a) -&gt; m [Int]
forall a (m :: * -&gt; *).
(TensorLikeRaw a, MonadThrow m) =&gt;
Maybe a -&gt; m [Int]
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#guessInnerDims"><span class="hs-identifier hs-var">guessInnerDims</span></a></span><span> </span><span class="annot"><span class="annottext">(Maybe (Vector a) -&gt; m [Int])
-&gt; (Maybe (Vector n a) -&gt; Maybe (Vector a))
-&gt; Maybe (Vector n a)
-&gt; m [Int]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Vector n a -&gt; Vector a) -&gt; Maybe (Vector n a) -&gt; Maybe (Vector a)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-identifier hs-var">fmap</span></span><span> </span><span class="annot"><span class="annottext">Vector n a -&gt; Vector a
forall a (n :: Nat). KnownNat n =&gt; Vector n a -&gt; Vector a
</span><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-var">SV.SomeSized</span></a></span><span>
</span><span id="line-1670"></span><span>
</span><span id="line-1671"></span><span>  </span><span id="local-6989586621679681395"><span class="annot"><span class="annottext">tensorPeekElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; IO (Vector n a)
</span><a href="#local-6989586621679681395"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPeekElemOff</span></a></span></span><span> </span><span id="local-6989586621679681394"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681394"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681393"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681393"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span id="local-6989586621679681392"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681392"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Vector a -&gt; Vector n a
forall (v :: * -&gt; *) (n :: Nat) a. v a -&gt; Vector v n a
</span><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-var">SVI.Vector</span></a></span><span> </span><span class="annot"><span class="annottext">(Vector a -&gt; Vector n a) -&gt; IO (Vector a) -&gt; IO (Vector n a)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; IO (Vector a)
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; IO a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPeekElemOff"><span class="hs-identifier hs-var">tensorPeekElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681394"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681393"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681392"><span class="hs-identifier hs-var">dims'</span></a></span><span>
</span><span id="line-1672"></span><span>
</span><span id="line-1673"></span><span>  </span><span id="local-6989586621679681390"><span class="annot"><span class="annottext">tensorPokeElemOff :: Ptr () -&gt; Int -&gt; [Int] -&gt; Vector n a -&gt; IO ()
</span><a href="#local-6989586621679681390"><span class="hs-identifier hs-var hs-var hs-var hs-var">tensorPokeElemOff</span></a></span></span><span> </span><span id="local-6989586621679681389"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681389"><span class="hs-identifier hs-var">ptr</span></a></span></span><span> </span><span id="local-6989586621679681388"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681388"><span class="hs-identifier hs-var">offset</span></a></span></span><span> </span><span id="local-6989586621679681387"><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681387"><span class="hs-identifier hs-var">dims'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; Int -&gt; [Int] -&gt; Vector a -&gt; IO ()
forall a. TensorLikeRaw a =&gt; Ptr () -&gt; Int -&gt; [Int] -&gt; a -&gt; IO ()
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#tensorPokeElemOff"><span class="hs-identifier hs-var">tensorPokeElemOff</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621679681389"><span class="hs-identifier hs-var">ptr</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679681388"><span class="hs-identifier hs-var">offset</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679681387"><span class="hs-identifier hs-var">dims'</span></a></span><span> </span><span class="annot"><span class="annottext">(Vector a -&gt; IO ())
-&gt; (Vector n a -&gt; Vector a) -&gt; Vector n a -&gt; IO ()
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Vector n a -&gt; Vector a
forall a (n :: Nat). KnownNat n =&gt; Vector n a -&gt; Vector a
</span><a href="../file:///nix/store/iniy1fidkgw9wpamsmciy1vlgi3wzvrs-vector-sized-lib-vector-sized-1.5.0-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-var">SV.SomeSized</span></a></span></span></span><span>
</span><span id="line-1674"></span><span>
</span><span id="line-1675"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetTensorOptions"><span class="hs-identifier hs-type">sSetTensorOptions</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1676"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679682515"><span class="annot"><a href="#local-6989586621679682515"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679682514"><span class="annot"><a href="#local-6989586621679682514"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679682513"><span class="annot"><a href="#local-6989586621679682513"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679682512"><span class="annot"><a href="#local-6989586621679682512"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679682511"><span class="annot"><a href="#local-6989586621679682511"><span class="hs-identifier hs-type">gradientFrom</span></a></span></span><span> </span><span id="local-6989586621679682510"><span class="annot"><a href="#local-6989586621679682510"><span class="hs-identifier hs-type">layoutFrom</span></a></span></span><span> </span><span id="local-6989586621679682509"><span class="annot"><a href="#local-6989586621679682509"><span class="hs-identifier hs-type">deviceFrom</span></a></span></span><span> </span><span id="local-6989586621679682508"><span class="annot"><a href="#local-6989586621679682508"><span class="hs-identifier hs-type">dataTypeFrom</span></a></span></span><span> </span><span id="local-6989586621679682507"><span class="annot"><a href="#local-6989586621679682507"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1677"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682515"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1678"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-type">SLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682514"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1679"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682513"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1680"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-type">SDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682512"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1681"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682511"><span class="hs-identifier hs-type">gradientFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682510"><span class="hs-identifier hs-type">layoutFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682509"><span class="hs-identifier hs-type">deviceFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682508"><span class="hs-identifier hs-type">dataTypeFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682507"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1682"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682515"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682514"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682513"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682512"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679682507"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1683"></span><span id="sSetTensorOptions"><span class="annot"><span class="annottext">sSetTensorOptions :: SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
-&gt; IO (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetTensorOptions"><span class="hs-identifier hs-var hs-var">sSetTensorOptions</span></a></span></span><span> </span><span id="local-6989586621679681385"><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679681385"><span class="hs-identifier hs-var">gradient'</span></a></span></span><span> </span><span id="local-6989586621679681384"><span class="annot"><span class="annottext">SLayout layout
</span><a href="#local-6989586621679681384"><span class="hs-identifier hs-var">layout</span></a></span></span><span> </span><span id="local-6989586621679681383"><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679681383"><span class="hs-identifier hs-var">device</span></a></span></span><span> </span><span id="local-6989586621679681382"><span class="annot"><span class="annottext">SDataType dataType
</span><a href="#local-6989586621679681382"><span class="hs-identifier hs-var">dataType</span></a></span></span><span> </span><span id="local-6989586621679681381"><span class="annot"><span class="annottext">Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
</span><a href="#local-6989586621679681381"><span class="hs-identifier hs-var">t</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-1684"></span><span>  </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-var">UnsafeTensor</span></a></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape)
-&gt; IO (ForeignPtr Tensor)
-&gt; IO (Tensor gradient layout device dataType shape)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; ForeignPtr TensorOptions
 -&gt; CBool
 -&gt; CBool
 -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
-&gt; TensorOptions
-&gt; Bool
-&gt; Bool
-&gt; IO (ForeignPtr Tensor)
forall a ca x1 cx1 x2 cx2 x3 cx3 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable x3 cx3,
 Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; cx3 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; x3 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast4</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr TensorOptions
-&gt; CBool
-&gt; CBool
-&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.tensor_to_obb</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
</span><a href="#local-6989586621679681381"><span class="hs-identifier hs-var">t</span></a></span><span> </span><span class="annot"><span class="annottext">TensorOptions
</span><a href="#local-6989586621679681380"><span class="hs-identifier hs-var">opts</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681379"><span class="hs-identifier hs-var">nonBlocking</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679681378"><span class="hs-identifier hs-var">copy</span></a></span><span>
</span><span id="line-1685"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-1686"></span><span>    </span><span id="local-6989586621679681380"><span class="annot"><span class="annottext">opts :: TensorOptions
</span><a href="#local-6989586621679681380"><span class="hs-identifier hs-var hs-var">opts</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; TensorOptions
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType).
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; TensorOptions
</span><a href="Torch.GraduallyTyped.Internal.TensorOptions.html#tensorOptions"><span class="hs-identifier hs-var">tensorOptions</span></a></span><span> </span><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679681385"><span class="hs-identifier hs-var">gradient'</span></a></span><span> </span><span class="annot"><span class="annottext">SLayout layout
</span><a href="#local-6989586621679681384"><span class="hs-identifier hs-var">layout</span></a></span><span> </span><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679681383"><span class="hs-identifier hs-var">device</span></a></span><span> </span><span class="annot"><span class="annottext">SDataType dataType
</span><a href="#local-6989586621679681382"><span class="hs-identifier hs-var">dataType</span></a></span><span>
</span><span id="line-1687"></span><span>
</span><span id="line-1688"></span><span>    </span><span id="local-6989586621679681379"><span class="annot"><span class="annottext">nonBlocking :: Bool
</span><a href="#local-6989586621679681379"><span class="hs-identifier hs-var hs-var">nonBlocking</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-1689"></span><span>    </span><span id="local-6989586621679681378"><span class="annot"><span class="annottext">copy :: Bool
</span><a href="#local-6989586621679681378"><span class="hs-identifier hs-var hs-var">copy</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-1690"></span><span>
</span><span id="line-1691"></span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#setTensorOptions"><span class="hs-identifier hs-type">setTensorOptions</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-1692"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679681376"><span class="annot"><a href="#local-6989586621679681376"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679681375"><span class="annot"><a href="#local-6989586621679681375"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679681374"><span class="annot"><a href="#local-6989586621679681374"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679681373"><span class="annot"><a href="#local-6989586621679681373"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679681372"><span class="annot"><a href="#local-6989586621679681372"><span class="hs-identifier hs-type">gradientFrom</span></a></span></span><span> </span><span id="local-6989586621679681371"><span class="annot"><a href="#local-6989586621679681371"><span class="hs-identifier hs-type">layoutFrom</span></a></span></span><span> </span><span id="local-6989586621679681370"><span class="annot"><a href="#local-6989586621679681370"><span class="hs-identifier hs-type">deviceFrom</span></a></span></span><span> </span><span id="local-6989586621679681369"><span class="annot"><a href="#local-6989586621679681369"><span class="hs-identifier hs-type">dataTypeFrom</span></a></span></span><span> </span><span id="local-6989586621679681368"><span class="annot"><a href="#local-6989586621679681368"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-1693"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681376"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1694"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681375"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1695"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681374"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-1696"></span><span>    </span><span class="annot"><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-type">SingI</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681373"><span class="hs-identifier hs-type">dataType</span></a></span><span>
</span><span id="line-1697"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-1698"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681372"><span class="hs-identifier hs-type">gradientFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681371"><span class="hs-identifier hs-type">layoutFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681370"><span class="hs-identifier hs-type">deviceFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681369"><span class="hs-identifier hs-type">dataTypeFrom</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681368"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-1699"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681376"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681375"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681374"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681373"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679681368"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1700"></span><span id="setTensorOptions"><span class="annot"><span class="annottext">setTensorOptions :: Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
-&gt; IO (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#setTensorOptions"><span class="hs-identifier hs-var hs-var">setTensorOptions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (gradientFrom :: Gradient RequiresGradient)
       (layoutFrom :: Layout LayoutType)
       (deviceFrom :: Device (DeviceType Nat))
       (dataTypeFrom :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape
-&gt; IO (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sSetTensorOptions"><span class="hs-identifier hs-var">sSetTensorOptions</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI gradient =&gt; Sing gradient
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681376"><span class="hs-identifier hs-type">gradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI layout =&gt; Sing layout
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681375"><span class="hs-identifier hs-type">layout</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI device =&gt; Sing device
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681374"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SingI dataType =&gt; Sing dataType
forall k (a :: k). SingI a =&gt; Sing a
</span><a href="../file:///nix/store/44d7539k7gqlplsaxjj66fck1zc772zc-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">sing</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679681373"><span class="hs-identifier hs-type">dataType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-1701"></span><span>
</span><span id="line-1702"></span><span class="hs-comment">-- instance</span><span>
</span><span id="line-1703"></span><span class="hs-comment">--   ( SingI gradient,</span><span>
</span><span id="line-1704"></span><span class="hs-comment">--     SingI layout,</span><span>
</span><span id="line-1705"></span><span class="hs-comment">--     SingI device,</span><span>
</span><span id="line-1706"></span><span class="hs-comment">--     SingI dType</span><span>
</span><span id="line-1707"></span><span class="hs-comment">--   ) =&gt;</span><span>
</span><span id="line-1708"></span><span class="hs-comment">--   TensorLike (Tensor gradient layout device ('DataType dType) ('Shape dims)) dType dims</span><span>
</span><span id="line-1709"></span><span class="hs-comment">--   where</span><span>
</span><span id="line-1710"></span><span class="hs-comment">--   sToTensor gradient' layout device t = pure $ sSetTensorOptions gradient' layout device dataType t</span><span>
</span><span id="line-1711"></span><span class="hs-comment">--     where</span><span>
</span><span id="line-1712"></span><span class="hs-comment">--       dataType = SDataType $ sing @dType</span><span>
</span><span id="line-1713"></span><span>
</span><span id="line-1714"></span><span class="hs-comment">--   fromTensor = setTensorOptions @gradient @layout @device @('DataType dType)</span><span>
</span><span id="line-1715"></span></pre></body></html>