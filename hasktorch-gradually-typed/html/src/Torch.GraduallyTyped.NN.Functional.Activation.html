<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><span id="line-2"></span><span>
</span><span id="line-3"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.GraduallyTyped.NN.Functional.Activation</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-4"></span><span>
</span><span id="line-5"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad.Catch</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">MonadThrow</span></span><span class="hs-special">)</span><span>
</span><span id="line-6"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">System.IO.Unsafe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">unsafePerformIO</span></span><span class="hs-special">)</span><span>
</span><span id="line-7"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html"><span class="hs-identifier">Torch.GraduallyTyped.Tensor.MathOperations.Pointwise</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#addScalar"><span class="hs-identifier">addScalar</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-identifier">mulScalar</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#powScalar"><span class="hs-identifier">powScalar</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#tanh"><span class="hs-identifier">tanh</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-8"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html"><span class="hs-identifier">Torch.GraduallyTyped.Tensor.Type</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier">Tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-9"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast1</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast2</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast3</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-10"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Native</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-11"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Torch.Scalar</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Scalar</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-12"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Float</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">pure</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">($)</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">(*)</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">(+)</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">(.)</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-operator">(/)</span></span><span class="hs-special">)</span><span>
</span><span id="line-13"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">pi</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">sqrt</span></span><span class="hs-special">)</span><span>
</span><span id="line-14"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.GC</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">unsafeThrowableIO</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-15"></span><span>
</span><span id="line-16"></span><span class="hs-comment">-- $setup</span><span>
</span><span id="line-17"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch.GraduallyTyped.Prelude.List (SList (..))</span><span>
</span><span id="line-18"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch.GraduallyTyped</span><span>
</span><span id="line-19"></span><span>
</span><span id="line-20"></span><span class="hs-comment">-- | Thresholds each element of the input Tensor.</span><span>
</span><span id="line-21"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#threshold"><span class="hs-identifier hs-type">threshold</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-22"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688992"><span class="annot"><a href="#local-6989586621679688992"><span class="hs-identifier hs-type">threshold</span></a></span></span><span> </span><span id="local-6989586621679688991"><span class="annot"><a href="#local-6989586621679688991"><span class="hs-identifier hs-type">value</span></a></span></span><span> </span><span id="local-6989586621679688990"><span class="annot"><a href="#local-6989586621679688990"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688989"><span class="annot"><a href="#local-6989586621679688989"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688988"><span class="annot"><a href="#local-6989586621679688988"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688987"><span class="annot"><a href="#local-6989586621679688987"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688986"><span class="annot"><a href="#local-6989586621679688986"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688985"><span class="annot"><a href="#local-6989586621679688985"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-23"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688992"><span class="hs-identifier hs-type">threshold</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688991"><span class="hs-identifier hs-type">value</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688985"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-24"></span><span>  </span><span class="hs-comment">-- | threshold</span><span>
</span><span id="line-25"></span><span>  </span><span class="annot"><a href="#local-6989586621679688992"><span class="hs-identifier hs-type">threshold</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-26"></span><span>  </span><span class="hs-comment">-- | value</span><span>
</span><span id="line-27"></span><span>  </span><span class="annot"><a href="#local-6989586621679688991"><span class="hs-identifier hs-type">value</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-28"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-29"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688990"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688989"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688988"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688987"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688986"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-30"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-31"></span><span>  </span><span class="annot"><a href="#local-6989586621679688985"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688990"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688989"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688988"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688987"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688986"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-32"></span><span id="threshold"><span class="annot"><span class="annottext">threshold :: threshold
-&gt; value
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#threshold"><span class="hs-identifier hs-var hs-var">threshold</span></a></span></span><span> </span><span id="local-6989586621679688984"><span class="annot"><span class="annottext">threshold
</span><a href="#local-6989586621679688984"><span class="hs-identifier hs-var">thresholdValue</span></a></span></span><span> </span><span id="local-6989586621679688983"><span class="annot"><span class="annottext">value
</span><a href="#local-6989586621679688983"><span class="hs-identifier hs-var">value</span></a></span></span><span> </span><span id="local-6989586621679688982"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688982"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-33"></span><span>  </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; ForeignPtr Scalar
 -&gt; ForeignPtr Scalar
 -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; threshold
-&gt; value
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 x2 cx2 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast3</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr Scalar -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.threshold_tss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688982"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">threshold
</span><a href="#local-6989586621679688984"><span class="hs-identifier hs-var">thresholdValue</span></a></span><span> </span><span class="annot"><span class="annottext">value
</span><a href="#local-6989586621679688983"><span class="hs-identifier hs-var">value</span></a></span><span>
</span><span id="line-34"></span><span>
</span><span id="line-35"></span><span class="hs-comment">-- | Applies the rectified linear unit function element-wise, that is,</span><span>
</span><span id="line-36"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-37"></span><span class="hs-comment">-- \text{ReLU}(x) = max(0, x).</span><span>
</span><span id="line-38"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-39"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#relu"><span class="hs-identifier hs-type">relu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-40"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688979"><span class="annot"><a href="#local-6989586621679688979"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688978"><span class="annot"><a href="#local-6989586621679688978"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688977"><span class="annot"><a href="#local-6989586621679688977"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688976"><span class="annot"><a href="#local-6989586621679688976"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688975"><span class="annot"><a href="#local-6989586621679688975"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-41"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-42"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688979"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688978"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688977"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688976"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688975"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-43"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-44"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688979"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688978"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688977"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688976"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688975"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-45"></span><span id="relu"><span class="annot"><span class="annottext">relu :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#relu"><span class="hs-identifier hs-var hs-var">relu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.relu_t</span></a></span><span>
</span><span id="line-46"></span><span>
</span><span id="line-47"></span><span class="hs-comment">-- | Applies the gaussian error linear unit function element-wise.</span><span>
</span><span id="line-48"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#gelu"><span class="hs-identifier hs-type">gelu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-49"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688972"><span class="annot"><a href="#local-6989586621679688972"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688971"><span class="annot"><a href="#local-6989586621679688971"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688970"><span class="annot"><a href="#local-6989586621679688970"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688969"><span class="annot"><a href="#local-6989586621679688969"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688968"><span class="annot"><a href="#local-6989586621679688968"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-50"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-51"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688972"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688971"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688970"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688969"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688968"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-52"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-53"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688972"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688971"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688970"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688969"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688968"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-54"></span><span id="gelu"><span class="annot"><span class="annottext">gelu :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#gelu"><span class="hs-identifier hs-var hs-var">gelu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.gelu_t</span></a></span><span>
</span><span id="line-55"></span><span>
</span><span id="line-56"></span><span class="hs-comment">-- | Applies the gaussian error linear unit function element-wise.</span><span>
</span><span id="line-57"></span><span class="hs-comment">--</span><span>
</span><span id="line-58"></span><span class="hs-comment">-- This is the implementation of the GELU activation function from</span><span>
</span><span id="line-59"></span><span class="hs-comment">-- Google's BERT repo (and coincidentally also from OpenAI's GPT).</span><span>
</span><span id="line-60"></span><span class="hs-comment">-- See also https://arxiv.org/abs/1606.08415.</span><span>
</span><span id="line-61"></span><span class="hs-comment">--</span><span>
</span><span id="line-62"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sFull (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNil)) 0.5</span><span>
</span><span id="line-63"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- geluNew t</span><span>
</span><span id="line-64"></span><span class="hs-comment">-- &gt;&gt;&gt; fromTensor @Float t'</span><span>
</span><span id="line-65"></span><span class="hs-comment">-- 0.345714</span><span>
</span><span id="line-66"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew"><span class="hs-identifier hs-type">geluNew</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-67"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688965"><span class="annot"><a href="#local-6989586621679688965"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688964"><span class="annot"><a href="#local-6989586621679688964"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688963"><span class="annot"><a href="#local-6989586621679688963"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688962"><span class="annot"><a href="#local-6989586621679688962"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688961"><span class="annot"><a href="#local-6989586621679688961"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688960"><span class="annot"><a href="#local-6989586621679688960"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-68"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688960"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-69"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-70"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688965"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688964"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688963"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688962"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688961"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-71"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-72"></span><span>  </span><span class="annot"><a href="#local-6989586621679688960"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688965"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688964"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688963"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688962"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688961"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-73"></span><span id="geluNew"><span class="annot"><span class="annottext">geluNew :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew"><span class="hs-identifier hs-var hs-var">geluNew</span></a></span></span><span> </span><span id="local-6989586621679688959"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688959"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-74"></span><span>  </span><span id="local-6989586621679688958"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688958"><span class="hs-identifier hs-var">xHalfed</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688959"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Float -&gt; m (Tensor gradient layout device dataType shape)
forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-operator hs-var">`mulScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">0.5</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span>
</span><span id="line-75"></span><span>  </span><span id="local-6989586621679688957"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688957"><span class="hs-identifier hs-var">xCubed</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688959"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Float -&gt; m (Tensor gradient layout device dataType shape)
forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#powScalar"><span class="hs-operator hs-var">`powScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">3.0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span>
</span><span id="line-76"></span><span>  </span><span id="local-6989586621679688956"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688956"><span class="hs-identifier hs-var">xCubedScaled</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688957"><span class="hs-identifier hs-var">xCubed</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Float -&gt; m (Tensor gradient layout device dataType shape)
forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-operator hs-var">`mulScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">0.044715</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span>
</span><span id="line-77"></span><span>  </span><span id="local-6989586621679688955"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688955"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688959"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688956"><span class="hs-identifier hs-var">xCubedScaled</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Float -&gt; m (Tensor gradient layout device dataType shape)
forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-operator hs-var">`mulScalar`</span></a></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">Prelude.sqrt</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="annottext">Float
forall a. Floating a =&gt; a
</span><span class="hs-identifier hs-var">Prelude.pi</span></span><span class="hs-special">)</span><span>
</span><span id="line-78"></span><span>  </span><span id="local-6989586621679688954"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688954"><span class="hs-identifier hs-var">x''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#tanh"><span class="hs-identifier hs-var">tanh</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688955"><span class="hs-identifier hs-var">x'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Float -&gt; m (Tensor gradient layout device dataType shape)
forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#addScalar"><span class="hs-operator hs-var">`addScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span>
</span><span id="line-79"></span><span>  </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688958"><span class="hs-identifier hs-var">xHalfed</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688954"><span class="hs-identifier hs-var">x''</span></a></span><span>
</span><span id="line-80"></span><span>
</span><span id="line-81"></span><span class="hs-comment">-- | Applies the HardTanh function element-wise.</span><span>
</span><span id="line-82"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardtanh"><span class="hs-identifier hs-type">hardtanh</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-83"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688952"><span class="annot"><a href="#local-6989586621679688952"><span class="hs-identifier hs-type">minValue</span></a></span></span><span> </span><span id="local-6989586621679688951"><span class="annot"><a href="#local-6989586621679688951"><span class="hs-identifier hs-type">maxValue</span></a></span></span><span> </span><span id="local-6989586621679688950"><span class="annot"><a href="#local-6989586621679688950"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688949"><span class="annot"><a href="#local-6989586621679688949"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688948"><span class="annot"><a href="#local-6989586621679688948"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688947"><span class="annot"><a href="#local-6989586621679688947"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688946"><span class="annot"><a href="#local-6989586621679688946"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688945"><span class="annot"><a href="#local-6989586621679688945"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-84"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688952"><span class="hs-identifier hs-type">minValue</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688951"><span class="hs-identifier hs-type">maxValue</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688945"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-85"></span><span>  </span><span class="hs-comment">-- | minimum value</span><span>
</span><span id="line-86"></span><span>  </span><span class="annot"><a href="#local-6989586621679688952"><span class="hs-identifier hs-type">minValue</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-87"></span><span>  </span><span class="hs-comment">-- | maximum value</span><span>
</span><span id="line-88"></span><span>  </span><span class="annot"><a href="#local-6989586621679688951"><span class="hs-identifier hs-type">maxValue</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-89"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-90"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688950"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688949"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688948"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688947"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688946"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-91"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-92"></span><span>  </span><span class="annot"><a href="#local-6989586621679688945"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688950"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688949"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688948"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688947"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688946"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-93"></span><span id="hardtanh"><span class="annot"><span class="annottext">hardtanh :: minValue
-&gt; maxValue
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardtanh"><span class="hs-identifier hs-var hs-var">hardtanh</span></a></span></span><span> </span><span id="local-6989586621679688944"><span class="annot"><span class="annottext">minValue
</span><a href="#local-6989586621679688944"><span class="hs-identifier hs-var">minValue</span></a></span></span><span> </span><span id="local-6989586621679688943"><span class="annot"><span class="annottext">maxValue
</span><a href="#local-6989586621679688943"><span class="hs-identifier hs-var">maxValue</span></a></span></span><span> </span><span id="local-6989586621679688942"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688942"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; ForeignPtr Scalar
 -&gt; ForeignPtr Scalar
 -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; minValue
-&gt; maxValue
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 x2 cx2 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast3</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr Scalar -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hardtanh_tss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688942"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">minValue
</span><a href="#local-6989586621679688944"><span class="hs-identifier hs-var">minValue</span></a></span><span> </span><span class="annot"><span class="annottext">maxValue
</span><a href="#local-6989586621679688943"><span class="hs-identifier hs-var">maxValue</span></a></span><span>
</span><span id="line-94"></span><span>
</span><span id="line-95"></span><span class="hs-comment">-- | Applies the hardswish function element-wise.</span><span>
</span><span id="line-96"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardswish"><span class="hs-identifier hs-type">hardswish</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-97"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688939"><span class="annot"><a href="#local-6989586621679688939"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688938"><span class="annot"><a href="#local-6989586621679688938"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688937"><span class="annot"><a href="#local-6989586621679688937"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688936"><span class="annot"><a href="#local-6989586621679688936"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688935"><span class="annot"><a href="#local-6989586621679688935"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-98"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-99"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688939"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688938"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688937"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688936"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688935"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-100"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-101"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688939"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688938"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688937"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688936"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688935"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-102"></span><span id="hardswish"><span class="annot"><span class="annottext">hardswish :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardswish"><span class="hs-identifier hs-var hs-var">hardswish</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; Tensor gradient layout device dataType shape
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; Tensor gradient layout device dataType shape)
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hardswish_t</span></a></span><span>
</span><span id="line-103"></span><span>
</span><span id="line-104"></span><span class="hs-comment">-- | Applies the exponential linear unit function element-wise, with alpha input,</span><span>
</span><span id="line-105"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-106"></span><span class="hs-comment">-- \text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1)).</span><span>
</span><span id="line-107"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-108"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#elu"><span class="hs-identifier hs-type">elu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-109"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688932"><span class="annot"><a href="#local-6989586621679688932"><span class="hs-identifier hs-type">alpha</span></a></span></span><span> </span><span id="local-6989586621679688931"><span class="annot"><a href="#local-6989586621679688931"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688930"><span class="annot"><a href="#local-6989586621679688930"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688929"><span class="annot"><a href="#local-6989586621679688929"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688928"><span class="annot"><a href="#local-6989586621679688928"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688927"><span class="annot"><a href="#local-6989586621679688927"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688926"><span class="annot"><a href="#local-6989586621679688926"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688932"><span class="hs-identifier hs-type">alpha</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688926"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-111"></span><span>  </span><span class="hs-comment">-- | alpha value for ELU formulation</span><span>
</span><span id="line-112"></span><span>  </span><span class="annot"><a href="#local-6989586621679688932"><span class="hs-identifier hs-type">alpha</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-113"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-114"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688931"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688930"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688929"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688928"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688927"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-115"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-116"></span><span>  </span><span class="annot"><a href="#local-6989586621679688926"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688931"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688930"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688929"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688928"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688927"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-117"></span><span id="elu"><span class="annot"><span class="annottext">elu :: alpha
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#elu"><span class="hs-identifier hs-var hs-var">elu</span></a></span></span><span> </span><span id="local-6989586621679688925"><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679688925"><span class="hs-identifier hs-var">alpha</span></a></span></span><span> </span><span id="local-6989586621679688924"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688924"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; alpha
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.elu_ts</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688924"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679688925"><span class="hs-identifier hs-var">alpha</span></a></span><span>
</span><span id="line-118"></span><span>
</span><span id="line-119"></span><span class="hs-comment">-- | Applies the scaled exponential linear unit function element-wise, that is,</span><span>
</span><span id="line-120"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-121"></span><span class="hs-comment">-- \text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)),</span><span>
</span><span id="line-122"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-123"></span><span class="hs-comment">-- with \(\alpha = 1.6732632423543772848170429916717\)</span><span>
</span><span id="line-124"></span><span class="hs-comment">-- and \(\text{scale}=1.0507009873554804934193349852946\).</span><span>
</span><span id="line-125"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#selu"><span class="hs-identifier hs-type">selu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-126"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688921"><span class="annot"><a href="#local-6989586621679688921"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688920"><span class="annot"><a href="#local-6989586621679688920"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688919"><span class="annot"><a href="#local-6989586621679688919"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688918"><span class="annot"><a href="#local-6989586621679688918"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688917"><span class="annot"><a href="#local-6989586621679688917"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688916"><span class="annot"><a href="#local-6989586621679688916"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-127"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688916"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-128"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-129"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688921"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688920"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688919"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688918"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688917"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-130"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-131"></span><span>  </span><span class="annot"><a href="#local-6989586621679688916"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688921"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688920"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688919"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688918"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688917"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-132"></span><span id="selu"><span class="annot"><span class="annottext">selu :: Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#selu"><span class="hs-identifier hs-var hs-var">selu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; (Tensor gradient layout device dataType shape
    -&gt; IO (Tensor gradient layout device dataType shape))
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.selu_t</span></a></span><span>
</span><span id="line-133"></span><span>
</span><span id="line-134"></span><span class="hs-comment">-- | Applies the continuously differentiable exponential linear unit function element-wise, that is,</span><span>
</span><span id="line-135"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-136"></span><span class="hs-comment">-- \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1)).</span><span>
</span><span id="line-137"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-138"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#celu"><span class="hs-identifier hs-type">celu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-139"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688913"><span class="annot"><a href="#local-6989586621679688913"><span class="hs-identifier hs-type">alpha</span></a></span></span><span> </span><span id="local-6989586621679688912"><span class="annot"><a href="#local-6989586621679688912"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688911"><span class="annot"><a href="#local-6989586621679688911"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688910"><span class="annot"><a href="#local-6989586621679688910"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688909"><span class="annot"><a href="#local-6989586621679688909"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688908"><span class="annot"><a href="#local-6989586621679688908"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688907"><span class="annot"><a href="#local-6989586621679688907"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-140"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688913"><span class="hs-identifier hs-type">alpha</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688907"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-141"></span><span>  </span><span class="hs-comment">-- | alpha</span><span>
</span><span id="line-142"></span><span>  </span><span class="annot"><a href="#local-6989586621679688913"><span class="hs-identifier hs-type">alpha</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-143"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-144"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688912"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688911"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688910"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688909"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688908"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-145"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-146"></span><span>  </span><span class="annot"><a href="#local-6989586621679688907"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688912"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688911"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688910"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688909"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688908"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-147"></span><span id="celu"><span class="annot"><span class="annottext">celu :: alpha
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#celu"><span class="hs-identifier hs-var hs-var">celu</span></a></span></span><span> </span><span id="local-6989586621679688906"><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679688906"><span class="hs-identifier hs-var">alpha</span></a></span></span><span> </span><span id="local-6989586621679688905"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688905"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; alpha
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.celu_ts</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688905"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679688906"><span class="hs-identifier hs-var">alpha</span></a></span><span>
</span><span id="line-148"></span><span>
</span><span id="line-149"></span><span class="hs-comment">-- | Applies the element-wise function:</span><span>
</span><span id="line-150"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-151"></span><span class="hs-comment">-- \text{LeakyReLU}(x) = \max(0,x) + \text{negativeSlope} * \min(0,x),</span><span>
</span><span id="line-152"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-153"></span><span class="hs-comment">-- the the angle of the negative slope can be controlled.</span><span>
</span><span id="line-154"></span><span class="hs-comment">-- A typical value for it is 0.01.</span><span>
</span><span id="line-155"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#leakyRelu"><span class="hs-identifier hs-type">leakyRelu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-156"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688902"><span class="annot"><a href="#local-6989586621679688902"><span class="hs-identifier hs-type">negativeSlope</span></a></span></span><span> </span><span id="local-6989586621679688901"><span class="annot"><a href="#local-6989586621679688901"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688900"><span class="annot"><a href="#local-6989586621679688900"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688899"><span class="annot"><a href="#local-6989586621679688899"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688898"><span class="annot"><a href="#local-6989586621679688898"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688897"><span class="annot"><a href="#local-6989586621679688897"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688896"><span class="annot"><a href="#local-6989586621679688896"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-157"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688902"><span class="hs-identifier hs-type">negativeSlope</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688896"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-158"></span><span>  </span><span class="hs-comment">-- | negative slope</span><span>
</span><span id="line-159"></span><span>  </span><span class="annot"><a href="#local-6989586621679688902"><span class="hs-identifier hs-type">negativeSlope</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-160"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-161"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688901"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688900"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688899"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688898"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688897"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-162"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-163"></span><span>  </span><span class="annot"><a href="#local-6989586621679688896"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688901"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688900"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688899"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688898"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688897"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-164"></span><span id="leakyRelu"><span class="annot"><span class="annottext">leakyRelu :: negativeSlope
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#leakyRelu"><span class="hs-identifier hs-var hs-var">leakyRelu</span></a></span></span><span> </span><span id="local-6989586621679688895"><span class="annot"><span class="annottext">negativeSlope
</span><a href="#local-6989586621679688895"><span class="hs-identifier hs-var">negativeSlope</span></a></span></span><span> </span><span id="local-6989586621679688894"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688894"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; negativeSlope
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.leaky_relu_ts</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688894"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">negativeSlope
</span><a href="#local-6989586621679688895"><span class="hs-identifier hs-var">negativeSlope</span></a></span><span>
</span><span id="line-165"></span><span>
</span><span id="line-166"></span><span class="hs-comment">-- | Applies the parameterized rectified linear unit function element-wise, that is,</span><span>
</span><span id="line-167"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-168"></span><span class="hs-comment">-- \text{PReLU}(x) = max(0, x) + \text{weight} * min(0, x).</span><span>
</span><span id="line-169"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-170"></span><span class="hs-comment">-- The weight parameter is typically learnable.</span><span>
</span><span id="line-171"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#prelu"><span class="hs-identifier hs-type">prelu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-172"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679688891"><span class="annot"><a href="#local-6989586621679688891"><span class="hs-identifier hs-type">gradient'</span></a></span></span><span> </span><span id="local-6989586621679688890"><span class="annot"><a href="#local-6989586621679688890"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679688889"><span class="annot"><a href="#local-6989586621679688889"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679688888"><span class="annot"><a href="#local-6989586621679688888"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679688887"><span class="annot"><a href="#local-6989586621679688887"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679688886"><span class="annot"><a href="#local-6989586621679688886"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679688885"><span class="annot"><a href="#local-6989586621679688885"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-173"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">MonadThrow</span></span><span> </span><span class="annot"><a href="#local-6989586621679688885"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-174"></span><span>  </span><span class="hs-comment">-- | weight (typically learnable)</span><span>
</span><span id="line-175"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688891"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688889"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688888"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688887"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688886"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-176"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-177"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688890"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688889"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688888"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688887"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688886"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-178"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-179"></span><span>  </span><span class="annot"><a href="#local-6989586621679688885"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688890"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688889"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688888"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688887"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679688886"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-180"></span><span id="prelu"><span class="annot"><span class="annottext">prelu :: Tensor gradient' layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#prelu"><span class="hs-identifier hs-var hs-var">prelu</span></a></span></span><span> </span><span id="local-6989586621679688884"><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679688884"><span class="hs-identifier hs-var">weight</span></a></span></span><span> </span><span id="local-6989586621679688883"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688883"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor gradient layout device dataType shape)
 -&gt; m (Tensor gradient layout device dataType shape))
-&gt; IO (Tensor gradient layout device dataType shape)
-&gt; m (Tensor gradient layout device dataType shape)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient' layout device dataType shape
-&gt; IO (Tensor gradient layout device dataType shape)
forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.prelu_tt</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679688883"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679688884"><span class="hs-identifier hs-var">weight</span></a></span><span>
</span><span id="line-181"></span></pre></body></html>