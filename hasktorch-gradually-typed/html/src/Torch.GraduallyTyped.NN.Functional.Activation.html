<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><span id="line-2"></span><span>
</span><span id="line-3"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.GraduallyTyped.NN.Functional.Activation</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-4"></span><span>
</span><span id="line-5"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier">Control.Monad.Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier">MonadThrow</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-6"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">System.IO.Unsafe</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">unsafePerformIO</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-7"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html"><span class="hs-identifier">Torch.GraduallyTyped.Tensor.MathOperations.Pointwise</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#addScalar"><span class="hs-identifier">addScalar</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-identifier">mulScalar</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#powScalar"><span class="hs-identifier">powScalar</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#tanh"><span class="hs-identifier">tanh</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-8"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html"><span class="hs-identifier">Torch.GraduallyTyped.Tensor.Type</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier">Tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-9"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast1</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast2</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">cast3</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-10"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Native</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-11"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Torch.Scalar</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier">Scalar</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-12"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">Prelude</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/ghc-prim-0.8.0/src"><span class="hs-identifier">Float</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">pure</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator">($)</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator">(*)</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator">(+)</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator">(.)</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator">(/)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-13"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">Prelude</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">pi</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier">sqrt</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-14"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.GC</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">unsafeThrowableIO</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-15"></span><span>
</span><span id="line-16"></span><span class="hs-comment">-- $setup</span><span>
</span><span id="line-17"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch.GraduallyTyped.Prelude.List (SList (..))</span><span>
</span><span id="line-18"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch.GraduallyTyped</span><span>
</span><span id="line-19"></span><span>
</span><span id="line-20"></span><span class="hs-comment">-- | Thresholds each element of the input Tensor.</span><span>
</span><span id="line-21"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#threshold"><span class="hs-identifier hs-type">threshold</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-22"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694294"><span class="annot"><a href="#local-6989586621679694294"><span class="hs-identifier hs-type">threshold</span></a></span></span><span> </span><span id="local-6989586621679694292"><span class="annot"><a href="#local-6989586621679694292"><span class="hs-identifier hs-type">value</span></a></span></span><span> </span><span id="local-6989586621679694289"><span class="annot"><a href="#local-6989586621679694289"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694288"><span class="annot"><a href="#local-6989586621679694288"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694287"><span class="annot"><a href="#local-6989586621679694287"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694286"><span class="annot"><a href="#local-6989586621679694286"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694285"><span class="annot"><a href="#local-6989586621679694285"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679694291"><span class="annot"><a href="#local-6989586621679694291"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-23"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694294"><span class="hs-identifier hs-type">threshold</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694292"><span class="hs-identifier hs-type">value</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694291"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-24"></span><span>  </span><span class="hs-comment">-- | threshold</span><span>
</span><span id="line-25"></span><span>  </span><span class="annot"><a href="#local-6989586621679694294"><span class="hs-identifier hs-type">threshold</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-26"></span><span>  </span><span class="hs-comment">-- | value</span><span>
</span><span id="line-27"></span><span>  </span><span class="annot"><a href="#local-6989586621679694292"><span class="hs-identifier hs-type">value</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-28"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-29"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694289"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694288"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694287"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694286"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694285"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-30"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-31"></span><span>  </span><span class="annot"><a href="#local-6989586621679694291"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694289"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694288"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694287"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694286"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694285"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-32"></span><span id="threshold"><span class="annot"><span class="annottext">threshold :: forall threshold value (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar threshold, Scalar value, MonadThrow m) =&gt;
threshold
-&gt; value
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#threshold"><span class="hs-identifier hs-var hs-var">threshold</span></a></span></span><span> </span><span id="local-6989586621679694102"><span class="annot"><span class="annottext">threshold
</span><a href="#local-6989586621679694102"><span class="hs-identifier hs-var">thresholdValue</span></a></span></span><span> </span><span id="local-6989586621679694101"><span class="annot"><span class="annottext">value
</span><a href="#local-6989586621679694101"><span class="hs-identifier hs-var">value</span></a></span></span><span> </span><span id="local-6989586621679694100"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694100"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-33"></span><span>  </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca x1 cx1 x2 cx2 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast3</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr Scalar -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.threshold_tss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694100"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">threshold
</span><a href="#local-6989586621679694102"><span class="hs-identifier hs-var">thresholdValue</span></a></span><span> </span><span class="annot"><span class="annottext">value
</span><a href="#local-6989586621679694101"><span class="hs-identifier hs-var">value</span></a></span><span>
</span><span id="line-34"></span><span>
</span><span id="line-35"></span><span class="hs-comment">-- | Applies the rectified linear unit function element-wise, that is,</span><span>
</span><span id="line-36"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-37"></span><span class="hs-comment">-- \text{ReLU}(x) = max(0, x).</span><span>
</span><span id="line-38"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-39"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#relu"><span class="hs-identifier hs-type">relu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-40"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694252"><span class="annot"><a href="#local-6989586621679694252"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694251"><span class="annot"><a href="#local-6989586621679694251"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694250"><span class="annot"><a href="#local-6989586621679694250"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694249"><span class="annot"><a href="#local-6989586621679694249"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694248"><span class="annot"><a href="#local-6989586621679694248"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-41"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-42"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694252"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694251"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694250"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694249"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694248"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-43"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-44"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694252"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694251"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694250"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694249"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694248"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-45"></span><span id="relu"><span class="annot"><span class="annottext">relu :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#relu"><span class="hs-identifier hs-var hs-var">relu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a. IO a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier hs-var">unsafePerformIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">.</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.relu_t</span></a></span><span>
</span><span id="line-46"></span><span>
</span><span id="line-47"></span><span class="hs-comment">-- | Applies the gaussian error linear unit function element-wise.</span><span>
</span><span id="line-48"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#gelu"><span class="hs-identifier hs-type">gelu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-49"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694093"><span class="annot"><a href="#local-6989586621679694093"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694092"><span class="annot"><a href="#local-6989586621679694092"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694091"><span class="annot"><a href="#local-6989586621679694091"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694090"><span class="annot"><a href="#local-6989586621679694090"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694089"><span class="annot"><a href="#local-6989586621679694089"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-50"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-51"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694093"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694092"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694091"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694090"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694089"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-52"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-53"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694093"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694092"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694091"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694090"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694089"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-54"></span><span id="gelu"><span class="annot"><span class="annottext">gelu :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#gelu"><span class="hs-identifier hs-var hs-var">gelu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a. IO a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier hs-var">unsafePerformIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">.</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.gelu_t</span></a></span><span>
</span><span id="line-55"></span><span>
</span><span id="line-56"></span><span class="hs-comment">-- | Applies the gaussian error linear unit function element-wise.</span><span>
</span><span id="line-57"></span><span class="hs-comment">--</span><span>
</span><span id="line-58"></span><span class="hs-comment">-- This is the implementation of the GELU activation function from</span><span>
</span><span id="line-59"></span><span class="hs-comment">-- Google's BERT repo (and coincidentally also from OpenAI's GPT).</span><span>
</span><span id="line-60"></span><span class="hs-comment">-- See also https://arxiv.org/abs/1606.08415.</span><span>
</span><span id="line-61"></span><span class="hs-comment">--</span><span>
</span><span id="line-62"></span><span class="hs-comment">-- &gt;&gt;&gt; t &lt;- sFull (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNil)) 0.5</span><span>
</span><span id="line-63"></span><span class="hs-comment">-- &gt;&gt;&gt; t' &lt;- geluNew t</span><span>
</span><span id="line-64"></span><span class="hs-comment">-- &gt;&gt;&gt; fromTensor @Float t'</span><span>
</span><span id="line-65"></span><span class="hs-comment">-- 0.345714</span><span>
</span><span id="line-66"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew"><span class="hs-identifier hs-type">geluNew</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-67"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694228"><span class="annot"><a href="#local-6989586621679694228"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694227"><span class="annot"><a href="#local-6989586621679694227"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694226"><span class="annot"><a href="#local-6989586621679694226"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694225"><span class="annot"><a href="#local-6989586621679694225"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694224"><span class="annot"><a href="#local-6989586621679694224"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679694229"><span class="annot"><a href="#local-6989586621679694229"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-68"></span><span>  </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694229"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-69"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-70"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694228"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694227"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694226"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694225"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694224"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-71"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-72"></span><span>  </span><span class="annot"><a href="#local-6989586621679694229"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694228"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694227"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694226"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694225"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694224"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-73"></span><span id="geluNew"><span class="annot"><span class="annottext">geluNew :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
MonadThrow m =&gt;
Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew"><span class="hs-identifier hs-var hs-var">geluNew</span></a></span></span><span> </span><span id="local-6989586621679694054"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694054"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-74"></span><span>  </span><span id="local-6989586621679694053"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694053"><span class="hs-identifier hs-var">xHalfed</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694054"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-operator hs-var">`mulScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">0.5</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/ghc-prim-0.8.0/src"><span class="hs-identifier hs-type">Float</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-75"></span><span>  </span><span id="local-6989586621679694052"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694052"><span class="hs-identifier hs-var">xCubed</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694054"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#powScalar"><span class="hs-operator hs-var">`powScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">3.0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/ghc-prim-0.8.0/src"><span class="hs-identifier hs-type">Float</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-76"></span><span>  </span><span id="local-6989586621679694051"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694051"><span class="hs-identifier hs-var">xCubedScaled</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694052"><span class="hs-identifier hs-var">xCubed</span></a></span><span> </span><span class="annot"><span class="annottext">forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-operator hs-var">`mulScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">0.044715</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/ghc-prim-0.8.0/src"><span class="hs-identifier hs-type">Float</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-77"></span><span>  </span><span id="local-6989586621679694050"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694050"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694054"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">forall a. Num a =&gt; a -&gt; a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">+</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694051"><span class="hs-identifier hs-var">xCubedScaled</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#mulScalar"><span class="hs-operator hs-var">`mulScalar`</span></a></span><span> </span><span class="annot"><span class="annottext">forall a. Floating a =&gt; a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier hs-var">Prelude.sqrt</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/ghc-prim-0.8.0/src"><span class="hs-identifier hs-type">Float</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">/</span></a></span><span> </span><span class="annot"><span class="annottext">forall a. Floating a =&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier hs-var">Prelude.pi</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-78"></span><span>  </span><span id="local-6989586621679694049"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694049"><span class="hs-identifier hs-var">x''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#tanh"><span class="hs-identifier hs-var">tanh</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694050"><span class="hs-identifier hs-var">x'</span></a></span><span> </span><span class="annot"><span class="annottext">forall other (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar other, MonadThrow m) =&gt;
Tensor gradient layout device dataType shape
-&gt; other -&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#addScalar"><span class="hs-operator hs-var">`addScalar`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/ghc-prim-0.8.0/src"><span class="hs-identifier hs-type">Float</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-79"></span><span>  </span><span class="annot"><span class="annottext">forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier hs-var">pure</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694053"><span class="hs-identifier hs-var">xHalfed</span></a></span><span> </span><span class="annot"><span class="annottext">forall a. Num a =&gt; a -&gt; a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">*</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694049"><span class="hs-identifier hs-var">x''</span></a></span><span>
</span><span id="line-80"></span><span>
</span><span id="line-81"></span><span class="hs-comment">-- | Applies the HardTanh function element-wise.</span><span>
</span><span id="line-82"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardtanh"><span class="hs-identifier hs-type">hardtanh</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-83"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694047"><span class="annot"><a href="#local-6989586621679694047"><span class="hs-identifier hs-type">minValue</span></a></span></span><span> </span><span id="local-6989586621679694046"><span class="annot"><a href="#local-6989586621679694046"><span class="hs-identifier hs-type">maxValue</span></a></span></span><span> </span><span id="local-6989586621679694045"><span class="annot"><a href="#local-6989586621679694045"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694044"><span class="annot"><a href="#local-6989586621679694044"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694043"><span class="annot"><a href="#local-6989586621679694043"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694042"><span class="annot"><a href="#local-6989586621679694042"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694041"><span class="annot"><a href="#local-6989586621679694041"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679694040"><span class="annot"><a href="#local-6989586621679694040"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-84"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694047"><span class="hs-identifier hs-type">minValue</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694046"><span class="hs-identifier hs-type">maxValue</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694040"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-85"></span><span>  </span><span class="hs-comment">-- | minimum value</span><span>
</span><span id="line-86"></span><span>  </span><span class="annot"><a href="#local-6989586621679694047"><span class="hs-identifier hs-type">minValue</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-87"></span><span>  </span><span class="hs-comment">-- | maximum value</span><span>
</span><span id="line-88"></span><span>  </span><span class="annot"><a href="#local-6989586621679694046"><span class="hs-identifier hs-type">maxValue</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-89"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-90"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694045"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694044"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694043"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694042"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694041"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-91"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-92"></span><span>  </span><span class="annot"><a href="#local-6989586621679694040"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694045"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694044"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694043"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694042"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694041"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-93"></span><span id="hardtanh"><span class="annot"><span class="annottext">hardtanh :: forall threshold value (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar threshold, Scalar value, MonadThrow m) =&gt;
threshold
-&gt; value
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardtanh"><span class="hs-identifier hs-var hs-var">hardtanh</span></a></span></span><span> </span><span id="local-6989586621679694029"><span class="annot"><span class="annottext">minValue
</span><a href="#local-6989586621679694029"><span class="hs-identifier hs-var">minValue</span></a></span></span><span> </span><span id="local-6989586621679694028"><span class="annot"><span class="annottext">maxValue
</span><a href="#local-6989586621679694028"><span class="hs-identifier hs-var">maxValue</span></a></span></span><span> </span><span id="local-6989586621679694027"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694027"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca x1 cx1 x2 cx2 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast3</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr Scalar -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hardtanh_tss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694027"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">minValue
</span><a href="#local-6989586621679694029"><span class="hs-identifier hs-var">minValue</span></a></span><span> </span><span class="annot"><span class="annottext">maxValue
</span><a href="#local-6989586621679694028"><span class="hs-identifier hs-var">maxValue</span></a></span><span>
</span><span id="line-94"></span><span>
</span><span id="line-95"></span><span class="hs-comment">-- | Applies the hardswish function element-wise.</span><span>
</span><span id="line-96"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardswish"><span class="hs-identifier hs-type">hardswish</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-97"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694024"><span class="annot"><a href="#local-6989586621679694024"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694023"><span class="annot"><a href="#local-6989586621679694023"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694022"><span class="annot"><a href="#local-6989586621679694022"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694021"><span class="annot"><a href="#local-6989586621679694021"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694020"><span class="annot"><a href="#local-6989586621679694020"><span class="hs-identifier hs-type">shape</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-98"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-99"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694024"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694023"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694022"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694021"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694020"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-100"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-101"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694024"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694023"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694022"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694021"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694020"><span class="hs-identifier hs-type">shape</span></a></span><span>
</span><span id="line-102"></span><span id="hardswish"><span class="annot"><span class="annottext">hardswish :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#hardswish"><span class="hs-identifier hs-var hs-var">hardswish</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a. IO a -&gt; a
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-identifier hs-var">unsafePerformIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">.</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.hardswish_t</span></a></span><span>
</span><span id="line-103"></span><span>
</span><span id="line-104"></span><span class="hs-comment">-- | Applies the exponential linear unit function element-wise, with alpha input,</span><span>
</span><span id="line-105"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-106"></span><span class="hs-comment">-- \text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1)).</span><span>
</span><span id="line-107"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-108"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#elu"><span class="hs-identifier hs-type">elu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-109"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694184"><span class="annot"><a href="#local-6989586621679694184"><span class="hs-identifier hs-type">alpha</span></a></span></span><span> </span><span id="local-6989586621679694182"><span class="annot"><a href="#local-6989586621679694182"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694181"><span class="annot"><a href="#local-6989586621679694181"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694180"><span class="annot"><a href="#local-6989586621679694180"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694179"><span class="annot"><a href="#local-6989586621679694179"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694178"><span class="annot"><a href="#local-6989586621679694178"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679694183"><span class="annot"><a href="#local-6989586621679694183"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694184"><span class="hs-identifier hs-type">alpha</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694183"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-111"></span><span>  </span><span class="hs-comment">-- | alpha value for ELU formulation</span><span>
</span><span id="line-112"></span><span>  </span><span class="annot"><a href="#local-6989586621679694184"><span class="hs-identifier hs-type">alpha</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-113"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-114"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694182"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694181"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694180"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694179"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694178"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-115"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-116"></span><span>  </span><span class="annot"><a href="#local-6989586621679694183"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694182"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694181"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694180"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694179"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694178"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-117"></span><span id="elu"><span class="annot"><span class="annottext">elu :: forall alpha (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar alpha, MonadThrow m) =&gt;
alpha
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#elu"><span class="hs-identifier hs-var hs-var">elu</span></a></span></span><span> </span><span id="local-6989586621679694008"><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679694008"><span class="hs-identifier hs-var">alpha</span></a></span></span><span> </span><span id="local-6989586621679694007"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694007"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.elu_ts</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679694007"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679694008"><span class="hs-identifier hs-var">alpha</span></a></span><span>
</span><span id="line-118"></span><span>
</span><span id="line-119"></span><span class="hs-comment">-- | Applies the scaled exponential linear unit function element-wise, that is,</span><span>
</span><span id="line-120"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-121"></span><span class="hs-comment">-- \text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)),</span><span>
</span><span id="line-122"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-123"></span><span class="hs-comment">-- with \(\alpha = 1.6732632423543772848170429916717\)</span><span>
</span><span id="line-124"></span><span class="hs-comment">-- and \(\text{scale}=1.0507009873554804934193349852946\).</span><span>
</span><span id="line-125"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#selu"><span class="hs-identifier hs-type">selu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-126"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694004"><span class="annot"><a href="#local-6989586621679694004"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694003"><span class="annot"><a href="#local-6989586621679694003"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694002"><span class="annot"><a href="#local-6989586621679694002"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694001"><span class="annot"><a href="#local-6989586621679694001"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694000"><span class="annot"><a href="#local-6989586621679694000"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679693999"><span class="annot"><a href="#local-6989586621679693999"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-127"></span><span>  </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693999"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-128"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-129"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694004"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694003"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694002"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694001"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694000"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-130"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-131"></span><span>  </span><span class="annot"><a href="#local-6989586621679693999"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694004"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694003"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694002"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694001"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694000"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-132"></span><span id="selu"><span class="annot"><span class="annottext">selu :: forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
MonadThrow m =&gt;
Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#selu"><span class="hs-identifier hs-var hs-var">selu</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">.</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.selu_t</span></a></span><span>
</span><span id="line-133"></span><span>
</span><span id="line-134"></span><span class="hs-comment">-- | Applies the continuously differentiable exponential linear unit function element-wise, that is,</span><span>
</span><span id="line-135"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-136"></span><span class="hs-comment">-- \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1)).</span><span>
</span><span id="line-137"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-138"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#celu"><span class="hs-identifier hs-type">celu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-139"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679693992"><span class="annot"><a href="#local-6989586621679693992"><span class="hs-identifier hs-type">alpha</span></a></span></span><span> </span><span id="local-6989586621679693991"><span class="annot"><a href="#local-6989586621679693991"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679693990"><span class="annot"><a href="#local-6989586621679693990"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679693989"><span class="annot"><a href="#local-6989586621679693989"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679693988"><span class="annot"><a href="#local-6989586621679693988"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679693987"><span class="annot"><a href="#local-6989586621679693987"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679693986"><span class="annot"><a href="#local-6989586621679693986"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-140"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693992"><span class="hs-identifier hs-type">alpha</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693986"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-141"></span><span>  </span><span class="hs-comment">-- | alpha</span><span>
</span><span id="line-142"></span><span>  </span><span class="annot"><a href="#local-6989586621679693992"><span class="hs-identifier hs-type">alpha</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-143"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-144"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693991"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693990"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693989"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693988"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693987"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-145"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-146"></span><span>  </span><span class="annot"><a href="#local-6989586621679693986"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693991"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693990"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693989"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693988"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693987"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-147"></span><span id="celu"><span class="annot"><span class="annottext">celu :: forall alpha (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar alpha, MonadThrow m) =&gt;
alpha
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#celu"><span class="hs-identifier hs-var hs-var">celu</span></a></span></span><span> </span><span id="local-6989586621679693978"><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679693978"><span class="hs-identifier hs-var">alpha</span></a></span></span><span> </span><span id="local-6989586621679693977"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679693977"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.celu_ts</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679693977"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">alpha
</span><a href="#local-6989586621679693978"><span class="hs-identifier hs-var">alpha</span></a></span><span>
</span><span id="line-148"></span><span>
</span><span id="line-149"></span><span class="hs-comment">-- | Applies the element-wise function:</span><span>
</span><span id="line-150"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-151"></span><span class="hs-comment">-- \text{LeakyReLU}(x) = \max(0,x) + \text{negativeSlope} * \min(0,x),</span><span>
</span><span id="line-152"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-153"></span><span class="hs-comment">-- the the angle of the negative slope can be controlled.</span><span>
</span><span id="line-154"></span><span class="hs-comment">-- A typical value for it is 0.01.</span><span>
</span><span id="line-155"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#leakyRelu"><span class="hs-identifier hs-type">leakyRelu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-156"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679693974"><span class="annot"><a href="#local-6989586621679693974"><span class="hs-identifier hs-type">negativeSlope</span></a></span></span><span> </span><span id="local-6989586621679693973"><span class="annot"><a href="#local-6989586621679693973"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679693972"><span class="annot"><a href="#local-6989586621679693972"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679693971"><span class="annot"><a href="#local-6989586621679693971"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679693970"><span class="annot"><a href="#local-6989586621679693970"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679693969"><span class="annot"><a href="#local-6989586621679693969"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679693968"><span class="annot"><a href="#local-6989586621679693968"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-157"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Scalar</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693974"><span class="hs-identifier hs-type">negativeSlope</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693968"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-158"></span><span>  </span><span class="hs-comment">-- | negative slope</span><span>
</span><span id="line-159"></span><span>  </span><span class="annot"><a href="#local-6989586621679693974"><span class="hs-identifier hs-type">negativeSlope</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-160"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-161"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693973"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693972"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693971"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693970"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693969"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-162"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-163"></span><span>  </span><span class="annot"><a href="#local-6989586621679693968"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693973"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693972"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693971"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693970"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679693969"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-164"></span><span id="leakyRelu"><span class="annot"><span class="annottext">leakyRelu :: forall alpha (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
(Scalar alpha, MonadThrow m) =&gt;
alpha
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#leakyRelu"><span class="hs-identifier hs-var hs-var">leakyRelu</span></a></span></span><span> </span><span id="local-6989586621679693960"><span class="annot"><span class="annottext">negativeSlope
</span><a href="#local-6989586621679693960"><span class="hs-identifier hs-var">negativeSlope</span></a></span></span><span> </span><span id="local-6989586621679693959"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679693959"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Scalar -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.leaky_relu_ts</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679693959"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">negativeSlope
</span><a href="#local-6989586621679693960"><span class="hs-identifier hs-var">negativeSlope</span></a></span><span>
</span><span id="line-165"></span><span>
</span><span id="line-166"></span><span class="hs-comment">-- | Applies the parameterized rectified linear unit function element-wise, that is,</span><span>
</span><span id="line-167"></span><span class="hs-comment">-- \[</span><span>
</span><span id="line-168"></span><span class="hs-comment">-- \text{PReLU}(x) = max(0, x) + \text{weight} * min(0, x).</span><span>
</span><span id="line-169"></span><span class="hs-comment">-- \]</span><span>
</span><span id="line-170"></span><span class="hs-comment">-- The weight parameter is typically learnable.</span><span>
</span><span id="line-171"></span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#prelu"><span class="hs-identifier hs-type">prelu</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-172"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679694143"><span class="annot"><a href="#local-6989586621679694143"><span class="hs-identifier hs-type">gradient'</span></a></span></span><span> </span><span id="local-6989586621679694138"><span class="annot"><a href="#local-6989586621679694138"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679694142"><span class="annot"><a href="#local-6989586621679694142"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679694141"><span class="annot"><a href="#local-6989586621679694141"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679694140"><span class="annot"><a href="#local-6989586621679694140"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679694139"><span class="annot"><a href="#local-6989586621679694139"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679694144"><span class="annot"><a href="#local-6989586621679694144"><span class="hs-identifier hs-type">m</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-173"></span><span>  </span><span class="annot"><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/exceptions-0.10.4/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694144"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-174"></span><span>  </span><span class="hs-comment">-- | weight (typically learnable)</span><span>
</span><span id="line-175"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694143"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694142"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694141"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694140"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694139"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-176"></span><span>  </span><span class="hs-comment">-- | input</span><span>
</span><span id="line-177"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694138"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694142"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694141"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694140"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694139"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-178"></span><span>  </span><span class="hs-comment">-- | output</span><span>
</span><span id="line-179"></span><span>  </span><span class="annot"><a href="#local-6989586621679694144"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694138"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694142"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694141"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694140"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679694139"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-180"></span><span id="prelu"><span class="annot"><span class="annottext">prelu :: forall (gradient' :: Gradient RequiresGradient)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (m :: * -&gt; *).
MonadThrow m =&gt;
Tensor gradient' layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#prelu"><span class="hs-identifier hs-var hs-var">prelu</span></a></span></span><span> </span><span id="local-6989586621679693951"><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679693951"><span class="hs-identifier hs-var">weight</span></a></span></span><span> </span><span id="local-6989586621679693950"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679693950"><span class="hs-identifier hs-var">tensor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><a href="../file:///nix/store/9z106fx3296sgdwa5s05xi381ayi6qfk-ghc-9.2.4-doc/share/doc/ghc/html/libraries/base-4.16.3.0/src"><span class="hs-operator hs-var">$</span></a></span><span> </span><span class="annot"><span class="annottext">forall a ca x1 cx1 y cy.
(Castable a ca, Castable x1 cx1, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">cast2</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor -&gt; ForeignPtr Tensor -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.prelu_tt</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679693950"><span class="hs-identifier hs-var">tensor</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient' layout device dataType shape
</span><a href="#local-6989586621679693951"><span class="hs-identifier hs-var">weight</span></a></span><span>
</span><span id="line-181"></span></pre></body></html>