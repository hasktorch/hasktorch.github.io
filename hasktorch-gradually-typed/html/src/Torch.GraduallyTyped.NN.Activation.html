<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE DeriveGeneric #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE KindSignatures #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE TupleSections #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# OPTIONS_GHC -Wall #-}</span><span>
</span><span id="line-15"></span><span>
</span><span id="line-16"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.GraduallyTyped.NN.Activation</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-17"></span><span>
</span><span id="line-18"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.Generics</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-19"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Nat</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">Symbol</span></span><span class="hs-special">)</span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html"><span class="hs-identifier">Torch.GraduallyTyped.NN.Class</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier">HasInitialize</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier">HasStateDict</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html"><span class="hs-identifier">Torch.GraduallyTyped.NN.Functional.Activation</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#gelu"><span class="hs-identifier">gelu</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew"><span class="hs-identifier">geluNew</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#relu"><span class="hs-identifier">relu</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.NonLinearActivation.html"><span class="hs-identifier">Torch.GraduallyTyped.NN.Functional.NonLinearActivation</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.NonLinearActivation.html#SoftmaxF"><span class="hs-identifier">SoftmaxF</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.NonLinearActivation.html#softmax"><span class="hs-identifier">softmax</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.html"><span class="hs-identifier">Torch.GraduallyTyped.Shape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#By"><span class="hs-identifier">By</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SSelectDim"><span class="hs-identifier">SSelectDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier">SelectDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html"><span class="hs-identifier">Torch.GraduallyTyped.Tensor.MathOperations.Pointwise</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#tanh"><span class="hs-identifier">tanh</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html"><span class="hs-identifier">Torch.GraduallyTyped.Tensor.Type</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier">Tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">tanh</span></span><span class="hs-special">)</span><span>
</span><span id="line-27"></span><span>
</span><span id="line-28"></span><span id="local-6989586621679762177"><span id="local-6989586621679762178"></span></span><span class="hs-keyword">data</span><span> </span><span id="Softmax"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-var">Softmax</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679762176"><span class="annot"><a href="#local-6989586621679762176"><span class="hs-identifier hs-type">selectDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SelectDim"><span class="hs-identifier hs-type">SelectDim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#By"><span class="hs-identifier hs-type">By</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Symbol</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-29"></span><span>  </span><span id="Softmax"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-var">Softmax</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-30"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679762287"><span class="annot"><a href="#local-6989586621679762287"><span class="hs-identifier hs-type">selectDim</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-31"></span><span>    </span><span class="hs-special">{</span><span id="softmaxSelectDim"><span class="annot"><span class="annottext">Softmax selectDim -&gt; SSelectDim selectDim
</span><a href="Torch.GraduallyTyped.NN.Activation.html#softmaxSelectDim"><span class="hs-identifier hs-var hs-var">softmaxSelectDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SSelectDim"><span class="hs-identifier hs-type">SSelectDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762287"><span class="hs-identifier hs-type">selectDim</span></a></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-32"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-type">Softmax</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762287"><span class="hs-identifier hs-type">selectDim</span></a></span><span>
</span><span id="line-33"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(forall x. Softmax selectDim -&gt; Rep (Softmax selectDim) x)
-&gt; (forall x. Rep (Softmax selectDim) x -&gt; Softmax selectDim)
-&gt; Generic (Softmax selectDim)
forall x. Rep (Softmax selectDim) x -&gt; Softmax selectDim
forall x. Softmax selectDim -&gt; Rep (Softmax selectDim) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (selectDim :: SelectDim (By Symbol Nat)) x.
Rep (Softmax selectDim) x -&gt; Softmax selectDim
forall (selectDim :: SelectDim (By Symbol Nat)) x.
Softmax selectDim -&gt; Rep (Softmax selectDim) x
$cto :: forall (selectDim :: SelectDim (By Symbol Nat)) x.
Rep (Softmax selectDim) x -&gt; Softmax selectDim
$cfrom :: forall (selectDim :: SelectDim (By Symbol Nat)) x.
Softmax selectDim -&gt; Rep (Softmax selectDim) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-34"></span><span>
</span><span id="line-35"></span><span id="local-6989586621679762169"><span id="local-6989586621679762170"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-type">Softmax</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762170"><span class="hs-identifier hs-type">selectDim</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SSelectDim"><span class="hs-identifier hs-type">SSelectDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762170"><span class="hs-identifier hs-type">selectDim</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679762169"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762169"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-36"></span><span>  </span><span id="local-6989586621679762166"><span class="annot"><span class="annottext">initialize :: SSelectDim selectDim -&gt; generator -&gt; (Softmax selectDim, generator)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var hs-var hs-var hs-var">initialize</span></a></span></span><span> </span><span id="local-6989586621679762164"><span class="annot"><span class="annottext">SSelectDim selectDim
</span><a href="#local-6989586621679762164"><span class="hs-identifier hs-var">selectDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SSelectDim selectDim -&gt; Softmax selectDim
forall (selectDim :: SelectDim (By Symbol Nat)).
SSelectDim selectDim -&gt; Softmax selectDim
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-var">Softmax</span></a></span><span> </span><span class="annot"><span class="annottext">SSelectDim selectDim
</span><a href="#local-6989586621679762164"><span class="hs-identifier hs-var">selectDim</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span></span><span>
</span><span id="line-37"></span><span>
</span><span id="line-38"></span><span id="local-6989586621679762163"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-type">Softmax</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762163"><span class="hs-identifier hs-type">selectDim</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SSelectDim"><span class="hs-identifier hs-type">SSelectDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762163"><span class="hs-identifier hs-type">selectDim</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-39"></span><span>  </span><span id="local-6989586621679762159"><span class="annot"><span class="annottext">fromStateDict :: SSelectDim selectDim -&gt; StateDictKey -&gt; m (Softmax selectDim)
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromStateDict</span></a></span></span><span> </span><span id="local-6989586621679762157"><span class="annot"><span class="annottext">SSelectDim selectDim
</span><a href="#local-6989586621679762157"><span class="hs-identifier hs-var">selectDim</span></a></span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Softmax selectDim -&gt; m (Softmax selectDim)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SSelectDim selectDim -&gt; Softmax selectDim
forall (selectDim :: SelectDim (By Symbol Nat)).
SSelectDim selectDim -&gt; Softmax selectDim
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-var">Softmax</span></a></span><span> </span><span class="annot"><span class="annottext">SSelectDim selectDim
</span><a href="#local-6989586621679762157"><span class="hs-identifier hs-var">selectDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-40"></span><span>  </span><span id="local-6989586621679762156"><span class="annot"><span class="annottext">toStateDict :: StateDictKey -&gt; Softmax selectDim -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var hs-var hs-var hs-var">toStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Softmax selectDim
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">() -&gt; m ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span></span><span>
</span><span id="line-41"></span><span>
</span><span id="line-42"></span><span id="local-6989586621679762147"><span id="local-6989586621679762148"><span id="local-6989586621679762149"><span id="local-6989586621679762150"><span id="local-6989586621679762151"><span id="local-6989586621679762152"><span id="local-6989586621679762153"><span id="local-6989586621679762154"><span class="hs-keyword">instance</span><span>
</span><span id="line-43"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679762154"><span class="hs-identifier hs-type">output</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762153"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762152"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762151"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762150"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Functional.NonLinearActivation.html#SoftmaxF"><span class="hs-identifier hs-type">SoftmaxF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762149"><span class="hs-identifier hs-type">selectDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762148"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-44"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-45"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-type">Softmax</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762149"><span class="hs-identifier hs-type">selectDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-46"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762153"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762152"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762151"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762150"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762148"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-47"></span><span>    </span><span class="annot"><a href="#local-6989586621679762147"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-48"></span><span>    </span><span class="annot"><a href="#local-6989586621679762154"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-49"></span><span>    </span><span class="annot"><a href="#local-6989586621679762147"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-50"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-51"></span><span>  </span><span id="local-6989586621679762144"><span class="annot"><span class="annottext">forward :: Softmax selectDim
-&gt; Tensor requiresGradient layout device dataType shape
-&gt; generator
-&gt; m (output, generator)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Softmax"><span class="hs-identifier hs-type">Softmax</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679762142"><span class="annot"><span class="annottext">SSelectDim selectDim
softmaxSelectDim :: SSelectDim selectDim
softmaxSelectDim :: forall (selectDim :: SelectDim (By Symbol Nat)).
Softmax selectDim -&gt; SSelectDim selectDim
</span><a href="#local-6989586621679762142"><span class="hs-glyph hs-var hs-var">..</span></a></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679762141"><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762141"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(output, generator) -&gt; m (output, generator)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">((output, generator) -&gt; m (output, generator))
-&gt; (generator -&gt; (output, generator))
-&gt; generator
-&gt; m (output, generator)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SSelectDim selectDim
-&gt; Tensor requiresGradient layout device dataType shape
-&gt; Tensor
     requiresGradient layout device dataType (SoftmaxF selectDim shape)
forall (selectDim :: SelectDim (By Symbol Nat))
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
SSelectDim selectDim
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor
     gradient layout device dataType (SoftmaxF selectDim shape)
</span><a href="Torch.GraduallyTyped.NN.Functional.NonLinearActivation.html#softmax"><span class="hs-identifier hs-var">softmax</span></a></span><span> </span><span class="annot"><span class="annottext">SSelectDim selectDim
</span><a href="#local-6989586621679762142"><span class="hs-identifier hs-var">softmaxSelectDim</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762141"><span class="hs-identifier hs-var">input</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span></span></span></span></span></span></span></span><span>
</span><span id="line-52"></span><span>
</span><span id="line-53"></span><span class="hs-keyword">data</span><span> </span><span id="Relu"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-var">Relu</span></a></span></span><span> </span><span class="hs-keyword">where</span><span> </span><span id="Relu"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-var">Relu</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-type">Relu</span></a></span><span>
</span><span id="line-54"></span><span>
</span><span id="line-55"></span><span id="local-6989586621679762138"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-type">Relu</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679762138"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762138"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-56"></span><span>  </span><span id="local-6989586621679762136"><span class="annot"><span class="annottext">initialize :: () -&gt; generator -&gt; (Relu, generator)
</span><a href="#local-6989586621679762136"><span class="hs-identifier hs-var hs-var hs-var hs-var">initialize</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Relu
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-var">Relu</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span><span>
</span><span id="line-57"></span><span>
</span><span id="line-58"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-type">Relu</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-59"></span><span>  </span><span id="local-6989586621679762133"><span class="annot"><span class="annottext">fromStateDict :: () -&gt; StateDictKey -&gt; m Relu
</span><a href="#local-6989586621679762133"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Relu -&gt; m Relu
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Relu
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-var">Relu</span></a></span><span>
</span><span id="line-60"></span><span>  </span><span id="local-6989586621679762132"><span class="annot"><span class="annottext">toStateDict :: StateDictKey -&gt; Relu -&gt; m ()
</span><a href="#local-6989586621679762132"><span class="hs-identifier hs-var hs-var hs-var hs-var">toStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Relu
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">() -&gt; m ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-61"></span><span>
</span><span id="line-62"></span><span id="local-6989586621679762126"><span id="local-6989586621679762127"><span id="local-6989586621679762128"><span id="local-6989586621679762129"><span id="local-6989586621679762130"><span id="local-6989586621679762131"><span class="hs-keyword">instance</span><span>
</span><span id="line-63"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-64"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-type">Relu</span></a></span><span>
</span><span id="line-65"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762131"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762130"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762129"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762128"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762127"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-66"></span><span>    </span><span class="annot"><a href="#local-6989586621679762126"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-67"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762131"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762130"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762129"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762128"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762127"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-68"></span><span>    </span><span class="annot"><a href="#local-6989586621679762126"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-69"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-70"></span><span>  </span><span id="local-6989586621679762124"><span class="annot"><span class="annottext">forward :: Relu
-&gt; Tensor requiresGradient layout device dataType shape
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
</span><a href="#local-6989586621679762124"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span class="annot"><span class="annottext">Relu
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Relu"><span class="hs-identifier hs-var">Relu</span></a></span><span> </span><span id="local-6989586621679762123"><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762123"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Tensor requiresGradient layout device dataType shape, generator)
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">((Tensor requiresGradient layout device dataType shape, generator)
 -&gt; m (Tensor requiresGradient layout device dataType shape,
       generator))
-&gt; (generator
    -&gt; (Tensor requiresGradient layout device dataType shape,
        generator))
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
-&gt; Tensor requiresGradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#relu"><span class="hs-identifier hs-var">relu</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762123"><span class="hs-identifier hs-var">input</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-71"></span><span>
</span><span id="line-72"></span><span class="hs-keyword">data</span><span> </span><span id="Gelu"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-var">Gelu</span></a></span></span><span> </span><span class="hs-keyword">where</span><span> </span><span id="Gelu"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-var">Gelu</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-type">Gelu</span></a></span><span>
</span><span id="line-73"></span><span>
</span><span id="line-74"></span><span id="local-6989586621679762121"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-type">Gelu</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679762121"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762121"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-75"></span><span>  </span><span id="local-6989586621679762119"><span class="annot"><span class="annottext">initialize :: () -&gt; generator -&gt; (Gelu, generator)
</span><a href="#local-6989586621679762119"><span class="hs-identifier hs-var hs-var hs-var hs-var">initialize</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Gelu
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-var">Gelu</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span><span>
</span><span id="line-76"></span><span>
</span><span id="line-77"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-type">Gelu</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-78"></span><span>  </span><span id="local-6989586621679762116"><span class="annot"><span class="annottext">fromStateDict :: () -&gt; StateDictKey -&gt; m Gelu
</span><a href="#local-6989586621679762116"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Gelu -&gt; m Gelu
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Gelu
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-var">Gelu</span></a></span><span>
</span><span id="line-79"></span><span>  </span><span id="local-6989586621679762115"><span class="annot"><span class="annottext">toStateDict :: StateDictKey -&gt; Gelu -&gt; m ()
</span><a href="#local-6989586621679762115"><span class="hs-identifier hs-var hs-var hs-var hs-var">toStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Gelu
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">() -&gt; m ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-80"></span><span>
</span><span id="line-81"></span><span id="local-6989586621679762109"><span id="local-6989586621679762110"><span id="local-6989586621679762111"><span id="local-6989586621679762112"><span id="local-6989586621679762113"><span id="local-6989586621679762114"><span class="hs-keyword">instance</span><span>
</span><span id="line-82"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-83"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-type">Gelu</span></a></span><span>
</span><span id="line-84"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762114"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762113"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762112"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762111"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762110"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-85"></span><span>    </span><span class="annot"><a href="#local-6989586621679762109"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-86"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762114"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762113"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762112"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762111"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762110"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-87"></span><span>    </span><span class="annot"><a href="#local-6989586621679762109"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-88"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-89"></span><span>  </span><span id="local-6989586621679762107"><span class="annot"><span class="annottext">forward :: Gelu
-&gt; Tensor requiresGradient layout device dataType shape
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
</span><a href="#local-6989586621679762107"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span class="annot"><span class="annottext">Gelu
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Gelu"><span class="hs-identifier hs-var">Gelu</span></a></span><span> </span><span id="local-6989586621679762106"><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762106"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Tensor requiresGradient layout device dataType shape, generator)
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">((Tensor requiresGradient layout device dataType shape, generator)
 -&gt; m (Tensor requiresGradient layout device dataType shape,
       generator))
-&gt; (generator
    -&gt; (Tensor requiresGradient layout device dataType shape,
        generator))
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
-&gt; Tensor requiresGradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#gelu"><span class="hs-identifier hs-var">gelu</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762106"><span class="hs-identifier hs-var">input</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-90"></span><span>
</span><span id="line-91"></span><span class="hs-keyword">data</span><span> </span><span id="GeluNew"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-var">GeluNew</span></a></span></span><span> </span><span class="hs-keyword">where</span><span> </span><span id="GeluNew"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-var">GeluNew</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-type">GeluNew</span></a></span><span>
</span><span id="line-92"></span><span>
</span><span id="line-93"></span><span id="local-6989586621679762104"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-type">GeluNew</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679762104"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762104"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-94"></span><span>  </span><span id="local-6989586621679762102"><span class="annot"><span class="annottext">initialize :: () -&gt; generator -&gt; (GeluNew, generator)
</span><a href="#local-6989586621679762102"><span class="hs-identifier hs-var hs-var hs-var hs-var">initialize</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GeluNew
</span><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-var">GeluNew</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span><span>
</span><span id="line-95"></span><span>
</span><span id="line-96"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-type">GeluNew</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-97"></span><span>  </span><span id="local-6989586621679762099"><span class="annot"><span class="annottext">fromStateDict :: () -&gt; StateDictKey -&gt; m GeluNew
</span><a href="#local-6989586621679762099"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GeluNew -&gt; m GeluNew
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">GeluNew
</span><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-var">GeluNew</span></a></span><span>
</span><span id="line-98"></span><span>  </span><span id="local-6989586621679762098"><span class="annot"><span class="annottext">toStateDict :: StateDictKey -&gt; GeluNew -&gt; m ()
</span><a href="#local-6989586621679762098"><span class="hs-identifier hs-var hs-var hs-var hs-var">toStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">GeluNew
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">() -&gt; m ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-99"></span><span>
</span><span id="line-100"></span><span id="local-6989586621679762092"><span id="local-6989586621679762093"><span id="local-6989586621679762094"><span id="local-6989586621679762095"><span id="local-6989586621679762096"><span id="local-6989586621679762097"><span class="hs-keyword">instance</span><span>
</span><span id="line-101"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-102"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-type">GeluNew</span></a></span><span>
</span><span id="line-103"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762097"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762096"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762095"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762094"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762093"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-104"></span><span>    </span><span class="annot"><a href="#local-6989586621679762092"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-105"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762097"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762096"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762095"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762094"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762093"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-106"></span><span>    </span><span class="annot"><a href="#local-6989586621679762092"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-107"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-108"></span><span>  </span><span id="local-6989586621679762090"><span class="annot"><span class="annottext">forward :: GeluNew
-&gt; Tensor requiresGradient layout device dataType shape
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
</span><a href="#local-6989586621679762090"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span class="annot"><span class="annottext">GeluNew
</span><a href="Torch.GraduallyTyped.NN.Activation.html#GeluNew"><span class="hs-identifier hs-var">GeluNew</span></a></span><span> </span><span id="local-6989586621679762089"><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762089"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Tensor requiresGradient layout device dataType shape, generator)
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">((Tensor requiresGradient layout device dataType shape, generator)
 -&gt; m (Tensor requiresGradient layout device dataType shape,
       generator))
-&gt; (generator
    -&gt; (Tensor requiresGradient layout device dataType shape,
        generator))
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
-&gt; Tensor requiresGradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.NN.Functional.Activation.html#geluNew"><span class="hs-identifier hs-var">geluNew</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762089"><span class="hs-identifier hs-var">input</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-109"></span><span>
</span><span id="line-110"></span><span class="hs-keyword">data</span><span> </span><span id="Tanh"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-var">Tanh</span></a></span></span><span> </span><span class="hs-keyword">where</span><span> </span><span id="Tanh"><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-var">Tanh</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-type">Tanh</span></a></span><span>
</span><span id="line-111"></span><span>
</span><span id="line-112"></span><span id="local-6989586621679762087"><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-type">Tanh</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679762087"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762087"><span class="hs-identifier hs-type">generator</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-113"></span><span>  </span><span id="local-6989586621679762085"><span class="annot"><span class="annottext">initialize :: () -&gt; generator -&gt; (Tanh, generator)
</span><a href="#local-6989586621679762085"><span class="hs-identifier hs-var hs-var hs-var hs-var">initialize</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tanh
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-var">Tanh</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span><span>
</span><span id="line-114"></span><span>
</span><span id="line-115"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-type">Tanh</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-116"></span><span>  </span><span id="local-6989586621679762082"><span class="annot"><span class="annottext">fromStateDict :: () -&gt; StateDictKey -&gt; m Tanh
</span><a href="#local-6989586621679762082"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tanh -&gt; m Tanh
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tanh
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-var">Tanh</span></a></span><span>
</span><span id="line-117"></span><span>  </span><span id="local-6989586621679762081"><span class="annot"><span class="annottext">toStateDict :: StateDictKey -&gt; Tanh -&gt; m ()
</span><a href="#local-6989586621679762081"><span class="hs-identifier hs-var hs-var hs-var hs-var">toStateDict</span></a></span></span><span> </span><span class="annot"><span class="annottext">StateDictKey
</span><span class="hs-identifier">_</span></span><span> </span><span class="annot"><span class="annottext">Tanh
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">() -&gt; m ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-118"></span><span>
</span><span id="line-119"></span><span id="local-6989586621679762075"><span id="local-6989586621679762076"><span id="local-6989586621679762077"><span id="local-6989586621679762078"><span id="local-6989586621679762079"><span id="local-6989586621679762080"><span class="hs-keyword">instance</span><span>
</span><span id="line-120"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-121"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-type">Tanh</span></a></span><span>
</span><span id="line-122"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762080"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762079"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762078"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762077"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762076"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-123"></span><span>    </span><span class="annot"><a href="#local-6989586621679762075"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-124"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762080"><span class="hs-identifier hs-type">requiresGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762079"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762078"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762077"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679762076"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-125"></span><span>    </span><span class="annot"><a href="#local-6989586621679762075"><span class="hs-identifier hs-type">generator</span></a></span><span>
</span><span id="line-126"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-127"></span><span>  </span><span id="local-6989586621679762073"><span class="annot"><span class="annottext">forward :: Tanh
-&gt; Tensor requiresGradient layout device dataType shape
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
</span><a href="#local-6989586621679762073"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span class="annot"><span class="annottext">Tanh
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-var">Tanh</span></a></span><span> </span><span id="local-6989586621679762072"><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762072"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Tensor requiresGradient layout device dataType shape, generator)
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">((Tensor requiresGradient layout device dataType shape, generator)
 -&gt; m (Tensor requiresGradient layout device dataType shape,
       generator))
-&gt; (generator
    -&gt; (Tensor requiresGradient layout device dataType shape,
        generator))
-&gt; generator
-&gt; m (Tensor requiresGradient layout device dataType shape,
      generator)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
-&gt; Tensor requiresGradient layout device dataType shape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#tanh"><span class="hs-identifier hs-var">tanh</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor requiresGradient layout device dataType shape
</span><a href="#local-6989586621679762072"><span class="hs-identifier hs-var">input</span></a></span><span class="hs-special">,</span><span class="hs-special">)</span></span></span></span></span></span></span></pre></body></html>