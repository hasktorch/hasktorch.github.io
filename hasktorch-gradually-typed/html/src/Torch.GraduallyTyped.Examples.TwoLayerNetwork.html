<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE DerivingStrategies #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE NamedFieldPuns #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE OverloadedStrings #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE PartialTypeSignatures #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE RoleAnnotations #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-15"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-16"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-17"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-18"></span><span>
</span><span id="line-19"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.GraduallyTyped.Examples.TwoLayerNetwork</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-20"></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/apj4fhhmp4gbch73305ll5kzmxzh18bj-stm-lib-stm-2.5.0.0-haddock-doc/share/doc/stm/html/src"><span class="hs-identifier">Control.Concurrent.STM.TVar</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">newTVarIO</span></span><span class="hs-special">)</span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/48s8mi00b2kvxgar4z446h72vmm1c3v3-lens-lib-lens-5.0.1-haddock-doc/share/doc/lens/html/src"><span class="hs-identifier">Control.Lens</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/48s8mi00b2kvxgar4z446h72vmm1c3v3-lens-lib-lens-5.0.1-haddock-doc/share/doc/lens/html/src"><span class="hs-identifier">element</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/48s8mi00b2kvxgar4z446h72vmm1c3v3-lens-lib-lens-5.0.1-haddock-doc/share/doc/lens/html/src"><span class="hs-operator">(^?)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">replicateM</span></span><span class="hs-special">)</span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/ifacyhfpnb9wrd20q18ymi87z39k1h67-exceptions-lib-exceptions-0.10.4-haddock-doc/share/doc/exceptions/html/src"><span class="hs-identifier">Control.Monad.Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/ifacyhfpnb9wrd20q18ymi87z39k1h67-exceptions-lib-exceptions-0.10.4-haddock-doc/share/doc/exceptions/html/src"><span class="hs-identifier">MonadThrow</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad.Cont</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">runContT</span></span><span class="hs-special">)</span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad.IO.Class</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">MonadIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">liftIO</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-27"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-identifier">Control.Monad.Indexed</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator">(&gt;&gt;&gt;=)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-28"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier">Control.Monad.Indexed.State</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier">IxStateT</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-29"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad.State</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">MonadState</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">StateT</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">evalStateT</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">execStateT</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">gets</span></span><span class="hs-special">)</span><span>
</span><span id="line-30"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad.Trans</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">MonadTrans</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-31"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-identifier">Data.Functor.Indexed</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-identifier">IxPointed</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-identifier">ireturn</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator">(&lt;&lt;$&gt;&gt;)</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator">(&lt;&lt;*&gt;&gt;)</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-32"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><span class="hs-identifier">Data.Map</span></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">Map</span></span><span>
</span><span id="line-33"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/h1pd6gy8cjy2zwwf9bl5ni48j4q56732-normaldistribution-lib-normaldistribution-1.1.0.3-haddock-doc/share/doc/normaldistribution/html/src"><span class="hs-identifier">Data.Random.Normal</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/h1pd6gy8cjy2zwwf9bl5ni48j4q56732-normaldistribution-lib-normaldistribution-1.1.0.3-haddock-doc/share/doc/normaldistribution/html/src"><span class="hs-identifier">normal</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-34"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><span class="hs-identifier">Data.Set</span></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">Set</span></span><span>
</span><span id="line-35"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Text</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">Text</span></span><span class="hs-special">)</span><span>
</span><span id="line-36"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/33j53czfh9iaqr8w1k6rjcna7zg8rw55-vector-sized-lib-vector-sized-1.4.3.1-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier">Data.Vector.Sized</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">VS</span></span><span>
</span><span id="line-37"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign.ForeignPtr</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">ForeignPtr</span></span><span class="hs-special">)</span><span>
</span><span id="line-38"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier">Pipes</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">P</span></span><span>
</span><span id="line-39"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/gi3ii02mxr2l6qd73gq29asx1kn1yvi3-pipes-concurrency-lib-pipes-concurrency-2.0.12-haddock-doc/share/doc/pipes-concurrency/html/src"><span class="hs-identifier">Pipes.Concurrent</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">P</span></span><span>
</span><span id="line-40"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier">Pipes.Prelude</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">P</span></span><span>
</span><span id="line-41"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier">System.Random</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier">Random</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier">RandomGen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier">getStdGen</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-42"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.html"><span class="hs-identifier">Torch.GraduallyTyped</span></a></span><span>
</span><span id="line-43"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-44"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Class</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-45"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.GC</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">unsafeThrowableIO</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-46"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Native</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-47"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Optim</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-48"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Type</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-49"></span><span>
</span><span id="line-50"></span><span class="hs-comment">-- | Compute the sine cardinal (sinc) function,</span><span>
</span><span id="line-51"></span><span class="hs-comment">-- see https://mathworld.wolfram.com/SincFunction.html.</span><span>
</span><span id="line-52"></span><span id="local-6989586621679609854"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#sinc"><span class="hs-identifier hs-type">sinc</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Floating</span></span><span> </span><span class="annot"><a href="#local-6989586621679609854"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679609854"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679609854"><span class="hs-identifier hs-type">a</span></a></span></span><span>
</span><span id="line-53"></span><span id="sinc"><span class="annot"><span class="annottext">sinc :: a -&gt; a
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#sinc"><span class="hs-identifier hs-var hs-var">sinc</span></a></span></span><span> </span><span id="local-6989586621679609852"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609852"><span class="hs-identifier hs-var">a</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">a -&gt; a
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">Prelude.sin</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609852"><span class="hs-identifier hs-var">a</span></a></span><span> </span><span class="annot"><span class="annottext">a -&gt; a -&gt; a
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609852"><span class="hs-identifier hs-var">a</span></a></span><span>
</span><span id="line-54"></span><span>
</span><span id="line-55"></span><span class="hs-comment">-- | Compute the sine cardinal (sinc) function and add normally distributed noise</span><span>
</span><span id="line-56"></span><span class="hs-comment">-- of strength epsilon. We use the 'normal' function from 'Data.Random.Normal'</span><span>
</span><span id="line-57"></span><span class="hs-comment">-- which requires a random number generator with 'RandomGen' instance.</span><span>
</span><span id="line-58"></span><span id="local-6989586621679610458"><span id="local-6989586621679610459"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#noisySinc"><span class="hs-identifier hs-type">noisySinc</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Floating</span></span><span> </span><span class="annot"><a href="#local-6989586621679610459"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier hs-type">Random</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610459"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier hs-type">RandomGen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610458"><span class="hs-identifier hs-type">g</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679610459"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679610459"><span class="hs-identifier hs-type">a</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679610458"><span class="hs-identifier hs-type">g</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610459"><span class="hs-identifier hs-type">a</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679610458"><span class="hs-identifier hs-type">g</span></a></span><span class="hs-special">)</span></span></span><span>
</span><span id="line-59"></span><span id="noisySinc"><span class="annot"><span class="annottext">noisySinc :: a -&gt; a -&gt; g -&gt; (a, g)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#noisySinc"><span class="hs-identifier hs-var hs-var">noisySinc</span></a></span></span><span> </span><span id="local-6989586621679609848"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609848"><span class="hs-identifier hs-var">eps</span></a></span></span><span> </span><span id="local-6989586621679609847"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609847"><span class="hs-identifier hs-var">a</span></a></span></span><span> </span><span id="local-6989586621679609846"><span class="annot"><span class="annottext">g
</span><a href="#local-6989586621679609846"><span class="hs-identifier hs-var">g</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">let</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609845"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609845"><span class="hs-identifier hs-var">noise</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609844"><span class="annot"><span class="annottext">g
</span><a href="#local-6989586621679609844"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">g -&gt; (a, g)
forall g a. (RandomGen g, Random a, Floating a) =&gt; g -&gt; (a, g)
</span><a href="../file:///nix/store/h1pd6gy8cjy2zwwf9bl5ni48j4q56732-normaldistribution-lib-normaldistribution-1.1.0.3-haddock-doc/share/doc/normaldistribution/html/src"><span class="hs-identifier hs-var">normal</span></a></span><span> </span><span class="annot"><span class="annottext">g
</span><a href="#local-6989586621679609846"><span class="hs-identifier hs-var">g</span></a></span><span> </span><span class="hs-keyword">in</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">a -&gt; a
forall a. Floating a =&gt; a -&gt; a
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#sinc"><span class="hs-identifier hs-var">sinc</span></a></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609847"><span class="hs-identifier hs-var">a</span></a></span><span> </span><span class="annot"><span class="annottext">a -&gt; a -&gt; a
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609848"><span class="hs-identifier hs-var">eps</span></a></span><span> </span><span class="annot"><span class="annottext">a -&gt; a -&gt; a
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609845"><span class="hs-identifier hs-var">noise</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">g
</span><a href="#local-6989586621679609844"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-60"></span><span>
</span><span id="line-61"></span><span class="hs-comment">-- | Datatype to represent a dataset of sine cardinal (sinc) inputs and outputs.</span><span>
</span><span id="line-62"></span><span class="hs-comment">-- The 'name' field is used to identify the split of the dataset.</span><span>
</span><span id="line-63"></span><span class="hs-keyword">data</span><span> </span><span id="SincData"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-var">SincData</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="SincData"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-var">SincData</span></a></span></span><span> </span><span class="hs-special">{</span><span id="name"><span class="annot"><span class="annottext">SincData -&gt; Text
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#name"><span class="hs-identifier hs-var hs-var">name</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Text</span></span><span class="hs-special">,</span><span> </span><span id="unSincData"><span class="annot"><span class="annottext">SincData -&gt; [(Float, Float)]
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#unSincData"><span class="hs-identifier hs-var hs-var">unSincData</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">}</span><span> </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609835"><span id="local-6989586621679609837"><span class="annot"><span class="annottext">SincData -&gt; SincData -&gt; Bool
(SincData -&gt; SincData -&gt; Bool)
-&gt; (SincData -&gt; SincData -&gt; Bool) -&gt; Eq SincData
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
/= :: SincData -&gt; SincData -&gt; Bool
$c/= :: SincData -&gt; SincData -&gt; Bool
== :: SincData -&gt; SincData -&gt; Bool
$c== :: SincData -&gt; SincData -&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609819"><span id="local-6989586621679609821"><span id="local-6989586621679609823"><span id="local-6989586621679609825"><span id="local-6989586621679609827"><span id="local-6989586621679609829"><span id="local-6989586621679609831"><span class="annot"><span class="annottext">Eq SincData
Eq SincData
-&gt; (SincData -&gt; SincData -&gt; Ordering)
-&gt; (SincData -&gt; SincData -&gt; Bool)
-&gt; (SincData -&gt; SincData -&gt; Bool)
-&gt; (SincData -&gt; SincData -&gt; Bool)
-&gt; (SincData -&gt; SincData -&gt; Bool)
-&gt; (SincData -&gt; SincData -&gt; SincData)
-&gt; (SincData -&gt; SincData -&gt; SincData)
-&gt; Ord SincData
SincData -&gt; SincData -&gt; Bool
SincData -&gt; SincData -&gt; Ordering
SincData -&gt; SincData -&gt; SincData
forall a.
Eq a
-&gt; (a -&gt; a -&gt; Ordering)
-&gt; (a -&gt; a -&gt; Bool)
-&gt; (a -&gt; a -&gt; Bool)
-&gt; (a -&gt; a -&gt; Bool)
-&gt; (a -&gt; a -&gt; Bool)
-&gt; (a -&gt; a -&gt; a)
-&gt; (a -&gt; a -&gt; a)
-&gt; Ord a
min :: SincData -&gt; SincData -&gt; SincData
$cmin :: SincData -&gt; SincData -&gt; SincData
max :: SincData -&gt; SincData -&gt; SincData
$cmax :: SincData -&gt; SincData -&gt; SincData
&gt;= :: SincData -&gt; SincData -&gt; Bool
$c&gt;= :: SincData -&gt; SincData -&gt; Bool
&gt; :: SincData -&gt; SincData -&gt; Bool
$c&gt; :: SincData -&gt; SincData -&gt; Bool
&lt;= :: SincData -&gt; SincData -&gt; Bool
$c&lt;= :: SincData -&gt; SincData -&gt; Bool
&lt; :: SincData -&gt; SincData -&gt; Bool
$c&lt; :: SincData -&gt; SincData -&gt; Bool
compare :: SincData -&gt; SincData -&gt; Ordering
$ccompare :: SincData -&gt; SincData -&gt; Ordering
$cp1Ord :: Eq SincData
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Ord</span></span></span></span></span></span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-64"></span><span>
</span><span id="line-65"></span><span class="hs-comment">-- | Create a dataset of noisy sine cardinal (sinc) values of a desired size.</span><span>
</span><span id="line-66"></span><span id="local-6989586621679609921"><span id="local-6989586621679609922"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkSincData"><span class="hs-identifier hs-type">mkSincData</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-67"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier hs-type">RandomGen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609922"><span class="hs-identifier hs-type">g</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Monad</span></span><span> </span><span class="annot"><a href="#local-6989586621679609921"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-68"></span><span>  </span><span class="hs-comment">-- | name of the dataset</span><span>
</span><span id="line-69"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Text</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-70"></span><span>  </span><span class="hs-comment">-- | number of samples</span><span>
</span><span id="line-71"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-72"></span><span>  </span><span class="hs-comment">-- | dataset in the state monad over the random generator</span><span>
</span><span id="line-73"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">StateT</span></span><span> </span><span class="annot"><a href="#local-6989586621679609922"><span class="hs-identifier hs-type">g</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609921"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-type">SincData</span></a></span></span></span><span>
</span><span id="line-74"></span><span id="mkSincData"><span class="annot"><span class="annottext">mkSincData :: Text -&gt; Int -&gt; StateT g m SincData
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkSincData"><span class="hs-identifier hs-var hs-var">mkSincData</span></a></span></span><span> </span><span id="local-6989586621679609816"><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609816"><span class="hs-identifier hs-var">name'</span></a></span></span><span> </span><span id="local-6989586621679609815"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609815"><span class="hs-identifier hs-var">size</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-75"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609814"><span class="annot"><span class="annottext">next' :: StateT g m (Float, Float)
</span><a href="#local-6989586621679609814"><span class="hs-identifier hs-var hs-var">next'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-76"></span><span>        </span><span id="local-6989586621679609813"><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679609813"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">20</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Float -&gt; Float) -&gt; StateT g m Float -&gt; StateT g m Float
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">(g -&gt; (Float, g)) -&gt; StateT g m Float
forall s (m :: * -&gt; *) a. MonadState s m =&gt; (s -&gt; (a, s)) -&gt; m a
</span><span class="hs-identifier hs-var">state</span></span><span> </span><span class="annot"><span class="annottext">g -&gt; (Float, g)
forall g a. (RandomGen g, Random a, Floating a) =&gt; g -&gt; (a, g)
</span><a href="../file:///nix/store/h1pd6gy8cjy2zwwf9bl5ni48j4q56732-normaldistribution-lib-normaldistribution-1.1.0.3-haddock-doc/share/doc/normaldistribution/html/src"><span class="hs-identifier hs-var">normal</span></a></span><span>
</span><span id="line-77"></span><span>        </span><span id="local-6989586621679609810"><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679609810"><span class="hs-identifier hs-var">y</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(g -&gt; (Float, g)) -&gt; StateT g m Float
forall s (m :: * -&gt; *) a. MonadState s m =&gt; (s -&gt; (a, s)) -&gt; m a
</span><span class="hs-identifier hs-var">state</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; g -&gt; (Float, g)
forall a g.
(Floating a, Random a, RandomGen g) =&gt;
a -&gt; a -&gt; g -&gt; (a, g)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#noisySinc"><span class="hs-identifier hs-var">noisySinc</span></a></span><span> </span><span class="annot"><span class="annottext">Float
</span><span class="hs-number">0.05</span></span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679609813"><span class="hs-identifier hs-var">x</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-78"></span><span>        </span><span class="annot"><span class="annottext">(Float, Float) -&gt; StateT g m (Float, Float)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679609813"><span class="hs-identifier hs-var">x</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679609810"><span class="hs-identifier hs-var">y</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-79"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Text -&gt; [(Float, Float)] -&gt; SincData
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-var">SincData</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609816"><span class="hs-identifier hs-var">name'</span></a></span><span> </span><span class="annot"><span class="annottext">([(Float, Float)] -&gt; SincData)
-&gt; StateT g m [(Float, Float)] -&gt; StateT g m SincData
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; StateT g m (Float, Float) -&gt; StateT g m [(Float, Float)]
forall (m :: * -&gt; *) a. Applicative m =&gt; Int -&gt; m a -&gt; m [a]
</span><span class="hs-identifier hs-var">replicateM</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609815"><span class="hs-identifier hs-var">size</span></a></span><span> </span><span class="annot"><span class="annottext">StateT g m (Float, Float)
</span><a href="#local-6989586621679609814"><span class="hs-identifier hs-var">next'</span></a></span><span>
</span><span id="line-80"></span><span>
</span><span id="line-81"></span><span class="hs-comment">-- | 'Dataset' instance used for streaming sine cardinal (sinc) examples.</span><span>
</span><span id="line-82"></span><span class="hs-keyword">instance</span><span> </span><span class="annot"><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-type">Dataset</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-type">SincData</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-83"></span><span>  </span><span id="local-6989586621679609805"><span class="annot"><span class="annottext">getItem :: SincData -&gt; Int -&gt; IO (Float, Float)
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var hs-var hs-var hs-var">getItem</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-type">SincData</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679609803"><span class="annot"><span class="annottext">[(Float, Float)]
</span><a href="#local-6989586621679609803"><span class="hs-identifier hs-var">d</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679609802"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609802"><span class="hs-identifier hs-var">k</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Float, Float)
-&gt; ((Float, Float) -&gt; IO (Float, Float))
-&gt; Maybe (Float, Float)
-&gt; IO (Float, Float)
forall b a. b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b
</span><span class="hs-identifier hs-var">maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">String -&gt; IO (Float, Float)
forall (m :: * -&gt; *) a. MonadFail m =&gt; String -&gt; m a
</span><span class="hs-identifier hs-var">fail</span></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;invalid key&quot;</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Float, Float) -&gt; IO (Float, Float)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Maybe (Float, Float) -&gt; IO (Float, Float))
-&gt; Maybe (Float, Float) -&gt; IO (Float, Float)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[(Float, Float)]
</span><a href="#local-6989586621679609803"><span class="hs-identifier hs-var">d</span></a></span><span> </span><span class="annot"><span class="annottext">[(Float, Float)]
-&gt; Getting (First (Float, Float)) [(Float, Float)] (Float, Float)
-&gt; Maybe (Float, Float)
forall s a. s -&gt; Getting (First a) s a -&gt; Maybe a
</span><a href="../file:///nix/store/48s8mi00b2kvxgar4z446h72vmm1c3v3-lens-lib-lens-5.0.1-haddock-doc/share/doc/lens/html/src"><span class="hs-operator hs-var">^?</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; IndexedTraversal' Int [(Float, Float)] (Float, Float)
forall (t :: * -&gt; *) a.
Traversable t =&gt;
Int -&gt; IndexedTraversal' Int (t a) a
</span><a href="../file:///nix/store/48s8mi00b2kvxgar4z446h72vmm1c3v3-lens-lib-lens-5.0.1-haddock-doc/share/doc/lens/html/src"><span class="hs-identifier hs-var">element</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609802"><span class="hs-identifier hs-var">k</span></a></span><span>
</span><span id="line-84"></span><span>  </span><span id="local-6989586621679609800"><span class="annot"><span class="annottext">keys :: SincData -&gt; Set Int
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var hs-var hs-var hs-var">keys</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#SincData"><span class="hs-identifier hs-type">SincData</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-identifier">_</span></span><span> </span><span id="local-6989586621679609798"><span class="annot"><span class="annottext">[(Float, Float)]
</span><a href="#local-6989586621679609798"><span class="hs-identifier hs-var">d</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Set Int
forall a. Ord a =&gt; [a] -&gt; Set a
</span><span class="hs-identifier hs-var">Set.fromList</span></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">..</span><span> </span><span class="annot"><span class="annottext">[(Float, Float)] -&gt; Int
forall (t :: * -&gt; *) a. Foldable t =&gt; t a -&gt; Int
</span><span class="hs-identifier hs-var">Prelude.length</span></span><span> </span><span class="annot"><span class="annottext">[(Float, Float)]
</span><a href="#local-6989586621679609798"><span class="hs-identifier hs-var">d</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">]</span><span>
</span><span id="line-85"></span><span>
</span><span id="line-86"></span><span class="hs-comment">-- | Data type to represent a simple two-layer neural network.</span><span>
</span><span id="line-87"></span><span class="hs-comment">-- It is a product type of two layer types, @fstLayer@ and @sndLayer@,</span><span>
</span><span id="line-88"></span><span class="hs-comment">-- an activation function, @activation@, and a dropout layer, @dropout@.</span><span>
</span><span id="line-89"></span><span class="hs-keyword">data</span><span> </span><span id="TwoLayerNetwork"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-var">TwoLayerNetwork</span></a></span></span><span> </span><span id="local-6989586621679610389"><span class="annot"><a href="#local-6989586621679610389"><span class="hs-identifier hs-type">fstLayer</span></a></span></span><span> </span><span id="local-6989586621679610388"><span class="annot"><a href="#local-6989586621679610388"><span class="hs-identifier hs-type">activation</span></a></span></span><span> </span><span id="local-6989586621679610387"><span class="annot"><a href="#local-6989586621679610387"><span class="hs-identifier hs-type">dropout</span></a></span></span><span> </span><span id="local-6989586621679610386"><span class="annot"><a href="#local-6989586621679610386"><span class="hs-identifier hs-type">sndLayer</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="TwoLayerNetwork"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-var">TwoLayerNetwork</span></a></span></span><span>
</span><span id="line-90"></span><span>  </span><span class="hs-special">{</span><span> </span><span id="tlnFstLayer"><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; fstLayer
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#tlnFstLayer"><span class="hs-identifier hs-var hs-var">tlnFstLayer</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679610389"><span class="hs-identifier hs-type">fstLayer</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-91"></span><span>    </span><span id="tlnActivation"><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; activation
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#tlnActivation"><span class="hs-identifier hs-var hs-var">tlnActivation</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679610388"><span class="hs-identifier hs-type">activation</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-92"></span><span>    </span><span id="tlnDropout"><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; dropout
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#tlnDropout"><span class="hs-identifier hs-var hs-var">tlnDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679610387"><span class="hs-identifier hs-type">dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-93"></span><span>    </span><span id="tlnSndLayer"><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; sndLayer
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#tlnSndLayer"><span class="hs-identifier hs-var hs-var">tlnSndLayer</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679610386"><span class="hs-identifier hs-type">sndLayer</span></a></span><span>
</span><span id="line-94"></span><span>  </span><span class="hs-special">}</span><span>
</span><span id="line-95"></span><span>
</span><span id="line-96"></span><span class="hs-comment">-- | The specification of a two-layer network is the product of the</span><span>
</span><span id="line-97"></span><span class="hs-comment">-- specifications of its two layers and the activation function.</span><span>
</span><span id="line-98"></span><span class="hs-keyword">type</span><span> </span><span class="hs-keyword">instance</span><span>
</span><span id="line-99"></span><span>  </span><span id="ModelSpec"><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-var">ModelSpec</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span id="local-6989586621679609790"><span class="annot"><a href="#local-6989586621679609790"><span class="hs-identifier hs-type hs-type">fstLayer</span></a></span></span><span> </span><span id="local-6989586621679609789"><span class="annot"><a href="#local-6989586621679609789"><span class="hs-identifier hs-type hs-type">activation</span></a></span></span><span> </span><span id="local-6989586621679609788"><span class="annot"><a href="#local-6989586621679609788"><span class="hs-identifier hs-type hs-type">dropout</span></a></span></span><span> </span><span id="local-6989586621679609787"><span class="annot"><a href="#local-6989586621679609787"><span class="hs-identifier hs-type hs-type">sndLayer</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-100"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609790"><span class="hs-identifier hs-type">fstLayer</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609789"><span class="hs-identifier hs-type">activation</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609788"><span class="hs-identifier hs-type">dropout</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609787"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-101"></span><span>
</span><span id="line-102"></span><span class="hs-comment">-- | To initialize a two-layer network,</span><span>
</span><span id="line-103"></span><span class="hs-comment">-- we need its specification and a random generator.</span><span>
</span><span id="line-104"></span><span class="hs-comment">-- The random generator is used to initialize the weights of the network.</span><span>
</span><span id="line-105"></span><span class="hs-comment">-- The specification is used to determine the properties of the two neural layers,</span><span>
</span><span id="line-106"></span><span class="hs-comment">-- the activation function, and the dropout layer.</span><span>
</span><span id="line-107"></span><span class="hs-comment">-- The four components are initialized separately and then combined into a single</span><span>
</span><span id="line-108"></span><span class="hs-comment">-- network.</span><span>
</span><span id="line-109"></span><span id="local-6989586621679609782"><span id="local-6989586621679609783"><span id="local-6989586621679609784"><span id="local-6989586621679609785"><span id="local-6989586621679609786"><span class="hs-keyword">instance</span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609786"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609786"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-111"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609784"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609784"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-112"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609783"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609783"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-113"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609782"><span class="hs-identifier hs-type">sndLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609782"><span class="hs-identifier hs-type">sndLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-114"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-115"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasInitialize"><span class="hs-identifier hs-type">HasInitialize</span></a></span><span>
</span><span id="line-116"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609786"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609784"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609783"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609782"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-117"></span><span>    </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-118"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609786"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609784"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609783"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609782"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-119"></span><span>    </span><span class="annot"><a href="#local-6989586621679609785"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-120"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-121"></span><span>  </span><span id="local-6989586621679609779"><span class="annot"><span class="annottext">initialize :: ModelSpec (TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; Generator generatorDevice
-&gt; m (TwoLayerNetwork fstLayer activation dropout sndLayer,
      Generator generatorDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var hs-var hs-var hs-var">initialize</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span id="local-6989586621679609777"><span class="annot"><a href="#local-6989586621679609777"><span class="hs-identifier hs-var">fstLayerSpec</span></a></span></span><span> </span><span id="local-6989586621679609776"><span class="annot"><a href="#local-6989586621679609776"><span class="hs-identifier hs-var">activationSpec</span></a></span></span><span> </span><span id="local-6989586621679609775"><span class="annot"><a href="#local-6989586621679609775"><span class="hs-identifier hs-var">dropoutSpec</span></a></span></span><span> </span><span id="local-6989586621679609774"><span class="annot"><a href="#local-6989586621679609774"><span class="hs-identifier hs-var">sndLayerSpec</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-122"></span><span>    </span><span class="annot"><span class="annottext">IxStateT
  m
  (Generator generatorDevice)
  (Generator generatorDevice)
  (TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; Generator generatorDevice
-&gt; m (TwoLayerNetwork fstLayer activation dropout sndLayer,
      Generator generatorDevice)
forall (m :: * -&gt; *) i j a. IxStateT m i j a -&gt; i -&gt; m (a, j)
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var hs-var">runIxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">(IxStateT
   m
   (Generator generatorDevice)
   (Generator generatorDevice)
   (TwoLayerNetwork fstLayer activation dropout sndLayer)
 -&gt; Generator generatorDevice
 -&gt; m (TwoLayerNetwork fstLayer activation dropout sndLayer,
       Generator generatorDevice))
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     (TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; Generator generatorDevice
-&gt; m (TwoLayerNetwork fstLayer activation dropout sndLayer,
      Generator generatorDevice)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-123"></span><span>      </span><span class="annot"><span class="annottext">fstLayer
-&gt; activation
-&gt; dropout
-&gt; sndLayer
-&gt; TwoLayerNetwork fstLayer activation dropout sndLayer
forall fstLayer activation dropout sndLayer.
fstLayer
-&gt; activation
-&gt; dropout
-&gt; sndLayer
-&gt; TwoLayerNetwork fstLayer activation dropout sndLayer
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-var">TwoLayerNetwork</span></a></span><span>
</span><span id="line-124"></span><span>        </span><span class="annot"><span class="annottext">(fstLayer
 -&gt; activation
 -&gt; dropout
 -&gt; sndLayer
 -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) fstLayer
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     (activation
      -&gt; dropout
      -&gt; sndLayer
      -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
forall k1 k2 (f :: k1 -&gt; k2 -&gt; * -&gt; *) a b (j :: k1) (k3 :: k2).
IxFunctor f =&gt;
(a -&gt; b) -&gt; f j k3 a -&gt; f j k3 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&lt;&lt;$&gt;&gt;</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (fstLayer, Generator generatorDevice))
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) fstLayer
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice
  -&gt; m (fstLayer, Generator generatorDevice))
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice) fstLayer)
-&gt; (ModelSpec fstLayer
    -&gt; Generator generatorDevice
    -&gt; m (fstLayer, Generator generatorDevice))
-&gt; ModelSpec fstLayer
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) fstLayer
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec fstLayer
-&gt; Generator generatorDevice
-&gt; m (fstLayer, Generator generatorDevice)
forall model (generatorDevice :: Device (DeviceType Nat)) output
       (generatorOutputDevice :: Device (DeviceType Nat)) (m :: * -&gt; *).
(HasInitialize model generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
ModelSpec model
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var">initialize</span></a></span><span> </span><span class="annot"><span class="annottext">(ModelSpec fstLayer
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice) fstLayer)
-&gt; ModelSpec fstLayer
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) fstLayer
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec fstLayer
</span><a href="#local-6989586621679609777"><span class="hs-identifier hs-var">fstLayerSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-125"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m
  (Generator generatorDevice)
  (Generator generatorDevice)
  (activation
   -&gt; dropout
   -&gt; sndLayer
   -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     activation
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     (dropout
      -&gt; sndLayer
      -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
forall k1 (f :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a b
       (k2 :: k1).
IxApplicative f =&gt;
f i j (a -&gt; b) -&gt; f j k2 a -&gt; f i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&lt;&lt;*&gt;&gt;</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (activation, Generator generatorDevice))
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     activation
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice
  -&gt; m (activation, Generator generatorDevice))
 -&gt; IxStateT
      m
      (Generator generatorDevice)
      (Generator generatorDevice)
      activation)
-&gt; (ModelSpec activation
    -&gt; Generator generatorDevice
    -&gt; m (activation, Generator generatorDevice))
-&gt; ModelSpec activation
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     activation
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec activation
-&gt; Generator generatorDevice
-&gt; m (activation, Generator generatorDevice)
forall model (generatorDevice :: Device (DeviceType Nat)) output
       (generatorOutputDevice :: Device (DeviceType Nat)) (m :: * -&gt; *).
(HasInitialize model generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
ModelSpec model
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var">initialize</span></a></span><span> </span><span class="annot"><span class="annottext">(ModelSpec activation
 -&gt; IxStateT
      m
      (Generator generatorDevice)
      (Generator generatorDevice)
      activation)
-&gt; ModelSpec activation
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     activation
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec activation
</span><a href="#local-6989586621679609776"><span class="hs-identifier hs-var">activationSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-126"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m
  (Generator generatorDevice)
  (Generator generatorDevice)
  (dropout
   -&gt; sndLayer
   -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) dropout
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     (sndLayer -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
forall k1 (f :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a b
       (k2 :: k1).
IxApplicative f =&gt;
f i j (a -&gt; b) -&gt; f j k2 a -&gt; f i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&lt;&lt;*&gt;&gt;</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (dropout, Generator generatorDevice))
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) dropout
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice
  -&gt; m (dropout, Generator generatorDevice))
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice) dropout)
-&gt; (ModelSpec dropout
    -&gt; Generator generatorDevice
    -&gt; m (dropout, Generator generatorDevice))
-&gt; ModelSpec dropout
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) dropout
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec dropout
-&gt; Generator generatorDevice
-&gt; m (dropout, Generator generatorDevice)
forall model (generatorDevice :: Device (DeviceType Nat)) output
       (generatorOutputDevice :: Device (DeviceType Nat)) (m :: * -&gt; *).
(HasInitialize model generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
ModelSpec model
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var">initialize</span></a></span><span> </span><span class="annot"><span class="annottext">(ModelSpec dropout
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice) dropout)
-&gt; ModelSpec dropout
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) dropout
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec dropout
</span><a href="#local-6989586621679609775"><span class="hs-identifier hs-var">dropoutSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-127"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m
  (Generator generatorDevice)
  (Generator generatorDevice)
  (sndLayer -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) sndLayer
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     (TwoLayerNetwork fstLayer activation dropout sndLayer)
forall k1 (f :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a b
       (k2 :: k1).
IxApplicative f =&gt;
f i j (a -&gt; b) -&gt; f j k2 a -&gt; f i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&lt;&lt;*&gt;&gt;</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (sndLayer, Generator generatorDevice))
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) sndLayer
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice
  -&gt; m (sndLayer, Generator generatorDevice))
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice) sndLayer)
-&gt; (ModelSpec sndLayer
    -&gt; Generator generatorDevice
    -&gt; m (sndLayer, Generator generatorDevice))
-&gt; ModelSpec sndLayer
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) sndLayer
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec sndLayer
-&gt; Generator generatorDevice
-&gt; m (sndLayer, Generator generatorDevice)
forall model (generatorDevice :: Device (DeviceType Nat)) output
       (generatorOutputDevice :: Device (DeviceType Nat)) (m :: * -&gt; *).
(HasInitialize model generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
ModelSpec model
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var">initialize</span></a></span><span> </span><span class="annot"><span class="annottext">(ModelSpec sndLayer
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice) sndLayer)
-&gt; ModelSpec sndLayer
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice) sndLayer
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec sndLayer
</span><a href="#local-6989586621679609774"><span class="hs-identifier hs-var">sndLayerSpec</span></a></span><span class="hs-special">)</span></span></span></span></span></span><span>
</span><span id="line-128"></span><span>
</span><span id="line-129"></span><span class="hs-comment">-- | @HasStateDict@ instance for a two-layer network.</span><span>
</span><span id="line-130"></span><span class="hs-comment">-- It allows for conversion of a two-layer network into a state dictionary and back.</span><span>
</span><span id="line-131"></span><span class="hs-comment">--</span><span>
</span><span id="line-132"></span><span class="hs-comment">-- To create a two-layer network from a state dictionary,</span><span>
</span><span id="line-133"></span><span class="hs-comment">-- we need to first create its two neural layers, the activation function, and the dropout layer from the state dictionary.</span><span>
</span><span id="line-134"></span><span class="hs-comment">-- Afterwards, we combine the four components into a single network.</span><span>
</span><span id="line-135"></span><span class="hs-comment">--</span><span>
</span><span id="line-136"></span><span class="hs-comment">-- The state dictionary of the two-layer network is the union of the</span><span>
</span><span id="line-137"></span><span class="hs-comment">-- state dictionaries its layers.</span><span>
</span><span id="line-138"></span><span id="local-6989586621679609767"><span id="local-6989586621679609768"><span id="local-6989586621679609769"><span id="local-6989586621679609770"><span class="hs-keyword">instance</span><span>
</span><span id="line-139"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609770"><span class="hs-identifier hs-type">fstLayer</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609769"><span class="hs-identifier hs-type">activation</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609768"><span class="hs-identifier hs-type">dropout</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609767"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-140"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609770"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609769"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609768"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609767"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-141"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-142"></span><span>  </span><span id="local-6989586621679609763"><span class="annot"><span class="annottext">fromStateDict :: ModelSpec (TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; Text -&gt; m (TwoLayerNetwork fstLayer activation dropout sndLayer)
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var hs-var hs-var hs-var">fromStateDict</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span id="local-6989586621679609761"><span class="annot"><a href="#local-6989586621679609761"><span class="hs-identifier hs-var">fstLayerSpec</span></a></span></span><span> </span><span id="local-6989586621679609760"><span class="annot"><a href="#local-6989586621679609760"><span class="hs-identifier hs-var">activationSpec</span></a></span></span><span> </span><span id="local-6989586621679609759"><span class="annot"><a href="#local-6989586621679609759"><span class="hs-identifier hs-var">dropoutSpec</span></a></span></span><span> </span><span id="local-6989586621679609758"><span class="annot"><a href="#local-6989586621679609758"><span class="hs-identifier hs-var">sndLayerSpec</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679609757"><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609757"><span class="hs-identifier hs-var">k</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-143"></span><span>    </span><span class="annot"><span class="annottext">fstLayer
-&gt; activation
-&gt; dropout
-&gt; sndLayer
-&gt; TwoLayerNetwork fstLayer activation dropout sndLayer
forall fstLayer activation dropout sndLayer.
fstLayer
-&gt; activation
-&gt; dropout
-&gt; sndLayer
-&gt; TwoLayerNetwork fstLayer activation dropout sndLayer
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-var">TwoLayerNetwork</span></a></span><span>
</span><span id="line-144"></span><span>      </span><span class="annot"><span class="annottext">(fstLayer
 -&gt; activation
 -&gt; dropout
 -&gt; sndLayer
 -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; m fstLayer
-&gt; m (activation
      -&gt; dropout
      -&gt; sndLayer
      -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec fstLayer -&gt; Text -&gt; m fstLayer
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec fstLayer
</span><a href="#local-6989586621679609761"><span class="hs-identifier hs-var">fstLayerSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609757"><span class="hs-identifier hs-var">k</span></a></span><span>
</span><span id="line-145"></span><span>      </span><span class="annot"><span class="annottext">m (activation
   -&gt; dropout
   -&gt; sndLayer
   -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; m activation
-&gt; m (dropout
      -&gt; sndLayer
      -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec activation -&gt; Text -&gt; m activation
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec activation
</span><a href="#local-6989586621679609760"><span class="hs-identifier hs-var">activationSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609757"><span class="hs-identifier hs-var">k</span></a></span><span>
</span><span id="line-146"></span><span>      </span><span class="annot"><span class="annottext">m (dropout
   -&gt; sndLayer
   -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; m dropout
-&gt; m (sndLayer
      -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec dropout -&gt; Text -&gt; m dropout
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec dropout
</span><a href="#local-6989586621679609759"><span class="hs-identifier hs-var">dropoutSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609757"><span class="hs-identifier hs-var">k</span></a></span><span>
</span><span id="line-147"></span><span>      </span><span class="annot"><span class="annottext">m (sndLayer
   -&gt; TwoLayerNetwork fstLayer activation dropout sndLayer)
-&gt; m sndLayer
-&gt; m (TwoLayerNetwork fstLayer activation dropout sndLayer)
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec sndLayer -&gt; Text -&gt; m sndLayer
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec sndLayer
</span><a href="#local-6989586621679609758"><span class="hs-identifier hs-var">sndLayerSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609757"><span class="hs-identifier hs-var">k</span></a></span><span>
</span><span id="line-148"></span><span>  </span><span id="local-6989586621679609756"><span class="annot"><span class="annottext">toStateDict :: Text
-&gt; TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var hs-var hs-var hs-var">toStateDict</span></a></span></span><span> </span><span id="local-6989586621679609754"><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609754"><span class="hs-identifier hs-var">k</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679609750"><span id="local-6989586621679609751"><span id="local-6989586621679609752"><span id="local-6989586621679609753"><span class="annot"><span class="annottext">fstLayer
activation
dropout
sndLayer
tlnSndLayer :: sndLayer
tlnDropout :: dropout
tlnActivation :: activation
tlnFstLayer :: fstLayer
tlnSndLayer :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; sndLayer
tlnDropout :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; dropout
tlnActivation :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; activation
tlnFstLayer :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; fstLayer
</span><a href="#local-6989586621679609750"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-149"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Text -&gt; fstLayer -&gt; m ()
forall model (m :: * -&gt; *).
(HasStateDict model, MonadThrow m, MonadState StateDict m) =&gt;
Text -&gt; model -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var">toStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609754"><span class="hs-identifier hs-var">k</span></a></span><span> </span><span class="annot"><span class="annottext">fstLayer
</span><a href="#local-6989586621679609753"><span class="hs-identifier hs-var">tlnFstLayer</span></a></span><span>
</span><span id="line-150"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Text -&gt; activation -&gt; m ()
forall model (m :: * -&gt; *).
(HasStateDict model, MonadThrow m, MonadState StateDict m) =&gt;
Text -&gt; model -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var">toStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609754"><span class="hs-identifier hs-var">k</span></a></span><span> </span><span class="annot"><span class="annottext">activation
</span><a href="#local-6989586621679609752"><span class="hs-identifier hs-var">tlnActivation</span></a></span><span>
</span><span id="line-151"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Text -&gt; dropout -&gt; m ()
forall model (m :: * -&gt; *).
(HasStateDict model, MonadThrow m, MonadState StateDict m) =&gt;
Text -&gt; model -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var">toStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609754"><span class="hs-identifier hs-var">k</span></a></span><span> </span><span class="annot"><span class="annottext">dropout
</span><a href="#local-6989586621679609751"><span class="hs-identifier hs-var">tlnDropout</span></a></span><span>
</span><span id="line-152"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Text -&gt; sndLayer -&gt; m ()
forall model (m :: * -&gt; *).
(HasStateDict model, MonadThrow m, MonadState StateDict m) =&gt;
Text -&gt; model -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var">toStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><a href="#local-6989586621679609754"><span class="hs-identifier hs-var">k</span></a></span><span> </span><span class="annot"><span class="annottext">sndLayer
</span><a href="#local-6989586621679609750"><span class="hs-identifier hs-var">tlnSndLayer</span></a></span><span>
</span><span id="line-153"></span><span>    </span><span class="annot"><span class="annottext">() -&gt; m ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span></span></span></span></span><span>
</span><span id="line-154"></span><span>
</span><span id="line-155"></span><span class="hs-comment">-- | Specifies the type of the two-layer network.</span><span>
</span><span id="line-156"></span><span class="hs-keyword">type</span><span> </span><span id="TwoLayerNetworkF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetworkF"><span class="hs-identifier hs-var">TwoLayerNetworkF</span></a></span></span><span> </span><span id="local-6989586621679609749"><span class="annot"><a href="#local-6989586621679609749"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679609748"><span class="annot"><a href="#local-6989586621679609748"><span class="hs-identifier hs-type">hasDropout</span></a></span></span><span> </span><span id="local-6989586621679609747"><span class="annot"><a href="#local-6989586621679609747"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679609746"><span class="annot"><a href="#local-6989586621679609746"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679609745"><span class="annot"><a href="#local-6989586621679609745"><span class="hs-identifier hs-type">inputDim</span></a></span></span><span> </span><span id="local-6989586621679609744"><span class="annot"><a href="#local-6989586621679609744"><span class="hs-identifier hs-type">outputDim</span></a></span></span><span> </span><span id="local-6989586621679609743"><span class="annot"><a href="#local-6989586621679609743"><span class="hs-identifier hs-type">hiddenDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-157"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span>
</span><span id="line-158"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span>
</span><span id="line-159"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNFstLayerF"><span class="hs-identifier hs-type">TLNFstLayerF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609749"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609747"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609746"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609745"><span class="hs-identifier hs-type">inputDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609743"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-160"></span><span>        </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNActivationF"><span class="hs-identifier hs-type">TLNActivationF</span></a></span><span>
</span><span id="line-161"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNDropoutF"><span class="hs-identifier hs-type">TLNDropoutF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609748"><span class="hs-identifier hs-type">hasDropout</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-162"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNSndLayerF"><span class="hs-identifier hs-type">TLNSndLayerF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609749"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609747"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609746"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609744"><span class="hs-identifier hs-type">outputDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609743"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-163"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-164"></span><span>
</span><span id="line-165"></span><span class="hs-comment">-- | Specifies the type of the first layer of the neural network.</span><span>
</span><span id="line-166"></span><span class="hs-keyword">type</span><span> </span><span id="TLNFstLayerF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNFstLayerF"><span class="hs-identifier hs-var">TLNFstLayerF</span></a></span></span><span> </span><span id="local-6989586621679609739"><span class="annot"><a href="#local-6989586621679609739"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679609738"><span class="annot"><a href="#local-6989586621679609738"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679609737"><span class="annot"><a href="#local-6989586621679609737"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679609736"><span class="annot"><a href="#local-6989586621679609736"><span class="hs-identifier hs-type">inputDim</span></a></span></span><span> </span><span id="local-6989586621679609735"><span class="annot"><a href="#local-6989586621679609735"><span class="hs-identifier hs-type">hiddenDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-167"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span>
</span><span id="line-168"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Linear.html#GLinear"><span class="hs-identifier hs-type">GLinear</span></a></span><span>
</span><span id="line-169"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Linear.html#LinearWeightF"><span class="hs-identifier hs-type">LinearWeightF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609739"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609738"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609737"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609736"><span class="hs-identifier hs-type">inputDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609735"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-170"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Linear.html#LinearBiasF"><span class="hs-identifier hs-type">LinearBiasF</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Type.html#WithBias"><span class="hs-identifier hs-type">WithBias</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609739"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609738"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609737"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609735"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-171"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-172"></span><span>
</span><span id="line-173"></span><span class="hs-comment">-- | Specifies the type of the activation function</span><span>
</span><span id="line-174"></span><span class="hs-keyword">type</span><span> </span><span id="TLNActivationF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNActivationF"><span class="hs-identifier hs-var">TLNActivationF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-type">Tanh</span></a></span><span>
</span><span id="line-175"></span><span>
</span><span id="line-176"></span><span class="hs-comment">-- | Specifies the type of the dropout layer</span><span>
</span><span id="line-177"></span><span class="hs-keyword">type</span><span> </span><span class="hs-keyword">family</span><span> </span><span id="TLNDropoutF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNDropoutF"><span class="hs-identifier hs-var">TLNDropoutF</span></a></span></span><span> </span><span id="local-6989586621679609734"><span class="annot"><a href="#local-6989586621679609734"><span class="hs-identifier hs-type">hasDropout</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-178"></span><span>  </span><span id="TLNDropoutF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNDropoutF"><span class="hs-identifier hs-var">TLNDropoutF</span></a></span></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Type.html#WithDropout"><span class="hs-identifier hs-type">WithDropout</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-179"></span><span>  </span><span id="TLNDropoutF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNDropoutF"><span class="hs-identifier hs-var">TLNDropoutF</span></a></span></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Type.html#WithoutDropout"><span class="hs-identifier hs-type">WithoutDropout</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-180"></span><span>
</span><span id="line-181"></span><span class="hs-comment">-- | Specifies the type of the second layer of the neural network.</span><span>
</span><span id="line-182"></span><span class="hs-keyword">type</span><span> </span><span id="TLNSndLayerF"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TLNSndLayerF"><span class="hs-identifier hs-var">TLNSndLayerF</span></a></span></span><span> </span><span id="local-6989586621679609733"><span class="annot"><a href="#local-6989586621679609733"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679609732"><span class="annot"><a href="#local-6989586621679609732"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679609731"><span class="annot"><a href="#local-6989586621679609731"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679609730"><span class="annot"><a href="#local-6989586621679609730"><span class="hs-identifier hs-type">outputDim</span></a></span></span><span> </span><span id="local-6989586621679609729"><span class="annot"><a href="#local-6989586621679609729"><span class="hs-identifier hs-type">hiddenDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-183"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span>
</span><span id="line-184"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Linear.html#GLinear"><span class="hs-identifier hs-type">GLinear</span></a></span><span>
</span><span id="line-185"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Linear.html#LinearWeightF"><span class="hs-identifier hs-type">LinearWeightF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609733"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609732"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609731"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609729"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609730"><span class="hs-identifier hs-type">outputDim</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-186"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-type">NamedModel</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Linear.html#LinearBiasF"><span class="hs-identifier hs-type">LinearBiasF</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Type.html#WithBias"><span class="hs-identifier hs-type">WithBias</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609733"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609732"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609731"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609730"><span class="hs-identifier hs-type">outputDim</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-187"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-188"></span><span>
</span><span id="line-189"></span><span class="hs-comment">-- | Creates a value that specifies the parameters of a two-layer neural network.</span><span>
</span><span id="line-190"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#twoLayerNetworkSpec"><span class="hs-identifier hs-type">twoLayerNetworkSpec</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-191"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679609942"><span class="annot"><a href="#local-6989586621679609942"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679609941"><span class="annot"><a href="#local-6989586621679609941"><span class="hs-identifier hs-type">hasDropout</span></a></span></span><span> </span><span id="local-6989586621679609940"><span class="annot"><a href="#local-6989586621679609940"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679609939"><span class="annot"><a href="#local-6989586621679609939"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679609938"><span class="annot"><a href="#local-6989586621679609938"><span class="hs-identifier hs-type">inputDim</span></a></span></span><span> </span><span id="local-6989586621679609937"><span class="annot"><a href="#local-6989586621679609937"><span class="hs-identifier hs-type">outputDim</span></a></span></span><span> </span><span id="local-6989586621679609936"><span class="annot"><a href="#local-6989586621679609936"><span class="hs-identifier hs-type">hiddenDim</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-192"></span><span>  </span><span class="hs-comment">-- | whether or not to compute gradients for the parametrs</span><span>
</span><span id="line-193"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-type">SGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609942"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-194"></span><span>  </span><span class="hs-comment">-- | whether or not to use dropout</span><span>
</span><span id="line-195"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Type.html#SHasDropout"><span class="hs-identifier hs-type">SHasDropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609941"><span class="hs-identifier hs-type">hasDropout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-196"></span><span>  </span><span class="hs-comment">-- | which device to use</span><span>
</span><span id="line-197"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609940"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-198"></span><span>  </span><span class="hs-comment">-- | which data type to use</span><span>
</span><span id="line-199"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-type">SDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609939"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-200"></span><span>  </span><span class="hs-comment">-- | input dimension</span><span>
</span><span id="line-201"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-type">SDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609938"><span class="hs-identifier hs-type">inputDim</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-202"></span><span>  </span><span class="hs-comment">-- | output dimension</span><span>
</span><span id="line-203"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-type">SDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609937"><span class="hs-identifier hs-type">outputDim</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-204"></span><span>  </span><span class="hs-comment">-- | hidden dimension</span><span>
</span><span id="line-205"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#SDim"><span class="hs-identifier hs-type">SDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609936"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-206"></span><span>  </span><span class="hs-comment">-- | dropout rate</span><span>
</span><span id="line-207"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-208"></span><span>  </span><span class="hs-comment">-- | specification for the network</span><span>
</span><span id="line-209"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetworkF"><span class="hs-identifier hs-type">TwoLayerNetworkF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609942"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609941"><span class="hs-identifier hs-type">hasDropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609940"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609939"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609938"><span class="hs-identifier hs-type">inputDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609937"><span class="hs-identifier hs-type">outputDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609936"><span class="hs-identifier hs-type">hiddenDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-210"></span><span id="twoLayerNetworkSpec"><span class="annot"><span class="annottext">twoLayerNetworkSpec :: SGradient gradient
-&gt; SHasDropout hasDropout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; SDim inputDim
-&gt; SDim outputDim
-&gt; SDim hiddenDim
-&gt; Double
-&gt; ModelSpec
     (TwoLayerNetworkF
        gradient hasDropout device dataType inputDim outputDim hiddenDim)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#twoLayerNetworkSpec"><span class="hs-identifier hs-var hs-var">twoLayerNetworkSpec</span></a></span></span><span> </span><span id="local-6989586621679609727"><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679609727"><span class="hs-identifier hs-var">gradient</span></a></span></span><span> </span><span id="local-6989586621679609726"><span class="annot"><span class="annottext">SHasDropout hasDropout
</span><a href="#local-6989586621679609726"><span class="hs-identifier hs-var">hasDropout</span></a></span></span><span> </span><span id="local-6989586621679609725"><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679609725"><span class="hs-identifier hs-var">device</span></a></span></span><span> </span><span id="local-6989586621679609724"><span class="annot"><span class="annottext">SDataType dataType
</span><a href="#local-6989586621679609724"><span class="hs-identifier hs-var">dataType</span></a></span></span><span> </span><span id="local-6989586621679609723"><span class="annot"><span class="annottext">SDim inputDim
</span><a href="#local-6989586621679609723"><span class="hs-identifier hs-var">inputDim</span></a></span></span><span> </span><span id="local-6989586621679609722"><span class="annot"><span class="annottext">SDim outputDim
</span><a href="#local-6989586621679609722"><span class="hs-identifier hs-var">outputDim</span></a></span></span><span> </span><span id="local-6989586621679609721"><span class="annot"><span class="annottext">SDim hiddenDim
</span><a href="#local-6989586621679609721"><span class="hs-identifier hs-var">hiddenDim</span></a></span></span><span> </span><span id="local-6989586621679609720"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609720"><span class="hs-identifier hs-var">dropoutP</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-211"></span><span>  </span><span class="annot"><span class="annottext">Text
-&gt; TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 gradient
                 ('Layout 'Dense)
                 device
                 dataType
                 ('Shape '[hiddenDim, inputDim])))
           (NamedModel
              (TensorSpec
                 gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
     Tanh
     (ModelSpec (TLNDropoutF hasDropout))
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 gradient
                 ('Layout 'Dense)
                 device
                 dataType
                 ('Shape '[outputDim, hiddenDim])))
           (NamedModel
              (TensorSpec
                 gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))))
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    gradient
                    ('Layout 'Dense)
                    device
                    dataType
                    ('Shape '[hiddenDim, inputDim])))
              (NamedModel
                 (TensorSpec
                    gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
        Tanh
        (ModelSpec (TLNDropoutF hasDropout))
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    gradient
                    ('Layout 'Dense)
                    device
                    dataType
                    ('Shape '[outputDim, hiddenDim])))
              (NamedModel
                 (TensorSpec
                    gradient
                    ('Layout 'Dense)
                    device
                    dataType
                    ('Shape '[outputDim]))))))
forall model. Text -&gt; model -&gt; NamedModel model
</span><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-var">NamedModel</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-string">&quot;twoLayerNetwork&quot;</span></span><span> </span><span class="annot"><span class="annottext">(TwoLayerNetwork
   (NamedModel
      (GLinear
         (NamedModel
            (TensorSpec
               gradient
               ('Layout 'Dense)
               device
               dataType
               ('Shape '[hiddenDim, inputDim])))
         (NamedModel
            (TensorSpec
               gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
   Tanh
   (ModelSpec (TLNDropoutF hasDropout))
   (NamedModel
      (GLinear
         (NamedModel
            (TensorSpec
               gradient
               ('Layout 'Dense)
               device
               dataType
               ('Shape '[outputDim, hiddenDim])))
         (NamedModel
            (TensorSpec
               gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))))
 -&gt; NamedModel
      (TwoLayerNetwork
         (NamedModel
            (GLinear
               (NamedModel
                  (TensorSpec
                     gradient
                     ('Layout 'Dense)
                     device
                     dataType
                     ('Shape '[hiddenDim, inputDim])))
               (NamedModel
                  (TensorSpec
                     gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
         Tanh
         (ModelSpec (TLNDropoutF hasDropout))
         (NamedModel
            (GLinear
               (NamedModel
                  (TensorSpec
                     gradient
                     ('Layout 'Dense)
                     device
                     dataType
                     ('Shape '[outputDim, hiddenDim])))
               (NamedModel
                  (TensorSpec
                     gradient
                     ('Layout 'Dense)
                     device
                     dataType
                     ('Shape '[outputDim])))))))
-&gt; TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 gradient
                 ('Layout 'Dense)
                 device
                 dataType
                 ('Shape '[hiddenDim, inputDim])))
           (NamedModel
              (TensorSpec
                 gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
     Tanh
     (ModelSpec (TLNDropoutF hasDropout))
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 gradient
                 ('Layout 'Dense)
                 device
                 dataType
                 ('Shape '[outputDim, hiddenDim])))
           (NamedModel
              (TensorSpec
                 gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))))
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    gradient
                    ('Layout 'Dense)
                    device
                    dataType
                    ('Shape '[hiddenDim, inputDim])))
              (NamedModel
                 (TensorSpec
                    gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
        Tanh
        (ModelSpec (TLNDropoutF hasDropout))
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    gradient
                    ('Layout 'Dense)
                    device
                    dataType
                    ('Shape '[outputDim, hiddenDim])))
              (NamedModel
                 (TensorSpec
                    gradient
                    ('Layout 'Dense)
                    device
                    dataType
                    ('Shape '[outputDim]))))))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-212"></span><span>    </span><span class="annot"><span class="annottext">NamedModel
  (GLinear
     (NamedModel
        (TensorSpec
           gradient
           ('Layout 'Dense)
           device
           dataType
           ('Shape '[hiddenDim, inputDim])))
     (NamedModel
        (TensorSpec
           gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim]))))
-&gt; Tanh
-&gt; ModelSpec (TLNDropoutF hasDropout)
-&gt; NamedModel
     (GLinear
        (NamedModel
           (TensorSpec
              gradient
              ('Layout 'Dense)
              device
              dataType
              ('Shape '[outputDim, hiddenDim])))
        (NamedModel
           (TensorSpec
              gradient ('Layout 'Dense) device dataType ('Shape '[outputDim]))))
-&gt; TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 gradient
                 ('Layout 'Dense)
                 device
                 dataType
                 ('Shape '[hiddenDim, inputDim])))
           (NamedModel
              (TensorSpec
                 gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
     Tanh
     (ModelSpec (TLNDropoutF hasDropout))
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 gradient
                 ('Layout 'Dense)
                 device
                 dataType
                 ('Shape '[outputDim, hiddenDim])))
           (NamedModel
              (TensorSpec
                 gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))))
forall fstLayer activation dropout sndLayer.
fstLayer
-&gt; activation
-&gt; dropout
-&gt; sndLayer
-&gt; TwoLayerNetwork fstLayer activation dropout sndLayer
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-var">TwoLayerNetwork</span></a></span><span>
</span><span id="line-213"></span><span>      </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Text
-&gt; GLinear
     (NamedModel
        (TensorSpec
           gradient
           ('Layout 'Dense)
           device
           dataType
           ('Shape '[hiddenDim, inputDim])))
     (NamedModel
        (TensorSpec
           gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))
-&gt; NamedModel
     (GLinear
        (NamedModel
           (TensorSpec
              gradient
              ('Layout 'Dense)
              device
              dataType
              ('Shape '[hiddenDim, inputDim])))
        (NamedModel
           (TensorSpec
              gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim]))))
forall model. Text -&gt; model -&gt; NamedModel model
</span><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-var">NamedModel</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-string">&quot;fstLayer&quot;</span></span><span> </span><span class="annot"><span class="annottext">(GLinear
   (NamedModel
      (TensorSpec
         gradient
         ('Layout 'Dense)
         device
         dataType
         ('Shape '[hiddenDim, inputDim])))
   (NamedModel
      (TensorSpec
         gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))
 -&gt; NamedModel
      (GLinear
         (NamedModel
            (TensorSpec
               gradient
               ('Layout 'Dense)
               device
               dataType
               ('Shape '[hiddenDim, inputDim])))
         (NamedModel
            (TensorSpec
               gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))))
-&gt; GLinear
     (NamedModel
        (TensorSpec
           gradient
           ('Layout 'Dense)
           device
           dataType
           ('Shape '[hiddenDim, inputDim])))
     (NamedModel
        (TensorSpec
           gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim])))
-&gt; NamedModel
     (GLinear
        (NamedModel
           (TensorSpec
              gradient
              ('Layout 'Dense)
              device
              dataType
              ('Shape '[hiddenDim, inputDim])))
        (NamedModel
           (TensorSpec
              gradient ('Layout 'Dense) device dataType ('Shape '[hiddenDim]))))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SHasBias 'WithBias
-&gt; SGradient gradient
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; SDim inputDim
-&gt; SDim hiddenDim
-&gt; ModelSpec
     (GLinear
        (NamedModel
           (LinearWeightF gradient device dataType inputDim hiddenDim))
        (NamedModel
           (LinearBiasF 'WithBias gradient device dataType hiddenDim)))
forall (hasBias :: HasBias) (gradient :: Gradient RequiresGradient)
       (device :: Device (DeviceType Nat)) (dataType :: DataType DType)
       (inputDim :: Dim (Name Symbol) (Size Nat))
       (outputDim :: Dim (Name Symbol) (Size Nat)).
SHasBias hasBias
-&gt; SGradient gradient
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; SDim inputDim
-&gt; SDim outputDim
-&gt; ModelSpec
     (GLinear
        (NamedModel
           (LinearWeightF gradient device dataType inputDim outputDim))
        (NamedModel
           (LinearBiasF hasBias gradient device dataType outputDim)))
</span><a href="Torch.GraduallyTyped.NN.Linear.html#linearSpec"><span class="hs-identifier hs-var">linearSpec</span></a></span><span> </span><span class="annot"><span class="annottext">SHasBias 'WithBias
</span><a href="Torch.GraduallyTyped.NN.Type.html#SWithBias"><span class="hs-identifier hs-var">SWithBias</span></a></span><span> </span><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679609727"><span class="hs-identifier hs-var">gradient</span></a></span><span> </span><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679609725"><span class="hs-identifier hs-var">device</span></a></span><span> </span><span class="annot"><span class="annottext">SDataType dataType
</span><a href="#local-6989586621679609724"><span class="hs-identifier hs-var">dataType</span></a></span><span> </span><span class="annot"><span class="annottext">SDim inputDim
</span><a href="#local-6989586621679609723"><span class="hs-identifier hs-var">inputDim</span></a></span><span> </span><span class="annot"><span class="annottext">SDim hiddenDim
</span><a href="#local-6989586621679609721"><span class="hs-identifier hs-var">hiddenDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-214"></span><span>      </span><span class="annot"><span class="annottext">Tanh
</span><a href="Torch.GraduallyTyped.NN.Activation.html#Tanh"><span class="hs-identifier hs-var">Tanh</span></a></span><span>
</span><span id="line-215"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">SHasDropout hasDropout
</span><a href="#local-6989586621679609726"><span class="hs-identifier hs-var">hasDropout</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-216"></span><span>          </span><span class="annot"><span class="annottext">SHasDropout hasDropout
</span><a href="Torch.GraduallyTyped.NN.Type.html#SWithDropout"><span class="hs-identifier hs-var">SWithDropout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Dropout
</span><a href="Torch.GraduallyTyped.NN.Dropout.html#Dropout"><span class="hs-identifier hs-var">Dropout</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609720"><span class="hs-identifier hs-var">dropoutP</span></a></span><span>
</span><span id="line-217"></span><span>          </span><span class="annot"><span class="annottext">SHasDropout hasDropout
</span><a href="Torch.GraduallyTyped.NN.Type.html#SWithoutDropout"><span class="hs-identifier hs-var">SWithoutDropout</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-218"></span><span>      </span><span class="hs-special">)</span><span>
</span><span id="line-219"></span><span>      </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Text
-&gt; GLinear
     (NamedModel
        (TensorSpec
           gradient
           ('Layout 'Dense)
           device
           dataType
           ('Shape '[outputDim, hiddenDim])))
     (NamedModel
        (TensorSpec
           gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))
-&gt; NamedModel
     (GLinear
        (NamedModel
           (TensorSpec
              gradient
              ('Layout 'Dense)
              device
              dataType
              ('Shape '[outputDim, hiddenDim])))
        (NamedModel
           (TensorSpec
              gradient ('Layout 'Dense) device dataType ('Shape '[outputDim]))))
forall model. Text -&gt; model -&gt; NamedModel model
</span><a href="Torch.GraduallyTyped.NN.Class.html#NamedModel"><span class="hs-identifier hs-var">NamedModel</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-string">&quot;sndLayer&quot;</span></span><span> </span><span class="annot"><span class="annottext">(GLinear
   (NamedModel
      (TensorSpec
         gradient
         ('Layout 'Dense)
         device
         dataType
         ('Shape '[outputDim, hiddenDim])))
   (NamedModel
      (TensorSpec
         gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))
 -&gt; NamedModel
      (GLinear
         (NamedModel
            (TensorSpec
               gradient
               ('Layout 'Dense)
               device
               dataType
               ('Shape '[outputDim, hiddenDim])))
         (NamedModel
            (TensorSpec
               gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))))
-&gt; GLinear
     (NamedModel
        (TensorSpec
           gradient
           ('Layout 'Dense)
           device
           dataType
           ('Shape '[outputDim, hiddenDim])))
     (NamedModel
        (TensorSpec
           gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])))
-&gt; NamedModel
     (GLinear
        (NamedModel
           (TensorSpec
              gradient
              ('Layout 'Dense)
              device
              dataType
              ('Shape '[outputDim, hiddenDim])))
        (NamedModel
           (TensorSpec
              gradient ('Layout 'Dense) device dataType ('Shape '[outputDim]))))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">SHasBias 'WithBias
-&gt; SGradient gradient
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; SDim hiddenDim
-&gt; SDim outputDim
-&gt; ModelSpec
     (GLinear
        (NamedModel
           (LinearWeightF gradient device dataType hiddenDim outputDim))
        (NamedModel
           (LinearBiasF 'WithBias gradient device dataType outputDim)))
forall (hasBias :: HasBias) (gradient :: Gradient RequiresGradient)
       (device :: Device (DeviceType Nat)) (dataType :: DataType DType)
       (inputDim :: Dim (Name Symbol) (Size Nat))
       (outputDim :: Dim (Name Symbol) (Size Nat)).
SHasBias hasBias
-&gt; SGradient gradient
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; SDim inputDim
-&gt; SDim outputDim
-&gt; ModelSpec
     (GLinear
        (NamedModel
           (LinearWeightF gradient device dataType inputDim outputDim))
        (NamedModel
           (LinearBiasF hasBias gradient device dataType outputDim)))
</span><a href="Torch.GraduallyTyped.NN.Linear.html#linearSpec"><span class="hs-identifier hs-var">linearSpec</span></a></span><span> </span><span class="annot"><span class="annottext">SHasBias 'WithBias
</span><a href="Torch.GraduallyTyped.NN.Type.html#SWithBias"><span class="hs-identifier hs-var">SWithBias</span></a></span><span> </span><span class="annot"><span class="annottext">SGradient gradient
</span><a href="#local-6989586621679609727"><span class="hs-identifier hs-var">gradient</span></a></span><span> </span><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679609725"><span class="hs-identifier hs-var">device</span></a></span><span> </span><span class="annot"><span class="annottext">SDataType dataType
</span><a href="#local-6989586621679609724"><span class="hs-identifier hs-var">dataType</span></a></span><span> </span><span class="annot"><span class="annottext">SDim hiddenDim
</span><a href="#local-6989586621679609721"><span class="hs-identifier hs-var">hiddenDim</span></a></span><span> </span><span class="annot"><span class="annottext">SDim outputDim
</span><a href="#local-6989586621679609722"><span class="hs-identifier hs-var">outputDim</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-220"></span><span>
</span><span id="line-221"></span><span class="hs-comment">-- | 'HasForward' instance used to define the forward pass of the model.</span><span>
</span><span id="line-222"></span><span class="hs-comment">-- The forward pass is defined as the composition of the forward passes of the two layers.</span><span>
</span><span id="line-223"></span><span class="hs-comment">-- A forward pass is a function that takes a two-layer network, an input tensor, and a random generator,</span><span>
</span><span id="line-224"></span><span class="hs-comment">-- and returns the output tensor and the updated generator.</span><span>
</span><span id="line-225"></span><span class="hs-comment">-- A nonlinearity, @tanh@, is applied to the output of the first layer.</span><span>
</span><span id="line-226"></span><span class="hs-comment">-- The input to the second layer is the output of the nonlinearity.</span><span>
</span><span id="line-227"></span><span id="local-6989586621679609695"><span id="local-6989586621679609696"><span id="local-6989586621679609697"><span id="local-6989586621679609698"><span id="local-6989586621679609699"><span id="local-6989586621679609700"><span id="local-6989586621679609701"><span id="local-6989586621679609702"><span id="local-6989586621679609703"><span id="local-6989586621679609704"><span id="local-6989586621679609705"><span id="local-6989586621679609706"><span id="local-6989586621679609707"><span id="local-6989586621679609708"><span id="local-6989586621679609709"><span id="local-6989586621679609710"><span id="local-6989586621679609711"><span id="local-6989586621679609712"><span class="hs-keyword">instance</span><span>
</span><span id="line-228"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-229"></span><span>      </span><span class="annot"><a href="#local-6989586621679609712"><span class="hs-identifier hs-type">fstLayer</span></a></span><span>
</span><span id="line-230"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609711"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609710"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609709"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609708"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609707"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-231"></span><span>      </span><span class="annot"><a href="#local-6989586621679609706"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-232"></span><span>      </span><span class="annot"><a href="#local-6989586621679609705"><span class="hs-identifier hs-type">output0</span></a></span><span>
</span><span id="line-233"></span><span>      </span><span class="annot"><a href="#local-6989586621679609704"><span class="hs-identifier hs-type">generatorDevice0</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-234"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-235"></span><span>      </span><span class="annot"><a href="#local-6989586621679609703"><span class="hs-identifier hs-type">activation</span></a></span><span>
</span><span id="line-236"></span><span>      </span><span class="annot"><a href="#local-6989586621679609705"><span class="hs-identifier hs-type">output0</span></a></span><span>
</span><span id="line-237"></span><span>      </span><span class="annot"><a href="#local-6989586621679609704"><span class="hs-identifier hs-type">generatorDevice0</span></a></span><span>
</span><span id="line-238"></span><span>      </span><span class="annot"><a href="#local-6989586621679609702"><span class="hs-identifier hs-type">output1</span></a></span><span>
</span><span id="line-239"></span><span>      </span><span class="annot"><a href="#local-6989586621679609701"><span class="hs-identifier hs-type">generatorDevice1</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-240"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-241"></span><span>      </span><span class="annot"><a href="#local-6989586621679609700"><span class="hs-identifier hs-type">dropout</span></a></span><span>
</span><span id="line-242"></span><span>      </span><span class="annot"><a href="#local-6989586621679609702"><span class="hs-identifier hs-type">output1</span></a></span><span>
</span><span id="line-243"></span><span>      </span><span class="annot"><a href="#local-6989586621679609701"><span class="hs-identifier hs-type">generatorDevice1</span></a></span><span>
</span><span id="line-244"></span><span>      </span><span class="annot"><a href="#local-6989586621679609699"><span class="hs-identifier hs-type">output2</span></a></span><span>
</span><span id="line-245"></span><span>      </span><span class="annot"><a href="#local-6989586621679609698"><span class="hs-identifier hs-type">generatorDevice2</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-246"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-247"></span><span>      </span><span class="annot"><a href="#local-6989586621679609697"><span class="hs-identifier hs-type">sndLayer</span></a></span><span>
</span><span id="line-248"></span><span>      </span><span class="annot"><a href="#local-6989586621679609699"><span class="hs-identifier hs-type">output2</span></a></span><span>
</span><span id="line-249"></span><span>      </span><span class="annot"><a href="#local-6989586621679609698"><span class="hs-identifier hs-type">generatorDevice2</span></a></span><span>
</span><span id="line-250"></span><span>      </span><span class="annot"><a href="#local-6989586621679609696"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-251"></span><span>      </span><span class="annot"><a href="#local-6989586621679609695"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span>
</span><span id="line-252"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-253"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-254"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609712"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609703"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609700"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609697"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-255"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609711"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609710"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609709"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609708"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609707"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-256"></span><span>    </span><span class="annot"><a href="#local-6989586621679609706"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-257"></span><span>    </span><span class="annot"><a href="#local-6989586621679609696"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-258"></span><span>    </span><span class="annot"><a href="#local-6989586621679609695"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span>
</span><span id="line-259"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-260"></span><span>  </span><span id="local-6989586621679609692"><span class="annot"><span class="annottext">forward :: TwoLayerNetwork fstLayer activation dropout sndLayer
-&gt; Tensor gradient layout dataType device shape
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679609687"><span id="local-6989586621679609688"><span id="local-6989586621679609689"><span id="local-6989586621679609690"><span class="annot"><span class="annottext">fstLayer
activation
dropout
sndLayer
tlnSndLayer :: sndLayer
tlnDropout :: dropout
tlnActivation :: activation
tlnFstLayer :: fstLayer
tlnSndLayer :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; sndLayer
tlnDropout :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; dropout
tlnActivation :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; activation
tlnFstLayer :: forall fstLayer activation dropout sndLayer.
TwoLayerNetwork fstLayer activation dropout sndLayer -&gt; fstLayer
</span><a href="#local-6989586621679609687"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679609686"><span class="annot"><span class="annottext">Tensor gradient layout dataType device shape
</span><a href="#local-6989586621679609686"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-261"></span><span>    </span><span class="annot"><span class="annottext">IxStateT
  m
  (Generator generatorDevice)
  (Generator generatorOutputDevice)
  output
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
forall (m :: * -&gt; *) i j a. IxStateT m i j a -&gt; i -&gt; m (a, j)
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var hs-var">runIxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">(IxStateT
   m
   (Generator generatorDevice)
   (Generator generatorOutputDevice)
   output
 -&gt; Generator generatorDevice
 -&gt; m (output, Generator generatorOutputDevice))
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorOutputDevice)
     output
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-262"></span><span>      </span><span class="annot"><span class="annottext">Tensor gradient layout dataType device shape
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorDevice)
     (Tensor gradient layout dataType device shape)
forall k (m :: k -&gt; k -&gt; * -&gt; *) a (i :: k).
IxPointed m =&gt;
a -&gt; m i i a
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-identifier hs-var">ireturn</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout dataType device shape
</span><a href="#local-6989586621679609686"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-263"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m
  (Generator generatorDevice)
  (Generator generatorDevice)
  (Tensor gradient layout dataType device shape)
-&gt; (Tensor gradient layout dataType device shape
    -&gt; IxStateT
         m (Generator generatorDevice) (Generator generatorDevice0) output0)
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice0) output0
forall k1 (m :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a
       (k2 :: k1) b.
IxMonad m =&gt;
m i j a -&gt; (a -&gt; m j k2 b) -&gt; m i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&gt;&gt;&gt;=</span></a></span><span> </span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (output0, Generator generatorDevice0))
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice0) output0
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice
  -&gt; m (output0, Generator generatorDevice0))
 -&gt; IxStateT
      m (Generator generatorDevice) (Generator generatorDevice0) output0)
-&gt; (Tensor gradient layout dataType device shape
    -&gt; Generator generatorDevice
    -&gt; m (output0, Generator generatorDevice0))
-&gt; Tensor gradient layout dataType device shape
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice0) output0
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">fstLayer
-&gt; Tensor gradient layout dataType device shape
-&gt; Generator generatorDevice
-&gt; m (output0, Generator generatorDevice0)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">fstLayer
</span><a href="#local-6989586621679609690"><span class="hs-identifier hs-var">tlnFstLayer</span></a></span><span>
</span><span id="line-264"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m (Generator generatorDevice) (Generator generatorDevice0) output0
-&gt; (output0
    -&gt; IxStateT
         m
         (Generator generatorDevice0)
         (Generator generatorDevice1)
         output1)
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice1) output1
forall k1 (m :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a
       (k2 :: k1) b.
IxMonad m =&gt;
m i j a -&gt; (a -&gt; m j k2 b) -&gt; m i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&gt;&gt;&gt;=</span></a></span><span> </span><span class="annot"><span class="annottext">(Generator generatorDevice0
 -&gt; m (output1, Generator generatorDevice1))
-&gt; IxStateT
     m (Generator generatorDevice0) (Generator generatorDevice1) output1
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice0
  -&gt; m (output1, Generator generatorDevice1))
 -&gt; IxStateT
      m
      (Generator generatorDevice0)
      (Generator generatorDevice1)
      output1)
-&gt; (output0
    -&gt; Generator generatorDevice0
    -&gt; m (output1, Generator generatorDevice1))
-&gt; output0
-&gt; IxStateT
     m (Generator generatorDevice0) (Generator generatorDevice1) output1
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">activation
-&gt; output0
-&gt; Generator generatorDevice0
-&gt; m (output1, Generator generatorDevice1)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">activation
</span><a href="#local-6989586621679609689"><span class="hs-identifier hs-var">tlnActivation</span></a></span><span>
</span><span id="line-265"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m (Generator generatorDevice) (Generator generatorDevice1) output1
-&gt; (output1
    -&gt; IxStateT
         m
         (Generator generatorDevice1)
         (Generator generatorDevice2)
         output2)
-&gt; IxStateT
     m (Generator generatorDevice) (Generator generatorDevice2) output2
forall k1 (m :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a
       (k2 :: k1) b.
IxMonad m =&gt;
m i j a -&gt; (a -&gt; m j k2 b) -&gt; m i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&gt;&gt;&gt;=</span></a></span><span> </span><span class="annot"><span class="annottext">(Generator generatorDevice1
 -&gt; m (output2, Generator generatorDevice2))
-&gt; IxStateT
     m (Generator generatorDevice1) (Generator generatorDevice2) output2
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice1
  -&gt; m (output2, Generator generatorDevice2))
 -&gt; IxStateT
      m
      (Generator generatorDevice1)
      (Generator generatorDevice2)
      output2)
-&gt; (output1
    -&gt; Generator generatorDevice1
    -&gt; m (output2, Generator generatorDevice2))
-&gt; output1
-&gt; IxStateT
     m (Generator generatorDevice1) (Generator generatorDevice2) output2
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">dropout
-&gt; output1
-&gt; Generator generatorDevice1
-&gt; m (output2, Generator generatorDevice2)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">dropout
</span><a href="#local-6989586621679609688"><span class="hs-identifier hs-var">tlnDropout</span></a></span><span>
</span><span id="line-266"></span><span>        </span><span class="annot"><span class="annottext">IxStateT
  m (Generator generatorDevice) (Generator generatorDevice2) output2
-&gt; (output2
    -&gt; IxStateT
         m
         (Generator generatorDevice2)
         (Generator generatorOutputDevice)
         output)
-&gt; IxStateT
     m
     (Generator generatorDevice)
     (Generator generatorOutputDevice)
     output
forall k1 (m :: k1 -&gt; k1 -&gt; * -&gt; *) (i :: k1) (j :: k1) a
       (k2 :: k1) b.
IxMonad m =&gt;
m i j a -&gt; (a -&gt; m j k2 b) -&gt; m i k2 b
</span><a href="../file:///nix/store/y93sl1ahi24xw24f0y8xsbhzkmpzqzqh-indexed-lib-indexed-0.1.3-haddock-doc/share/doc/indexed/html/src"><span class="hs-operator hs-var">&gt;&gt;&gt;=</span></a></span><span> </span><span class="annot"><span class="annottext">(Generator generatorDevice2
 -&gt; m (output, Generator generatorOutputDevice))
-&gt; IxStateT
     m
     (Generator generatorDevice2)
     (Generator generatorOutputDevice)
     output
forall (m :: * -&gt; *) i j a. (i -&gt; m (a, j)) -&gt; IxStateT m i j a
</span><a href="../file:///nix/store/8vjaffqxij7bih93d9mlzmc7ksanlw0i-indexed-extras-lib-indexed-extras-0.2-haddock-doc/share/doc/indexed-extras/html/src"><span class="hs-identifier hs-var">IxStateT</span></a></span><span> </span><span class="annot"><span class="annottext">((Generator generatorDevice2
  -&gt; m (output, Generator generatorOutputDevice))
 -&gt; IxStateT
      m
      (Generator generatorDevice2)
      (Generator generatorOutputDevice)
      output)
-&gt; (output2
    -&gt; Generator generatorDevice2
    -&gt; m (output, Generator generatorOutputDevice))
-&gt; output2
-&gt; IxStateT
     m
     (Generator generatorDevice2)
     (Generator generatorOutputDevice)
     output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">sndLayer
-&gt; output2
-&gt; Generator generatorDevice2
-&gt; m (output, Generator generatorOutputDevice)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">sndLayer
</span><a href="#local-6989586621679609687"><span class="hs-identifier hs-var">tlnSndLayer</span></a></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-267"></span><span>
</span><span id="line-268"></span><span id="local-6989586621679609668"><span id="local-6989586621679609669"><span id="local-6989586621679609670"><span id="local-6989586621679609671"><span id="local-6989586621679609672"><span id="local-6989586621679609673"><span id="local-6989586621679609674"><span id="local-6989586621679609675"><span id="local-6989586621679609676"><span id="local-6989586621679609677"><span id="local-6989586621679609678"><span id="local-6989586621679609679"><span id="local-6989586621679609680"><span id="local-6989586621679609681"><span id="local-6989586621679609682"><span id="local-6989586621679609683"><span id="local-6989586621679609684"><span id="local-6989586621679609685"><span class="hs-keyword">instance</span><span>
</span><span id="line-269"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-270"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609685"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609684"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609683"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609682"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-271"></span><span>      </span><span class="annot"><a href="#local-6989586621679609681"><span class="hs-identifier hs-type">input</span></a></span><span>
</span><span id="line-272"></span><span>      </span><span class="annot"><a href="#local-6989586621679609680"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-273"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609679"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609678"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609677"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609676"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609675"><span class="hs-identifier hs-type">shape</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-274"></span><span>      </span><span class="annot"><a href="#local-6989586621679609674"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-275"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609675"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609673"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-276"></span><span>    </span><span class="annot"><a href="#local-6989586621679609672"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-277"></span><span>      </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span>
</span><span id="line-278"></span><span>          </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609679"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%7C%3E"><span class="hs-operator hs-type">&lt;|&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609671"><span class="hs-identifier hs-type">gradient'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-279"></span><span>          </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609678"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609670"><span class="hs-identifier hs-type">layout'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-280"></span><span>          </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609677"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609669"><span class="hs-identifier hs-type">device'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-281"></span><span>          </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609676"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609668"><span class="hs-identifier hs-type">dataType'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-282"></span><span>          </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-283"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-284"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span>
</span><span id="line-285"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TwoLayerNetwork"><span class="hs-identifier hs-type">TwoLayerNetwork</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609685"><span class="hs-identifier hs-type">fstLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609684"><span class="hs-identifier hs-type">activation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609683"><span class="hs-identifier hs-type">dropout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609682"><span class="hs-identifier hs-type">sndLayer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-286"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609681"><span class="hs-identifier hs-type">input</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609671"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609670"><span class="hs-identifier hs-type">layout'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609669"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609668"><span class="hs-identifier hs-type">dataType'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609673"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-287"></span><span>    </span><span class="annot"><a href="#local-6989586621679609680"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span>
</span><span id="line-288"></span><span>    </span><span class="annot"><a href="#local-6989586621679609672"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-289"></span><span>    </span><span class="annot"><a href="#local-6989586621679609674"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span>
</span><span id="line-290"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-291"></span><span>  </span><span id="local-6989586621679609666"><span class="annot"><span class="annottext">forward :: TwoLayerNetwork fstLayer activation dropout sndLayer
-&gt; (input, Tensor gradient' layout' device' dataType' shape')
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="#local-6989586621679609666"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span id="local-6989586621679609665"><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer
</span><a href="#local-6989586621679609665"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609664"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609664"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609663"><span class="annot"><span class="annottext">Tensor gradient' layout' device' dataType' shape'
</span><a href="#local-6989586621679609663"><span class="hs-identifier hs-var">expectedOutput</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679609662"><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609662"><span class="hs-identifier hs-var">g</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-292"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679609661"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609661"><span class="hs-identifier hs-var">output</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609660"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609660"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (Tensor gradient layout device dataType shape,
      Generator generatorOutputDevice)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">TwoLayerNetwork fstLayer activation dropout sndLayer
</span><a href="#local-6989586621679609665"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609664"><span class="hs-identifier hs-var">input</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609662"><span class="hs-identifier hs-var">g</span></a></span><span>
</span><span id="line-293"></span><span>    </span><span id="local-6989586621679609659"><span class="annot"><span class="annottext">output
</span><a href="#local-6989586621679609659"><span class="hs-identifier hs-var">loss</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609661"><span class="hs-identifier hs-var">output</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; Tensor gradient' layout' device' dataType' shape'
-&gt; m (Tensor
        (gradient &lt;|&gt; gradient')
        (layout &lt;+&gt; layout')
        (device &lt;+&gt; device')
        (dataType &lt;+&gt; dataType')
        ('Shape '[]))
forall (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)])
       (gradient' :: Gradient RequiresGradient)
       (layout' :: Layout LayoutType) (device' :: Device (DeviceType Nat))
       (dataType' :: DataType DType)
       (shape' :: Shape [Dim (Name Symbol) (Size Nat)]).
(MonadThrow m, Catch (shape &lt;+&gt; shape')) =&gt;
Tensor gradient layout device dataType shape
-&gt; Tensor gradient' layout' device' dataType' shape'
-&gt; m (Tensor
        (gradient &lt;|&gt; gradient')
        (layout &lt;+&gt; layout')
        (device &lt;+&gt; device')
        (dataType &lt;+&gt; dataType')
        ('Shape '[]))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mseLoss"><span class="hs-operator hs-var">`mseLoss`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient' layout' device' dataType' shape'
</span><a href="#local-6989586621679609663"><span class="hs-identifier hs-var">expectedOutput</span></a></span><span>
</span><span id="line-294"></span><span>    </span><span class="annot"><span class="annottext">(output, Generator generatorOutputDevice)
-&gt; m (output, Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">output
</span><a href="#local-6989586621679609659"><span class="hs-identifier hs-var">loss</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609660"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-295"></span><span>
</span><span id="line-296"></span><span class="hs-comment">-- | Options for the Adam optimizer.</span><span>
</span><span id="line-297"></span><span class="hs-keyword">data</span><span> </span><span id="AdamOptions"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#AdamOptions"><span class="hs-identifier hs-var">AdamOptions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="AdamOptions"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#AdamOptions"><span class="hs-identifier hs-var">AdamOptions</span></a></span></span><span>
</span><span id="line-298"></span><span>  </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | learning rate</span><span>
</span><span id="line-299"></span><span>    </span><span id="learningRate"><span class="annot"><span class="annottext">AdamOptions -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#learningRate"><span class="hs-identifier hs-var hs-var">learningRate</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-300"></span><span>    </span><span class="hs-comment">-- | beta1</span><span>
</span><span id="line-301"></span><span>    </span><span id="beta1"><span class="annot"><span class="annottext">AdamOptions -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#beta1"><span class="hs-identifier hs-var hs-var">beta1</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-302"></span><span>    </span><span class="hs-comment">-- | beta2</span><span>
</span><span id="line-303"></span><span>    </span><span id="beta2"><span class="annot"><span class="annottext">AdamOptions -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#beta2"><span class="hs-identifier hs-var hs-var">beta2</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-304"></span><span>    </span><span class="hs-comment">-- | epsilon</span><span>
</span><span id="line-305"></span><span>    </span><span id="epsilon"><span class="annot"><span class="annottext">AdamOptions -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#epsilon"><span class="hs-identifier hs-var hs-var">epsilon</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-306"></span><span>    </span><span class="hs-comment">-- | weight decay</span><span>
</span><span id="line-307"></span><span>    </span><span id="weightDecay"><span class="annot"><span class="annottext">AdamOptions -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#weightDecay"><span class="hs-identifier hs-var hs-var">weightDecay</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-308"></span><span>    </span><span class="hs-comment">-- | use amsgrad</span><span>
</span><span id="line-309"></span><span>    </span><span id="amsgrad"><span class="annot"><span class="annottext">AdamOptions -&gt; Bool
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#amsgrad"><span class="hs-identifier hs-var hs-var">amsgrad</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-310"></span><span>  </span><span class="hs-special">}</span><span>
</span><span id="line-311"></span><span>
</span><span id="line-312"></span><span class="hs-comment">-- | Default Adam options.</span><span>
</span><span id="line-313"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#defaultAdamOptions"><span class="hs-identifier hs-type">defaultAdamOptions</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#AdamOptions"><span class="hs-identifier hs-type">AdamOptions</span></a></span><span>
</span><span id="line-314"></span><span id="defaultAdamOptions"><span class="annot"><span class="annottext">defaultAdamOptions :: AdamOptions
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#defaultAdamOptions"><span class="hs-identifier hs-var hs-var">defaultAdamOptions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-315"></span><span>  </span><span class="annot"><span class="annottext">AdamOptions :: Double
-&gt; Double -&gt; Double -&gt; Double -&gt; Double -&gt; Bool -&gt; AdamOptions
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#AdamOptions"><span class="hs-identifier hs-type">AdamOptions</span></a></span><span>
</span><span id="line-316"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="annot"><span class="annottext">learningRate :: Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#learningRate"><span class="hs-identifier hs-var">learningRate</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0.001</span></span><span class="hs-special">,</span><span>
</span><span id="line-317"></span><span>      </span><span class="annot"><span class="annottext">beta1 :: Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#beta1"><span class="hs-identifier hs-var">beta1</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0.9</span></span><span class="hs-special">,</span><span>
</span><span id="line-318"></span><span>      </span><span class="annot"><span class="annottext">beta2 :: Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#beta2"><span class="hs-identifier hs-var">beta2</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0.999</span></span><span class="hs-special">,</span><span>
</span><span id="line-319"></span><span>      </span><span class="annot"><span class="annottext">epsilon :: Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#epsilon"><span class="hs-identifier hs-var">epsilon</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1e-8</span></span><span class="hs-special">,</span><span>
</span><span id="line-320"></span><span>      </span><span class="annot"><span class="annottext">weightDecay :: Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#weightDecay"><span class="hs-identifier hs-var">weightDecay</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0.0</span></span><span class="hs-special">,</span><span>
</span><span id="line-321"></span><span>      </span><span class="annot"><span class="annottext">amsgrad :: Bool
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#amsgrad"><span class="hs-identifier hs-var">amsgrad</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-322"></span><span>    </span><span class="hs-special">}</span><span>
</span><span id="line-323"></span><span>
</span><span id="line-324"></span><span class="hs-comment">-- | Optimizer data type.</span><span>
</span><span id="line-325"></span><span class="hs-keyword">data</span><span> </span><span id="Optimizer"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-var">Optimizer</span></a></span></span><span> </span><span id="local-6989586621679609649"><span class="annot"><a href="#local-6989586621679609649"><span class="hs-identifier hs-type">model</span></a></span></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-326"></span><span>  </span><span id="UnsafeOptimizer"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#UnsafeOptimizer"><span class="hs-identifier hs-var">UnsafeOptimizer</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-327"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679610224"><span class="annot"><a href="#local-6989586621679610224"><span class="hs-identifier hs-type">model</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-328"></span><span>    </span><span class="hs-special">{</span><span> </span><span id="optimizerStateDictKeys"><span class="annot"><span class="annottext">Optimizer model -&gt; [Text]
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#optimizerStateDictKeys"><span class="hs-identifier hs-var hs-var">optimizerStateDictKeys</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#StateDictKey"><span class="hs-identifier hs-type">StateDictKey</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-329"></span><span>      </span><span id="optimizerPtr"><span class="annot"><span class="annottext">Optimizer model -&gt; ForeignPtr Optimizer
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#optimizerPtr"><span class="hs-identifier hs-var hs-var">optimizerPtr</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Optimizer</span></a></span><span>
</span><span id="line-330"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-331"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610224"><span class="hs-identifier hs-type">model</span></a></span><span>
</span><span id="line-332"></span><span>
</span><span id="line-333"></span><span class="hs-keyword">type</span><span> </span><span class="hs-keyword">role</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><span class="hs-identifier">nominal</span></span><span>
</span><span id="line-334"></span><span>
</span><span id="line-335"></span><span class="hs-comment">-- | Get the model state dictionary from an optimizer.</span><span>
</span><span id="line-336"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getStateDict"><span class="hs-identifier hs-type">getStateDict</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-337"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679610210"><span class="annot"><a href="#local-6989586621679610210"><span class="hs-identifier hs-type">model</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610210"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#StateDict"><span class="hs-identifier hs-type">StateDict</span></a></span><span>
</span><span id="line-338"></span><span id="getStateDict"><span class="annot"><span class="annottext">getStateDict :: Optimizer model -&gt; IO StateDict
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getStateDict"><span class="hs-identifier hs-var hs-var">getStateDict</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#UnsafeOptimizer"><span class="hs-identifier hs-type">UnsafeOptimizer</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679609642"><span id="local-6989586621679609643"><span class="annot"><span class="annottext">[Text]
ForeignPtr Optimizer
optimizerPtr :: ForeignPtr Optimizer
optimizerStateDictKeys :: [Text]
optimizerPtr :: forall model. Optimizer model -&gt; ForeignPtr Optimizer
optimizerStateDictKeys :: forall model. Optimizer model -&gt; [Text]
</span><a href="#local-6989586621679609642"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-339"></span><span>  </span><span class="hs-keyword">do</span><span>
</span><span id="line-340"></span><span>    </span><span id="local-6989586621679609641"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679609641"><span class="hs-identifier hs-var">tPtrs</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Optimizer -&gt; IO (ForeignPtr TensorList))
-&gt; ForeignPtr Optimizer -&gt; IO [ForeignPtr Tensor]
forall a ca y cy.
(Castable a ca, Castable y cy) =&gt;
(ca -&gt; IO cy) -&gt; a -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.cast1</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Optimizer -&gt; IO (ForeignPtr TensorList)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.getParams</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Optimizer
</span><a href="#local-6989586621679609642"><span class="hs-identifier hs-var">optimizerPtr</span></a></span><span>
</span><span id="line-341"></span><span>    </span><span class="annot"><span class="annottext">StateDict -&gt; IO StateDict
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(StateDict -&gt; IO StateDict)
-&gt; ([(Text, ForeignPtr Tensor)] -&gt; StateDict)
-&gt; [(Text, ForeignPtr Tensor)]
-&gt; IO StateDict
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">[(Text, ForeignPtr Tensor)] -&gt; StateDict
forall k a. Ord k =&gt; [(k, a)] -&gt; Map k a
</span><span class="hs-identifier hs-var">Map.fromList</span></span><span> </span><span class="annot"><span class="annottext">([(Text, ForeignPtr Tensor)] -&gt; IO StateDict)
-&gt; [(Text, ForeignPtr Tensor)] -&gt; IO StateDict
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Text] -&gt; [ForeignPtr Tensor] -&gt; [(Text, ForeignPtr Tensor)]
forall a b. [a] -&gt; [b] -&gt; [(a, b)]
</span><span class="hs-identifier hs-var">zip</span></span><span> </span><span class="annot"><span class="annottext">[Text]
</span><a href="#local-6989586621679609643"><span class="hs-identifier hs-var">optimizerStateDictKeys</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679609641"><span class="hs-identifier hs-var">tPtrs</span></a></span><span>
</span><span id="line-342"></span><span>
</span><span id="line-343"></span><span class="hs-comment">-- | Extract a model from an optimizer.</span><span>
</span><span id="line-344"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getModel"><span class="hs-identifier hs-type">getModel</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-345"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679609636"><span class="annot"><a href="#local-6989586621679609636"><span class="hs-identifier hs-type">model</span></a></span></span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609636"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609636"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609636"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="#local-6989586621679609636"><span class="hs-identifier hs-type">model</span></a></span><span>
</span><span id="line-346"></span><span id="getModel"><span class="annot"><span class="annottext">getModel :: ModelSpec model -&gt; Optimizer model -&gt; IO model
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getModel"><span class="hs-identifier hs-var hs-var">getModel</span></a></span></span><span> </span><span id="local-6989586621679609635"><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609635"><span class="hs-identifier hs-var">modelSpec</span></a></span></span><span> </span><span id="local-6989586621679609634"><span class="annot"><span class="annottext">Optimizer model
</span><a href="#local-6989586621679609634"><span class="hs-identifier hs-var">optimizer</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-347"></span><span>  </span><span id="local-6989586621679609633"><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609633"><span class="hs-identifier hs-var">stateDict</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Optimizer model -&gt; IO StateDict
forall model. Optimizer model -&gt; IO StateDict
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getStateDict"><span class="hs-identifier hs-var">getStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Optimizer model
</span><a href="#local-6989586621679609634"><span class="hs-identifier hs-var">optimizer</span></a></span><span>
</span><span id="line-348"></span><span>  </span><span class="annot"><span class="annottext">(StateT StateDict IO model -&gt; StateDict -&gt; IO model)
-&gt; StateDict -&gt; StateT StateDict IO model -&gt; IO model
forall a b c. (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
</span><span class="hs-identifier hs-var">flip</span></span><span> </span><span class="annot"><span class="annottext">StateT StateDict IO model -&gt; StateDict -&gt; IO model
forall (m :: * -&gt; *) s a. Monad m =&gt; StateT s m a -&gt; s -&gt; m a
</span><span class="hs-identifier hs-var">evalStateT</span></span><span> </span><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609633"><span class="hs-identifier hs-var">stateDict</span></a></span><span> </span><span class="annot"><span class="annottext">(StateT StateDict IO model -&gt; IO model)
-&gt; StateT StateDict IO model -&gt; IO model
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec model -&gt; Text -&gt; StateT StateDict IO model
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609635"><span class="hs-identifier hs-var">modelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
forall a. Monoid a =&gt; a
</span><span class="hs-identifier hs-var">mempty</span></span><span>
</span><span id="line-349"></span><span>
</span><span id="line-350"></span><span class="hs-comment">-- | Create a new Adam optimizer from a model.</span><span>
</span><span id="line-351"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkAdam"><span class="hs-identifier hs-type">mkAdam</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-352"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679609915"><span class="annot"><a href="#local-6989586621679609915"><span class="hs-identifier hs-type">model</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-353"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609915"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-354"></span><span>  </span><span class="hs-comment">-- | Adam options</span><span>
</span><span id="line-355"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#AdamOptions"><span class="hs-identifier hs-type">AdamOptions</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-356"></span><span>  </span><span class="hs-comment">-- | initial model</span><span>
</span><span id="line-357"></span><span>  </span><span class="annot"><a href="#local-6989586621679609915"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-358"></span><span>  </span><span class="hs-comment">-- | Adam optimizer</span><span>
</span><span id="line-359"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609915"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-360"></span><span id="mkAdam"><span class="annot"><span class="annottext">mkAdam :: AdamOptions -&gt; model -&gt; IO (Optimizer model)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkAdam"><span class="hs-identifier hs-var hs-var">mkAdam</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#AdamOptions"><span class="hs-identifier hs-type">AdamOptions</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679609625"><span id="local-6989586621679609626"><span id="local-6989586621679609627"><span id="local-6989586621679609628"><span id="local-6989586621679609629"><span id="local-6989586621679609630"><span class="annot"><span class="annottext">Bool
Double
amsgrad :: Bool
weightDecay :: Double
epsilon :: Double
beta2 :: Double
beta1 :: Double
learningRate :: Double
amsgrad :: AdamOptions -&gt; Bool
weightDecay :: AdamOptions -&gt; Double
epsilon :: AdamOptions -&gt; Double
beta2 :: AdamOptions -&gt; Double
beta1 :: AdamOptions -&gt; Double
learningRate :: AdamOptions -&gt; Double
</span><a href="#local-6989586621679609625"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679609624"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609624"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-361"></span><span>  </span><span id="local-6989586621679609623"><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609623"><span class="hs-identifier hs-var">stateDict</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(StateT StateDict IO () -&gt; StateDict -&gt; IO StateDict)
-&gt; StateDict -&gt; StateT StateDict IO () -&gt; IO StateDict
forall a b c. (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
</span><span class="hs-identifier hs-var">flip</span></span><span> </span><span class="annot"><span class="annottext">StateT StateDict IO () -&gt; StateDict -&gt; IO StateDict
forall (m :: * -&gt; *) s a. Monad m =&gt; StateT s m a -&gt; s -&gt; m s
</span><span class="hs-identifier hs-var">execStateT</span></span><span> </span><span class="annot"><span class="annottext">StateDict
forall k a. Map k a
</span><span class="hs-identifier hs-var">Map.empty</span></span><span> </span><span class="annot"><span class="annottext">(StateT StateDict IO () -&gt; IO StateDict)
-&gt; StateT StateDict IO () -&gt; IO StateDict
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Text -&gt; model -&gt; StateT StateDict IO ()
forall model (m :: * -&gt; *).
(HasStateDict model, MonadThrow m, MonadState StateDict m) =&gt;
Text -&gt; model -&gt; m ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#toStateDict"><span class="hs-identifier hs-var">toStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Text
forall a. Monoid a =&gt; a
</span><span class="hs-identifier hs-var">mempty</span></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609624"><span class="hs-identifier hs-var">model</span></a></span><span>
</span><span id="line-362"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609621"><span class="annot"><span class="annottext">[Text]
</span><a href="#local-6989586621679609621"><span class="hs-identifier hs-var">stateDictKeys</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609620"><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679609620"><span class="hs-identifier hs-var">tPtrs</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[(Text, ForeignPtr Tensor)] -&gt; ([Text], [ForeignPtr Tensor])
forall a b. [(a, b)] -&gt; ([a], [b])
</span><span class="hs-identifier hs-var">unzip</span></span><span> </span><span class="annot"><span class="annottext">([(Text, ForeignPtr Tensor)] -&gt; ([Text], [ForeignPtr Tensor]))
-&gt; [(Text, ForeignPtr Tensor)] -&gt; ([Text], [ForeignPtr Tensor])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">StateDict -&gt; [(Text, ForeignPtr Tensor)]
forall k a. Map k a -&gt; [(k, a)]
</span><span class="hs-identifier hs-var">Map.toList</span></span><span> </span><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609623"><span class="hs-identifier hs-var">stateDict</span></a></span><span>
</span><span id="line-363"></span><span>  </span><span class="annot"><span class="annottext">[Text] -&gt; ForeignPtr Optimizer -&gt; Optimizer model
forall model. [Text] -&gt; ForeignPtr Optimizer -&gt; Optimizer model
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#UnsafeOptimizer"><span class="hs-identifier hs-var">UnsafeOptimizer</span></a></span><span> </span><span class="annot"><span class="annottext">[Text]
</span><a href="#local-6989586621679609621"><span class="hs-identifier hs-var">stateDictKeys</span></a></span><span>
</span><span id="line-364"></span><span>    </span><span class="annot"><span class="annottext">(ForeignPtr Optimizer -&gt; Optimizer model)
-&gt; IO (ForeignPtr Optimizer) -&gt; IO (Optimizer model)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">(CDouble
 -&gt; CDouble
 -&gt; CDouble
 -&gt; CDouble
 -&gt; CDouble
 -&gt; CBool
 -&gt; ForeignPtr TensorList
 -&gt; IO (ForeignPtr Optimizer))
-&gt; Double
-&gt; Double
-&gt; Double
-&gt; Double
-&gt; Double
-&gt; Bool
-&gt; [ForeignPtr Tensor]
-&gt; IO (ForeignPtr Optimizer)
forall a ca x1 cx1 x2 cx2 x3 cx3 x4 cx4 x5 cx5 x6 cx6 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable x3 cx3,
 Castable x4 cx4, Castable x5 cx5, Castable x6 cx6,
 Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; cx3 -&gt; cx4 -&gt; cx5 -&gt; cx6 -&gt; IO cy)
-&gt; a -&gt; x1 -&gt; x2 -&gt; x3 -&gt; x4 -&gt; x5 -&gt; x6 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.cast7</span></a></span><span> </span><span class="annot"><span class="annottext">CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CBool
-&gt; ForeignPtr TensorList
-&gt; IO (ForeignPtr Optimizer)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.adam</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609630"><span class="hs-identifier hs-var">learningRate</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609629"><span class="hs-identifier hs-var">beta1</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609628"><span class="hs-identifier hs-var">beta2</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609627"><span class="hs-identifier hs-var">epsilon</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609626"><span class="hs-identifier hs-var">weightDecay</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679609625"><span class="hs-identifier hs-var">amsgrad</span></a></span><span> </span><span class="annot"><span class="annottext">[ForeignPtr Tensor]
</span><a href="#local-6989586621679609620"><span class="hs-identifier hs-var">tPtrs</span></a></span><span>
</span><span id="line-365"></span><span>
</span><span id="line-366"></span><span class="hs-comment">-- | Compute the mean squared error between two tensors.</span><span>
</span><span id="line-367"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mseLoss"><span class="hs-identifier hs-type">mseLoss</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-368"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679610242"><span class="annot"><a href="#local-6989586621679610242"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679610239"><span class="annot"><a href="#local-6989586621679610239"><span class="hs-identifier hs-type">gradient</span></a></span></span><span> </span><span id="local-6989586621679610238"><span class="annot"><a href="#local-6989586621679610238"><span class="hs-identifier hs-type">layout</span></a></span></span><span> </span><span id="local-6989586621679610237"><span class="annot"><a href="#local-6989586621679610237"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679610236"><span class="annot"><a href="#local-6989586621679610236"><span class="hs-identifier hs-type">dataType</span></a></span></span><span> </span><span id="local-6989586621679610241"><span class="annot"><a href="#local-6989586621679610241"><span class="hs-identifier hs-type">shape</span></a></span></span><span> </span><span id="local-6989586621679610235"><span class="annot"><a href="#local-6989586621679610235"><span class="hs-identifier hs-type">gradient'</span></a></span></span><span> </span><span id="local-6989586621679610234"><span class="annot"><a href="#local-6989586621679610234"><span class="hs-identifier hs-type">layout'</span></a></span></span><span> </span><span id="local-6989586621679610233"><span class="annot"><a href="#local-6989586621679610233"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679610232"><span class="annot"><a href="#local-6989586621679610232"><span class="hs-identifier hs-type">dataType'</span></a></span></span><span> </span><span id="local-6989586621679610240"><span class="annot"><a href="#local-6989586621679610240"><span class="hs-identifier hs-type">shape'</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-369"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/ifacyhfpnb9wrd20q18ymi87z39k1h67-exceptions-lib-exceptions-0.10.4-haddock-doc/share/doc/exceptions/html/src"><span class="hs-identifier hs-type">MonadThrow</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610242"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610241"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610240"><span class="hs-identifier hs-type">shape'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-370"></span><span>  </span><span class="hs-comment">-- | prediction tensor</span><span>
</span><span id="line-371"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610239"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610238"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610237"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610236"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610241"><span class="hs-identifier hs-type">shape</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-372"></span><span>  </span><span class="hs-comment">-- | target tensor</span><span>
</span><span id="line-373"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610235"><span class="hs-identifier hs-type">gradient'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610234"><span class="hs-identifier hs-type">layout'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610233"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610232"><span class="hs-identifier hs-type">dataType'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610240"><span class="hs-identifier hs-type">shape'</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-374"></span><span>  </span><span class="hs-comment">-- | output tensor</span><span>
</span><span id="line-375"></span><span>  </span><span class="annot"><a href="#local-6989586621679610242"><span class="hs-identifier hs-type">m</span></a></span><span>
</span><span id="line-376"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span>
</span><span id="line-377"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610239"><span class="hs-identifier hs-type">gradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%7C%3E"><span class="hs-operator hs-type">&lt;|&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610235"><span class="hs-identifier hs-type">gradient'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-378"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610238"><span class="hs-identifier hs-type">layout</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610234"><span class="hs-identifier hs-type">layout'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-379"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610237"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610233"><span class="hs-identifier hs-type">device'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-380"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610236"><span class="hs-identifier hs-type">dataType</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610232"><span class="hs-identifier hs-type">dataType'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-381"></span><span>        </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-382"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-383"></span><span id="local-6989586621679609615"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609615"><span class="hs-identifier hs-var">prediction</span></a></span></span><span> </span><span id="mseLoss"><span class="annot"><span class="annottext">mseLoss :: Tensor gradient layout device dataType shape
-&gt; Tensor gradient' layout' device' dataType' shape'
-&gt; m (Tensor
        (gradient &lt;|&gt; gradient')
        (layout &lt;+&gt; layout')
        (device &lt;+&gt; device')
        (dataType &lt;+&gt; dataType')
        ('Shape '[]))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mseLoss"><span class="hs-operator hs-var hs-var">`mseLoss`</span></a></span></span><span> </span><span id="local-6989586621679609614"><span class="annot"><span class="annottext">Tensor gradient' layout' device' dataType' shape'
</span><a href="#local-6989586621679609614"><span class="hs-identifier hs-var">target</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-384"></span><span>  </span><span class="annot"><span class="annottext">IO
  (Tensor
     (Or (Gradient RequiresGradient) gradient gradient')
     (Unify (Layout LayoutType) layout layout')
     (Unify (Device (DeviceType Nat)) device device')
     (Unify (DataType DType) dataType dataType')
     ('Shape '[]))
-&gt; m (Tensor
        (Or (Gradient RequiresGradient) gradient gradient')
        (Unify (Layout LayoutType) layout layout')
        (Unify (Device (DeviceType Nat)) device device')
        (Unify (DataType DType) dataType dataType')
        ('Shape '[]))
forall a (m :: * -&gt; *). MonadThrow m =&gt; IO a -&gt; m a
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">unsafeThrowableIO</span></a></span><span> </span><span class="annot"><span class="annottext">(IO
   (Tensor
      (Or (Gradient RequiresGradient) gradient gradient')
      (Unify (Layout LayoutType) layout layout')
      (Unify (Device (DeviceType Nat)) device device')
      (Unify (DataType DType) dataType dataType')
      ('Shape '[]))
 -&gt; m (Tensor
         (Or (Gradient RequiresGradient) gradient gradient')
         (Unify (Layout LayoutType) layout layout')
         (Unify (Device (DeviceType Nat)) device device')
         (Unify (DataType DType) dataType dataType')
         ('Shape '[])))
-&gt; IO
     (Tensor
        (Or (Gradient RequiresGradient) gradient gradient')
        (Unify (Layout LayoutType) layout layout')
        (Unify (Device (DeviceType Nat)) device device')
        (Unify (DataType DType) dataType dataType')
        ('Shape '[]))
-&gt; m (Tensor
        (Or (Gradient RequiresGradient) gradient gradient')
        (Unify (Layout LayoutType) layout layout')
        (Unify (Device (DeviceType Nat)) device device')
        (Unify (DataType DType) dataType dataType')
        ('Shape '[]))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-385"></span><span>    </span><span class="annot"><span class="annottext">(ForeignPtr Tensor
 -&gt; ForeignPtr Tensor -&gt; Int64 -&gt; IO (ForeignPtr Tensor))
-&gt; Tensor gradient layout device dataType shape
-&gt; Tensor gradient' layout' device' dataType' shape'
-&gt; Int
-&gt; IO
     (Tensor
        (Or (Gradient RequiresGradient) gradient gradient')
        (Unify (Layout LayoutType) layout layout')
        (Unify (Device (DeviceType Nat)) device device')
        (Unify (DataType DType) dataType dataType')
        ('Shape '[]))
forall a ca x1 cx1 x2 cx2 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.cast3</span></a></span><span>
</span><span id="line-386"></span><span>      </span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; ForeignPtr Tensor -&gt; Int64 -&gt; IO (ForeignPtr Tensor)
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.mse_loss_ttl</span></a></span><span>
</span><span id="line-387"></span><span>      </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609615"><span class="hs-identifier hs-var">prediction</span></a></span><span>
</span><span id="line-388"></span><span>      </span><span class="annot"><span class="annottext">Tensor gradient' layout' device' dataType' shape'
</span><a href="#local-6989586621679609614"><span class="hs-identifier hs-var">target</span></a></span><span>
</span><span id="line-389"></span><span>      </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span> </span><span class="hs-comment">-- reduce mean</span><span>
</span><span id="line-390"></span><span>
</span><span id="line-391"></span><span class="hs-comment">-- | Perform one step of optimization.</span><span>
</span><span id="line-392"></span><span id="local-6989586621679610058"><span id="local-6989586621679610059"><span id="local-6989586621679610060"><span id="local-6989586621679610061"><span id="local-6989586621679610062"><span id="local-6989586621679610063"><span id="local-6989586621679610064"><span id="local-6989586621679610065"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#stepWithGenerator"><span class="hs-identifier hs-type">stepWithGenerator</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-393"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610065"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-394"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#SGetGeneratorDevice"><span class="hs-identifier hs-type">SGetGeneratorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610064"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-395"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#SGetGeneratorDevice"><span class="hs-identifier hs-type">SGetGeneratorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610063"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-396"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610062"><span class="hs-identifier hs-type">lossShape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-397"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610061"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-398"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-399"></span><span>  </span><span class="hs-comment">-- | optimizer for the model</span><span>
</span><span id="line-400"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610065"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-401"></span><span>  </span><span class="hs-comment">-- | model specification</span><span>
</span><span id="line-402"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610065"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-403"></span><span>  </span><span class="hs-comment">-- | loss function to minimize</span><span>
</span><span id="line-404"></span><span>  </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679610065"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610064"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610061"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610060"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610059"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610058"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610062"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610063"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-405"></span><span>  </span><span class="hs-comment">-- | random generator</span><span>
</span><span id="line-406"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610064"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-407"></span><span>  </span><span class="hs-comment">-- | loss and updated generator</span><span>
</span><span id="line-408"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610061"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610060"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610059"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610058"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610062"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679610063"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span><span>
</span><span id="line-409"></span><span id="stepWithGenerator"><span class="annot"><span class="annottext">stepWithGenerator :: Optimizer model
-&gt; ModelSpec model
-&gt; (model
    -&gt; Generator generatorDevice
    -&gt; IO
         (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
          Generator generatorOutputDevice))
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#stepWithGenerator"><span class="hs-identifier hs-var hs-var">stepWithGenerator</span></a></span></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#UnsafeOptimizer"><span class="hs-identifier hs-type">UnsafeOptimizer</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679609609"><span id="local-6989586621679609610"><span class="annot"><span class="annottext">[Text]
ForeignPtr Optimizer
optimizerPtr :: ForeignPtr Optimizer
optimizerStateDictKeys :: [Text]
optimizerPtr :: forall model. Optimizer model -&gt; ForeignPtr Optimizer
optimizerStateDictKeys :: forall model. Optimizer model -&gt; [Text]
</span><a href="#local-6989586621679609609"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679609608"><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609608"><span class="hs-identifier hs-var">modelSpec</span></a></span></span><span> </span><span id="local-6989586621679609607"><span class="annot"><span class="annottext">model
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609607"><span class="hs-identifier hs-var">lossFn</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#UnsafeGenerator"><span class="hs-identifier hs-type">UnsafeGenerator</span></a></span><span> </span><span id="local-6989586621679609605"><span class="annot"><span class="annottext">TVar
  (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator))
</span><a href="#local-6989586621679609605"><span class="hs-identifier hs-var">tvar</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-410"></span><span>  </span><span class="hs-keyword">do</span><span>
</span><span id="line-411"></span><span>    </span><span id="local-6989586621679609604"><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609604"><span class="hs-identifier hs-var">genPtr</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">TVar
  (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator))
-&gt; IO (ForeignPtr Generator)
forall (device :: Device (DeviceType Nat)).
SGetGeneratorDevice device =&gt;
TVar (Either (SDevice device, Word64) (ForeignPtr Generator))
-&gt; IO (ForeignPtr Generator)
</span><a href="Torch.GraduallyTyped.Random.html#getGenPtr"><span class="hs-identifier hs-var">getGenPtr</span></a></span><span> </span><span class="annot"><span class="annottext">TVar
  (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator))
</span><a href="#local-6989586621679609605"><span class="hs-identifier hs-var">tvar</span></a></span><span>
</span><span id="line-412"></span><span>    </span><span class="hs-keyword">let</span><span> </span><span class="annot"><a href="#local-6989586621679609602"><span class="hs-identifier hs-type">rawLossFn</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.TensorList</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Generator</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.StdTuple</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Tensor</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Generator</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-413"></span><span>        </span><span id="local-6989586621679609602"><span class="annot"><span class="annottext">rawLossFn :: ForeignPtr TensorList
-&gt; ForeignPtr Generator
-&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator)))
</span><a href="#local-6989586621679609602"><span class="hs-identifier hs-var hs-var">rawLossFn</span></a></span></span><span> </span><span id="local-6989586621679609601"><span class="annot"><span class="annottext">ForeignPtr TensorList
</span><a href="#local-6989586621679609601"><span class="hs-identifier hs-var">tlPtr</span></a></span></span><span> </span><span id="local-6989586621679609600"><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609600"><span class="hs-identifier hs-var">genPtr''</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-414"></span><span>          </span><span id="local-6989586621679609599"><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609599"><span class="hs-identifier hs-var">g''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">TVar
  (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator))
-&gt; Generator generatorDevice
forall (device :: Device (DeviceType Nat)).
TVar (Either (SDevice device, Word64) (ForeignPtr Generator))
-&gt; Generator device
</span><a href="Torch.GraduallyTyped.Random.html#UnsafeGenerator"><span class="hs-identifier hs-var">UnsafeGenerator</span></a></span><span> </span><span class="annot"><span class="annottext">(TVar
   (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator))
 -&gt; Generator generatorDevice)
-&gt; IO
     (TVar
        (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator)))
-&gt; IO (Generator generatorDevice)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Either (SDevice generatorDevice, Word64) (ForeignPtr Generator)
-&gt; IO
     (TVar
        (Either (SDevice generatorDevice, Word64) (ForeignPtr Generator)))
forall a. a -&gt; IO (TVar a)
</span><span class="hs-identifier hs-var">newTVarIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Generator
-&gt; Either (SDevice generatorDevice, Word64) (ForeignPtr Generator)
forall a b. b -&gt; Either a b
</span><span class="hs-identifier hs-var">Right</span></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609600"><span class="hs-identifier hs-var">genPtr''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-415"></span><span>          </span><span id="local-6989586621679609598"><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609598"><span class="hs-identifier hs-var">stateDict'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#StateDict"><span class="hs-identifier hs-type">StateDict</span></a></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList
-&gt; ([ForeignPtr Tensor] -&gt; IO StateDict) -&gt; IO StateDict
forall a b r. Castable a b =&gt; b -&gt; (a -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.uncast</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList
</span><a href="#local-6989586621679609601"><span class="hs-identifier hs-var">tlPtr</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">StateDict -&gt; IO StateDict
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(StateDict -&gt; IO StateDict)
-&gt; ([ForeignPtr Tensor] -&gt; StateDict)
-&gt; [ForeignPtr Tensor]
-&gt; IO StateDict
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">[(Text, ForeignPtr Tensor)] -&gt; StateDict
forall k a. Ord k =&gt; [(k, a)] -&gt; Map k a
</span><span class="hs-identifier hs-var">Map.fromList</span></span><span> </span><span class="annot"><span class="annottext">([(Text, ForeignPtr Tensor)] -&gt; StateDict)
-&gt; ([ForeignPtr Tensor] -&gt; [(Text, ForeignPtr Tensor)])
-&gt; [ForeignPtr Tensor]
-&gt; StateDict
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">[Text] -&gt; [ForeignPtr Tensor] -&gt; [(Text, ForeignPtr Tensor)]
forall a b. [a] -&gt; [b] -&gt; [(a, b)]
</span><span class="hs-identifier hs-var">zip</span></span><span> </span><span class="annot"><span class="annottext">[Text]
</span><a href="#local-6989586621679609610"><span class="hs-identifier hs-var">optimizerStateDictKeys</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-416"></span><span>          </span><span id="local-6989586621679609596"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609596"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(StateT StateDict IO model -&gt; StateDict -&gt; IO model)
-&gt; StateDict -&gt; StateT StateDict IO model -&gt; IO model
forall a b c. (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
</span><span class="hs-identifier hs-var">flip</span></span><span> </span><span class="annot"><span class="annottext">StateT StateDict IO model -&gt; StateDict -&gt; IO model
forall (m :: * -&gt; *) s a. Monad m =&gt; StateT s m a -&gt; s -&gt; m a
</span><span class="hs-identifier hs-var">evalStateT</span></span><span> </span><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609598"><span class="hs-identifier hs-var">stateDict'</span></a></span><span> </span><span class="annot"><span class="annottext">(StateT StateDict IO model -&gt; IO model)
-&gt; StateT StateDict IO model -&gt; IO model
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec model -&gt; Text -&gt; StateT StateDict IO model
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609608"><span class="hs-identifier hs-var">modelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
forall a. Monoid a =&gt; a
</span><span class="hs-identifier hs-var">mempty</span></span><span>
</span><span id="line-417"></span><span>          </span><span class="hs-comment">-- model &lt;- getModel modelSpec optim</span><span>
</span><span id="line-418"></span><span>          </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-type">UnsafeTensor</span></a></span><span> </span><span id="local-6989586621679609594"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679609594"><span class="hs-identifier hs-var">tPtr</span></a></span></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#UnsafeGenerator"><span class="hs-identifier hs-type">UnsafeGenerator</span></a></span><span> </span><span id="local-6989586621679609593"><span class="annot"><span class="annottext">TVar
  (Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator))
</span><a href="#local-6989586621679609593"><span class="hs-identifier hs-var">tvar'''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">model
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609607"><span class="hs-identifier hs-var">lossFn</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609596"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609599"><span class="hs-identifier hs-var">g''</span></a></span><span>
</span><span id="line-419"></span><span>          </span><span id="local-6989586621679609592"><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609592"><span class="hs-identifier hs-var">genPtr'''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">TVar
  (Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator))
-&gt; IO (ForeignPtr Generator)
forall (device :: Device (DeviceType Nat)).
SGetGeneratorDevice device =&gt;
TVar (Either (SDevice device, Word64) (ForeignPtr Generator))
-&gt; IO (ForeignPtr Generator)
</span><a href="Torch.GraduallyTyped.Random.html#getGenPtr"><span class="hs-identifier hs-var">getGenPtr</span></a></span><span> </span><span class="annot"><span class="annottext">TVar
  (Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator))
</span><a href="#local-6989586621679609593"><span class="hs-identifier hs-var">tvar'''</span></a></span><span>
</span><span id="line-420"></span><span>          </span><span class="annot"><span class="annottext">(ForeignPtr Tensor, ForeignPtr Generator)
-&gt; (ForeignPtr (StdTuple '(Tensor, Generator))
    -&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator))))
-&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator)))
forall a b r. Castable a b =&gt; a -&gt; (b -&gt; IO r) -&gt; IO r
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.cast</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679609594"><span class="hs-identifier hs-var">tPtr</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609592"><span class="hs-identifier hs-var">genPtr'''</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">ForeignPtr (StdTuple '(Tensor, Generator))
-&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator)))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span>
</span><span id="line-421"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679609590"><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679609590"><span class="hs-identifier hs-var">lossPtr</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609589"><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609589"><span class="hs-identifier hs-var">genPtr'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">ForeignPtr</span></span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Generator</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(ForeignPtr Optimizer
 -&gt; ForeignPtr Generator
 -&gt; (ForeignPtr TensorList
     -&gt; ForeignPtr Generator
     -&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator))))
 -&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator))))
-&gt; ForeignPtr Optimizer
-&gt; ForeignPtr Generator
-&gt; (ForeignPtr TensorList
    -&gt; ForeignPtr Generator
    -&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator))))
-&gt; IO (ForeignPtr Tensor, ForeignPtr Generator)
forall a ca x1 cx1 x2 cx2 y cy.
(Castable a ca, Castable x1 cx1, Castable x2 cx2, Castable y cy) =&gt;
(ca -&gt; cx1 -&gt; cx2 -&gt; IO cy) -&gt; a -&gt; x1 -&gt; x2 -&gt; IO y
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.cast3</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Optimizer
-&gt; ForeignPtr Generator
-&gt; (ForeignPtr TensorList
    -&gt; ForeignPtr Generator
    -&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator))))
-&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator)))
</span><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-var">ATen.stepWithGenerator</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Optimizer
</span><a href="#local-6989586621679609609"><span class="hs-identifier hs-var">optimizerPtr</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609604"><span class="hs-identifier hs-var">genPtr</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr TensorList
-&gt; ForeignPtr Generator
-&gt; IO (ForeignPtr (StdTuple '(Tensor, Generator)))
</span><a href="#local-6989586621679609602"><span class="hs-identifier hs-var">rawLossFn</span></a></span><span>
</span><span id="line-422"></span><span>    </span><span id="local-6989586621679609587"><span class="annot"><span class="annottext">TVar
  (Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator))
</span><a href="#local-6989586621679609587"><span class="hs-identifier hs-var">g'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Either
  (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator)
-&gt; IO
     (TVar
        (Either
           (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator)))
forall a. a -&gt; IO (TVar a)
</span><span class="hs-identifier hs-var">newTVarIO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Generator
-&gt; Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator)
forall a b. b -&gt; Either a b
</span><span class="hs-identifier hs-var">Right</span></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Generator
</span><a href="#local-6989586621679609589"><span class="hs-identifier hs-var">genPtr'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-423"></span><span>    </span><span class="annot"><span class="annottext">(Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
 Generator generatorOutputDevice)
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ForeignPtr Tensor
-&gt; Tensor lossGradient lossLayout lossDataType lossDevice lossShape
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
ForeignPtr Tensor -&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#UnsafeTensor"><span class="hs-identifier hs-var">UnsafeTensor</span></a></span><span> </span><span class="annot"><span class="annottext">ForeignPtr Tensor
</span><a href="#local-6989586621679609590"><span class="hs-identifier hs-var">lossPtr</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">TVar
  (Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator))
-&gt; Generator generatorOutputDevice
forall (device :: Device (DeviceType Nat)).
TVar (Either (SDevice device, Word64) (ForeignPtr Generator))
-&gt; Generator device
</span><a href="Torch.GraduallyTyped.Random.html#UnsafeGenerator"><span class="hs-identifier hs-var">UnsafeGenerator</span></a></span><span> </span><span class="annot"><span class="annottext">TVar
  (Either
     (SDevice generatorOutputDevice, Word64) (ForeignPtr Generator))
</span><a href="#local-6989586621679609587"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-424"></span><span>
</span><span id="line-425"></span><span class="hs-comment">-- | Train the model for one epoch.</span><span>
</span><span id="line-426"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#train"><span class="hs-identifier hs-type">train</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-427"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679609909"><span class="annot"><a href="#local-6989586621679609909"><span class="hs-identifier hs-type">m</span></a></span></span><span> </span><span id="local-6989586621679609908"><span class="annot"><a href="#local-6989586621679609908"><span class="hs-identifier hs-type">model</span></a></span></span><span> </span><span id="local-6989586621679609907"><span class="annot"><a href="#local-6989586621679609907"><span class="hs-identifier hs-type">input</span></a></span></span><span> </span><span id="local-6989586621679609906"><span class="annot"><a href="#local-6989586621679609906"><span class="hs-identifier hs-type">generatorDevice</span></a></span></span><span> </span><span id="local-6989586621679609905"><span class="annot"><a href="#local-6989586621679609905"><span class="hs-identifier hs-type">lossGradient</span></a></span></span><span> </span><span id="local-6989586621679609904"><span class="annot"><a href="#local-6989586621679609904"><span class="hs-identifier hs-type">lossLayout</span></a></span></span><span> </span><span id="local-6989586621679609903"><span class="annot"><a href="#local-6989586621679609903"><span class="hs-identifier hs-type">lossDataType</span></a></span></span><span> </span><span id="local-6989586621679609902"><span class="annot"><a href="#local-6989586621679609902"><span class="hs-identifier hs-type">lossDevice</span></a></span></span><span> </span><span id="local-6989586621679609901"><span class="annot"><a href="#local-6989586621679609901"><span class="hs-identifier hs-type">lossShape</span></a></span></span><span> </span><span id="local-6989586621679609900"><span class="annot"><a href="#local-6989586621679609900"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-428"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadIO</span></span><span> </span><span class="annot"><a href="#local-6989586621679609909"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-429"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609908"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-430"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609908"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609907"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609906"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609905"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609904"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609903"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609902"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609901"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679609900"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-431"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609908"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609907"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609900"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609905"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609904"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609903"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609902"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609901"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679609900"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-432"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#SGetGeneratorDevice"><span class="hs-identifier hs-type">SGetGeneratorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609906"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-433"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#SGetGeneratorDevice"><span class="hs-identifier hs-type">SGetGeneratorDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609900"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-434"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609905"><span class="hs-identifier hs-type">lossGradient</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-435"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-type">SGetShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609901"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-436"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609901"><span class="hs-identifier hs-type">lossShape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-437"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609905"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithGradient"><span class="hs-identifier hs-type">WithGradient</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-438"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-439"></span><span>  </span><span class="hs-comment">-- | optimizer for the model</span><span>
</span><span id="line-440"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609908"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-441"></span><span>  </span><span class="hs-comment">-- | model specification</span><span>
</span><span id="line-442"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#ModelSpec"><span class="hs-identifier hs-type">ModelSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609908"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-443"></span><span>  </span><span class="hs-comment">-- | stream of training examples</span><span>
</span><span id="line-444"></span><span>  </span><span class="annot"><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-type">P.ListT</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609909"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609907"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-445"></span><span>  </span><span class="hs-comment">-- | random generator</span><span>
</span><span id="line-446"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609906"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-447"></span><span>  </span><span class="hs-comment">-- | returned is either the original generator or the average training loss and a new generator</span><span>
</span><span id="line-448"></span><span>  </span><span class="annot"><a href="#local-6989586621679609909"><span class="hs-identifier hs-type">m</span></a></span><span>
</span><span id="line-449"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Either</span></span><span>
</span><span id="line-450"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609906"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-451"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679609904"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609903"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609902"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609900"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-452"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-453"></span><span id="train"><span class="annot"><span class="annottext">train :: Optimizer model
-&gt; ModelSpec model
-&gt; ListT m input
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#train"><span class="hs-identifier hs-var hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679609585"><span class="annot"><span class="annottext">Optimizer model
</span><a href="#local-6989586621679609585"><span class="hs-identifier hs-var">optim</span></a></span></span><span> </span><span id="local-6989586621679609584"><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609584"><span class="hs-identifier hs-var">modelSpec</span></a></span></span><span> </span><span id="local-6989586621679609583"><span class="annot"><span class="annottext">ListT m input
</span><a href="#local-6989586621679609583"><span class="hs-identifier hs-var">examples</span></a></span></span><span> </span><span id="local-6989586621679609582"><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609582"><span class="hs-identifier hs-var">g</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-454"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609581"><span class="annot"><span class="annottext">producer :: Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609581"><span class="hs-identifier hs-var hs-var">producer</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Producer input m ()
-&gt; Producer Int m () -&gt; Proxy X () () (input, Int) m ()
forall (m :: * -&gt; *) a r b x' x.
Monad m =&gt;
Producer a m r -&gt; Producer b m r -&gt; Proxy x' x () (a, b) m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.zip</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ListT m input -&gt; Producer input m ()
forall (m :: * -&gt; *) a. ListT m a -&gt; Producer a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var hs-var">P.enumerate</span></a></span><span> </span><span class="annot"><span class="annottext">ListT m input
</span><a href="#local-6989586621679609583"><span class="hs-identifier hs-var">examples</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; Producer Int m ()
forall (m :: * -&gt; *) (f :: * -&gt; *) a x' x.
(Functor m, Foldable f) =&gt;
f a -&gt; Proxy x' x () a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.each</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">..</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-455"></span><span>  </span><span id="local-6989586621679609577"><span class="annot"><span class="annottext">Either () ((input, Int), Proxy X () () (input, Int) m ())
</span><a href="#local-6989586621679609577"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
-&gt; m (Either () ((input, Int), Proxy X () () (input, Int) m ()))
forall (m :: * -&gt; *) a r.
Monad m =&gt;
Producer a m r -&gt; m (Either r (a, Producer a m r))
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.next</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609581"><span class="hs-identifier hs-var">producer</span></a></span><span>
</span><span id="line-456"></span><span>  </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Either () ((input, Int), Proxy X () () (input, Int) m ())
</span><a href="#local-6989586621679609577"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-457"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Left</span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Either
  (Generator generatorDevice)
  (Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[]),
   Generator generatorOutputDevice)
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Either
   (Generator generatorDevice)
   (Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Generator generatorOutputDevice)
 -&gt; m (Either
         (Generator generatorDevice)
         (Tensor
            ('Gradient 'WithoutGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice)))
-&gt; (Generator generatorDevice
    -&gt; Either
         (Generator generatorDevice)
         (Tensor
            ('Gradient 'WithoutGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice))
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
-&gt; Either
     (Generator generatorDevice)
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
forall a b. a -&gt; Either a b
</span><span class="hs-identifier hs-var">Left</span></span><span> </span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (Either
         (Generator generatorDevice)
         (Tensor
            ('Gradient 'WithoutGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice)))
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609582"><span class="hs-identifier hs-var">g</span></a></span><span>
</span><span id="line-458"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Right</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span id="local-6989586621679609575"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609575"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609574"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609574"><span class="hs-identifier hs-var">iter</span></a></span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609573"><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609573"><span class="hs-identifier hs-var">producer'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-459"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609572"><span class="annot"><span class="annottext">step :: ((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; (input, Int)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609572"><span class="hs-identifier hs-var hs-var">step</span></a></span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span id="local-6989586621679609571"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609571"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609570"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609570"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609569"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609569"><span class="hs-identifier hs-var">input'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609568"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609568"><span class="hs-identifier hs-var">iter'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (m :: * -&gt; *) a. MonadIO m =&gt; IO a -&gt; m a
</span><span class="hs-identifier hs-var">liftIO</span></span><span> </span><span class="annot"><span class="annottext">(IO
   ((Tensor
       ('Gradient 'WithoutGradient)
       lossLayout
       lossDataType
       lossDevice
       ('Shape '[]),
     Int),
    Generator generatorOutputDevice)
 -&gt; m ((Tensor
          ('Gradient 'WithoutGradient)
          lossLayout
          lossDataType
          lossDevice
          ('Shape '[]),
        Int),
       Generator generatorOutputDevice))
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-460"></span><span>            </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609567"><span class="annot"><span class="annottext">forward' :: model
-&gt; Generator generatorOutputDevice
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609567"><span class="hs-identifier hs-var hs-var">forward'</span></a></span></span><span> </span><span id="local-6989586621679609566"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609566"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679609565"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609565"><span class="hs-identifier hs-var">g''</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-461"></span><span>                  </span><span class="hs-special">(</span><span id="local-6989586621679609564"><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609564"><span class="hs-identifier hs-var">loss'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609563"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609563"><span class="hs-identifier hs-var">g'''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">model
-&gt; input
-&gt; Generator generatorOutputDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609566"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609569"><span class="hs-identifier hs-var">input'</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609565"><span class="hs-identifier hs-var">g''</span></a></span><span>
</span><span id="line-462"></span><span>                  </span><span id="local-6989586621679609562"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609562"><span class="hs-identifier hs-var">loss''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">SShape ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithGradient)
     lossLayout
     lossDataType
     lossDevice
     lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (shape' :: Shape [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetShape shape, MonadThrow m, Catch (shape &lt;+&gt; shape')) =&gt;
SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-var">sCheckedShape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SList '[] -&gt; SShape ('Shape '[])
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SList dims -&gt; SShape ('Shape dims)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-var">SShape</span></a></span><span> </span><span class="annot"><span class="annottext">SList '[]
forall a. SList '[]
</span><a href="../file:///nix/store/lnxln0gj8camy4zp976hwp5qw721jzi5-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">SNil</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithGradient)
   lossLayout
   lossDataType
   lossDevice
   lossShape
 -&gt; IO
      (Tensor
         ('Gradient 'WithGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[])))
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (m :: * -&gt; *) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">SGradient ('Gradient 'WithGradient)
-&gt; Tensor lossGradient lossLayout lossDataType lossDevice lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
forall (gradient' :: Gradient RequiresGradient) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetGradient gradient, MonadThrow m,
 Catch (gradient &lt;+&gt; gradient')) =&gt;
SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-var">sCheckedGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
-&gt; SGradient ('Gradient 'WithGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithGradient"><span class="hs-identifier hs-var">SWithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609564"><span class="hs-identifier hs-var">loss'</span></a></span><span>
</span><span id="line-463"></span><span>                  </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithGradient)
   lossLayout
   lossDataType
   lossDevice
   ('Shape '[]),
 Generator generatorOutputDevice)
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609562"><span class="hs-identifier hs-var">loss''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609563"><span class="hs-identifier hs-var">g'''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-464"></span><span>            </span><span class="hs-special">(</span><span id="local-6989586621679609554"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609554"><span class="hs-identifier hs-var">loss'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609553"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609553"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Optimizer model
-&gt; ModelSpec model
-&gt; (model
    -&gt; Generator generatorOutputDevice
    -&gt; IO
         (Tensor
            ('Gradient 'WithGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice))
-&gt; Generator generatorOutputDevice
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
forall model (generatorDevice :: Device (DeviceType Nat))
       (generatorOutputDevice :: Device (DeviceType Nat))
       (lossShape :: Shape [Dim (Name Symbol) (Size Nat)])
       (lossGradient :: Gradient RequiresGradient)
       (lossLayout :: Layout LayoutType)
       (lossDataType :: Device (DeviceType Nat))
       (lossDevice :: DataType DType).
(HasStateDict model, SGetGeneratorDevice generatorDevice,
 SGetGeneratorDevice generatorOutputDevice,
 Catch (lossShape &lt;+&gt; 'Shape '[]),
 Catch (lossGradient &lt;+&gt; 'Gradient 'WithGradient)) =&gt;
Optimizer model
-&gt; ModelSpec model
-&gt; (model
    -&gt; Generator generatorDevice
    -&gt; IO
         (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
          Generator generatorOutputDevice))
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#stepWithGenerator"><span class="hs-identifier hs-var">stepWithGenerator</span></a></span><span> </span><span class="annot"><span class="annottext">Optimizer model
</span><a href="#local-6989586621679609585"><span class="hs-identifier hs-var">optim</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609584"><span class="hs-identifier hs-var">modelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">model
-&gt; Generator generatorOutputDevice
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609567"><span class="hs-identifier hs-var">forward'</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609570"><span class="hs-identifier hs-var">g'</span></a></span><span>
</span><span id="line-465"></span><span>            </span><span id="local-6989586621679609552"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609552"><span class="hs-identifier hs-var">loss''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; IO
     (Tensor ('Gradient 'WithoutGradient) layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withoutGradient"><span class="hs-identifier hs-var">withoutGradient</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609554"><span class="hs-identifier hs-var">loss'</span></a></span><span>
</span><span id="line-466"></span><span>            </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609571"><span class="hs-identifier hs-var">loss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[])
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609552"><span class="hs-identifier hs-var">loss''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609568"><span class="hs-identifier hs-var">iter'</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609553"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-467"></span><span>          </span><span id="local-6989586621679609550"><span class="annot"><span class="annottext">init' :: m ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
</span><a href="#local-6989586621679609550"><span class="hs-identifier hs-var hs-var">init'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (m :: * -&gt; *) a. MonadIO m =&gt; IO a -&gt; m a
</span><span class="hs-identifier hs-var">liftIO</span></span><span> </span><span class="annot"><span class="annottext">(IO
   ((Tensor
       ('Gradient 'WithoutGradient)
       lossLayout
       lossDataType
       lossDevice
       ('Shape '[]),
     Int),
    Generator generatorOutputDevice)
 -&gt; m ((Tensor
          ('Gradient 'WithoutGradient)
          lossLayout
          lossDataType
          lossDevice
          ('Shape '[]),
        Int),
       Generator generatorOutputDevice))
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-468"></span><span>            </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609549"><span class="annot"><span class="annottext">forward' :: model
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609549"><span class="hs-identifier hs-var hs-var">forward'</span></a></span></span><span> </span><span id="local-6989586621679609548"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609548"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679609547"><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609547"><span class="hs-identifier hs-var">g'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-469"></span><span>                  </span><span class="hs-special">(</span><span id="local-6989586621679609546"><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609546"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609545"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609545"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">model
-&gt; input
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609548"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609575"><span class="hs-identifier hs-var">input</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609547"><span class="hs-identifier hs-var">g'</span></a></span><span>
</span><span id="line-470"></span><span>                  </span><span id="local-6989586621679609544"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609544"><span class="hs-identifier hs-var">loss'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">SShape ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithGradient)
     lossLayout
     lossDataType
     lossDevice
     lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (shape' :: Shape [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetShape shape, MonadThrow m, Catch (shape &lt;+&gt; shape')) =&gt;
SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-var">sCheckedShape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SList '[] -&gt; SShape ('Shape '[])
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SList dims -&gt; SShape ('Shape dims)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-var">SShape</span></a></span><span> </span><span class="annot"><span class="annottext">SList '[]
forall a. SList '[]
</span><a href="../file:///nix/store/lnxln0gj8camy4zp976hwp5qw721jzi5-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">SNil</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithGradient)
   lossLayout
   lossDataType
   lossDevice
   lossShape
 -&gt; IO
      (Tensor
         ('Gradient 'WithGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[])))
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (m :: * -&gt; *) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">SGradient ('Gradient 'WithGradient)
-&gt; Tensor lossGradient lossLayout lossDataType lossDevice lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
forall (gradient' :: Gradient RequiresGradient) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetGradient gradient, MonadThrow m,
 Catch (gradient &lt;+&gt; gradient')) =&gt;
SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-var">sCheckedGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
-&gt; SGradient ('Gradient 'WithGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithGradient"><span class="hs-identifier hs-var">SWithGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609546"><span class="hs-identifier hs-var">loss</span></a></span><span>
</span><span id="line-471"></span><span>                  </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithGradient)
   lossLayout
   lossDataType
   lossDevice
   ('Shape '[]),
 Generator generatorOutputDevice)
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609544"><span class="hs-identifier hs-var">loss'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609545"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-472"></span><span>            </span><span class="hs-special">(</span><span id="local-6989586621679609543"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609543"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609542"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609542"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Optimizer model
-&gt; ModelSpec model
-&gt; (model
    -&gt; Generator generatorDevice
    -&gt; IO
         (Tensor
            ('Gradient 'WithGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice))
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
forall model (generatorDevice :: Device (DeviceType Nat))
       (generatorOutputDevice :: Device (DeviceType Nat))
       (lossShape :: Shape [Dim (Name Symbol) (Size Nat)])
       (lossGradient :: Gradient RequiresGradient)
       (lossLayout :: Layout LayoutType)
       (lossDataType :: Device (DeviceType Nat))
       (lossDevice :: DataType DType).
(HasStateDict model, SGetGeneratorDevice generatorDevice,
 SGetGeneratorDevice generatorOutputDevice,
 Catch (lossShape &lt;+&gt; 'Shape '[]),
 Catch (lossGradient &lt;+&gt; 'Gradient 'WithGradient)) =&gt;
Optimizer model
-&gt; ModelSpec model
-&gt; (model
    -&gt; Generator generatorDevice
    -&gt; IO
         (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
          Generator generatorOutputDevice))
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#stepWithGenerator"><span class="hs-identifier hs-var">stepWithGenerator</span></a></span><span> </span><span class="annot"><span class="annottext">Optimizer model
</span><a href="#local-6989586621679609585"><span class="hs-identifier hs-var">optim</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec model
</span><a href="#local-6989586621679609584"><span class="hs-identifier hs-var">modelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">model
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor
        ('Gradient 'WithGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609549"><span class="hs-identifier hs-var">forward'</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609582"><span class="hs-identifier hs-var">g</span></a></span><span>
</span><span id="line-473"></span><span>            </span><span id="local-6989586621679609541"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609541"><span class="hs-identifier hs-var">loss'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Tensor gradient layout device dataType shape
-&gt; IO
     (Tensor ('Gradient 'WithoutGradient) layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#withoutGradient"><span class="hs-identifier hs-var">withoutGradient</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609543"><span class="hs-identifier hs-var">loss</span></a></span><span>
</span><span id="line-474"></span><span>            </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609541"><span class="hs-identifier hs-var">loss'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609574"><span class="hs-identifier hs-var">iter</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609542"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-475"></span><span>          </span><span id="local-6989586621679609540"><span class="annot"><span class="annottext">done :: ((Tensor gradient layout device dataType shape, divisor), b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
</span><a href="#local-6989586621679609540"><span class="hs-identifier hs-var hs-var">done</span></a></span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span id="local-6989586621679609539"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609539"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609538"><span class="annot"><span class="annottext">divisor
</span><a href="#local-6989586621679609538"><span class="hs-identifier hs-var">iter'</span></a></span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609537"><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679609537"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Either a (Tensor gradient layout device dataType shape, b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Either a (Tensor gradient layout device dataType shape, b)
 -&gt; f (Either a (Tensor gradient layout device dataType shape, b)))
-&gt; ((Tensor gradient layout device dataType shape, b)
    -&gt; Either a (Tensor gradient layout device dataType shape, b))
-&gt; (Tensor gradient layout device dataType shape, b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape, b)
-&gt; Either a (Tensor gradient layout device dataType shape, b)
forall a b. b -&gt; Either a b
</span><span class="hs-identifier hs-var">Right</span></span><span> </span><span class="annot"><span class="annottext">((Tensor gradient layout device dataType shape, b)
 -&gt; f (Either a (Tensor gradient layout device dataType shape, b)))
-&gt; (Tensor gradient layout device dataType shape, b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609539"><span class="hs-identifier hs-var">loss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; divisor -&gt; Tensor gradient layout device dataType shape
forall divisor (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Scalar divisor =&gt;
Tensor gradient layout device dataType shape
-&gt; divisor -&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#divScalar"><span class="hs-operator hs-var">`divScalar`</span></a></span><span> </span><span class="annot"><span class="annottext">divisor
</span><a href="#local-6989586621679609538"><span class="hs-identifier hs-var">iter'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679609537"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-476"></span><span>      </span><span class="annot"><span class="annottext">(((Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[]),
   Int),
  Generator generatorOutputDevice)
 -&gt; (input, Int)
 -&gt; m ((Tensor
          ('Gradient 'WithoutGradient)
          lossLayout
          lossDataType
          lossDevice
          ('Shape '[]),
        Int),
       Generator generatorOutputDevice))
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
-&gt; (((Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Int),
     Generator generatorOutputDevice)
    -&gt; m (Either
            (Generator generatorDevice)
            (Tensor
               ('Gradient 'WithoutGradient)
               lossLayout
               lossDataType
               lossDevice
               ('Shape '[]),
             Generator generatorOutputDevice)))
-&gt; Proxy X () () (input, Int) m ()
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall (m :: * -&gt; *) x a b.
Monad m =&gt;
(x -&gt; a -&gt; m x) -&gt; m x -&gt; (x -&gt; m b) -&gt; Producer a m () -&gt; m b
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.foldM</span></a></span><span> </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; (input, Int)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609572"><span class="hs-identifier hs-var">step</span></a></span><span> </span><span class="annot"><span class="annottext">m ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
</span><a href="#local-6989586621679609550"><span class="hs-identifier hs-var">init'</span></a></span><span> </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall (f :: * -&gt; *) divisor
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) b a.
(Applicative f, Scalar divisor) =&gt;
((Tensor gradient layout device dataType shape, divisor), b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
</span><a href="#local-6989586621679609540"><span class="hs-identifier hs-var">done</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609573"><span class="hs-identifier hs-var">producer'</span></a></span><span>
</span><span id="line-477"></span><span>
</span><span id="line-478"></span><span class="hs-comment">-- | Evaluate the model on the given examples.</span><span>
</span><span id="line-479"></span><span id="local-6989586621679609883"><span id="local-6989586621679609884"><span id="local-6989586621679609885"><span id="local-6989586621679609886"><span id="local-6989586621679609887"><span id="local-6989586621679609888"><span id="local-6989586621679609889"><span id="local-6989586621679609890"><span id="local-6989586621679609891"><span id="local-6989586621679609892"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#eval"><span class="hs-identifier hs-type">eval</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-480"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadIO</span></span><span> </span><span class="annot"><a href="#local-6989586621679609892"><span class="hs-identifier hs-type">m</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-481"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasStateDict"><span class="hs-identifier hs-type">HasStateDict</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609891"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-482"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609891"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609890"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609889"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609888"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609887"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609886"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609885"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609884"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679609883"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-483"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.NN.Class.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609891"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609890"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609883"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609888"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609887"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609886"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609885"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609884"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679609883"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-484"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetGradient"><span class="hs-identifier hs-type">SGetGradient</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609888"><span class="hs-identifier hs-type">lossGradient</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-485"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#SGetShape"><span class="hs-identifier hs-type">SGetShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609884"><span class="hs-identifier hs-type">lossShape</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-486"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609884"><span class="hs-identifier hs-type">lossShape</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-487"></span><span>    </span><span class="annot"><a href="Torch.GraduallyTyped.Prelude.html#Catch"><span class="hs-identifier hs-type">Catch</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679609888"><span class="hs-identifier hs-type">lossGradient</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Unify.html#%3C%2B%3E"><span class="hs-operator hs-type">&lt;+&gt;</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-488"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-489"></span><span>  </span><span class="hs-comment">-- | model</span><span>
</span><span id="line-490"></span><span>  </span><span class="annot"><a href="#local-6989586621679609891"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-491"></span><span>  </span><span class="hs-comment">-- | stream of examples</span><span>
</span><span id="line-492"></span><span>  </span><span class="annot"><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-type">P.ListT</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609892"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609890"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-493"></span><span>  </span><span class="hs-comment">-- | random generator</span><span>
</span><span id="line-494"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609889"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-495"></span><span>  </span><span class="hs-comment">-- | returned is either the original generator or the average evaluation loss and a new generator</span><span>
</span><span id="line-496"></span><span>  </span><span class="annot"><a href="#local-6989586621679609892"><span class="hs-identifier hs-type">m</span></a></span><span>
</span><span id="line-497"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Either</span></span><span>
</span><span id="line-498"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609889"><span class="hs-identifier hs-type">generatorDevice</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-499"></span><span>        </span><span class="hs-special">(</span><span class="annot"><a href="Torch.GraduallyTyped.Tensor.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#Gradient"><span class="hs-identifier hs-type">Gradient</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.RequiresGradient.html#WithoutGradient"><span class="hs-identifier hs-type">WithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679609887"><span class="hs-identifier hs-type">lossLayout</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609886"><span class="hs-identifier hs-type">lossDataType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609885"><span class="hs-identifier hs-type">lossDevice</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><a href="Torch.GraduallyTyped.Shape.Type.html#Shape"><span class="hs-identifier hs-type">Shape</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Random.html#Generator"><span class="hs-identifier hs-type">Generator</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609883"><span class="hs-identifier hs-type">generatorOutputDevice</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-500"></span><span>    </span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-501"></span><span id="eval"><span class="annot"><span class="annottext">eval :: model
-&gt; ListT m input
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#eval"><span class="hs-identifier hs-var hs-var">eval</span></a></span></span><span> </span><span id="local-6989586621679609533"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609533"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679609532"><span class="annot"><span class="annottext">ListT m input
</span><a href="#local-6989586621679609532"><span class="hs-identifier hs-var">examples</span></a></span></span><span> </span><span id="local-6989586621679609531"><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609531"><span class="hs-identifier hs-var">g</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-502"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609530"><span class="annot"><span class="annottext">producer :: Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609530"><span class="hs-identifier hs-var hs-var">producer</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Producer input m ()
-&gt; Producer Int m () -&gt; Proxy X () () (input, Int) m ()
forall (m :: * -&gt; *) a r b x' x.
Monad m =&gt;
Producer a m r -&gt; Producer b m r -&gt; Proxy x' x () (a, b) m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.zip</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ListT m input -&gt; Producer input m ()
forall (m :: * -&gt; *) a. ListT m a -&gt; Producer a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var hs-var">P.enumerate</span></a></span><span> </span><span class="annot"><span class="annottext">ListT m input
</span><a href="#local-6989586621679609532"><span class="hs-identifier hs-var">examples</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; Producer Int m ()
forall (m :: * -&gt; *) (f :: * -&gt; *) a x' x.
(Functor m, Foldable f) =&gt;
f a -&gt; Proxy x' x () a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.each</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">..</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-503"></span><span>  </span><span id="local-6989586621679609529"><span class="annot"><span class="annottext">Either () ((input, Int), Proxy X () () (input, Int) m ())
</span><a href="#local-6989586621679609529"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
-&gt; m (Either () ((input, Int), Proxy X () () (input, Int) m ()))
forall (m :: * -&gt; *) a r.
Monad m =&gt;
Producer a m r -&gt; m (Either r (a, Producer a m r))
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.next</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609530"><span class="hs-identifier hs-var">producer</span></a></span><span>
</span><span id="line-504"></span><span>  </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Either () ((input, Int), Proxy X () () (input, Int) m ())
</span><a href="#local-6989586621679609529"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-505"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Left</span></span><span> </span><span class="annot"><span class="annottext">()
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Either
  (Generator generatorDevice)
  (Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[]),
   Generator generatorOutputDevice)
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Either
   (Generator generatorDevice)
   (Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Generator generatorOutputDevice)
 -&gt; m (Either
         (Generator generatorDevice)
         (Tensor
            ('Gradient 'WithoutGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice)))
-&gt; (Generator generatorDevice
    -&gt; Either
         (Generator generatorDevice)
         (Tensor
            ('Gradient 'WithoutGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice))
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
-&gt; Either
     (Generator generatorDevice)
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Generator generatorOutputDevice)
forall a b. a -&gt; Either a b
</span><span class="hs-identifier hs-var">Left</span></span><span> </span><span class="annot"><span class="annottext">(Generator generatorDevice
 -&gt; m (Either
         (Generator generatorDevice)
         (Tensor
            ('Gradient 'WithoutGradient)
            lossLayout
            lossDataType
            lossDevice
            ('Shape '[]),
          Generator generatorOutputDevice)))
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609531"><span class="hs-identifier hs-var">g</span></a></span><span>
</span><span id="line-506"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Right</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span id="local-6989586621679609528"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609528"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609527"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609527"><span class="hs-identifier hs-var">iter</span></a></span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609526"><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609526"><span class="hs-identifier hs-var">producer'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-507"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609525"><span class="annot"><span class="annottext">step :: ((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; (input, Int)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609525"><span class="hs-identifier hs-var hs-var">step</span></a></span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span id="local-6989586621679609524"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609524"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609523"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609523"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609522"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609522"><span class="hs-identifier hs-var">input'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609521"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609521"><span class="hs-identifier hs-var">iter'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (m :: * -&gt; *) a. MonadIO m =&gt; IO a -&gt; m a
</span><span class="hs-identifier hs-var">liftIO</span></span><span> </span><span class="annot"><span class="annottext">(IO
   ((Tensor
       ('Gradient 'WithoutGradient)
       lossLayout
       lossDataType
       lossDevice
       ('Shape '[]),
     Int),
    Generator generatorOutputDevice)
 -&gt; m ((Tensor
          ('Gradient 'WithoutGradient)
          lossLayout
          lossDataType
          lossDevice
          ('Shape '[]),
        Int),
       Generator generatorOutputDevice))
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-508"></span><span>            </span><span class="hs-special">(</span><span id="local-6989586621679609520"><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609520"><span class="hs-identifier hs-var">loss'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609519"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609519"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">model
-&gt; input
-&gt; Generator generatorOutputDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609533"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609522"><span class="hs-identifier hs-var">input'</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609523"><span class="hs-identifier hs-var">g'</span></a></span><span>
</span><span id="line-509"></span><span>            </span><span id="local-6989586621679609518"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609518"><span class="hs-identifier hs-var">loss''</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">SShape ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (shape' :: Shape [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetShape shape, MonadThrow m, Catch (shape &lt;+&gt; shape')) =&gt;
SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-var">sCheckedShape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SList '[] -&gt; SShape ('Shape '[])
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SList dims -&gt; SShape ('Shape dims)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-var">SShape</span></a></span><span> </span><span class="annot"><span class="annottext">SList '[]
forall a. SList '[]
</span><a href="../file:///nix/store/lnxln0gj8camy4zp976hwp5qw721jzi5-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">SNil</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithoutGradient)
   lossLayout
   lossDataType
   lossDevice
   lossShape
 -&gt; IO
      (Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[])))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (m :: * -&gt; *) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">SGradient ('Gradient 'WithoutGradient)
-&gt; Tensor lossGradient lossLayout lossDataType lossDevice lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
forall (gradient' :: Gradient RequiresGradient) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetGradient gradient, MonadThrow m,
 Catch (gradient &lt;+&gt; gradient')) =&gt;
SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-var">sCheckedGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
-&gt; SGradient ('Gradient 'WithoutGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithoutGradient"><span class="hs-identifier hs-var">SWithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609520"><span class="hs-identifier hs-var">loss'</span></a></span><span>
</span><span id="line-510"></span><span>            </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609524"><span class="hs-identifier hs-var">loss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[])
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609518"><span class="hs-identifier hs-var">loss''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609521"><span class="hs-identifier hs-var">iter'</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609519"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-511"></span><span>          </span><span id="local-6989586621679609516"><span class="annot"><span class="annottext">init' :: m ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
</span><a href="#local-6989586621679609516"><span class="hs-identifier hs-var hs-var">init'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO
  ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (m :: * -&gt; *) a. MonadIO m =&gt; IO a -&gt; m a
</span><span class="hs-identifier hs-var">liftIO</span></span><span> </span><span class="annot"><span class="annottext">(IO
   ((Tensor
       ('Gradient 'WithoutGradient)
       lossLayout
       lossDataType
       lossDevice
       ('Shape '[]),
     Int),
    Generator generatorOutputDevice)
 -&gt; m ((Tensor
          ('Gradient 'WithoutGradient)
          lossLayout
          lossDataType
          lossDevice
          ('Shape '[]),
        Int),
       Generator generatorOutputDevice))
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-512"></span><span>            </span><span class="hs-special">(</span><span id="local-6989586621679609515"><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609515"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609514"><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609514"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">model
-&gt; input
-&gt; Generator generatorDevice
-&gt; IO
     (Tensor lossGradient lossLayout lossDataType lossDevice lossShape,
      Generator generatorOutputDevice)
forall model input (generatorDevice :: Device (DeviceType Nat))
       output (generatorOutputDevice :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(HasForward
   model input generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
model
-&gt; input
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679609533"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679609528"><span class="hs-identifier hs-var">input</span></a></span><span> </span><span class="annot"><span class="annottext">Generator generatorDevice
</span><a href="#local-6989586621679609531"><span class="hs-identifier hs-var">g</span></a></span><span>
</span><span id="line-513"></span><span>            </span><span id="local-6989586621679609513"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609513"><span class="hs-identifier hs-var">loss'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">SShape ('Shape '[])
-&gt; Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (shape' :: Shape [Dim (Name Symbol) (Size Nat)])
       (m :: * -&gt; *) (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetShape shape, MonadThrow m, Catch (shape &lt;+&gt; shape')) =&gt;
SShape shape'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient layout device dataType shape')
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedShape"><span class="hs-identifier hs-var">sCheckedShape</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SList '[] -&gt; SShape ('Shape '[])
forall (dims :: [Dim (Name Symbol) (Size Nat)]).
SList dims -&gt; SShape ('Shape dims)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SShape"><span class="hs-identifier hs-var">SShape</span></a></span><span> </span><span class="annot"><span class="annottext">SList '[]
forall a. SList '[]
</span><a href="../file:///nix/store/lnxln0gj8camy4zp976hwp5qw721jzi5-singletons-lib-singletons-2.7-haddock-doc/share/doc/singletons/html/src"><span class="hs-identifier hs-var">SNil</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithoutGradient)
   lossLayout
   lossDataType
   lossDevice
   lossShape
 -&gt; IO
      (Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[])))
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]))
forall (m :: * -&gt; *) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">SGradient ('Gradient 'WithoutGradient)
-&gt; Tensor lossGradient lossLayout lossDataType lossDevice lossShape
-&gt; IO
     (Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        lossShape)
forall (gradient' :: Gradient RequiresGradient) (m :: * -&gt; *)
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
(SGetGradient gradient, MonadThrow m,
 Catch (gradient &lt;+&gt; gradient')) =&gt;
SGradient gradient'
-&gt; Tensor gradient layout device dataType shape
-&gt; m (Tensor gradient' layout device dataType shape)
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sCheckedGradient"><span class="hs-identifier hs-var">sCheckedGradient</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
-&gt; SGradient ('Gradient 'WithoutGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithoutGradient"><span class="hs-identifier hs-var">SWithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor lossGradient lossLayout lossDataType lossDevice lossShape
</span><a href="#local-6989586621679609515"><span class="hs-identifier hs-var">loss</span></a></span><span>
</span><span id="line-514"></span><span>            </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; IO
     ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  lossLayout
  lossDataType
  lossDevice
  ('Shape '[])
</span><a href="#local-6989586621679609513"><span class="hs-identifier hs-var">loss'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609527"><span class="hs-identifier hs-var">iter</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator generatorOutputDevice
</span><a href="#local-6989586621679609514"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-515"></span><span>          </span><span id="local-6989586621679609512"><span class="annot"><span class="annottext">done :: ((Tensor gradient layout device dataType shape, divisor), b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
</span><a href="#local-6989586621679609512"><span class="hs-identifier hs-var hs-var">done</span></a></span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span id="local-6989586621679609511"><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609511"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609510"><span class="annot"><span class="annottext">divisor
</span><a href="#local-6989586621679609510"><span class="hs-identifier hs-var">iter'</span></a></span></span><span class="hs-special">)</span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609509"><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679609509"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Either a (Tensor gradient layout device dataType shape, b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Either a (Tensor gradient layout device dataType shape, b)
 -&gt; f (Either a (Tensor gradient layout device dataType shape, b)))
-&gt; ((Tensor gradient layout device dataType shape, b)
    -&gt; Either a (Tensor gradient layout device dataType shape, b))
-&gt; (Tensor gradient layout device dataType shape, b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor gradient layout device dataType shape, b)
-&gt; Either a (Tensor gradient layout device dataType shape, b)
forall a b. b -&gt; Either a b
</span><span class="hs-identifier hs-var">Right</span></span><span> </span><span class="annot"><span class="annottext">((Tensor gradient layout device dataType shape, b)
 -&gt; f (Either a (Tensor gradient layout device dataType shape, b)))
-&gt; (Tensor gradient layout device dataType shape, b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
</span><a href="#local-6989586621679609511"><span class="hs-identifier hs-var">loss</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor gradient layout device dataType shape
-&gt; divisor -&gt; Tensor gradient layout device dataType shape
forall divisor (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]).
Scalar divisor =&gt;
Tensor gradient layout device dataType shape
-&gt; divisor -&gt; Tensor gradient layout device dataType shape
</span><a href="Torch.GraduallyTyped.Tensor.MathOperations.Pointwise.html#divScalar"><span class="hs-operator hs-var">`divScalar`</span></a></span><span> </span><span class="annot"><span class="annottext">divisor
</span><a href="#local-6989586621679609510"><span class="hs-identifier hs-var">iter'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679609509"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-516"></span><span>      </span><span class="annot"><span class="annottext">(((Tensor
     ('Gradient 'WithoutGradient)
     lossLayout
     lossDataType
     lossDevice
     ('Shape '[]),
   Int),
  Generator generatorOutputDevice)
 -&gt; (input, Int)
 -&gt; m ((Tensor
          ('Gradient 'WithoutGradient)
          lossLayout
          lossDataType
          lossDevice
          ('Shape '[]),
        Int),
       Generator generatorOutputDevice))
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
-&gt; (((Tensor
        ('Gradient 'WithoutGradient)
        lossLayout
        lossDataType
        lossDevice
        ('Shape '[]),
      Int),
     Generator generatorOutputDevice)
    -&gt; m (Either
            (Generator generatorDevice)
            (Tensor
               ('Gradient 'WithoutGradient)
               lossLayout
               lossDataType
               lossDevice
               ('Shape '[]),
             Generator generatorOutputDevice)))
-&gt; Proxy X () () (input, Int) m ()
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall (m :: * -&gt; *) x a b.
Monad m =&gt;
(x -&gt; a -&gt; m x) -&gt; m x -&gt; (x -&gt; m b) -&gt; Producer a m () -&gt; m b
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.foldM</span></a></span><span> </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; (input, Int)
-&gt; m ((Tensor
         ('Gradient 'WithoutGradient)
         lossLayout
         lossDataType
         lossDevice
         ('Shape '[]),
       Int),
      Generator generatorOutputDevice)
</span><a href="#local-6989586621679609525"><span class="hs-identifier hs-var">step</span></a></span><span> </span><span class="annot"><span class="annottext">m ((Tensor
      ('Gradient 'WithoutGradient)
      lossLayout
      lossDataType
      lossDevice
      ('Shape '[]),
    Int),
   Generator generatorOutputDevice)
</span><a href="#local-6989586621679609516"><span class="hs-identifier hs-var">init'</span></a></span><span> </span><span class="annot"><span class="annottext">((Tensor
    ('Gradient 'WithoutGradient)
    lossLayout
    lossDataType
    lossDevice
    ('Shape '[]),
  Int),
 Generator generatorOutputDevice)
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
forall (f :: * -&gt; *) divisor
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (shape :: Shape [Dim (Name Symbol) (Size Nat)]) b a.
(Applicative f, Scalar divisor) =&gt;
((Tensor gradient layout device dataType shape, divisor), b)
-&gt; f (Either a (Tensor gradient layout device dataType shape, b))
</span><a href="#local-6989586621679609512"><span class="hs-identifier hs-var">done</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy X () () (input, Int) m ()
</span><a href="#local-6989586621679609526"><span class="hs-identifier hs-var">producer'</span></a></span><span>
</span><span id="line-517"></span><span>
</span><span id="line-518"></span><span class="hs-comment">-- | Single-cycle learning rate schedule.</span><span>
</span><span id="line-519"></span><span class="hs-comment">-- See, for instance, https://arxiv.org/abs/1803.09820.</span><span>
</span><span id="line-520"></span><span class="hs-comment">--</span><span>
</span><span id="line-521"></span><span class="hs-comment">-- This is a simple schedule that is a stepwise linear interpolation</span><span>
</span><span id="line-522"></span><span class="hs-comment">-- between the initial, maximum, and final learning rates.</span><span>
</span><span id="line-523"></span><span class="hs-comment">-- The initial learning rate is zero.</span><span>
</span><span id="line-524"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#singleCycleLearningRateSchedule"><span class="hs-identifier hs-type">singleCycleLearningRateSchedule</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-525"></span><span>  </span><span class="hs-comment">-- | peak learning rate after warmup</span><span>
</span><span id="line-526"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-527"></span><span>  </span><span class="hs-comment">-- | learning rate at the end of the schedule</span><span>
</span><span id="line-528"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-529"></span><span>  </span><span class="hs-comment">-- | total number of epochs</span><span>
</span><span id="line-530"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-531"></span><span>  </span><span class="hs-comment">-- | number of warm-up epochs</span><span>
</span><span id="line-532"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-533"></span><span>  </span><span class="hs-comment">-- | number of cool-down epochs</span><span>
</span><span id="line-534"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-535"></span><span>  </span><span class="hs-comment">-- | current epoch</span><span>
</span><span id="line-536"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-537"></span><span>  </span><span class="hs-comment">-- | current learning rate</span><span>
</span><span id="line-538"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-539"></span><span id="singleCycleLearningRateSchedule"><span class="annot"><span class="annottext">singleCycleLearningRateSchedule :: Double -&gt; Double -&gt; Int -&gt; Int -&gt; Int -&gt; Int -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#singleCycleLearningRateSchedule"><span class="hs-identifier hs-var hs-var">singleCycleLearningRateSchedule</span></a></span></span><span> </span><span id="local-6989586621679609507"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609507"><span class="hs-identifier hs-var">maxLearningRate</span></a></span></span><span> </span><span id="local-6989586621679609506"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609506"><span class="hs-identifier hs-var">finalLearningRate</span></a></span></span><span> </span><span id="local-6989586621679609505"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609505"><span class="hs-identifier hs-var">numEpochs</span></a></span></span><span> </span><span id="local-6989586621679609504"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609504"><span class="hs-identifier hs-var">numWarmupEpochs</span></a></span></span><span> </span><span id="local-6989586621679609503"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609503"><span class="hs-identifier hs-var">numCooldownEpochs</span></a></span></span><span> </span><span id="local-6989586621679609502"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span></span><span>
</span><span id="line-540"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&lt;=</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0.0</span></span><span>
</span><span id="line-541"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&lt;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&lt;=</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609504"><span class="hs-identifier hs-var">numWarmupEpochs</span></a></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-542"></span><span>    </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609498"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609498"><span class="hs-identifier hs-var">a</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609504"><span class="hs-identifier hs-var">numWarmupEpochs</span></a></span><span>
</span><span id="line-543"></span><span>     </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609498"><span class="hs-identifier hs-var">a</span></a></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609507"><span class="hs-identifier hs-var">maxLearningRate</span></a></span><span>
</span><span id="line-544"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609504"><span class="hs-identifier hs-var">numWarmupEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&lt;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span> </span><span class="annot"><span class="annottext">Bool -&gt; Bool -&gt; Bool
</span><span class="hs-operator hs-var">&amp;&amp;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&lt;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609505"><span class="hs-identifier hs-var">numEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609503"><span class="hs-identifier hs-var">numCooldownEpochs</span></a></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-545"></span><span>    </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609497"><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609497"><span class="hs-identifier hs-var">a</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-546"></span><span>          </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609505"><span class="hs-identifier hs-var">numEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609503"><span class="hs-identifier hs-var">numCooldownEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609502"><span class="hs-identifier hs-var">epoch</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-547"></span><span>            </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609505"><span class="hs-identifier hs-var">numEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609503"><span class="hs-identifier hs-var">numCooldownEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609504"><span class="hs-identifier hs-var">numWarmupEpochs</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-548"></span><span>     </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609497"><span class="hs-identifier hs-var">a</span></a></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609507"><span class="hs-identifier hs-var">maxLearningRate</span></a></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-glyph hs-var">-</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609497"><span class="hs-identifier hs-var">a</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609506"><span class="hs-identifier hs-var">finalLearningRate</span></a></span><span>
</span><span id="line-549"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609506"><span class="hs-identifier hs-var">finalLearningRate</span></a></span><span>
</span><span id="line-550"></span><span>
</span><span id="line-551"></span><span class="hs-comment">-- | Data type for monitoring the training and evaluation losses.</span><span>
</span><span id="line-552"></span><span class="hs-keyword">data</span><span> </span><span id="Monitor"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Monitor"><span class="hs-identifier hs-var">Monitor</span></a></span></span><span>
</span><span id="line-553"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="hs-comment">-- | monitor for training loss</span><span>
</span><span id="line-554"></span><span>    </span><span id="TrainingMonitor"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TrainingMonitor"><span class="hs-identifier hs-var">TrainingMonitor</span></a></span></span><span> </span><span class="hs-special">{</span><span id="mtLoss"><span class="annot"><span class="annottext">Monitor -&gt; Float
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mtLoss"><span class="hs-identifier hs-var hs-var">mtLoss</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">,</span><span> </span><span id="mtEpoch"><span class="annot"><span class="annottext">Monitor -&gt; Int
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mtEpoch"><span class="hs-identifier hs-var hs-var">mtEpoch</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">}</span><span>
</span><span id="line-555"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="hs-comment">-- | monitor for evaluation loss</span><span>
</span><span id="line-556"></span><span>    </span><span id="EvaluationMonitor"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#EvaluationMonitor"><span class="hs-identifier hs-var">EvaluationMonitor</span></a></span></span><span> </span><span class="hs-special">{</span><span id="meLoss"><span class="annot"><span class="annottext">Monitor -&gt; Float
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#meLoss"><span class="hs-identifier hs-var hs-var">meLoss</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">,</span><span> </span><span id="meEpoch"><span class="annot"><span class="annottext">Monitor -&gt; Int
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#meEpoch"><span class="hs-identifier hs-var hs-var">meEpoch</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">}</span><span>
</span><span id="line-557"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="annot"><span class="hs-keyword">stock</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609485"><span id="local-6989586621679609487"><span id="local-6989586621679609489"><span class="annot"><span class="annottext">Int -&gt; Monitor -&gt; ShowS
[Monitor] -&gt; ShowS
Monitor -&gt; String
(Int -&gt; Monitor -&gt; ShowS)
-&gt; (Monitor -&gt; String) -&gt; ([Monitor] -&gt; ShowS) -&gt; Show Monitor
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [Monitor] -&gt; ShowS
$cshowList :: [Monitor] -&gt; ShowS
show :: Monitor -&gt; String
$cshow :: Monitor -&gt; String
showsPrec :: Int -&gt; Monitor -&gt; ShowS
$cshowsPrec :: Int -&gt; Monitor -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-558"></span><span>
</span><span id="line-559"></span><span class="hs-comment">-- | A simple monitor that prints the training and evaluation losses to stdout.</span><span>
</span><span id="line-560"></span><span id="local-6989586621679609871"><span id="local-6989586621679609872"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#monitor"><span class="hs-identifier hs-type">monitor</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">MonadIO</span></span><span> </span><span class="annot"><a href="#local-6989586621679609872"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-type">P.Consumer</span></a></span><span> </span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#Monitor"><span class="hs-identifier hs-type">Monitor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609872"><span class="hs-identifier hs-type">m</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609871"><span class="hs-identifier hs-type">r</span></a></span></span></span><span>
</span><span id="line-561"></span><span id="monitor"><span class="annot"><span class="annottext">monitor :: Consumer Monitor m r
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#monitor"><span class="hs-identifier hs-var hs-var">monitor</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(Monitor -&gt; String) -&gt; Pipe Monitor String m r
forall (m :: * -&gt; *) a b r. Functor m =&gt; (a -&gt; b) -&gt; Pipe a b m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.map</span></a></span><span> </span><span class="annot"><span class="annottext">Monitor -&gt; String
forall a. Show a =&gt; a -&gt; String
</span><span class="hs-identifier hs-var">show</span></span><span> </span><span class="annot"><span class="annottext">Pipe Monitor String m r
-&gt; Proxy () String () X m r -&gt; Consumer Monitor m r
forall (m :: * -&gt; *) a' a b r c' c.
Functor m =&gt;
Proxy a' a () b m r -&gt; Proxy () b c' c m r -&gt; Proxy a' a c' c m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-operator hs-var">P.&gt;-&gt;</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy () String () X m r
forall (m :: * -&gt; *) r. MonadIO m =&gt; Consumer' String m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.stdoutLn'</span></a></span><span>
</span><span id="line-562"></span><span>
</span><span id="line-563"></span><span class="hs-comment">-- | Collate a stream of examples into batches.</span><span>
</span><span id="line-564"></span><span class="hs-comment">-- The returned batches are of the form @(input, target)@,</span><span>
</span><span id="line-565"></span><span class="hs-comment">-- where @(input, target)@ is a pair of tensors.</span><span>
</span><span id="line-566"></span><span class="hs-comment">-- @input@ and @target@ are both of shape:</span><span>
</span><span id="line-567"></span><span class="hs-comment">--</span><span>
</span><span id="line-568"></span><span class="hs-comment">-- &gt; Dim (SName @&quot;*&quot;) (UncheckedSize batchSize) :|: Dim (SName @&quot;*&quot;) (SSize @1) :|: SNil</span><span>
</span><span id="line-569"></span><span class="hs-comment">--</span><span>
</span><span id="line-570"></span><span class="hs-comment">-- where @batchSize@ is the number of examples in the batch.</span><span>
</span><span id="line-571"></span><span id="local-6989586621679609478"><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#collate"><span class="hs-identifier hs-type">collate</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-572"></span><span>  </span><span class="annot"><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-type">SDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679609478"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-573"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-574"></span><span>  </span><span class="annot"><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-type">P.ListT</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-575"></span><span>  </span><span class="annot"><span class="hs-identifier">_</span></span></span><span>
</span><span id="line-576"></span><span id="collate"><span class="annot"><span class="annottext">collate :: SDevice device
-&gt; Int
-&gt; ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT r IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#collate"><span class="hs-identifier hs-var hs-var">collate</span></a></span></span><span> </span><span id="local-6989586621679609476"><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679609476"><span class="hs-identifier hs-var">device</span></a></span></span><span> </span><span id="local-6989586621679609475"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609475"><span class="hs-identifier hs-var">batchSize</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-577"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609474"><span class="annot"><span class="annottext">collateFn :: [(Float, Float)]
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609474"><span class="hs-identifier hs-var hs-var">collateFn</span></a></span></span><span> </span><span id="local-6989586621679609473"><span class="annot"><span class="annottext">[(Float, Float)]
</span><a href="#local-6989586621679609473"><span class="hs-identifier hs-var">chunk</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-578"></span><span>        </span><span class="hs-keyword">let</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609472"><span class="annot"><span class="annottext">[Float]
</span><a href="#local-6989586621679609472"><span class="hs-identifier hs-var">xs</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609471"><span class="annot"><span class="annottext">[Float]
</span><a href="#local-6989586621679609471"><span class="hs-identifier hs-var">ys</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[(Float, Float)] -&gt; ([Float], [Float])
forall a b. [(a, b)] -&gt; ([a], [b])
</span><span class="hs-identifier hs-var">unzip</span></span><span> </span><span class="annot"><span class="annottext">[(Float, Float)]
</span><a href="#local-6989586621679609473"><span class="hs-identifier hs-var">chunk</span></a></span><span>
</span><span id="line-579"></span><span>            </span><span id="local-6989586621679609470"><span class="annot"><span class="annottext">xs' :: [Vector 1 Float]
</span><a href="#local-6989586621679609470"><span class="hs-identifier hs-var hs-var">xs'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Float -&gt; Vector 1 Float
forall a. a -&gt; Vector 1 a
</span><a href="../file:///nix/store/33j53czfh9iaqr8w1k6rjcna7zg8rw55-vector-sized-lib-vector-sized-1.4.3.1-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-var">VS.singleton</span></a></span><span> </span><span class="annot"><span class="annottext">(Float -&gt; Vector 1 Float) -&gt; [Float] -&gt; [Vector 1 Float]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Float]
</span><a href="#local-6989586621679609472"><span class="hs-identifier hs-var">xs</span></a></span><span>
</span><span id="line-580"></span><span>            </span><span id="local-6989586621679609468"><span class="annot"><span class="annottext">ys' :: [Vector 1 Float]
</span><a href="#local-6989586621679609468"><span class="hs-identifier hs-var hs-var">ys'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Float -&gt; Vector 1 Float
forall a. a -&gt; Vector 1 a
</span><a href="../file:///nix/store/33j53czfh9iaqr8w1k6rjcna7zg8rw55-vector-sized-lib-vector-sized-1.4.3.1-haddock-doc/share/doc/vector-sized/html/src"><span class="hs-identifier hs-var">VS.singleton</span></a></span><span> </span><span class="annot"><span class="annottext">(Float -&gt; Vector 1 Float) -&gt; [Float] -&gt; [Vector 1 Float]
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Float]
</span><a href="#local-6989586621679609471"><span class="hs-identifier hs-var">ys</span></a></span><span>
</span><span id="line-581"></span><span>            </span><span id="local-6989586621679609467"><span class="annot"><span class="annottext">sToTensor' :: [Vector 1 Float]
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609467"><span class="hs-identifier hs-var hs-var">sToTensor'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SGradient ('Gradient 'WithoutGradient)
-&gt; SLayout ('Layout 'Dense)
-&gt; SDevice device
-&gt; [Vector 1 Float]
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
forall a (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat))
       (m :: * -&gt; *).
(TensorLike a dType dims, MonadThrow m) =&gt;
SGradient gradient
-&gt; SLayout layout
-&gt; SDevice device
-&gt; a
-&gt; m (Tensor
        gradient layout device ('DataType dType) ('Shape dims))
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#sToTensor"><span class="hs-identifier hs-var">sToTensor</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
-&gt; SGradient ('Gradient 'WithoutGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithoutGradient"><span class="hs-identifier hs-var">SWithoutGradient</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SLayoutType 'Dense -&gt; SLayout ('Layout 'Dense)
forall (layoutType :: LayoutType).
SLayoutType layoutType -&gt; SLayout ('Layout layoutType)
</span><a href="Torch.GraduallyTyped.Layout.html#SLayout"><span class="hs-identifier hs-var">SLayout</span></a></span><span> </span><span class="annot"><span class="annottext">SLayoutType 'Dense
</span><a href="Torch.GraduallyTyped.Layout.html#SDense"><span class="hs-identifier hs-var">SDense</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">SDevice device
</span><a href="#local-6989586621679609476"><span class="hs-identifier hs-var">device</span></a></span><span>
</span><span id="line-582"></span><span>         </span><span class="hs-keyword">in</span><span> </span><span class="hs-special">(</span><span class="hs-special">,</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor
   ('Gradient 'WithoutGradient)
   ('Layout 'Dense)
   device
   ('DataType 'Float)
   ('Shape
      '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])
 -&gt; Tensor
      ('Gradient 'WithoutGradient)
      ('Layout 'Dense)
      device
      ('DataType 'Float)
      ('Shape
         '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])
 -&gt; (Tensor
       ('Gradient 'WithoutGradient)
       ('Layout 'Dense)
       device
       ('DataType 'Float)
       ('Shape
          '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
     Tensor
       ('Gradient 'WithoutGradient)
       ('Layout 'Dense)
       device
       ('DataType 'Float)
       ('Shape
          '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])
      -&gt; (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            device
            ('DataType 'Float)
            ('Shape
               '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
          Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            device
            ('DataType 'Float)
            ('Shape
               '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Vector 1 Float]
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609467"><span class="hs-identifier hs-var">sToTensor'</span></a></span><span> </span><span class="annot"><span class="annottext">[Vector 1 Float]
</span><a href="#local-6989586621679609470"><span class="hs-identifier hs-var">xs'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     device
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])
   -&gt; (Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         device
         ('DataType 'Float)
         ('Shape
            '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
       Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         device
         ('DataType 'Float)
         ('Shape
            '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">[Vector 1 Float]
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609467"><span class="hs-identifier hs-var">sToTensor'</span></a></span><span> </span><span class="annot"><span class="annottext">[Vector 1 Float]
</span><a href="#local-6989586621679609468"><span class="hs-identifier hs-var">ys'</span></a></span><span>
</span><span id="line-583"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">ContT
  r
  IO
  (ListT
     IO
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT r IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">P.lift</span></span><span> </span><span class="annot"><span class="annottext">(ContT
   r
   IO
   (ListT
      IO
      (Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         device
         ('DataType 'Float)
         ('Shape
            '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
       Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         device
         ('DataType 'Float)
         ('Shape
            '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
 -&gt; Proxy
      X
      ()
      ()
      Monitor
      (ContT r IO)
      (ListT
         IO
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            device
            ('DataType 'Float)
            ('Shape
               '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
          Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            device
            ('DataType 'Float)
            ('Shape
               '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))))
-&gt; (ListT IO (Float, Float)
    -&gt; ContT
         r
         IO
         (ListT
            IO
            (Tensor
               ('Gradient 'WithoutGradient)
               ('Layout 'Dense)
               device
               ('DataType 'Float)
               ('Shape
                  '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
             Tensor
               ('Gradient 'WithoutGradient)
               ('Layout 'Dense)
               device
               ('DataType 'Float)
               ('Shape
                  '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))))
-&gt; ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT r IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Buffer
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     device
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
   Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     device
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
-&gt; Int
-&gt; ([(Float, Float)]
    -&gt; Maybe
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            device
            ('DataType 'Float)
            ('Shape
               '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
          Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            device
            ('DataType 'Float)
            ('Shape
               '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
-&gt; ListT IO (Float, Float)
-&gt; ContT
     r
     IO
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
forall (m :: * -&gt; *) batch sample r.
(MonadIO m, MonadBaseControl IO m) =&gt;
Buffer batch
-&gt; Int
-&gt; ([sample] -&gt; Maybe batch)
-&gt; ListT m sample
-&gt; ContT r m (ListT m batch)
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">bufferedCollate</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
-&gt; Buffer
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
forall a. Int -&gt; Buffer a
</span><a href="../file:///nix/store/gi3ii02mxr2l6qd73gq29asx1kn1yvi3-pipes-concurrency-lib-pipes-concurrency-2.0.12-haddock-doc/share/doc/pipes-concurrency/html/src"><span class="hs-identifier hs-var">P.bounded</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609475"><span class="hs-identifier hs-var">batchSize</span></a></span><span> </span><span class="annot"><span class="annottext">[(Float, Float)]
-&gt; Maybe
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        device
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609474"><span class="hs-identifier hs-var">collateFn</span></a></span><span>
</span><span id="line-584"></span><span>
</span><span id="line-585"></span><span class="hs-comment">-- | Run the two-layer network training loop on a toy dataset.</span><span>
</span><span id="line-586"></span><span class="annot"><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#runTwoLayerNetworkExample"><span class="hs-identifier hs-type">runTwoLayerNetworkExample</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-587"></span><span id="runTwoLayerNetworkExample"><span class="annot"><span class="annottext">runTwoLayerNetworkExample :: IO ()
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#runTwoLayerNetworkExample"><span class="hs-identifier hs-var hs-var">runTwoLayerNetworkExample</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-588"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- seed for the random number generator</span><span>
</span><span id="line-589"></span><span>      </span><span id="local-6989586621679609459"><span class="annot"><span class="annottext">seed :: Word64
</span><a href="#local-6989586621679609459"><span class="hs-identifier hs-var hs-var">seed</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Word64
</span><span class="hs-number">0</span></span><span>
</span><span id="line-590"></span><span>
</span><span id="line-591"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- compute device</span><span>
</span><span id="line-592"></span><span>      </span><span id="local-6989586621679609458"><span class="annot"><span class="annottext">device :: SDevice ('Device 'CPU)
</span><a href="#local-6989586621679609458"><span class="hs-identifier hs-var hs-var">device</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SDeviceType 'CPU -&gt; SDevice ('Device 'CPU)
forall (deviceType :: DeviceType Nat).
SDeviceType deviceType -&gt; SDevice ('Device deviceType)
</span><a href="Torch.GraduallyTyped.Device.html#SDevice"><span class="hs-identifier hs-var">SDevice</span></a></span><span> </span><span class="annot"><span class="annottext">SDeviceType 'CPU
</span><a href="Torch.GraduallyTyped.Device.html#SCPU"><span class="hs-identifier hs-var">SCPU</span></a></span><span>
</span><span id="line-593"></span><span>
</span><span id="line-594"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- input dimension of the network</span><span>
</span><span id="line-595"></span><span>      </span><span id="local-6989586621679609455"><span class="annot"><span class="annottext">inputDim :: SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
</span><a href="#local-6989586621679609455"><span class="hs-identifier hs-var hs-var">inputDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">KnownSymbol &quot;*&quot; =&gt; SName ('Name &quot;*&quot;)
forall (name :: Symbol). KnownSymbol name =&gt; SName ('Name name)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SName"><span class="hs-identifier hs-var">SName</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span> </span><span class="annot"><span class="annottext">SName ('Name &quot;*&quot;)
-&gt; SSize ('Size 1) -&gt; SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#%3A%26%3A"><span class="hs-operator hs-var">:&amp;:</span></a></span><span> </span><span class="annot"><span class="annottext">KnownNat 1 =&gt; SSize ('Size 1)
forall (size :: Nat). KnownNat size =&gt; SSize ('Size size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SSize"><span class="hs-identifier hs-var">SSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-596"></span><span>      </span><span class="hs-comment">-- output dimension of the network</span><span>
</span><span id="line-597"></span><span>      </span><span id="local-6989586621679609451"><span class="annot"><span class="annottext">outputDim :: SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
</span><a href="#local-6989586621679609451"><span class="hs-identifier hs-var hs-var">outputDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">KnownSymbol &quot;*&quot; =&gt; SName ('Name &quot;*&quot;)
forall (name :: Symbol). KnownSymbol name =&gt; SName ('Name name)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SName"><span class="hs-identifier hs-var">SName</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span> </span><span class="annot"><span class="annottext">SName ('Name &quot;*&quot;)
-&gt; SSize ('Size 1) -&gt; SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#%3A%26%3A"><span class="hs-operator hs-var">:&amp;:</span></a></span><span> </span><span class="annot"><span class="annottext">KnownNat 1 =&gt; SSize ('Size 1)
forall (size :: Nat). KnownNat size =&gt; SSize ('Size size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SSize"><span class="hs-identifier hs-var">SSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-598"></span><span>      </span><span class="hs-comment">-- hidden dimension of the network</span><span>
</span><span id="line-599"></span><span>      </span><span id="local-6989586621679609450"><span class="annot"><span class="annottext">hiddenDim :: SDim ('Dim ('Name &quot;*&quot;) ('Size 100))
</span><a href="#local-6989586621679609450"><span class="hs-identifier hs-var hs-var">hiddenDim</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">KnownSymbol &quot;*&quot; =&gt; SName ('Name &quot;*&quot;)
forall (name :: Symbol). KnownSymbol name =&gt; SName ('Name name)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SName"><span class="hs-identifier hs-var">SName</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-string">&quot;*&quot;</span></span><span> </span><span class="annot"><span class="annottext">SName ('Name &quot;*&quot;)
-&gt; SSize ('Size 100) -&gt; SDim ('Dim ('Name &quot;*&quot;) ('Size 100))
forall (name :: Name Symbol) (size :: Size Nat).
SName name -&gt; SSize size -&gt; SDim ('Dim name size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#%3A%26%3A"><span class="hs-operator hs-var">:&amp;:</span></a></span><span> </span><span class="annot"><span class="annottext">KnownNat 100 =&gt; SSize ('Size 100)
forall (size :: Nat). KnownNat size =&gt; SSize ('Size size)
</span><a href="Torch.GraduallyTyped.Shape.Type.html#SSize"><span class="hs-identifier hs-var">SSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">100</span></span><span>
</span><span id="line-600"></span><span>
</span><span id="line-601"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- create the model specifications</span><span>
</span><span id="line-602"></span><span>      </span><span id="local-6989586621679609449"><span class="annot"><span class="annottext">mkModelSpec :: SRequiresGradient requiresGradient
-&gt; SHasDropout hasDropout
-&gt; ModelSpec
     (TwoLayerNetworkF
        ('Gradient requiresGradient)
        hasDropout
        ('Device 'CPU)
        ('DataType 'Float)
        ('Dim ('Name &quot;*&quot;) ('Size 1))
        ('Dim ('Name &quot;*&quot;) ('Size 1))
        ('Dim ('Name &quot;*&quot;) ('Size 100)))
</span><a href="#local-6989586621679609449"><span class="hs-identifier hs-var hs-var">mkModelSpec</span></a></span></span><span> </span><span id="local-6989586621679609448"><span class="annot"><span class="annottext">SRequiresGradient requiresGradient
</span><a href="#local-6989586621679609448"><span class="hs-identifier hs-var">hasGradient</span></a></span></span><span> </span><span id="local-6989586621679609447"><span class="annot"><span class="annottext">SHasDropout hasDropout
</span><a href="#local-6989586621679609447"><span class="hs-identifier hs-var">hasDropout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-603"></span><span>        </span><span class="annot"><span class="annottext">SGradient ('Gradient requiresGradient)
-&gt; SHasDropout hasDropout
-&gt; SDevice ('Device 'CPU)
-&gt; SDataType ('DataType 'Float)
-&gt; SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
-&gt; SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
-&gt; SDim ('Dim ('Name &quot;*&quot;) ('Size 100))
-&gt; Double
-&gt; ModelSpec
     (TwoLayerNetworkF
        ('Gradient requiresGradient)
        hasDropout
        ('Device 'CPU)
        ('DataType 'Float)
        ('Dim ('Name &quot;*&quot;) ('Size 1))
        ('Dim ('Name &quot;*&quot;) ('Size 1))
        ('Dim ('Name &quot;*&quot;) ('Size 100)))
forall (gradient :: Gradient RequiresGradient)
       (hasDropout :: HasDropout) (device :: Device (DeviceType Nat))
       (dataType :: DataType DType)
       (inputDim :: Dim (Name Symbol) (Size Nat))
       (outputDim :: Dim (Name Symbol) (Size Nat))
       (hiddenDim :: Dim (Name Symbol) (Size Nat)).
SGradient gradient
-&gt; SHasDropout hasDropout
-&gt; SDevice device
-&gt; SDataType dataType
-&gt; SDim inputDim
-&gt; SDim outputDim
-&gt; SDim hiddenDim
-&gt; Double
-&gt; ModelSpec
     (TwoLayerNetworkF
        gradient hasDropout device dataType inputDim outputDim hiddenDim)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#twoLayerNetworkSpec"><span class="hs-identifier hs-var">twoLayerNetworkSpec</span></a></span><span>
</span><span id="line-604"></span><span>          </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
forall (requiresGradient :: RequiresGradient).
SRequiresGradient requiresGradient
-&gt; SGradient ('Gradient requiresGradient)
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SGradient"><span class="hs-identifier hs-var">SGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient requiresGradient
</span><a href="#local-6989586621679609448"><span class="hs-identifier hs-var">hasGradient</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-605"></span><span>          </span><span class="annot"><span class="annottext">SHasDropout hasDropout
</span><a href="#local-6989586621679609447"><span class="hs-identifier hs-var">hasDropout</span></a></span><span>
</span><span id="line-606"></span><span>          </span><span class="annot"><span class="annottext">SDevice ('Device 'CPU)
</span><a href="#local-6989586621679609458"><span class="hs-identifier hs-var">device</span></a></span><span>
</span><span id="line-607"></span><span>          </span><span class="hs-special">(</span><span class="annot"><span class="annottext">SDType 'Float -&gt; SDataType ('DataType 'Float)
forall (dType :: DType).
SDType dType -&gt; SDataType ('DataType dType)
</span><a href="Torch.GraduallyTyped.DType.html#SDataType"><span class="hs-identifier hs-var">SDataType</span></a></span><span> </span><span class="annot"><span class="annottext">SDType 'Float
</span><a href="Torch.GraduallyTyped.DType.html#SFloat"><span class="hs-identifier hs-var">SFloat</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-608"></span><span>          </span><span class="annot"><span class="annottext">SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
</span><a href="#local-6989586621679609455"><span class="hs-identifier hs-var">inputDim</span></a></span><span>
</span><span id="line-609"></span><span>          </span><span class="annot"><span class="annottext">SDim ('Dim ('Name &quot;*&quot;) ('Size 1))
</span><a href="#local-6989586621679609451"><span class="hs-identifier hs-var">outputDim</span></a></span><span>
</span><span id="line-610"></span><span>          </span><span class="annot"><span class="annottext">SDim ('Dim ('Name &quot;*&quot;) ('Size 100))
</span><a href="#local-6989586621679609450"><span class="hs-identifier hs-var">hiddenDim</span></a></span><span>
</span><span id="line-611"></span><span>          </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0.1</span></span><span>
</span><span id="line-612"></span><span>      </span><span class="hs-comment">-- during training, we need to turn dropout on and keep track of the gradient</span><span>
</span><span id="line-613"></span><span>      </span><span id="local-6989586621679609444"><span class="annot"><span class="annottext">trainingModelSpec :: NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     (ModelSpec (TLNDropoutF 'WithDropout))
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609444"><span class="hs-identifier hs-var hs-var">trainingModelSpec</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
-&gt; SHasDropout 'WithDropout
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        (ModelSpec (TLNDropoutF 'WithDropout))
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
forall (requiresGradient :: RequiresGradient)
       (hasDropout :: HasDropout).
SRequiresGradient requiresGradient
-&gt; SHasDropout hasDropout
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        (ModelSpec (TLNDropoutF hasDropout))
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609449"><span class="hs-identifier hs-var">mkModelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithGradient"><span class="hs-identifier hs-var">SWithGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SHasDropout 'WithDropout
</span><a href="Torch.GraduallyTyped.NN.Type.html#SWithDropout"><span class="hs-identifier hs-var">SWithDropout</span></a></span><span>
</span><span id="line-614"></span><span>      </span><span class="hs-comment">-- during evaluation, we don't need to turn dropout on, nor do we need to keep track of the gradient</span><span>
</span><span id="line-615"></span><span>      </span><span id="local-6989586621679609443"><span class="annot"><span class="annottext">evaluationModelSpec :: NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     (ModelSpec (TLNDropoutF 'WithoutDropout))
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609443"><span class="hs-identifier hs-var hs-var">evaluationModelSpec</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
-&gt; SHasDropout 'WithoutDropout
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        (ModelSpec (TLNDropoutF 'WithoutDropout))
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
forall (requiresGradient :: RequiresGradient)
       (hasDropout :: HasDropout).
SRequiresGradient requiresGradient
-&gt; SHasDropout hasDropout
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        (ModelSpec (TLNDropoutF hasDropout))
        (NamedModel
           (GLinear
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (TensorSpec
                    ('Gradient requiresGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609449"><span class="hs-identifier hs-var">mkModelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">SRequiresGradient 'WithoutGradient
</span><a href="Torch.GraduallyTyped.RequiresGradient.html#SWithoutGradient"><span class="hs-identifier hs-var">SWithoutGradient</span></a></span><span> </span><span class="annot"><span class="annottext">SHasDropout 'WithoutDropout
</span><a href="Torch.GraduallyTyped.NN.Type.html#SWithoutDropout"><span class="hs-identifier hs-var">SWithoutDropout</span></a></span><span>
</span><span id="line-616"></span><span>
</span><span id="line-617"></span><span>  </span><span class="hs-comment">-- create a Torch random generator from the seed</span><span>
</span><span id="line-618"></span><span>  </span><span id="local-6989586621679609442"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609442"><span class="hs-identifier hs-var">g0</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">SDevice ('Device 'CPU) -&gt; Word64 -&gt; IO (Generator ('Device 'CPU))
forall (m :: * -&gt; *) (device :: Device (DeviceType Nat)).
MonadThrow m =&gt;
SDevice device -&gt; Word64 -&gt; m (Generator device)
</span><a href="Torch.GraduallyTyped.Random.html#sMkGenerator"><span class="hs-identifier hs-var">sMkGenerator</span></a></span><span> </span><span class="annot"><span class="annottext">SDevice ('Device 'CPU)
</span><a href="#local-6989586621679609458"><span class="hs-identifier hs-var">device</span></a></span><span> </span><span class="annot"><span class="annottext">Word64
</span><a href="#local-6989586621679609459"><span class="hs-identifier hs-var">seed</span></a></span><span>
</span><span id="line-619"></span><span>
</span><span id="line-620"></span><span>  </span><span class="hs-comment">-- initialize the model from the model specification using the generator</span><span>
</span><span id="line-621"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679609440"><span class="annot"><span class="annottext">NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     Dropout
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609440"><span class="hs-identifier hs-var">model</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609439"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609439"><span class="hs-identifier hs-var">g1</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ModelSpec
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; Generator ('Device 'CPU)
-&gt; IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           Dropout
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))),
      Generator ('Device 'CPU))
forall model (generatorDevice :: Device (DeviceType Nat)) output
       (generatorOutputDevice :: Device (DeviceType Nat)) (m :: * -&gt; *).
(HasInitialize model generatorDevice output generatorOutputDevice,
 MonadThrow m) =&gt;
ModelSpec model
-&gt; Generator generatorDevice
-&gt; m (output, Generator generatorOutputDevice)
</span><a href="Torch.GraduallyTyped.NN.Class.html#initialize"><span class="hs-identifier hs-var">initialize</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     Dropout
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609444"><span class="hs-identifier hs-var">trainingModelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609442"><span class="hs-identifier hs-var">g0</span></a></span><span>
</span><span id="line-622"></span><span>
</span><span id="line-623"></span><span>  </span><span class="hs-comment">-- define collation function</span><span>
</span><span id="line-624"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679609438"><span class="annot"><span class="annottext">collate' :: ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
</span><a href="#local-6989586621679609438"><span class="hs-identifier hs-var hs-var">collate'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-625"></span><span>        </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- batch size</span><span>
</span><span id="line-626"></span><span>            </span><span id="local-6989586621679609437"><span class="annot"><span class="annottext">batchSize :: Int
</span><a href="#local-6989586621679609437"><span class="hs-identifier hs-var hs-var">batchSize</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">100</span></span><span>
</span><span id="line-627"></span><span>         </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">SDevice ('Device 'CPU)
-&gt; Int
-&gt; ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
forall (device :: Device (DeviceType Nat)) r.
SDevice device
-&gt; Int
-&gt; ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT r IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           device
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#collate"><span class="hs-identifier hs-var">Torch.GraduallyTyped.Examples.TwoLayerNetwork.collate</span></a></span><span> </span><span class="annot"><span class="annottext">SDevice ('Device 'CPU)
</span><a href="#local-6989586621679609458"><span class="hs-identifier hs-var">device</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609437"><span class="hs-identifier hs-var">batchSize</span></a></span><span>
</span><span id="line-628"></span><span>
</span><span id="line-629"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- total number of epochs</span><span>
</span><span id="line-630"></span><span>      </span><span id="local-6989586621679609436"><span class="annot"><span class="annottext">numEpochs :: Int
</span><a href="#local-6989586621679609436"><span class="hs-identifier hs-var hs-var">numEpochs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">100</span></span><span>
</span><span id="line-631"></span><span>      </span><span class="hs-comment">-- learning rate schedule</span><span>
</span><span id="line-632"></span><span>      </span><span id="local-6989586621679609435"><span class="annot"><span class="annottext">learningRateSchedule :: Int -&gt; Double
</span><a href="#local-6989586621679609435"><span class="hs-identifier hs-var hs-var">learningRateSchedule</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-633"></span><span>        </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- peak learning rate after warmup</span><span>
</span><span id="line-634"></span><span>            </span><span id="local-6989586621679609434"><span class="annot"><span class="annottext">maxLearningRate :: Double
</span><a href="#local-6989586621679609434"><span class="hs-identifier hs-var hs-var">maxLearningRate</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1e-2</span></span><span>
</span><span id="line-635"></span><span>            </span><span class="hs-comment">-- learning rate at the end of the schedule</span><span>
</span><span id="line-636"></span><span>            </span><span id="local-6989586621679609433"><span class="annot"><span class="annottext">finalLearningRate :: Double
</span><a href="#local-6989586621679609433"><span class="hs-identifier hs-var hs-var">finalLearningRate</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1e-4</span></span><span>
</span><span id="line-637"></span><span>            </span><span class="hs-comment">-- warmup epochs</span><span>
</span><span id="line-638"></span><span>            </span><span id="local-6989586621679609432"><span class="annot"><span class="annottext">numWarmupEpochs :: Int
</span><a href="#local-6989586621679609432"><span class="hs-identifier hs-var hs-var">numWarmupEpochs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">10</span></span><span>
</span><span id="line-639"></span><span>            </span><span class="hs-comment">-- cooldown epochs</span><span>
</span><span id="line-640"></span><span>            </span><span id="local-6989586621679609431"><span class="annot"><span class="annottext">numCooldownEpochs :: Int
</span><a href="#local-6989586621679609431"><span class="hs-identifier hs-var hs-var">numCooldownEpochs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">10</span></span><span>
</span><span id="line-641"></span><span>         </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Int -&gt; Int -&gt; Int -&gt; Int -&gt; Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#singleCycleLearningRateSchedule"><span class="hs-identifier hs-var">singleCycleLearningRateSchedule</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609434"><span class="hs-identifier hs-var">maxLearningRate</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679609433"><span class="hs-identifier hs-var">finalLearningRate</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609436"><span class="hs-identifier hs-var">numEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609432"><span class="hs-identifier hs-var">numWarmupEpochs</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609431"><span class="hs-identifier hs-var">numCooldownEpochs</span></a></span><span>
</span><span id="line-642"></span><span>
</span><span id="line-643"></span><span>  </span><span class="hs-comment">-- create the dataset(s) using a Haskell random generator</span><span>
</span><span id="line-644"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679609430"><span class="annot"><span class="annottext">SincData
</span><a href="#local-6989586621679609430"><span class="hs-identifier hs-var">trainingData</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609429"><span class="annot"><span class="annottext">SincData
</span><a href="#local-6989586621679609429"><span class="hs-identifier hs-var">evaluationData</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609428"><span class="annot"><span class="annottext">DatasetOptions
</span><a href="#local-6989586621679609428"><span class="hs-identifier hs-var">streamingState</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span>
</span><span id="line-645"></span><span>    </span><span class="annot"><span class="annottext">IO StdGen
forall (m :: * -&gt; *). MonadIO m =&gt; m StdGen
</span><a href="../file:///nix/store/fv40valsp1qi9dy3snb8jgiq0a9h8rjk-random-lib-random-1.2.0-haddock-doc/share/doc/random/html/src"><span class="hs-identifier hs-var">getStdGen</span></a></span><span>
</span><span id="line-646"></span><span>      </span><span class="annot"><span class="annottext">IO StdGen
-&gt; (StdGen -&gt; IO (SincData, SincData, DatasetOptions))
-&gt; IO (SincData, SincData, DatasetOptions)
forall (m :: * -&gt; *) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">StateT StdGen IO (SincData, SincData, DatasetOptions)
-&gt; StdGen -&gt; IO (SincData, SincData, DatasetOptions)
forall (m :: * -&gt; *) s a. Monad m =&gt; StateT s m a -&gt; s -&gt; m a
</span><span class="hs-identifier hs-var">evalStateT</span></span><span>
</span><span id="line-647"></span><span>        </span><span class="hs-special">(</span><span> </span><span class="hs-special">(</span><span class="hs-special">,</span><span class="hs-special">,</span><span class="hs-special">)</span><span>
</span><span id="line-648"></span><span>            </span><span class="hs-comment">-- create a dataset of 10000 unique training examples</span><span>
</span><span id="line-649"></span><span>            </span><span class="annot"><span class="annottext">(SincData
 -&gt; SincData
 -&gt; DatasetOptions
 -&gt; (SincData, SincData, DatasetOptions))
-&gt; StateT StdGen IO SincData
-&gt; StateT
     StdGen
     IO
     (SincData
      -&gt; DatasetOptions -&gt; (SincData, SincData, DatasetOptions))
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Text -&gt; Int -&gt; StateT StdGen IO SincData
forall g (m :: * -&gt; *).
(RandomGen g, Monad m) =&gt;
Text -&gt; Int -&gt; StateT g m SincData
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkSincData"><span class="hs-identifier hs-var">mkSincData</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-string">&quot;training&quot;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">10000</span></span><span>
</span><span id="line-650"></span><span>              </span><span class="hs-comment">-- create a dataset of 500 unique evaluation examples</span><span>
</span><span id="line-651"></span><span>              </span><span class="annot"><span class="annottext">StateT
  StdGen
  IO
  (SincData
   -&gt; DatasetOptions -&gt; (SincData, SincData, DatasetOptions))
-&gt; StateT StdGen IO SincData
-&gt; StateT
     StdGen IO (DatasetOptions -&gt; (SincData, SincData, DatasetOptions))
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">Text -&gt; Int -&gt; StateT StdGen IO SincData
forall g (m :: * -&gt; *).
(RandomGen g, Monad m) =&gt;
Text -&gt; Int -&gt; StateT g m SincData
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkSincData"><span class="hs-identifier hs-var">mkSincData</span></a></span><span> </span><span class="annot"><span class="annottext">Text
</span><span class="hs-string">&quot;evaluation&quot;</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">500</span></span><span>
</span><span id="line-652"></span><span>              </span><span class="hs-comment">-- configure the data loader for random shuffling</span><span>
</span><span id="line-653"></span><span>              </span><span class="annot"><span class="annottext">StateT
  StdGen IO (DatasetOptions -&gt; (SincData, SincData, DatasetOptions))
-&gt; StateT StdGen IO DatasetOptions
-&gt; StateT StdGen IO (SincData, SincData, DatasetOptions)
forall (f :: * -&gt; *) a b. Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">(StdGen -&gt; DatasetOptions) -&gt; StateT StdGen IO DatasetOptions
forall s (m :: * -&gt; *) a. MonadState s m =&gt; (s -&gt; a) -&gt; m a
</span><span class="hs-identifier hs-var">gets</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span id="local-6989586621679609427"><span class="annot"><span class="annottext">StdGen
</span><a href="#local-6989586621679609427"><span class="hs-identifier hs-var">stdGen</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int -&gt; DatasetOptions
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">datasetOpts</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">{</span><span class="annot"><span class="annottext">shuffle :: Sample
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">shuffle</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">StdGen -&gt; Sample
forall g. RandomGen g =&gt; g -&gt; Sample
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">Shuffle</span></a></span><span> </span><span class="annot"><span class="annottext">StdGen
</span><a href="#local-6989586621679609427"><span class="hs-identifier hs-var">stdGen</span></a></span><span class="hs-special">}</span><span class="hs-special">)</span><span>
</span><span id="line-654"></span><span>        </span><span class="hs-special">)</span><span>
</span><span id="line-655"></span><span>
</span><span id="line-656"></span><span>  </span><span class="hs-comment">-- create an Adam optimizer with learning rate 1e-4 using the model.</span><span>
</span><span id="line-657"></span><span>  </span><span class="hs-comment">-- the optimizer is responsible for computing the gradient of the loss</span><span>
</span><span id="line-658"></span><span>  </span><span class="hs-comment">-- with respect to the model parameters and updating the model parameters.</span><span>
</span><span id="line-659"></span><span>  </span><span id="local-6989586621679609423"><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
</span><a href="#local-6989586621679609423"><span class="hs-identifier hs-var">optim</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">IO
  (Optimizer
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           Dropout
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
-&gt; IO
     (Optimizer
        (NamedModel
           (TwoLayerNetwork
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
              Tanh
              Dropout
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
forall (m :: * -&gt; *) a. MonadIO m =&gt; IO a -&gt; m a
</span><span class="hs-identifier hs-var">liftIO</span></span><span> </span><span class="annot"><span class="annottext">(IO
   (Optimizer
      (NamedModel
         (TwoLayerNetwork
            (NamedModel
               (GLinear
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape
                           '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
            Tanh
            Dropout
            (NamedModel
               (GLinear
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape
                           '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
 -&gt; IO
      (Optimizer
         (NamedModel
            (TwoLayerNetwork
               (NamedModel
                  (GLinear
                     (NamedModel
                        (Tensor
                           ('Gradient 'WithGradient)
                           ('Layout 'Dense)
                           ('Device 'CPU)
                           ('DataType 'Float)
                           ('Shape
                              '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                     (NamedModel
                        (Tensor
                           ('Gradient 'WithGradient)
                           ('Layout 'Dense)
                           ('Device 'CPU)
                           ('DataType 'Float)
                           ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
               Tanh
               Dropout
               (NamedModel
                  (GLinear
                     (NamedModel
                        (Tensor
                           ('Gradient 'WithGradient)
                           ('Layout 'Dense)
                           ('Device 'CPU)
                           ('DataType 'Float)
                           ('Shape
                              '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                     (NamedModel
                        (Tensor
                           ('Gradient 'WithGradient)
                           ('Layout 'Dense)
                           ('Device 'CPU)
                           ('DataType 'Float)
                           ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))))
-&gt; IO
     (Optimizer
        (NamedModel
           (TwoLayerNetwork
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
              Tanh
              Dropout
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
-&gt; IO
     (Optimizer
        (NamedModel
           (TwoLayerNetwork
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
              Tanh
              Dropout
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">AdamOptions
-&gt; NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
-&gt; IO
     (Optimizer
        (NamedModel
           (TwoLayerNetwork
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
              Tanh
              Dropout
              (NamedModel
                 (GLinear
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape
                             '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                    (NamedModel
                       (Tensor
                          ('Gradient 'WithGradient)
                          ('Layout 'Dense)
                          ('Device 'CPU)
                          ('DataType 'Float)
                          ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
forall model.
HasStateDict model =&gt;
AdamOptions -&gt; model -&gt; IO (Optimizer model)
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#mkAdam"><span class="hs-identifier hs-var">mkAdam</span></a></span><span> </span><span class="annot"><span class="annottext">AdamOptions
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#defaultAdamOptions"><span class="hs-identifier hs-var">defaultAdamOptions</span></a></span><span> </span><span class="hs-special">{</span><span class="annot"><span class="annottext">learningRate :: Double
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#learningRate"><span class="hs-identifier hs-var">learningRate</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1e-4</span></span><span class="hs-special">}</span><span> </span><span class="annot"><span class="annottext">NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     Dropout
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609440"><span class="hs-identifier hs-var">model</span></a></span><span>
</span><span id="line-660"></span><span>
</span><span id="line-661"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- one epoch of training and evaluation</span><span>
</span><span id="line-662"></span><span>      </span><span id="local-6989586621679609422"><span class="annot"><span class="annottext">step :: (DatasetOptions, Generator ('Device 'CPU))
-&gt; Int
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (DatasetOptions, Generator ('Device 'CPU))
</span><a href="#local-6989586621679609422"><span class="hs-identifier hs-var hs-var">step</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609421"><span class="annot"><span class="annottext">DatasetOptions
</span><a href="#local-6989586621679609421"><span class="hs-identifier hs-var">streamingState'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609420"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609420"><span class="hs-identifier hs-var">g</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679609419"><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609419"><span class="hs-identifier hs-var">epoch</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-663"></span><span>        </span><span class="hs-comment">-- let learningRate = learningRateSchedule epoch</span><span>
</span><span id="line-664"></span><span>        </span><span class="hs-comment">-- ATen.setLearningRate optim learningRate</span><span>
</span><span id="line-665"></span><span>
</span><span id="line-666"></span><span>        </span><span class="hs-comment">-- train for one epoch on the training set</span><span>
</span><span id="line-667"></span><span>        </span><span class="hs-special">(</span><span id="local-6989586621679609418"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609418"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609417"><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609417"><span class="hs-identifier hs-var">shuffle</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-668"></span><span>          </span><span class="hs-special">(</span><span id="local-6989586621679609416"><span class="annot"><span class="annottext">ListT IO (Float, Float)
</span><a href="#local-6989586621679609416"><span class="hs-identifier hs-var">trainingStream</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609415"><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609415"><span class="hs-identifier hs-var">shuffle</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ContT () IO (ListT IO (Float, Float), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (ListT IO (Float, Float), Sample)
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">P.lift</span></span><span> </span><span class="annot"><span class="annottext">(ContT () IO (ListT IO (Float, Float), Sample)
 -&gt; Proxy
      X () () Monitor (ContT () IO) (ListT IO (Float, Float), Sample))
-&gt; ContT () IO (ListT IO (Float, Float), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (ListT IO (Float, Float), Sample)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">DatasetOptions
-&gt; SincData -&gt; ContT () IO (ListT IO (Float, Float), Sample)
forall (m :: * -&gt; *) dataset k sample r.
(Dataset m dataset k sample, MonadIO m, MonadBaseControl IO m) =&gt;
DatasetOptions -&gt; dataset -&gt; ContT r m (ListT m sample, Sample)
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">streamFromMap</span></a></span><span> </span><span class="annot"><span class="annottext">DatasetOptions
</span><a href="#local-6989586621679609421"><span class="hs-identifier hs-var">streamingState'</span></a></span><span> </span><span class="annot"><span class="annottext">SincData
</span><a href="#local-6989586621679609430"><span class="hs-identifier hs-var">trainingData</span></a></span><span>
</span><span id="line-669"></span><span>          </span><span id="local-6989586621679609413"><span class="annot"><span class="annottext">Either
  (Generator ('Device 'CPU))
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape '[]),
   Generator ('Device 'CPU))
</span><a href="#local-6989586621679609413"><span class="hs-identifier hs-var">trainingLoss</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-670"></span><span>            </span><span id="local-6989586621679609412"><span class="annot"><span class="annottext">ListT
  IO
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
   Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609412"><span class="hs-identifier hs-var">batchedStream</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
</span><a href="#local-6989586621679609438"><span class="hs-identifier hs-var">collate'</span></a></span><span> </span><span class="annot"><span class="annottext">ListT IO (Float, Float)
</span><a href="#local-6989586621679609416"><span class="hs-identifier hs-var">trainingStream</span></a></span><span>
</span><span id="line-671"></span><span>            </span><span class="annot"><span class="annottext">ContT
  ()
  IO
  (Either
     (Generator ('Device 'CPU))
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape '[]),
      Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">P.lift</span></span><span> </span><span class="annot"><span class="annottext">(ContT
   ()
   IO
   (Either
      (Generator ('Device 'CPU))
      (Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         ('Device 'CPU)
         ('DataType 'Float)
         ('Shape '[]),
       Generator ('Device 'CPU)))
 -&gt; Proxy
      X
      ()
      ()
      Monitor
      (ContT () IO)
      (Either
         (Generator ('Device 'CPU))
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            ('Device 'CPU)
            ('DataType 'Float)
            ('Shape '[]),
          Generator ('Device 'CPU))))
-&gt; (IO
      (Either
         (Generator ('Device 'CPU))
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            ('Device 'CPU)
            ('DataType 'Float)
            ('Shape '[]),
          Generator ('Device 'CPU)))
    -&gt; ContT
         ()
         IO
         (Either
            (Generator ('Device 'CPU))
            (Tensor
               ('Gradient 'WithoutGradient)
               ('Layout 'Dense)
               ('Device 'CPU)
               ('DataType 'Float)
               ('Shape '[]),
             Generator ('Device 'CPU))))
-&gt; IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">IO
  (Either
     (Generator ('Device 'CPU))
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape '[]),
      Generator ('Device 'CPU)))
-&gt; ContT
     ()
     IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">lift</span></span><span> </span><span class="annot"><span class="annottext">(IO
   (Either
      (Generator ('Device 'CPU))
      (Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         ('Device 'CPU)
         ('DataType 'Float)
         ('Shape '[]),
       Generator ('Device 'CPU)))
 -&gt; Proxy
      X
      ()
      ()
      Monitor
      (ContT () IO)
      (Either
         (Generator ('Device 'CPU))
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            ('Device 'CPU)
            ('DataType 'Float)
            ('Shape '[]),
          Generator ('Device 'CPU))))
-&gt; IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; ModelSpec
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           Dropout
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; ListT
     IO
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
-&gt; Generator ('Device 'CPU)
-&gt; IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall (m :: * -&gt; *) model input
       (generatorDevice :: Device (DeviceType Nat))
       (lossGradient :: Gradient RequiresGradient)
       (lossLayout :: Layout LayoutType)
       (lossDataType :: Device (DeviceType Nat))
       (lossDevice :: DataType DType)
       (lossShape :: Shape [Dim (Name Symbol) (Size Nat)])
       (generatorOutputDevice :: Device (DeviceType Nat)).
(MonadIO m, HasStateDict model,
 HasForward
   model
   input
   generatorDevice
   (Tensor lossGradient lossLayout lossDataType lossDevice lossShape)
   generatorOutputDevice,
 HasForward
   model
   input
   generatorOutputDevice
   (Tensor lossGradient lossLayout lossDataType lossDevice lossShape)
   generatorOutputDevice,
 SGetGeneratorDevice generatorDevice,
 SGetGeneratorDevice generatorOutputDevice,
 SGetGradient lossGradient, SGetShape lossShape,
 Catch (lossShape &lt;+&gt; 'Shape '[]),
 Catch (lossGradient &lt;+&gt; 'Gradient 'WithGradient)) =&gt;
Optimizer model
-&gt; ModelSpec model
-&gt; ListT m input
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#train"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
</span><a href="#local-6989586621679609423"><span class="hs-identifier hs-var">optim</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     Dropout
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609444"><span class="hs-identifier hs-var">trainingModelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">ListT
  IO
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
   Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609412"><span class="hs-identifier hs-var">batchedStream</span></a></span><span> </span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609420"><span class="hs-identifier hs-var">g</span></a></span><span>
</span><span id="line-672"></span><span>          </span><span class="hs-comment">-- returned is either the original generator or</span><span>
</span><span id="line-673"></span><span>          </span><span class="hs-comment">-- a pair of a generator and the average training loss</span><span>
</span><span id="line-674"></span><span>          </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Either
  (Generator ('Device 'CPU))
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape '[]),
   Generator ('Device 'CPU))
</span><a href="#local-6989586621679609413"><span class="hs-identifier hs-var">trainingLoss</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-675"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Left</span></span><span> </span><span id="local-6989586621679609411"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609411"><span class="hs-identifier hs-var">g'</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">(Generator ('Device 'CPU), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (Generator ('Device 'CPU), Sample)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609411"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609415"><span class="hs-identifier hs-var">shuffle</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-676"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Right</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609410"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  ('Layout 'Dense)
  ('Device 'CPU)
  ('DataType 'Float)
  ('Shape '[])
</span><a href="#local-6989586621679609410"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609409"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609409"><span class="hs-identifier hs-var">g'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-677"></span><span>              </span><span class="annot"><span class="annottext">Monitor -&gt; Proxy X () () Monitor (ContT () IO) ()
forall (m :: * -&gt; *) a x' x. Functor m =&gt; a -&gt; Proxy x' x () a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.yield</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float -&gt; Int -&gt; Monitor
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#TrainingMonitor"><span class="hs-identifier hs-var">TrainingMonitor</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  ('Layout 'Dense)
  ('Device 'CPU)
  ('DataType 'Float)
  ('Shape '[])
-&gt; Float
forall a (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)).
TensorLike a dType dims =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensor"><span class="hs-identifier hs-var">fromTensor</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  ('Layout 'Dense)
  ('Device 'CPU)
  ('DataType 'Float)
  ('Shape '[])
</span><a href="#local-6989586621679609410"><span class="hs-identifier hs-var">loss</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609419"><span class="hs-identifier hs-var">epoch</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-678"></span><span>              </span><span class="annot"><span class="annottext">(Generator ('Device 'CPU), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (Generator ('Device 'CPU), Sample)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609409"><span class="hs-identifier hs-var">g'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609415"><span class="hs-identifier hs-var">shuffle</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-679"></span><span>
</span><span id="line-680"></span><span>        </span><span class="hs-comment">-- evaluate on the evaluation set</span><span>
</span><span id="line-681"></span><span>        </span><span class="hs-special">(</span><span id="local-6989586621679609406"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609406"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609405"><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609405"><span class="hs-identifier hs-var">shuffle'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-682"></span><span>          </span><span class="hs-special">(</span><span id="local-6989586621679609404"><span class="annot"><span class="annottext">ListT IO (Float, Float)
</span><a href="#local-6989586621679609404"><span class="hs-identifier hs-var">evalStream</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609403"><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609403"><span class="hs-identifier hs-var">shuffle'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ContT () IO (ListT IO (Float, Float), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (ListT IO (Float, Float), Sample)
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">P.lift</span></span><span> </span><span class="annot"><span class="annottext">(ContT () IO (ListT IO (Float, Float), Sample)
 -&gt; Proxy
      X () () Monitor (ContT () IO) (ListT IO (Float, Float), Sample))
-&gt; ContT () IO (ListT IO (Float, Float), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (ListT IO (Float, Float), Sample)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">DatasetOptions
-&gt; SincData -&gt; ContT () IO (ListT IO (Float, Float), Sample)
forall (m :: * -&gt; *) dataset k sample r.
(Dataset m dataset k sample, MonadIO m, MonadBaseControl IO m) =&gt;
DatasetOptions -&gt; dataset -&gt; ContT r m (ListT m sample, Sample)
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">streamFromMap</span></a></span><span> </span><span class="annot"><span class="annottext">DatasetOptions
</span><a href="#local-6989586621679609421"><span class="hs-identifier hs-var">streamingState'</span></a></span><span> </span><span class="hs-special">{</span><span class="annot"><span class="annottext">Sample
shuffle :: Sample
shuffle :: Sample
</span><a href="#local-6989586621679609417"><span class="hs-identifier hs-var hs-var">shuffle</span></a></span><span class="hs-special">}</span><span> </span><span class="annot"><span class="annottext">SincData
</span><a href="#local-6989586621679609429"><span class="hs-identifier hs-var">evaluationData</span></a></span><span>
</span><span id="line-683"></span><span>          </span><span id="local-6989586621679609402"><span class="annot"><span class="annottext">Either
  (Generator ('Device 'CPU))
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape '[]),
   Generator ('Device 'CPU))
</span><a href="#local-6989586621679609402"><span class="hs-identifier hs-var">evalLoss</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-684"></span><span>            </span><span id="local-6989586621679609401"><span class="annot"><span class="annottext">ListT
  IO
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
   Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609401"><span class="hs-identifier hs-var">batchedStream</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">ListT IO (Float, Float)
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (ListT
        IO
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
         Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape
              '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)])))
</span><a href="#local-6989586621679609438"><span class="hs-identifier hs-var">collate'</span></a></span><span> </span><span class="annot"><span class="annottext">ListT IO (Float, Float)
</span><a href="#local-6989586621679609404"><span class="hs-identifier hs-var">evalStream</span></a></span><span>
</span><span id="line-685"></span><span>            </span><span class="annot"><span class="annottext">ContT
  ()
  IO
  (Either
     (Generator ('Device 'CPU))
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape '[]),
      Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">P.lift</span></span><span> </span><span class="annot"><span class="annottext">(ContT
   ()
   IO
   (Either
      (Generator ('Device 'CPU))
      (Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         ('Device 'CPU)
         ('DataType 'Float)
         ('Shape '[]),
       Generator ('Device 'CPU)))
 -&gt; Proxy
      X
      ()
      ()
      Monitor
      (ContT () IO)
      (Either
         (Generator ('Device 'CPU))
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            ('Device 'CPU)
            ('DataType 'Float)
            ('Shape '[]),
          Generator ('Device 'CPU))))
-&gt; (IO
      (Either
         (Generator ('Device 'CPU))
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            ('Device 'CPU)
            ('DataType 'Float)
            ('Shape '[]),
          Generator ('Device 'CPU)))
    -&gt; ContT
         ()
         IO
         (Either
            (Generator ('Device 'CPU))
            (Tensor
               ('Gradient 'WithoutGradient)
               ('Layout 'Dense)
               ('Device 'CPU)
               ('DataType 'Float)
               ('Shape '[]),
             Generator ('Device 'CPU))))
-&gt; IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">IO
  (Either
     (Generator ('Device 'CPU))
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape '[]),
      Generator ('Device 'CPU)))
-&gt; ContT
     ()
     IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall (t :: (* -&gt; *) -&gt; * -&gt; *) (m :: * -&gt; *) a.
(MonadTrans t, Monad m) =&gt;
m a -&gt; t m a
</span><span class="hs-identifier hs-var">lift</span></span><span> </span><span class="annot"><span class="annottext">(IO
   (Either
      (Generator ('Device 'CPU))
      (Tensor
         ('Gradient 'WithoutGradient)
         ('Layout 'Dense)
         ('Device 'CPU)
         ('DataType 'Float)
         ('Shape '[]),
       Generator ('Device 'CPU)))
 -&gt; Proxy
      X
      ()
      ()
      Monitor
      (ContT () IO)
      (Either
         (Generator ('Device 'CPU))
         (Tensor
            ('Gradient 'WithoutGradient)
            ('Layout 'Dense)
            ('Device 'CPU)
            ('DataType 'Float)
            ('Shape '[]),
          Generator ('Device 'CPU))))
-&gt; IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-686"></span><span>              </span><span id="local-6989586621679609400"><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609400"><span class="hs-identifier hs-var">stateDict</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; IO StateDict
forall model. Optimizer model -&gt; IO StateDict
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getStateDict"><span class="hs-identifier hs-var">getStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
</span><a href="#local-6989586621679609423"><span class="hs-identifier hs-var">optim</span></a></span><span>
</span><span id="line-687"></span><span>              </span><span id="local-6989586621679609399"><span class="annot"><span class="annottext">NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     ()
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609399"><span class="hs-identifier hs-var">evaluationModel</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">(StateT
   StateDict
   IO
   (NamedModel
      (TwoLayerNetwork
         (NamedModel
            (GLinear
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape
                        '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
         Tanh
         ()
         (NamedModel
            (GLinear
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape
                        '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
 -&gt; StateDict
 -&gt; IO
      (NamedModel
         (TwoLayerNetwork
            (NamedModel
               (GLinear
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape
                           '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
            Tanh
            ()
            (NamedModel
               (GLinear
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape
                           '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
-&gt; StateDict
-&gt; StateT
     StateDict
     IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           ()
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           ()
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
forall a b c. (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
</span><span class="hs-identifier hs-var">flip</span></span><span> </span><span class="annot"><span class="annottext">StateT
  StateDict
  IO
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        ()
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; StateDict
-&gt; IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           ()
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
forall (m :: * -&gt; *) s a. Monad m =&gt; StateT s m a -&gt; s -&gt; m a
</span><span class="hs-identifier hs-var">evalStateT</span></span><span> </span><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609400"><span class="hs-identifier hs-var">stateDict</span></a></span><span> </span><span class="annot"><span class="annottext">(StateT
   StateDict
   IO
   (NamedModel
      (TwoLayerNetwork
         (NamedModel
            (GLinear
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape
                        '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
         Tanh
         ()
         (NamedModel
            (GLinear
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape
                        '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
               (NamedModel
                  (Tensor
                     ('Gradient 'WithoutGradient)
                     ('Layout 'Dense)
                     ('Device 'CPU)
                     ('DataType 'Float)
                     ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
 -&gt; IO
      (NamedModel
         (TwoLayerNetwork
            (NamedModel
               (GLinear
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape
                           '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
            Tanh
            ()
            (NamedModel
               (GLinear
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape
                           '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                  (NamedModel
                     (Tensor
                        ('Gradient 'WithoutGradient)
                        ('Layout 'Dense)
                        ('Device 'CPU)
                        ('DataType 'Float)
                        ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))))
-&gt; StateT
     StateDict
     IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           ()
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           ()
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">ModelSpec
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        ()
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; Text
-&gt; StateT
     StateDict
     IO
     (NamedModel
        (TwoLayerNetwork
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
           Tanh
           ()
           (NamedModel
              (GLinear
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape
                          '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
                 (NamedModel
                    (Tensor
                       ('Gradient 'WithoutGradient)
                       ('Layout 'Dense)
                       ('Device 'CPU)
                       ('DataType 'Float)
                       ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
forall model (m :: * -&gt; *).
(HasStateDict model, MonadIO m, MonadThrow m,
 MonadState StateDict m) =&gt;
ModelSpec model -&gt; Text -&gt; m model
</span><a href="Torch.GraduallyTyped.NN.Class.html#fromStateDict"><span class="hs-identifier hs-var">fromStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">ModelSpec
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        ()
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithoutGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     ()
     (NamedModel
        (GLinear
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (TensorSpec
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609443"><span class="hs-identifier hs-var">evaluationModelSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Text
forall a. Monoid a =&gt; a
</span><span class="hs-identifier hs-var">mempty</span></span><span>
</span><span id="line-688"></span><span>              </span><span class="annot"><span class="annottext">NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     ()
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
-&gt; ListT
     IO
     (Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
      Tensor
        ('Gradient 'WithoutGradient)
        ('Layout 'Dense)
        ('Device 'CPU)
        ('DataType 'Float)
        ('Shape
           '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
-&gt; Generator ('Device 'CPU)
-&gt; IO
     (Either
        (Generator ('Device 'CPU))
        (Tensor
           ('Gradient 'WithoutGradient)
           ('Layout 'Dense)
           ('Device 'CPU)
           ('DataType 'Float)
           ('Shape '[]),
         Generator ('Device 'CPU)))
forall (m :: * -&gt; *) model input
       (generatorDevice :: Device (DeviceType Nat))
       (lossGradient :: Gradient RequiresGradient)
       (lossLayout :: Layout LayoutType)
       (lossDataType :: Device (DeviceType Nat))
       (lossDevice :: DataType DType)
       (lossShape :: Shape [Dim (Name Symbol) (Size Nat)])
       (generatorOutputDevice :: Device (DeviceType Nat)).
(MonadIO m, HasStateDict model,
 HasForward
   model
   input
   generatorDevice
   (Tensor lossGradient lossLayout lossDataType lossDevice lossShape)
   generatorOutputDevice,
 HasForward
   model
   input
   generatorOutputDevice
   (Tensor lossGradient lossLayout lossDataType lossDevice lossShape)
   generatorOutputDevice,
 SGetGradient lossGradient, SGetShape lossShape,
 Catch (lossShape &lt;+&gt; 'Shape '[]),
 Catch (lossGradient &lt;+&gt; 'Gradient 'WithoutGradient)) =&gt;
model
-&gt; ListT m input
-&gt; Generator generatorDevice
-&gt; m (Either
        (Generator generatorDevice)
        (Tensor
           ('Gradient 'WithoutGradient)
           lossLayout
           lossDataType
           lossDevice
           ('Shape '[]),
         Generator generatorOutputDevice))
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#eval"><span class="hs-identifier hs-var">eval</span></a></span><span> </span><span class="annot"><span class="annottext">NamedModel
  (TwoLayerNetwork
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
     Tanh
     ()
     (NamedModel
        (GLinear
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape
                    '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
           (NamedModel
              (Tensor
                 ('Gradient 'WithoutGradient)
                 ('Layout 'Dense)
                 ('Device 'CPU)
                 ('DataType 'Float)
                 ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)]))))))
</span><a href="#local-6989586621679609399"><span class="hs-identifier hs-var">evaluationModel</span></a></span><span> </span><span class="annot"><span class="annottext">ListT
  IO
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]),
   Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape
        '[ 'Dim ('Name &quot;*&quot;) 'UncheckedSize, 'Dim ('Name &quot;*&quot;) ('Size 1)]))
</span><a href="#local-6989586621679609401"><span class="hs-identifier hs-var">batchedStream</span></a></span><span> </span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609418"><span class="hs-identifier hs-var">g'</span></a></span><span>
</span><span id="line-689"></span><span>          </span><span class="hs-comment">-- returned is either the original generator or</span><span>
</span><span id="line-690"></span><span>          </span><span class="hs-comment">-- a pair of a generator and the average evaluation loss</span><span>
</span><span id="line-691"></span><span>          </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Either
  (Generator ('Device 'CPU))
  (Tensor
     ('Gradient 'WithoutGradient)
     ('Layout 'Dense)
     ('Device 'CPU)
     ('DataType 'Float)
     ('Shape '[]),
   Generator ('Device 'CPU))
</span><a href="#local-6989586621679609402"><span class="hs-identifier hs-var">evalLoss</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-692"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Left</span></span><span> </span><span id="local-6989586621679609398"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609398"><span class="hs-identifier hs-var">g''</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">(Generator ('Device 'CPU), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (Generator ('Device 'CPU), Sample)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609398"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609403"><span class="hs-identifier hs-var">shuffle'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-693"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Right</span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609397"><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  ('Layout 'Dense)
  ('Device 'CPU)
  ('DataType 'Float)
  ('Shape '[])
</span><a href="#local-6989586621679609397"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609396"><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609396"><span class="hs-identifier hs-var">g''</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-694"></span><span>              </span><span class="annot"><span class="annottext">Monitor -&gt; Proxy X () () Monitor (ContT () IO) ()
forall (m :: * -&gt; *) a x' x. Functor m =&gt; a -&gt; Proxy x' x () a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.yield</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float -&gt; Int -&gt; Monitor
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#EvaluationMonitor"><span class="hs-identifier hs-var">EvaluationMonitor</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  ('Layout 'Dense)
  ('Device 'CPU)
  ('DataType 'Float)
  ('Shape '[])
-&gt; Float
forall a (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)])
       (gradient :: Gradient RequiresGradient)
       (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)).
TensorLike a dType dims =&gt;
Tensor gradient layout device ('DataType dType) ('Shape dims) -&gt; a
</span><a href="Torch.GraduallyTyped.Tensor.Type.html#fromTensor"><span class="hs-identifier hs-var">fromTensor</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  ('Gradient 'WithoutGradient)
  ('Layout 'Dense)
  ('Device 'CPU)
  ('DataType 'Float)
  ('Shape '[])
</span><a href="#local-6989586621679609397"><span class="hs-identifier hs-var">loss</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609419"><span class="hs-identifier hs-var">epoch</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-695"></span><span>              </span><span class="annot"><span class="annottext">(Generator ('Device 'CPU), Sample)
-&gt; Proxy
     X () () Monitor (ContT () IO) (Generator ('Device 'CPU), Sample)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609396"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609403"><span class="hs-identifier hs-var">shuffle'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-696"></span><span>
</span><span id="line-697"></span><span>        </span><span class="annot"><span class="annottext">(DatasetOptions, Generator ('Device 'CPU))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (DatasetOptions, Generator ('Device 'CPU))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">DatasetOptions
</span><a href="#local-6989586621679609421"><span class="hs-identifier hs-var">streamingState'</span></a></span><span> </span><span class="hs-special">{</span><span class="annot"><span class="annottext">shuffle :: Sample
</span><a href="../../../../hasktorch/html/src"><span class="hs-identifier hs-var">shuffle</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Sample
</span><a href="#local-6989586621679609405"><span class="hs-identifier hs-var">shuffle'</span></a></span><span class="hs-special">}</span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609406"><span class="hs-identifier hs-var">g''</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-698"></span><span>
</span><span id="line-699"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- initialize the training loop</span><span>
</span><span id="line-700"></span><span>      </span><span id="local-6989586621679609395"><span class="annot"><span class="annottext">init' :: Proxy
  X
  ()
  ()
  Monitor
  (ContT () IO)
  (DatasetOptions, Generator ('Device 'CPU))
</span><a href="#local-6989586621679609395"><span class="hs-identifier hs-var hs-var">init'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(DatasetOptions, Generator ('Device 'CPU))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (DatasetOptions, Generator ('Device 'CPU))
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">DatasetOptions
</span><a href="#local-6989586621679609428"><span class="hs-identifier hs-var">streamingState</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Generator ('Device 'CPU)
</span><a href="#local-6989586621679609439"><span class="hs-identifier hs-var">g1</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-701"></span><span>
</span><span id="line-702"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span class="hs-comment">-- finalize the training loop</span><span>
</span><span id="line-703"></span><span>      </span><span id="local-6989586621679609394"><span class="annot"><span class="annottext">done :: (a, b) -&gt; f ()
</span><a href="#local-6989586621679609394"><span class="hs-identifier hs-var hs-var">done</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679609393"><span class="annot"><span class="annottext">a
</span><a href="#local-6989586621679609393"><span class="hs-identifier hs-var">_streamingState'</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679609392"><span class="annot"><span class="annottext">b
</span><a href="#local-6989586621679609392"><span class="hs-identifier hs-var">_g2</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">() -&gt; f ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-704"></span><span>
</span><span id="line-705"></span><span>  </span><span class="hs-comment">-- run the training loop</span><span>
</span><span id="line-706"></span><span>  </span><span class="annot"><span class="annottext">(ContT () IO () -&gt; (() -&gt; IO ()) -&gt; IO ())
-&gt; (() -&gt; IO ()) -&gt; ContT () IO () -&gt; IO ()
forall a b c. (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
</span><span class="hs-identifier hs-var">flip</span></span><span> </span><span class="annot"><span class="annottext">ContT () IO () -&gt; (() -&gt; IO ()) -&gt; IO ()
forall k (r :: k) (m :: k -&gt; *) a. ContT r m a -&gt; (a -&gt; m r) -&gt; m r
</span><span class="hs-identifier hs-var hs-var">runContT</span></span><span> </span><span class="annot"><span class="annottext">() -&gt; IO ()
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(ContT () IO () -&gt; IO ())
-&gt; (Effect (ContT () IO) () -&gt; ContT () IO ())
-&gt; Effect (ContT () IO) ()
-&gt; IO ()
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Effect (ContT () IO) () -&gt; ContT () IO ()
forall (m :: * -&gt; *) r. Monad m =&gt; Effect m r -&gt; m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.runEffect</span></a></span><span> </span><span class="annot"><span class="annottext">(Effect (ContT () IO) () -&gt; IO ())
-&gt; Effect (ContT () IO) () -&gt; IO ()
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-707"></span><span>    </span><span class="annot"><span class="annottext">((DatasetOptions, Generator ('Device 'CPU))
 -&gt; Int
 -&gt; Proxy
      X
      ()
      ()
      Monitor
      (ContT () IO)
      (DatasetOptions, Generator ('Device 'CPU)))
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (DatasetOptions, Generator ('Device 'CPU))
-&gt; ((DatasetOptions, Generator ('Device 'CPU))
    -&gt; Proxy X () () Monitor (ContT () IO) ())
-&gt; Producer Int (Proxy X () () Monitor (ContT () IO)) ()
-&gt; Proxy X () () Monitor (ContT () IO) ()
forall (m :: * -&gt; *) x a b.
Monad m =&gt;
(x -&gt; a -&gt; m x) -&gt; m x -&gt; (x -&gt; m b) -&gt; Producer a m () -&gt; m b
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.foldM</span></a></span><span> </span><span class="annot"><span class="annottext">(DatasetOptions, Generator ('Device 'CPU))
-&gt; Int
-&gt; Proxy
     X
     ()
     ()
     Monitor
     (ContT () IO)
     (DatasetOptions, Generator ('Device 'CPU))
</span><a href="#local-6989586621679609422"><span class="hs-identifier hs-var">step</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy
  X
  ()
  ()
  Monitor
  (ContT () IO)
  (DatasetOptions, Generator ('Device 'CPU))
</span><a href="#local-6989586621679609395"><span class="hs-identifier hs-var">init'</span></a></span><span> </span><span class="annot"><span class="annottext">(DatasetOptions, Generator ('Device 'CPU))
-&gt; Proxy X () () Monitor (ContT () IO) ()
forall (f :: * -&gt; *) a b. Applicative f =&gt; (a, b) -&gt; f ()
</span><a href="#local-6989586621679609394"><span class="hs-identifier hs-var">done</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">[Int] -&gt; Producer Int (Proxy X () () Monitor (ContT () IO)) ()
forall (m :: * -&gt; *) (f :: * -&gt; *) a x' x.
(Functor m, Foldable f) =&gt;
f a -&gt; Proxy x' x () a m ()
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-identifier hs-var">P.each</span></a></span><span> </span><span class="hs-special">[</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">..</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679609436"><span class="hs-identifier hs-var">numEpochs</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-708"></span><span>      </span><span class="annot"><span class="annottext">Proxy X () () Monitor (ContT () IO) ()
-&gt; Proxy () Monitor () X (ContT () IO) ()
-&gt; Effect (ContT () IO) ()
forall (m :: * -&gt; *) a' a b r c' c.
Functor m =&gt;
Proxy a' a () b m r -&gt; Proxy () b c' c m r -&gt; Proxy a' a c' c m r
</span><a href="../file:///nix/store/2275zkfvd9na2mx7hmkq7wzrgjyxz84l-pipes-lib-pipes-4.3.15-haddock-doc/share/doc/pipes/html/src"><span class="hs-operator hs-var">P.&gt;-&gt;</span></a></span><span> </span><span class="annot"><span class="annottext">Proxy () Monitor () X (ContT () IO) ()
forall (m :: * -&gt; *) r. MonadIO m =&gt; Consumer Monitor m r
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#monitor"><span class="hs-identifier hs-var">monitor</span></a></span><span>
</span><span id="line-709"></span><span>
</span><span id="line-710"></span><span>  </span><span class="hs-comment">-- save the model's state dictionary to a file</span><span>
</span><span id="line-711"></span><span>  </span><span id="local-6989586621679609390"><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609390"><span class="hs-identifier hs-var">stateDict'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
-&gt; IO StateDict
forall model. Optimizer model -&gt; IO StateDict
</span><a href="Torch.GraduallyTyped.Examples.TwoLayerNetwork.html#getStateDict"><span class="hs-identifier hs-var">getStateDict</span></a></span><span> </span><span class="annot"><span class="annottext">Optimizer
  (NamedModel
     (TwoLayerNetwork
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 100), 'Dim ('Name &quot;*&quot;) ('Size 1)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 100)])))))
        Tanh
        Dropout
        (NamedModel
           (GLinear
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape
                       '[ 'Dim ('Name &quot;*&quot;) ('Size 1), 'Dim ('Name &quot;*&quot;) ('Size 100)])))
              (NamedModel
                 (Tensor
                    ('Gradient 'WithGradient)
                    ('Layout 'Dense)
                    ('Device 'CPU)
                    ('DataType 'Float)
                    ('Shape '[ 'Dim ('Name &quot;*&quot;) ('Size 1)])))))))
</span><a href="#local-6989586621679609423"><span class="hs-identifier hs-var">optim</span></a></span><span>
</span><span id="line-712"></span><span>  </span><span class="annot"><span class="annottext">StateDict -&gt; String -&gt; IO ()
</span><a href="Torch.GraduallyTyped.NN.Class.html#stateDictToFile"><span class="hs-identifier hs-var">stateDictToFile</span></a></span><span> </span><span class="annot"><span class="annottext">StateDict
</span><a href="#local-6989586621679609390"><span class="hs-identifier hs-var">stateDict'</span></a></span><span> </span><span class="annot"><span class="annottext">String
</span><span class="hs-string">&quot;twoLayerNetwork.pt&quot;</span></span><span>
</span><span id="line-713"></span></pre></body></html>