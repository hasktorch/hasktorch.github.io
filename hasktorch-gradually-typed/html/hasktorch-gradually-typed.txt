-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | experimental project for hasktorch
--   
--   experimental project for hasktorch
@package hasktorch-gradually-typed
@version 0.2.0.0

module Torch.Data.Parser

-- | <tt>Parser b i a</tt> is a parser that consumes a stream of <tt>i</tt>
--   tokens and as a result yields a value of type <tt>a</tt>, while
--   operating under the <tt>b</tt> non-determinism monad.
--   
--   For most purposes, the non-determinism monad <tt>b</tt> should be a
--   <a>MonadPlus</a>. Useful examples include <tt>[]</tt> or
--   <tt>Logic</tt> if you want backtracking, and <a>Maybe</a> if you want
--   no backtracking.
--   
--   Use <tt><a>StateT</a> s []</tt> if you want to maintain a state
--   <tt>s</tt> that is automatically reverted when backtracking via
--   <tt>[]</tt>.
--   
--   <tt>hoistFreeT</tt> can be used to change the backtracking monad.
--   
--   <a>FreeT</a> provides instances for <a>Functor</a>,
--   <a>Applicative</a>, <a>Monad</a>, <a>Alternative</a> and
--   <a>MonadPlus</a>.
type Parser (b :: Type -> Type) (i :: Type) (a :: Type) = FreeT ((->) i) b a

-- | Recurse over a parser.
--   
--   Tears down the free monad transformer over the '(-&gt;) i' functor
--   using iteration: <tt> recurse next parser = next parser (parser' -&gt;
--   next parser' (parser'' -&gt; next parser'' (parser''' -&gt; next
--   parser''' (...)))) </tt>
recurse :: forall t b i a. (Parser b i a -> (Parser b i a -> t b a) -> t b a) -> Parser b i a -> t b a
parseStream :: forall s b i a. Monad b => (s -> b (i, s)) -> Parser (StateT s b) i a -> s -> b (a, s)
parseString :: forall b i a. MonadPlus b => Parser (StateT [i] b) i a -> [i] -> b (a, [i])

-- | <tt>token</tt> is trivial parser that consumes a single token
--   <tt>i</tt> and yields it.
--   
--   Other parsers can be derived from this one using methods of the
--   <a>Functor</a>, <a>Applicative</a>, <a>Monad</a>, <a>Alternative</a>,
--   and <a>MonadPlus</a> typeclasses and the parser combinators in this
--   module.
token :: forall b i. Monad b => Parser b i i
eof :: Alternative b => Parser (StateT [i] b) i ()
notFollowedBy :: forall b i a. (Alternative b, Foldable b, MonadPlus b) => Parser (StateT [i] b) i a -> Parser (StateT [i] b) i ()

-- | <tt>satisfy p</tt> is a simple parser that consumes a single token
--   <tt>i</tt> and yields it if and only if <tt>p i</tt> evaluates to
--   <a>True</a>. Otherwise, the parser fails.
satisfy :: forall b i. MonadPlus b => (i -> Bool) -> Parser b i i

-- | <tt>is i</tt> is a simple parser that consumes a single token and
--   yields it if and only if it is equal to <tt>i</tt>. Otherwise, the
--   parser fails.
isToken :: forall b i. (MonadPlus b, Eq i) => i -> Parser b i i

-- | <tt>isNot i</tt> is a simple parser that consumes a single token and
--   yields it if and only if it is not equal to <tt>i</tt>. If the token
--   is equal to <tt>i</tt>, the parser fails.
isNotToken :: forall b i. (MonadPlus b, Eq i) => i -> Parser b i i

-- | Stateful scanner.
--   
--   <pre>
--   &gt;&gt;&gt; :{
--     f s a | "ell" `isInfixOf` (s ++ [a]) = Nothing
--           | otherwise                    = Just (s ++ [a])
--   :}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (scan f "" token) "hello 123"
--   ("hel","lo 123")
--   </pre>
scan :: (Alternative m, Monad m) => (s -> a -> Maybe s) -> s -> m a -> m [a]

-- | <tt>atMost n p</tt> applies the parser <tt>p</tt> at most <tt>n</tt>
--   times and returns every parsing result. If parsing of <tt>p</tt>
--   succeeds less the <tt>n</tt> times, <tt>repeatP n p</tt> succeeds as
--   well.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (atMost 2 (isToken 'a')) "aaaaaab"
--   ("aa","aaaab")
--   </pre>
atMost :: (Alternative m, Monad m) => Int -> m a -> m [a]

-- | <tt>eitherP p p'</tt> combines the two alternatives <tt>p</tt> and
--   <tt>p'</tt>.
eitherP :: Alternative f => f a -> f b -> f (Either a b)

-- | <tt>void p</tt> applies the parser <tt>p</tt> and discards its result.
void :: Functor f => f a -> f ()

-- | <tt>combine p p'</tt> merges the results of <tt>p</tt> and <tt>p'</tt>
--   using the <a>Semigroup</a> instance.
combine :: (Applicative f, Semigroup a) => f a -> f a -> f a

-- | <tt>combines ps</tt> merges the results of the parsers <tt>ps</tt>
--   using the <a>Monoid</a> instance.
combines :: (Applicative f, Monoid a) => [f a] -> f a

-- | <tt>isString s</tt> is a simple parser that consumes <a>Char</a>
--   tokens and yields them if and only if they assemble the <a>String</a>
--   <tt>s</tt>. Otherwise, the parser fails.
isString :: (Traversable t, MonadPlus b, Eq i) => t i -> Parser b i (t i)

-- | <tt>string</tt> matches any string
--   
--   <pre>
--   &gt;&gt;&gt; parseString @[] string "a string"
--   [("a string",""),("a strin","g"),("a stri","ng"),("a str","ing"),("a st","ring"),("a s","tring"),("a ","string"),("a"," string"),("","a string")]
--   </pre>
--   
--   <ul>
--   <li>- &gt;&gt;&gt; p = string @[] &gt;&gt;= s -&gt; (guard ("dog"
--   <a>isInfixOf</a> s) &gt;&gt; pure s)</li>
--   <li>- &gt;&gt;&gt; head $ parseString p "this is a string with a
--   dog"</li>
--   <li>- ("this is a string with a dog","")</li>
--   <li>- &gt;&gt;&gt; p = string @[] &gt;&gt;= s -&gt; (guard (not $
--   "dog" <a>isInfixOf</a> s) &gt;&gt; pure s)</li>
--   <li>- &gt;&gt;&gt; head $ parseString p "this is also string with a
--   dog"</li>
--   <li>- ("this is also string with a do","g")</li>
--   </ul>
string :: MonadPlus b => Parser b i [i]
intP :: (CharParsing m, Monad m) => m Int
doubleP :: (CharParsing m, Monad m) => m Double
instance (GHC.Base.Applicative f, Control.Monad.Logic.Class.MonadLogic b) => Control.Monad.Logic.Class.MonadLogic (Control.Monad.Trans.Free.FreeT f b)
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Combinators.Parsing (Control.Monad.Trans.Free.FreeT ((->) i) (Control.Monad.Trans.State.Lazy.StateT [i] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Char.CharParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Char] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Token.TokenParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Char] b))

module Torch.GraduallyTyped.Internal.Void
data Void

module Torch.GraduallyTyped.NN.Type
data HasBias
WithBias :: HasBias
WithoutBias :: HasBias
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Type.HasBias
instance GHC.Show.Show Torch.GraduallyTyped.NN.Type.HasBias
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Type.HasBias
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Type.HasBias

module Torch.GraduallyTyped.Prelude
type Type = Type
data Constraint
data Proxy (t :: k)
Proxy :: Proxy (t :: k)
type family (a :: Bool) || (b :: Bool) :: Bool
type family If (cond :: Bool) (tru :: k) (fls :: k) :: k
data Nat
type family (a :: Nat) + (b :: Nat) :: Nat
type family (a :: Nat) * (b :: Nat) :: Nat
type family TypeError (a :: ErrorMessage) :: b
data ErrorMessage
Text :: Symbol -> ErrorMessage
ShowType :: t -> ErrorMessage
(:<>:) :: ErrorMessage -> ErrorMessage -> ErrorMessage
(:$$:) :: ErrorMessage -> ErrorMessage -> ErrorMessage
data IsChecked a
Checked :: a -> IsChecked a
Unchecked :: a -> IsChecked a
pattern IsChecked :: a -> IsChecked a
pattern Demoted :: SingKind k => Demote k -> Sing (a :: k)
pattern Demoted' :: (SingKind k, Demote k ~ IsChecked t) => t -> Sing (a :: k)
forgetIsChecked :: IsChecked a -> a
type family All (c :: k -> Constraint) (xs :: [k]) :: Constraint
class KnownElem k x where {
    type family ElemValF k :: Type;
}
elemVal :: KnownElem k x => ElemValF k
class KnownList k (xs :: [k])
listVal :: KnownList k xs => [ElemValF k]

-- | Can be used to report stuck type families, see
--   <a>https://kcsongor.github.io/report-stuck-families/</a>. This family
--   is able to check whether its argument <tt>a</tt> is stuck and report
--   an error <tt>err</tt> in that case.
type family Assert err a

-- | Approximates a normal form on the type level. <a>Catch</a> forces its
--   argument <tt>a</tt> and returns an empty <a>Constraint</a> if and only
--   if the argument does not produce a <a>TypeError</a>.
--   
--   The first equation will recursively force the kind of the argument
--   until it reaches <a>Type</a> or a <a>TypeError</a>. In the former
--   case, it falls over to the second equation which will produce the
--   empty constraint. In the latter case, it gets stuck with 'Catch
--   (TypeError ...)', and the compiler will report the error message.
--   
--   Thanks to <a>https://kcsongor.github.io/kcsongor</a> for the
--   suggestion.
type family Catch a
type family Seq (a :: k) (b :: k') :: k'

-- | Returns the first element of a type-level tuple with the kind <tt>(k,
--   k')</tt> marked by a prefix quote.
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Fst '(Int, String)
--   Fst '(Int, String) :: *
--   = Int
--   
--   &gt;&gt;&gt; :kind! Fst '(Functor, String)
--   Fst '(Functor, String) :: (* -&gt; *) -&gt; Constraint
--   = Functor
--   </pre>
type family Fst (t :: (k, k')) :: k

-- | Returns the second element of a type-level tuple with the kind <tt>(k,
--   k')</tt> marked by a prefix quote.
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Snd '(Int, String)
--   Snd '(Int, String) :: *
--   = [Char]
--   
--   &gt;&gt;&gt; :kind! Snd '(Int, Monad)
--   Snd '(Int, Monad) :: (* -&gt; *) -&gt; Constraint
--   = Monad
--   </pre>
type family Snd (t :: (k, k')) :: k'

-- | Check that a given type is an element of a type-level list:
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Elem String '[]
--   Elem String '[] :: Bool
--   = 'False
--   
--   &gt;&gt;&gt; :kind! Elem String '[Int, String]
--   Elem String '[Int, String] :: Bool
--   = 'True
--   
--   &gt;&gt;&gt; :kind! Elem String '[Int, Bool]
--   Elem String '[Int, Bool] :: Bool
--   = 'False
--   </pre>
type family Elem (e :: t) (es :: [t]) :: Bool
type family Head (xs :: [a]) :: Maybe a
type family Tail (xs :: [a]) :: Maybe [a]
type family Length (xs :: [a]) :: Nat

-- | Test whether or not a given type contains another:
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Contains (Either Int String) Int
--   Contains (Either Int String) Int :: Bool
--   = 'True
--   
--   &gt;&gt;&gt; :kind! Contains (Either Int String) Bool
--   Contains (Either Int String) Bool :: Bool
--   = 'False
--   
--   &gt;&gt;&gt; :kind! Contains (Either Int String) Either
--   Contains (Either Int String) Either :: Bool
--   = 'True
--   </pre>
type family Contains (f :: k) (a :: k') :: Bool

-- | Extract all occurrences of a given type from another:
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Extract (Either Int String) Int
--   Extract (Either Int String) Int :: [*]
--   = '[Int]
--   
--   &gt;&gt;&gt; :kind! Extract (Either Int String) Bool
--   Extract (Either Int String) Bool :: [*]
--   = '[]
--   
--   &gt;&gt;&gt; :kind! Extract (Either Int String) Either
--   Extract (Either Int String) Either :: [* -&gt; * -&gt; *]
--   = '[Either]
--   </pre>
type family Extract (f :: k) (a :: k') :: [k']
type family FromMaybe (d :: k) (x :: Maybe k) :: k
type family FstMaybe (t :: Maybe (k, k')) :: Maybe k
type family SndMaybe (t :: Maybe (k, k')) :: Maybe k'
type family PrependMaybe (h :: Maybe a) (t :: Maybe [a]) :: Maybe [a]
type family MapMaybe (f :: k -> k') (a :: Maybe k) :: Maybe k'
type family BindMaybe (f :: k -> Maybe k') (a :: Maybe k) :: Maybe k'
type family JoinMaybe (a :: Maybe (Maybe k)) :: Maybe k
type family LiftM2Maybe (f :: k -> k' -> k'') (a :: Maybe k) (b :: Maybe k') :: Maybe k''
type family LiftTimesMaybe (a :: Maybe Nat) (b :: Maybe Nat) :: Maybe Nat
type family LiftTypeEqMaybe (a :: Maybe k) (b :: Maybe k') :: Constraint
type family Concat (xs :: [k]) (ys :: [k]) :: [k]
type Reverse xs = ReverseImplF xs '[]
whenM :: Monad m => m Bool -> m () -> m ()
unlessM :: Monad m => m Bool -> m () -> m ()
ifM :: Monad m => m Bool -> m a -> m a -> m a
guardM :: MonadPlus m => m Bool -> m ()

-- | The <a>&amp;&amp;</a> operator lifted to a monad. If the first
--   argument evaluates to <a>False</a> the second argument will not be
--   evaluated.
(&&^) :: Monad m => m Bool -> m Bool -> m Bool
infixr 3 &&^

-- | The <a>||</a> operator lifted to a monad. If the first argument
--   evaluates to <a>True</a> the second argument will not be evaluated.
(||^) :: Monad m => m Bool -> m Bool -> m Bool
infixr 2 ||^

-- | <a>&amp;&amp;</a> lifted to an Applicative. Unlike <a>&amp;&amp;^</a>
--   the operator is <b>not</b> short-circuiting.
(<&&>) :: Applicative a => a Bool -> a Bool -> a Bool
infixr 3 <&&>

-- | <a>||</a> lifted to an Applicative. Unlike <a>||^</a> the operator is
--   <b>not</b> short-circuiting.
(<||>) :: Applicative a => a Bool -> a Bool -> a Bool
infixr 2 <||>
instance GHC.Generics.Generic (Torch.GraduallyTyped.Prelude.IsChecked a)
instance GHC.Show.Show a => GHC.Show.Show (Torch.GraduallyTyped.Prelude.IsChecked a)
instance GHC.Classes.Ord a => GHC.Classes.Ord (Torch.GraduallyTyped.Prelude.IsChecked a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Torch.GraduallyTyped.Prelude.IsChecked a)
instance Torch.GraduallyTyped.Prelude.KnownList k '[]
instance forall k (x :: k) (xs :: [k]). (Torch.GraduallyTyped.Prelude.KnownElem k x, Torch.GraduallyTyped.Prelude.KnownList k xs) => Torch.GraduallyTyped.Prelude.KnownList k (x : xs)

module Torch.GraduallyTyped.Layout

-- | Data type that represents the memory layout of a tensor.
data LayoutType

-- | The memory layout of the tensor is dense (strided).
Dense :: LayoutType

-- | The memory layout of the tensor is sparse.
Sparse :: LayoutType
type DenseSym0 = 'Dense :: LayoutType
type SparseSym0 = 'Sparse :: LayoutType
data SLayoutType z_azcO
[SDense] :: SLayoutType ('Dense :: LayoutType)
[SSparse] :: SLayoutType ('Sparse :: LayoutType)
class KnownLayoutType (layoutType :: LayoutType)
layoutTypeVal :: KnownLayoutType layoutType => LayoutType

-- | Data type to represent whether or not the tensor's memory layout is
--   checked, that is, known to the compiler.
data Layout (layoutType :: Type)

-- | The tensor's memory layout is unknown to the compiler.
[UncheckedLayout] :: forall layoutType. Layout layoutType

-- | The tensor's memory layout is known to the compiler.
[Layout] :: forall layoutType. layoutType -> Layout layoutType
data SLayout (layout :: Layout LayoutType)
[SUncheckedLayout] :: LayoutType -> SLayout 'UncheckedLayout
[SLayout] :: forall layoutType. SLayoutType layoutType -> SLayout ('Layout layoutType)
class KnownLayout (layout :: Layout LayoutType)
layoutVal :: KnownLayout layout => Layout LayoutType
type family GetLayouts f
instance GHC.Show.Show layoutType => GHC.Show.Show (Torch.GraduallyTyped.Layout.Layout layoutType)
instance GHC.Show.Show (Torch.GraduallyTyped.Layout.SLayoutType layoutType)
instance GHC.Show.Show (Torch.GraduallyTyped.Layout.SLayout layout)
instance Torch.GraduallyTyped.Layout.KnownLayout 'Torch.GraduallyTyped.Layout.UncheckedLayout
instance Torch.GraduallyTyped.Layout.KnownLayoutType layoutType => Torch.GraduallyTyped.Layout.KnownLayout ('Torch.GraduallyTyped.Layout.Layout layoutType)
instance Data.Singletons.Internal.SingI layoutType => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Layout.Layout layoutType)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Layout.Layout Torch.GraduallyTyped.Layout.LayoutType)
instance Torch.GraduallyTyped.Layout.KnownLayoutType 'Torch.GraduallyTyped.Layout.Dense
instance Torch.GraduallyTyped.Layout.KnownLayoutType 'Torch.GraduallyTyped.Layout.Sparse
instance Data.Singletons.Internal.SingKind Torch.GraduallyTyped.Layout.LayoutType
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.Layout.Dense
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.Layout.Sparse
instance Torch.Internal.Class.Castable Torch.GraduallyTyped.Layout.LayoutType Torch.Internal.Type.Layout
instance GHC.Classes.Eq Torch.GraduallyTyped.Layout.LayoutType
instance GHC.Show.Show Torch.GraduallyTyped.Layout.LayoutType

module Torch.GraduallyTyped.Index.Type
data Index (index :: Type)
[UncheckedIndex] :: forall index. Index index
[Index] :: forall index. index -> Index index
data SIndex (index :: Index Nat)
[SUncheckedIndex] :: Natural -> SIndex 'UncheckedIndex
[SIndex] :: forall index. KnownNat index => SIndex ('Index index)
type family IndexF (index :: Index Nat) :: Nat
class KnownIndex (index :: Index Nat)
indexVal :: KnownIndex index => Index Natural
instance GHC.Show.Show index => GHC.Show.Show (Torch.GraduallyTyped.Index.Type.Index index)
instance GHC.Show.Show (Torch.GraduallyTyped.Index.Type.SIndex index)
instance Torch.GraduallyTyped.Index.Type.KnownIndex 'Torch.GraduallyTyped.Index.Type.UncheckedIndex
instance GHC.TypeNats.KnownNat index => Torch.GraduallyTyped.Index.Type.KnownIndex ('Torch.GraduallyTyped.Index.Type.Index index)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Index.Type.Index GHC.Types.Nat)
instance GHC.TypeNats.KnownNat index => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Index.Type.Index index)

module Torch.GraduallyTyped.Device

-- | Data type to represent compute devices.
data DeviceType (deviceId :: Type)

-- | The tensor is stored in the CPU's memory.
[CPU] :: forall deviceId. DeviceType deviceId

-- | The tensor is stored the memory of the GPU with ID <tt>deviceId</tt>.
[CUDA] :: forall deviceId. deviceId -> DeviceType deviceId
data SDeviceType (deviceType :: DeviceType Nat)
[SCPU] :: SDeviceType 'CPU
[SCUDA] :: forall deviceId. KnownNat deviceId => SDeviceType ('CUDA deviceId)
type family CUDAF (deviceType :: DeviceType Nat) :: Nat
class KnownDeviceType (deviceType :: DeviceType Nat)
deviceTypeVal :: KnownDeviceType deviceType => DeviceType Int16

-- | Data type to represent whether or not the compute device is checked,
--   that is, known to the compiler.
data Device (deviceType :: Type)

-- | The compute device is unknown to the compiler.
[UncheckedDevice] :: forall deviceType. Device deviceType

-- | The compute device is known to the compiler.
[Device] :: forall deviceType. deviceType -> Device deviceType
data SDevice (deviceType :: Device (DeviceType Nat))
[SUncheckedDevice] :: DeviceType Int16 -> SDevice 'UncheckedDevice
[SDevice] :: forall deviceType. SDeviceType deviceType -> SDevice ('Device deviceType)
class KnownDevice (device :: Device (DeviceType Nat))
deviceVal :: KnownDevice device => Device (DeviceType Int16)
type family GetDevices f
instance GHC.Show.Show deviceId => GHC.Show.Show (Torch.GraduallyTyped.Device.DeviceType deviceId)
instance GHC.Classes.Ord deviceId => GHC.Classes.Ord (Torch.GraduallyTyped.Device.DeviceType deviceId)
instance GHC.Classes.Eq deviceId => GHC.Classes.Eq (Torch.GraduallyTyped.Device.DeviceType deviceId)
instance GHC.Show.Show deviceType => GHC.Show.Show (Torch.GraduallyTyped.Device.Device deviceType)
instance GHC.Show.Show (Torch.GraduallyTyped.Device.SDeviceType deviceType)
instance GHC.Show.Show (Torch.GraduallyTyped.Device.SDevice device)
instance Torch.GraduallyTyped.Device.KnownDevice 'Torch.GraduallyTyped.Device.UncheckedDevice
instance Torch.GraduallyTyped.Device.KnownDeviceType deviceType => Torch.GraduallyTyped.Device.KnownDevice ('Torch.GraduallyTyped.Device.Device deviceType)
instance Data.Singletons.Internal.SingI deviceType => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Device.Device deviceType)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Device.Device (Torch.GraduallyTyped.Device.DeviceType GHC.Types.Nat))
instance Torch.GraduallyTyped.Device.KnownDeviceType 'Torch.GraduallyTyped.Device.CPU
instance GHC.TypeNats.KnownNat deviceId => Torch.GraduallyTyped.Device.KnownDeviceType ('Torch.GraduallyTyped.Device.CUDA deviceId)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Device.DeviceType GHC.Types.Nat)
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.Device.CPU
instance GHC.TypeNats.KnownNat deviceId => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Device.CUDA deviceId)

module Torch.GraduallyTyped.DType
data DType

-- | Bool
Bool :: DType

-- | Byte
UInt8 :: DType

-- | Char
Int8 :: DType

-- | Short
Int16 :: DType

-- | Int
Int32 :: DType

-- | Long
Int64 :: DType

-- | Half
Half :: DType

-- | Float
Float :: DType

-- | Double
Double :: DType

-- | ComplexHalf
ComplexHalf :: DType

-- | ComplexFloat
ComplexFloat :: DType

-- | ComplexDouble
ComplexDouble :: DType

-- | QInt8
QInt8 :: DType

-- | QUInt8
QUInt8 :: DType

-- | QInt32
QInt32 :: DType

-- | BFloat16
BFloat16 :: DType
type BoolSym0 = 'Bool :: DType
type UInt8Sym0 = 'UInt8 :: DType
type Int8Sym0 = 'Int8 :: DType
type Int16Sym0 = 'Int16 :: DType
type Int32Sym0 = 'Int32 :: DType
type Int64Sym0 = 'Int64 :: DType
type HalfSym0 = 'Half :: DType
type FloatSym0 = 'Float :: DType
type DoubleSym0 = 'Double :: DType
type ComplexHalfSym0 = 'ComplexHalf :: DType
type ComplexFloatSym0 = 'ComplexFloat :: DType
type ComplexDoubleSym0 = 'ComplexDouble :: DType
type QInt8Sym0 = 'QInt8 :: DType
type QUInt8Sym0 = 'QUInt8 :: DType
type QInt32Sym0 = 'QInt32 :: DType
type BFloat16Sym0 = 'BFloat16 :: DType
data SDType z_aCH8
[SBool] :: SDType ('Bool :: DType)
[SUInt8] :: SDType ('UInt8 :: DType)
[SInt8] :: SDType ('Int8 :: DType)
[SInt16] :: SDType ('Int16 :: DType)
[SInt32] :: SDType ('Int32 :: DType)
[SInt64] :: SDType ('Int64 :: DType)
[SHalf] :: SDType ('Half :: DType)
[SFloat] :: SDType ('Float :: DType)
[SDouble] :: SDType ('Double :: DType)
[SComplexHalf] :: SDType ('ComplexHalf :: DType)
[SComplexFloat] :: SDType ('ComplexFloat :: DType)
[SComplexDouble] :: SDType ('ComplexDouble :: DType)
[SQInt8] :: SDType ('QInt8 :: DType)
[SQUInt8] :: SDType ('QUInt8 :: DType)
[SQInt32] :: SDType ('QInt32 :: DType)
[SBFloat16] :: SDType ('BFloat16 :: DType)
class KnownDType (dType :: DType)
dTypeVal :: KnownDType dType => DType

-- | Data type to represent whether or not the tensor data type is checked,
--   that is, known to the compiler.
data DataType (dType :: Type)

-- | The tensor data type is unknown to the compiler.
[UncheckedDataType] :: forall dType. DataType dType

-- | The tensor data type is known to the compiler.
[DataType] :: forall dType. dType -> DataType dType
data SDataType (dataType :: DataType DType)
[SUncheckedDataType] :: DType -> SDataType 'UncheckedDataType
[SDataType] :: forall dType. SDType dType -> SDataType ('DataType dType)
class KnownDataType (dataType :: DataType DType)
dataTypeVal :: KnownDataType dataType => DataType DType
type family GetDataTypes f
instance GHC.Show.Show dType => GHC.Show.Show (Torch.GraduallyTyped.DType.DataType dType)
instance GHC.Show.Show (Torch.GraduallyTyped.DType.SDType dType)
instance GHC.Show.Show (Torch.GraduallyTyped.DType.SDataType dataType)
instance Torch.GraduallyTyped.DType.KnownDataType 'Torch.GraduallyTyped.DType.UncheckedDataType
instance Torch.GraduallyTyped.DType.KnownDType dType => Torch.GraduallyTyped.DType.KnownDataType ('Torch.GraduallyTyped.DType.DataType dType)
instance Data.Singletons.Internal.SingI dType => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.DType.DataType dType)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.DType.DataType Torch.GraduallyTyped.DType.DType)
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Bool
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.UInt8
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int8
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int16
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int32
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int64
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Half
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Float
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Double
instance Data.Singletons.Internal.SingKind Torch.GraduallyTyped.DType.DType
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Bool
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.UInt8
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Int8
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Int16
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Int32
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Int64
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Half
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Float
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.Double
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.ComplexHalf
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.ComplexFloat
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.ComplexDouble
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.QInt8
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.QUInt8
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.QInt32
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.DType.BFloat16
instance Torch.Internal.Class.Castable Torch.GraduallyTyped.DType.DType Torch.Internal.Type.ScalarType
instance GHC.Read.Read Torch.GraduallyTyped.DType.DType
instance GHC.Show.Show Torch.GraduallyTyped.DType.DType
instance GHC.Classes.Eq Torch.GraduallyTyped.DType.DType

module Torch.GraduallyTyped.RequiresGradient

-- | Data type to represent whether or not the tensor requires gradient
--   computations.
data RequiresGradient

-- | The tensor requires gradient computations.
WithGradient :: RequiresGradient

-- | Gradient computations for this tensor are disabled.
WithoutGradient :: RequiresGradient
type WithGradientSym0 = 'WithGradient :: RequiresGradient
type WithoutGradientSym0 = 'WithoutGradient :: RequiresGradient
data SRequiresGradient z_aFeE
[SWithGradient] :: SRequiresGradient ('WithGradient :: RequiresGradient)
[SWithoutGradient] :: SRequiresGradient ('WithoutGradient :: RequiresGradient)
class KnownRequiresGradient (requiresGradient :: RequiresGradient)
requiresGradientVal :: KnownRequiresGradient requiresGradient => RequiresGradient

-- | Data type to represent whether or not it is known by the compiler if
--   the tensor requires gradient computations.
data Gradient (requiresGradient :: Type)

-- | Whether or not the tensor requires gradient computations is unknown to
--   the compiler.
[UncheckedGradient] :: forall requiresGradient. Gradient requiresGradient

-- | Whether or not the tensor requires gradient computations is known to
--   the compiler.
[Gradient] :: forall requiresGradient. requiresGradient -> Gradient requiresGradient
data SGradient (gradient :: Gradient RequiresGradient)
[SUncheckedGradient] :: RequiresGradient -> SGradient 'UncheckedGradient
[SGradient] :: forall requiresGradient. SRequiresGradient requiresGradient -> SGradient ('Gradient requiresGradient)
class KnownGradient (gradient :: Gradient RequiresGradient)
gradientVal :: KnownGradient gradient => Gradient RequiresGradient
type family GetGradients f
instance GHC.Show.Show requiresGradient => GHC.Show.Show (Torch.GraduallyTyped.RequiresGradient.Gradient requiresGradient)
instance GHC.Show.Show (Torch.GraduallyTyped.RequiresGradient.SRequiresGradient requiresGradient)
instance GHC.Show.Show (Torch.GraduallyTyped.RequiresGradient.SGradient requiresGradient)
instance Torch.GraduallyTyped.RequiresGradient.KnownGradient 'Torch.GraduallyTyped.RequiresGradient.UncheckedGradient
instance Torch.GraduallyTyped.RequiresGradient.KnownRequiresGradient requiresGradient => Torch.GraduallyTyped.RequiresGradient.KnownGradient ('Torch.GraduallyTyped.RequiresGradient.Gradient requiresGradient)
instance Data.Singletons.Internal.SingI requiresGradient => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.RequiresGradient.Gradient requiresGradient)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.RequiresGradient.Gradient Torch.GraduallyTyped.RequiresGradient.RequiresGradient)
instance Torch.GraduallyTyped.RequiresGradient.KnownRequiresGradient 'Torch.GraduallyTyped.RequiresGradient.WithGradient
instance Torch.GraduallyTyped.RequiresGradient.KnownRequiresGradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient
instance Data.Singletons.Internal.SingKind Torch.GraduallyTyped.RequiresGradient.RequiresGradient
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.RequiresGradient.WithGradient
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient
instance GHC.Classes.Eq Torch.GraduallyTyped.RequiresGradient.RequiresGradient
instance GHC.Show.Show Torch.GraduallyTyped.RequiresGradient.RequiresGradient

module Torch.GraduallyTyped.Internal.TensorOptions
newtype TensorOptions
TensorOptions :: ForeignPtr TensorOptions -> TensorOptions
tensorOptions :: RequiresGradient -> LayoutType -> DeviceType Int16 -> DType -> TensorOptions
instance Torch.Internal.Class.Castable Torch.GraduallyTyped.Internal.TensorOptions.TensorOptions (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorOptions)

module Torch.GraduallyTyped.Scalar
class (Castable a (ForeignPtr Scalar)) => Scalar a
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Types.Float
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Types.Double
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Types.Int
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Integer.Type.Integer
instance Torch.Internal.Class.Castable GHC.Types.Float (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Double (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Int (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Integer.Type.Integer (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)

module Torch.GraduallyTyped.Shape.Type
data Size (size :: Type)
[UncheckedSize] :: forall size. Size size
[Size] :: forall size. size -> Size size
data SSize (size :: Size Nat)
[SUncheckedSize] :: Integer -> SSize 'UncheckedSize
[SSize] :: forall size. KnownNat size => SSize ('Size size)
type family SizeF (size :: Size Nat) :: Nat
class KnownSize (size :: Size Nat)
sizeVal :: KnownSize size => Size Integer
data Name (name :: Type)
[UncheckedName] :: forall name. Name name
[Name] :: forall name. name -> Name name
data SName (name :: Name Symbol)
[SUncheckedName] :: String -> SName 'UncheckedName
[SName] :: forall name. KnownSymbol name => SName ('Name name)
type family NameF (name :: Name Symbol) :: Symbol
class KnownName (name :: Name Symbol)
nameVal :: KnownName name => Name String
data Dim (name :: Type) (size :: Type)
[Dim] :: forall name size. {dimName :: name, dimSize :: size} -> Dim name size
data SDim (dim :: Dim (Name Symbol) (Size Nat))
[SDim] :: forall name size. {sDimName :: SName name, sDimSize :: SSize size} -> SDim ('Dim name size)
pattern (:&:) :: forall (name :: Name Symbol) (size :: Size Nat). SName name -> SSize size -> SDim ('Dim name size)
infix 9 :&:
class KnownDim (dim :: Dim (Name Symbol) (Size Nat))
dimVal :: KnownDim dim => Dim (Name String) (Size Integer)

-- | Data type to select dimensions by name or by index.
data By (name :: Type) (index :: Type)

-- | Select a dimension by name.
[ByName] :: forall name index. name -> By name index

-- | Select a dimension by index. Counting starts at zero for the first
--   dimension.
[ByIndex] :: forall name index. index -> By name index
data SBy (by :: By Symbol Nat)
[SByName] :: forall name. KnownSymbol name => SBy ('ByName name)
[SByIndex] :: forall index. KnownNat index => SBy ('ByIndex index)
type family ByNameF (by :: By Symbol Nat) :: Symbol
type family ByIndexF (by :: By Symbol Nat) :: Nat
class KnownBy (by :: By Symbol Nat)
byVal :: KnownBy by => By String Integer
data SelectDim (by :: Type)

-- | Unknown method of dimension selection.
[UncheckedSelectDim] :: forall by. SelectDim by

-- | Known method of dimension selection, that is, either by name or by
--   index.
[SelectDim] :: forall by. by -> SelectDim by
data SSelectDim (selectDim :: SelectDim (By Symbol Nat))
[SUncheckedSelectDim] :: By String Integer -> SSelectDim 'UncheckedSelectDim
[SSelectDim] :: forall by. SBy by -> SSelectDim ('SelectDim by)
class KnownSelectDim (selectDim :: SelectDim (By Symbol Nat))
selectDimVal :: KnownSelectDim selectDim => SelectDim (By String Integer)
data SelectDims (selectDims :: Type)
[UncheckedSelectDims] :: forall selectDims. SelectDims selectDims
[SelectDims] :: forall selectDims. selectDims -> SelectDims selectDims
data SSelectDims (selectDims :: SelectDims [By Symbol Nat])
[SUncheckedSelectDims] :: [By String Integer] -> SSelectDims 'UncheckedSelectDims
[SSelectDims] :: forall bys. SList bys -> SSelectDims ('SelectDims bys)
class KnownSelectDims (selectDims :: SelectDims [By Symbol Nat])
selectDimsVal :: KnownSelectDims selectDims => SelectDims [By String Integer]

-- | Data type to represent tensor shapes, that is, lists of dimensions.
data Shape (dims :: Type)

-- | The shape is fully unchecked. Neither the number of the dimensions nor
--   any dimension properties are known to the compiler.
[UncheckedShape] :: forall dims. Shape dims

-- | The shape is partially known to the compiler. The list of dimensions
--   has a known length, but may contain <tt>UncheckedDim</tt>, that is,
--   unknown dimensions.
[Shape] :: forall dims. dims -> Shape dims
data SShape (shape :: Shape [Dim (Name Symbol) (Size Nat)])
[SUncheckedShape] :: [Dim String Integer] -> SShape 'UncheckedShape
[SShape] :: forall dims. SList dims -> SShape ('Shape dims)
pattern (:|:) :: forall (dim :: Dim (Name Symbol) (Size Nat)) (dims :: [Dim (Name Symbol) (Size Nat)]). SDim dim -> SList dims -> SList (dim : dims)
infixr 8 :|:
class KnownShape (shape :: Shape [Dim (Name Symbol) (Size Nat)])
shapeVal :: KnownShape shape => Shape [Dim (Name String) (Size Integer)]
type family GetShapes f
instance GHC.Show.Show size => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Size size)
instance GHC.Show.Show name => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Name name)
instance (GHC.Show.Show name, GHC.Show.Show size) => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.Classes.Ord name, GHC.Classes.Ord size) => GHC.Classes.Ord (Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.Classes.Eq name, GHC.Classes.Eq size) => GHC.Classes.Eq (Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.Classes.Ord name, GHC.Classes.Ord index) => GHC.Classes.Ord (Torch.GraduallyTyped.Shape.Type.By name index)
instance (GHC.Classes.Eq name, GHC.Classes.Eq index) => GHC.Classes.Eq (Torch.GraduallyTyped.Shape.Type.By name index)
instance (GHC.Show.Show name, GHC.Show.Show index) => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.By name index)
instance GHC.Show.Show dims => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Shape dims)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SSize size)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SName name)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SDim dim)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SBy by)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SSelectDim selectDim)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SSelectDims selectDims)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SShape shape)
instance Torch.GraduallyTyped.Shape.Type.KnownShape 'Torch.GraduallyTyped.Shape.Type.UncheckedShape
instance Torch.GraduallyTyped.Shape.Type.KnownShape ('Torch.GraduallyTyped.Shape.Type.Shape '[])
instance (Torch.GraduallyTyped.Shape.Type.KnownShape ('Torch.GraduallyTyped.Shape.Type.Shape dims), Torch.GraduallyTyped.Shape.Type.KnownDim dim) => Torch.GraduallyTyped.Shape.Type.KnownShape ('Torch.GraduallyTyped.Shape.Type.Shape (dim : dims))
instance Data.Singletons.Internal.SingI dims => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.Shape dims)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.Shape [Torch.GraduallyTyped.Shape.Type.Dim (Torch.GraduallyTyped.Shape.Type.Name GHC.Types.Symbol) (Torch.GraduallyTyped.Shape.Type.Size GHC.Types.Nat)])
instance Torch.GraduallyTyped.Shape.Type.KnownSelectDims 'Torch.GraduallyTyped.Shape.Type.UncheckedSelectDims
instance Torch.GraduallyTyped.Shape.Type.KnownSelectDims ('Torch.GraduallyTyped.Shape.Type.SelectDims '[])
instance (Torch.GraduallyTyped.Shape.Type.KnownBy by, Torch.GraduallyTyped.Shape.Type.KnownSelectDims ('Torch.GraduallyTyped.Shape.Type.SelectDims bys)) => Torch.GraduallyTyped.Shape.Type.KnownSelectDims ('Torch.GraduallyTyped.Shape.Type.SelectDims (by : bys))
instance Data.Singletons.Internal.SingI bys => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.SelectDims bys)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.SelectDims [Torch.GraduallyTyped.Shape.Type.By GHC.Types.Symbol GHC.Types.Nat])
instance Torch.GraduallyTyped.Shape.Type.KnownSelectDim 'Torch.GraduallyTyped.Shape.Type.UncheckedSelectDim
instance Torch.GraduallyTyped.Shape.Type.KnownBy by => Torch.GraduallyTyped.Shape.Type.KnownSelectDim ('Torch.GraduallyTyped.Shape.Type.SelectDim by)
instance Data.Singletons.Internal.SingI by => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.SelectDim by)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.SelectDim (Torch.GraduallyTyped.Shape.Type.By GHC.Types.Symbol GHC.Types.Nat))
instance GHC.TypeLits.KnownSymbol name => Torch.GraduallyTyped.Shape.Type.KnownBy ('Torch.GraduallyTyped.Shape.Type.ByName name)
instance GHC.TypeNats.KnownNat index => Torch.GraduallyTyped.Shape.Type.KnownBy ('Torch.GraduallyTyped.Shape.Type.ByIndex index)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.By GHC.Types.Symbol GHC.Types.Nat)
instance GHC.TypeLits.KnownSymbol name => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.ByName name)
instance GHC.TypeNats.KnownNat index => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.ByIndex index)
instance (Torch.GraduallyTyped.Shape.Type.KnownName name, Torch.GraduallyTyped.Shape.Type.KnownSize size) => Torch.GraduallyTyped.Shape.Type.KnownDim ('Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.TypeLits.KnownSymbol name, GHC.TypeNats.KnownNat size) => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name name) ('Torch.GraduallyTyped.Shape.Type.Size size))
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.Dim (Torch.GraduallyTyped.Shape.Type.Name GHC.Types.Symbol) (Torch.GraduallyTyped.Shape.Type.Size GHC.Types.Nat))
instance Data.Bifunctor.Bifunctor Torch.GraduallyTyped.Shape.Type.Dim
instance Torch.GraduallyTyped.Shape.Type.KnownName 'Torch.GraduallyTyped.Shape.Type.UncheckedName
instance GHC.TypeLits.KnownSymbol name => Torch.GraduallyTyped.Shape.Type.KnownName ('Torch.GraduallyTyped.Shape.Type.Name name)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.Name GHC.Types.Symbol)
instance GHC.TypeLits.KnownSymbol name => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.Name name)
instance Torch.GraduallyTyped.Shape.Type.KnownSize 'Torch.GraduallyTyped.Shape.Type.UncheckedSize
instance GHC.TypeNats.KnownNat size => Torch.GraduallyTyped.Shape.Type.KnownSize ('Torch.GraduallyTyped.Shape.Type.Size size)
instance Data.Singletons.Internal.SingKind (Torch.GraduallyTyped.Shape.Type.Size GHC.Types.Nat)
instance GHC.TypeNats.KnownNat size => Data.Singletons.Internal.SingI ('Torch.GraduallyTyped.Shape.Type.Size size)
instance Torch.Internal.Class.Castable GHC.Base.String (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Dimname)
instance Torch.Internal.Class.Castable [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Dimname] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.DimnameList)
instance Torch.Internal.Class.Castable [GHC.Base.String] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.DimnameList)
instance Torch.Internal.Class.Castable [GHC.Integer.Type.Integer] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)

module Torch.GraduallyTyped.Index.Class
type family IndexOutOfBound (idx :: Nat) (dim :: Dim (Name Symbol) (Size Nat))
type family InRangeImplF (idx :: Index Nat) (dim :: Dim (Name Symbol) (Size Nat)) :: Bool
type family InRangeCheckF (idx :: Index Nat) (dim :: Dim (Name Symbol) (Size Nat)) (ok :: Bool) :: Constraint
type InRangeF idx dim = InRangeCheckF idx dim (InRangeImplF idx dim)

module Torch.GraduallyTyped.Index

module Torch.GraduallyTyped.Tensor.MathOperations.Other

module Torch.GraduallyTyped.Tensor.MathOperations.Spectral

module Torch.GraduallyTyped.Unify

-- | <tt>a <a>+</a> b</tt> unifies <tt>a</tt> and <tt>b</tt>. Think of it
--   as a kind-level monoid.
type family (<+>) (a :: k) (b :: k) :: k
infixr 8 <+>

-- | Desugared kind unification.
--   
--   TODO: add data type unification of scalar (Haskell) data types and
--   those of kind <tt>DataType</tt>. Perhaps convert the scalar (Haskell)
--   data type first to a <tt>DataType</tt> so that the kinds are aligned.
type family Unify k (a :: k) (b :: k) :: k
type UnifyRequiresGradientMessage (requiresGradient :: RequiresGradient) (requiresGradient' :: RequiresGradient) = "The supplied tensors must all either require or disable gradient calculation," % "but different gradient settings were found:" % "" % "    " <> requiresGradient <> " and " <> requiresGradient' <> "." % ""
type UnifyLayoutErrorMessage (layoutType :: k) (layoutType' :: k') = "The supplied tensors must have the same memory layout," % "but different layouts were found:" % "" % "    " <> layoutType <> " and " <> layoutType' <> "." % ""
type UnifyDeviceErrorMessage (deviceType :: k) (deviceType' :: k') = "The supplied tensors must be on the same device, " % "but different device locations were found:" % "" % "    " <> deviceType <> " and " <> deviceType' <> "." % ""
type UnifyDataTypeErrorMessage (dType :: k) (dType' :: k') = "The supplied tensors must have the same data type, " % "but different data types were found:" % "" % "    " <> dType <> " and " <> dType' <> "." % ""
type UnifyDimsErrorMessage (dims :: k) (dims' :: k') = "The supplied tensors must have shapes with identical number of dimensions," % "but dimension lists of different lengths were found." % "Here are the tails of both dimension lists:" % "" % "    " <> dims <> " and " <> dims' <> "." % "" % "Try extending, (un-)squeezing, or broadcasting the tensor(s)."
type UnifyNameErrorMessage (name :: k) (name' :: k') = "The supplied dimensions must be the same," % "but dimensions with different names were found:" % "" % "    " <> name <> " and " <> name' <> "." % "" % "Check spelling and whether or not this is really what you want." % "If you are certain, consider dropping or changing the names."
type UnifySizeErrorMessage (size :: k) (size' :: k') = "The supplied dimensions must be the same," % "but dimensions with different sizes were found:" % "" % "    " <> size <> " and " <> size' <> "." % "" % "Check whether or not this is really what you want." % "If you are certain, adjust the sizes such that they match."
type UnifyRightAssociativeL k a b c = Unify k (Unify k a b) c ~ Unify k a (Unify k b c)
type UnifyIdempotenceL2 k a b = Unify k a (Unify k a b) ~ Unify k a b
type UnifyIdempotenceL2C k a b = Unify k a (Unify k b a) ~ Unify k a b
type UnifyIdempotenceL3 k a b c = Unify k a (Unify k b (Unify k a c)) ~ Unify k a (Unify k b c)
type UnifyIdempotenceL3C k a b c = Unify k a (Unify k b (Unify k c a)) ~ Unify k a (Unify k b c)
type UnifyIdempotenceL4 k a b c d = Unify k a (Unify k b (Unify k c (Unify k a d))) ~ Unify k a (Unify k b (Unify k c d))
type UnifyIdempotenceL4C k a b c d = Unify k a (Unify k b (Unify k c (Unify k d a))) ~ Unify k a (Unify k b (Unify k c d))
type UnifyIdempotenceL5 k a b c d e = Unify k a (Unify k b (Unify k c (Unify k d (Unify k a e)))) ~ Unify k a (Unify k b (Unify k c (Unify k d e)))
type UnifyIdempotenceL5C k a b c d e = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e a)))) ~ Unify k a (Unify k b (Unify k c (Unify k d e)))
type UnifyIdempotenceL6 k a b c d e f = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k a f))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e f))))
type UnifyIdempotenceL6C k a b c d e f = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f a))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e f))))
type UnifyIdempotenceL7 k a b c d e f g = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k a g)))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f g)))))
type UnifyIdempotenceL7C k a b c d e f g = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g a)))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f g)))))
type UnifyIdempotenceL8 k a b c d e f g h = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k a h))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g h))))))
type UnifyIdempotenceL8C k a b c d e f g h = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h a))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g h))))))
type UnifyIdempotenceL9 k a b c d e f g h i = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h (Unify k a i)))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h i)))))))
type UnifyIdempotenceL9C k a b c d e f g h i = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h (Unify k i a)))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h i)))))))
type family (<|>) (a :: k) (b :: k) :: k
infixr 8 <|>
type family Or k (a :: k) (b :: k) :: k
type OrRightAssociativeL k a b c = Or k (Or k a b) c ~ Or k a (Or k b c)
type OrIdempotenceL2 k a b = Or k a (Or k a b) ~ Or k a b
type OrIdempotenceL2C k a b = Or k a (Or k b a) ~ Or k a b
type OrIdempotenceL3 k a b c = Or k a (Or k b (Or k a c)) ~ Or k a (Or k b c)
type OrIdempotenceL3C k a b c = Or k a (Or k b (Or k c a)) ~ Or k a (Or k b c)
type OrIdempotenceL4 k a b c d = Or k a (Or k b (Or k c (Or k a d))) ~ Or k a (Or k b (Or k c d))
type OrIdempotenceL4C k a b c d = Or k a (Or k b (Or k c (Or k d a))) ~ Or k a (Or k b (Or k c d))
type OrIdempotenceL5 k a b c d e = Or k a (Or k b (Or k c (Or k d (Or k a e)))) ~ Or k a (Or k b (Or k c (Or k d e)))
type OrIdempotenceL5C k a b c d e = Or k a (Or k b (Or k c (Or k d (Or k e a)))) ~ Or k a (Or k b (Or k c (Or k d e)))
type OrIdempotenceL6 k a b c d e f = Or k a (Or k b (Or k c (Or k d (Or k e (Or k a f))))) ~ Or k a (Or k b (Or k c (Or k d (Or k e f))))
type OrIdempotenceL6C k a b c d e f = Or k a (Or k b (Or k c (Or k d (Or k e (Or k f a))))) ~ Or k a (Or k b (Or k c (Or k d (Or k e f))))

module Torch.GraduallyTyped.Shape.Class
type family AddSizeF (size :: Size Nat) (size' :: Size Nat) :: Size Nat
type family AddDimF (dim :: Dim (Name Symbol) (Size Nat)) (dim' :: Dim (Name Symbol) (Size Nat)) :: Dim (Name Symbol) (Size Nat)
type family BroadcastSizeF (size :: Size Nat) (size' :: Size Nat) :: Maybe (Size Nat)
type family BroadcastDimF (dim :: Dim (Name Symbol) (Size Nat)) (dim' :: Dim (Name Symbol) (Size Nat)) :: Maybe (Dim (Name Symbol) (Size Nat))
type family NumelDimF (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe Nat
type family BroadcastDimsCheckF (dims :: [Dim (Name Symbol) (Size Nat)]) (dims' :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family BroadcastDimsImplF (reversedDims :: [Dim (Name Symbol) (Size Nat)]) (reversedDims' :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type BroadcastDimsF dims dims' = BroadcastDimsCheckF dims dims' (BroadcastDimsImplF (Reverse dims) (Reverse dims'))
type family BroadcastShapesF (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (shape' :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family NumelDimsF (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe Nat
type family NumelF (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Maybe Nat
type family GetDimAndIndexByNameF (index :: Nat) (result :: (Maybe (Dim (Name Symbol) (Size Nat)), Maybe Nat)) (name :: Symbol) (dims :: [Dim (Name Symbol) (Size Nat)]) :: (Maybe (Dim (Name Symbol) (Size Nat)), Maybe Nat)
type family GetDimByNameF (name :: Symbol) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe (Dim (Name Symbol) (Size Nat))
type family GetIndexByNameF (name :: Symbol) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe Nat
type family GetDimByIndexF (index :: Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe (Dim (Name Symbol) (Size Nat))
type family GetDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe (Dim (Name Symbol) (Size Nat))
type GetDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot return the first dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family GetDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe (Dim (Name Symbol) (Size Nat))) :: Dim (Name Symbol) (Size Nat)
type family GetDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Dim (Name Symbol) (Size Nat)
type family (!) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (_k :: k) :: Dim (Name Symbol) (Size Nat)

-- | Get dimension by index or by name
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @8 :|: SUncheckedName "feature" :&amp;: SSize @2 :|: SNil
--   
--   &gt;&gt;&gt; dim = sGetDim (SSelectDim $ SByName @"batch") shape
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim ('Name "batch") ('Size 8)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Checked "batch", dimSize = Checked 8}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dim = sGetDim (SSelectDim $ SByName @"feature") shape
--   
--   &gt;&gt;&gt; :type dim
--   dim
--     :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName 'UncheckedSize))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Unchecked "feature", dimSize = Checked 2}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dim = sGetDim (SSelectDim $ SByName @"sequence") shape
--   
--   &gt;&gt;&gt; :type dim
--   dim
--     :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName 'UncheckedSize))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   *** Exception: GetDimError {gdeBy = ByName "sequence"}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dim = sGetDim (SSelectDim $ SByIndex @0) shape
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim ('Name "batch") ('Size 8)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Checked "batch", dimSize = Checked 8}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; :type sGetDim (SSelectDim $ SByIndex @2) shape
--   sGetDim (SSelectDim $ SByIndex @2) shape
--     :: MonadThrow m =&gt; m (SDim (TypeError ...))
--   </pre>
sGetDim :: forall selectDim shape dim m. (dim ~ GetDimF selectDim shape, MonadThrow m) => SSelectDim selectDim -> SShape shape -> m (SDim dim)
data GetDimError
GetDimError :: By String Integer -> GetDimError
[gdeBy] :: GetDimError -> By String Integer
GetDimErrorWithDims :: By String Integer -> [Dim String Integer] -> GetDimError
[gdewdBy] :: GetDimError -> By String Integer
[gdewdDims] :: GetDimError -> [Dim String Integer]
getDim :: forall selectDim shape dim. dim ~ GetDimF selectDim shape => SSelectDim selectDim -> SShape shape -> SDim dim
type family ReplaceDimByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimNameByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (name :: Name Symbol) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimNameImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (name' :: Name Symbol) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimSizeByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (size' :: Size Nat) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimSizeImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (size' :: Size Nat) :: Maybe [Dim (Name Symbol) (Size Nat)]
type ReplaceDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) = "Cannot replace the first dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'" % "" % "with" % "" % "    '" <> dim <> "'." % ""
type family ReplaceDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Shape [Dim (Name Symbol) (Size Nat)]
type family InsertDimByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family InsertDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type InsertDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) = "Cannot insert the dimension" % "" % "    '" <> dim <> "'" % "" % "before the first dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family InsertDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family InsertDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Shape [Dim (Name Symbol) (Size Nat)]
type family RemoveDimByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family RemoveDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type RemoveDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot remove the dimension by" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family RemoveDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family RemoveDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
data UnifyNameError
UnifyNameError :: String -> String -> UnifyNameError
[uneExpect] :: UnifyNameError -> String
[uneActual] :: UnifyNameError -> String
sUnifyName :: forall m name name'. MonadThrow m => SName name -> SName name' -> m (SName (name <+> name'))
data UnifySizeError
UnifySizeError :: Integer -> Integer -> UnifySizeError
[useExpect] :: UnifySizeError -> Integer
[useActual] :: UnifySizeError -> Integer
sUnifySize :: forall m size size'. MonadThrow m => SSize size -> SSize size' -> m (SSize (size <+> size'))

-- | Unify two dimensions.
--   
--   <pre>
--   &gt;&gt;&gt; dimA = SName @"*" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dimB = SName @"batch" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dim = sUnifyDim dimA dimB
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim ('Name "batch") ('Size 0)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Checked "batch", dimSize = Checked 0}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dimC = SName @"feature" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; :type sUnifyDim dimB dimC
--   sUnifyDim dimB dimC
--     :: MonadThrow m =&gt; m (SDim ('Dim (TypeError ...) ('Size 0)))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dimD = SUncheckedName "batch" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dim = sUnifyDim dimA dimD
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName ('Size 0)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Unchecked "batch", dimSize = Checked 0}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dimE = SUncheckedName "feature" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dim = sUnifyDim dimB dimE
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName ('Size 0)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   *** Exception: UnifyNameError {uneExpect = "batch", uneActual = "feature"}
--   </pre>
sUnifyDim :: forall m dim dim'. MonadThrow m => SDim dim -> SDim dim' -> m (SDim (dim <+> dim'))
instance GHC.Show.Show Torch.GraduallyTyped.Shape.Class.GetDimError
instance GHC.Show.Show Torch.GraduallyTyped.Shape.Class.UnifyNameError
instance GHC.Show.Show Torch.GraduallyTyped.Shape.Class.UnifySizeError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Shape.Class.UnifySizeError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Shape.Class.UnifyNameError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Shape.Class.GetDimError

module Torch.GraduallyTyped.Tensor.Type

-- | A gradually typed tensor.
--   
--   <pre>
--                             Compute device, e.g. `'Device <a>CPU</a>
--                            
--                                            List of dimensions, e.g. `'Shape '[ 'Dim 'UncheckedName ('Size 8), 'Dim 'UncheckedName ('Size 1) ]`
--                                           
--   Tensor gradient layout device dataType shape
--                                  
--                                   Data type, e.g. `'DataType <a>Float</a>
--                    
--                     Memory layout, e.g. `'Layout <a>Dense</a>
--             
--              Whether or not the tensor requires a gradient, e.g. `'Gradient <a>WithGradient</a> for one that does
--   </pre>
newtype Tensor (gradient :: Gradient RequiresGradient) (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (shape :: Shape [Dim (Name Symbol) (Size Nat)])

-- | Unsafe constructor for tensors. Do not call this constructor directly,
--   use smart constructors like <tt>ones</tt> or <tt>randn</tt> instead.
[UnsafeTensor] :: forall gradient layout device dataType shape. ForeignPtr Tensor -> Tensor gradient layout device dataType shape

-- | Alias for an untyped tensor without gradients.
type UncheckedTensor = Tensor 'UncheckedGradient 'UncheckedLayout 'UncheckedDevice 'UncheckedDataType 'UncheckedShape

-- | Alias for an untyped tensor with gradients.
type UncheckedParameter = Tensor ('Gradient 'WithGradient) 'UncheckedLayout 'UncheckedDevice 'UncheckedDataType 'UncheckedShape

-- | Alias for a tensor on CPU memory without gradients.
type CPUTensor = Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU)

-- | Alias for a tensor on CPU memory with gradients.
type CPUParameter = Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU)

-- | Alias for a sparse tensor on CPU memory without gradients.
type SparseCPUTensor = Tensor ('Gradient 'WithoutGradient) ('Layout 'Sparse) ('Device 'CPU)

-- | Alias for a sparse tensor on CPU memory with gradients.
type SparseCPUParameter = Tensor ('Gradient 'WithGradient) ('Layout 'Sparse) ('Device 'CPU)

-- | Alias for a tensor on CUDA memory without gradients.
type CUDATensor deviceId = Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device ('CUDA deviceId))

-- | Alias for a tensor on CUDA memory with gradients.
type CUDAParameter deviceId = Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CUDA deviceId))

-- | Alias for a sparse tensor on CUDA memory without gradients.
type SparseCUDATensor deviceId = Tensor ('Gradient 'WithoutGradient) ('Layout 'Sparse) ('Device ('CUDA deviceId))

-- | Alias for a sparse tensor on CUDA memory with gradients.
type SparseCUDAParameter deviceId = Tensor ('Gradient 'WithGradient) ('Layout 'Sparse) ('Device ('CUDA deviceId))

-- | Takes a tensor that may or may not require gradient computations and
--   returns a copy that does not require them.
withoutGradient :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> IO (Tensor ('Gradient 'WithoutGradient) layout device dataType shape)

-- | Takes a tensor that does not requires gradient computations and
--   returns a copy that requires them.
withGradient :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> IO (Tensor ('Gradient 'WithGradient) layout device dataType shape)
class SGetGradient (gradient :: Gradient RequiresGradient)

-- | Returns the gradually typed information for whether or not gradient
--   computations for the tensor are turned on.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' gradient = sOnes gradient (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = sOnes' $ SGradient SWithGradient
--   
--   &gt;&gt;&gt; sGradient t
--   SGradient SWithGradient
--   
--   &gt;&gt;&gt; t = sOnes' $ SUncheckedGradient WithoutGradient
--   
--   &gt;&gt;&gt; sGradient t
--   SUncheckedGradient WithoutGradient
--   </pre>
sGradient :: forall layout device dataType shape. SGetGradient gradient => Tensor gradient layout device dataType shape -> SGradient gradient

-- | Returns the untyped memory layout of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' gradient = sOnes gradient (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = sOnes' $ SGradient SWithGradient
--   
--   &gt;&gt;&gt; gradient t
--   WithGradient
--   
--   &gt;&gt;&gt; t = sOnes' $ SUncheckedGradient WithoutGradient
--   
--   &gt;&gt;&gt; gradient t
--   WithoutGradient
--   </pre>
gradient :: forall layout device dataType shape. SGetGradient gradient => Tensor gradient layout device dataType shape -> RequiresGradient
data GradientError
GradientError :: RequiresGradient -> RequiresGradient -> GradientError
[geExpected] :: GradientError -> RequiresGradient
[geActual] :: GradientError -> RequiresGradient

-- | Checks whether or not the input tensor has the memory layout
--   <tt>layout</tt> and returns a statically annotated copy of it wrapped
--   in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the memory
--   layout <tt>layout</tt>. If it does not have it, then the result will
--   be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t =
--   sOnes (SGradient SWithGradient) (SUncheckedLayout Dense) (SDevice
--   SCPU) (SDataType SFloat) (SShape $ SName <tt>"batch" :&amp;: SSize
--   </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|: SNil)
--   &gt;&gt;&gt; t' &lt;- sCheckedLayout (SLayout SDense) t &gt;&gt;&gt;
--   :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense)
--   ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name "batch")
--   ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt; t' &lt;-
--   sCheckedLayout (SLayout SSparse) t *** Exception: LayoutError
--   {leExpected = Sparse, leActual = Dense}
sCheckedGradient :: forall gradient' m gradient layout device dataType shape. (SGetGradient gradient, MonadThrow m) => SGradient gradient' -> Tensor gradient layout device dataType shape -> m (Tensor (Seq (gradient <+> gradient') gradient') layout device dataType shape)
checkedGradient :: forall gradient' m gradient layout device dataType shape. (SingI gradient', SGetGradient gradient, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor (Seq (gradient <+> gradient') gradient') layout device dataType shape)

-- | Returns the input tensor but with <a>UncheckedLayout</a> as memory
--   layout type annotation. Any static information about the tensor's
--   memory layout is thus erased. However, the tensor's underlying data
--   structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedLayout t
--   uncheckedLayout t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          'UncheckedLayout
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedGradient :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor 'UncheckedGradient layout device dataType shape

-- | Returns a dense copy of the tensor.
toDense :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient ('Layout 'Dense) device dataType shape

-- | Returns a sparse copy of the tensor.
toSparse :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient ('Layout 'Sparse) device dataType shape
class SGetLayout (layout :: Layout LayoutType)

-- | Returns the gradually typed memory layout of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' layout = sOnes (SGradient SWithGradient) layout (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = sOnes' $ SLayout SDense
--   
--   &gt;&gt;&gt; sLayout t
--   SLayout SDense
--   
--   &gt;&gt;&gt; t = sOnes' $ SUncheckedLayout Dense
--   
--   &gt;&gt;&gt; sLayout t
--   SUncheckedLayout Dense
--   </pre>
sLayout :: forall gradient device dataType shape. SGetLayout layout => Tensor gradient layout device dataType shape -> SLayout layout

-- | Returns the untyped memory layout of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' layout = sOnes (SGradient SWithGradient) layout (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = sOnes' $ SLayout SDense
--   
--   &gt;&gt;&gt; layoutType t
--   Dense
--   
--   &gt;&gt;&gt; t = sOnes' $ SUncheckedLayout Dense
--   
--   &gt;&gt;&gt; layoutType t
--   Dense
--   </pre>
layoutType :: forall gradient device dataType shape. SGetLayout layout => Tensor gradient layout device dataType shape -> LayoutType
data LayoutError
LayoutError :: LayoutType -> LayoutType -> LayoutError
[leExpected] :: LayoutError -> LayoutType
[leActual] :: LayoutError -> LayoutType

-- | Checks whether or not the input tensor has the memory layout
--   <tt>layout</tt> and returns a statically annotated copy of it wrapped
--   in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the memory
--   layout <tt>layout</tt>. If it does not have it, then the result will
--   be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t =
--   sOnes (SGradient SWithGradient) (SUncheckedLayout Dense) (SDevice
--   SCPU) (SDataType SFloat) (SShape $ SName <tt>"batch" :&amp;: SSize
--   </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|: SNil)
--   &gt;&gt;&gt; t' &lt;- sCheckedLayout (SLayout SDense) t &gt;&gt;&gt;
--   :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense)
--   ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name "batch")
--   ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt; t' &lt;-
--   sCheckedLayout (SLayout SSparse) t *** Exception: LayoutError
--   {leExpected = Sparse, leActual = Dense}
sCheckedLayout :: forall layout' m gradient layout device dataType shape. (SGetLayout layout, MonadThrow m) => SLayout layout' -> Tensor gradient layout device dataType shape -> m (Tensor gradient (Seq (layout <+> layout') layout') device dataType shape)
checkedLayout :: forall layout' m gradient layout device dataType shape. (SingI layout', SGetLayout layout, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient (Seq (layout <+> layout') layout') device dataType shape)

-- | Returns the input tensor but with <a>UncheckedLayout</a> as memory
--   layout type annotation. Any static information about the tensor's
--   memory layout is thus erased. However, the tensor's underlying data
--   structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedLayout t
--   uncheckedLayout t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          'UncheckedLayout
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedLayout :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient 'UncheckedLayout device dataType shape

-- | Returns a copy of the tensor in CPU memory.
cpu :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout ('Device 'CPU) dataType shape

-- | Returns a copy of the tensor in CUDA memory.
cuda :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout ('Device ('CUDA 0)) dataType shape
class SGetDevice (device :: Device (DeviceType Nat))

-- | Returns the gradually typed compute device of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; ones' device = sOnes (SGradient SWithGradient) (SLayout SDense) device (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = ones' $ SDevice SCPU
--   
--   &gt;&gt;&gt; sDevice t
--   SDevice SCPU
--   
--   &gt;&gt;&gt; t = ones' $ SUncheckedDevice CPU
--   
--   &gt;&gt;&gt; sDevice t
--   SUncheckedDevice CPU
--   </pre>
sDevice :: forall gradient layout dataType shape. SGetDevice device => Tensor gradient layout device dataType shape -> SDevice device

-- | Returns the untyped compute device of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; ones' device = sOnes (SGradient SWithGradient) (SLayout SDense) device (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = ones' $ SDevice SCPU
--   
--   &gt;&gt;&gt; deviceType t
--   CPU
--   
--   &gt;&gt;&gt; t = ones' $ SUncheckedDevice CPU
--   
--   &gt;&gt;&gt; deviceType t
--   CPU
--   </pre>
deviceType :: forall gradient layout dataType shape. SGetDevice device => Tensor gradient layout device dataType shape -> DeviceType Int16
data DeviceError
DeviceError :: DeviceType Int16 -> DeviceType Int16 -> DeviceError
[deExpected] :: DeviceError -> DeviceType Int16
[deActual] :: DeviceError -> DeviceType Int16

-- | Checks whether or not the input tensor is in the memory of
--   <tt>device</tt> and returns a statically annotated copy of it wrapped
--   in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor is indeed on
--   <tt>device</tt>. If it is not, then the result will be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t =
--   sOnes (SGradient SWithGradient) (SLayout SDense) (SUncheckedDevice
--   CPU) (SDataType SFloat) (SShape $ SName <tt>"batch" :&amp;: SSize
--   </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|: SNil)
--   &gt;&gt;&gt; t' &lt;- sCheckedDevice (SDevice SCPU) t &gt;&gt;&gt;
--   :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense)
--   ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name "batch")
--   ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt; t' &lt;-
--   sCheckedDevice (SDevice (SCUDA @0)) t *** Exception: DeviceError
--   {deExpected = CUDA 0, deActual = CPU}
sCheckedDevice :: forall device' m gradient layout device dataType shape. (SGetDevice device, MonadThrow m) => SDevice device' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout (Seq (device <+> device') device') dataType shape)
checkedDevice :: forall device' m gradient layout device dataType shape. (SingI device', SGetDevice device, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout (Seq (device <+> device') device') dataType shape)

-- | Returns the input tensor but with <a>UncheckedDevice</a> as device
--   type annotation. Any static information about the tensor's device is
--   thus erased. However, the tensor's underlying data structure is not
--   changed.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDevice t
--   uncheckedDevice t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          'UncheckedDevice
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedDevice :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout 'UncheckedDevice dataType shape

-- | Returns a copy of the tensor converted to <a>Bool</a>.
bool :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape

-- | Returns a copy of the tensor converted to <a>UInt8</a>.
byte :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'UInt8) shape

-- | Returns a copy of the tensor converted to <a>Int8</a>.
char :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape

-- | Returns a copy of the tensor converted to <a>Int16</a>.
short :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int16) shape

-- | Returns a copy of the tensor converted to <a>Int32</a>.
int :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int32) shape

-- | Returns a copy of the tensor converted to <a>Int64</a>.
long :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int64) shape

-- | Returns a copy of the tensor converted to the 16-bit floating point
--   format <a>Half</a>.
half :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device ('DataType 'Half) shape

-- | Returns a copy of the tensor converted to the 32-bit floating point
--   format <a>Float</a>.
float :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device ('DataType 'Float) shape

-- | Returns a copy of the tensor converted to the 32-bit floating point
--   format <a>Double</a>.
double :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device ('DataType 'Double) shape
class SGetDataType (dataType :: DataType DType)

-- | Returns the gradually typed compute data type of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' dataType = sOnes (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) dataType (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = sOnes' $ SDataType SFloat
--   
--   &gt;&gt;&gt; sDataType t
--   SDataType SFloat
--   
--   &gt;&gt;&gt; t = sOnes' $ SUncheckedDataType Float
--   
--   &gt;&gt;&gt; sDataType t
--   SUncheckedDataType Float
--   </pre>
sDataType :: forall gradient layout device shape. SGetDataType dataType => Tensor gradient layout device dataType shape -> SDataType dataType

-- | Returns the untyped compute data type of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' dataType = sOnes (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) dataType (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t = sOnes' $ SDataType SFloat
--   
--   &gt;&gt;&gt; dType t
--   Float
--   
--   &gt;&gt;&gt; t = sOnes' $ SUncheckedDataType Float
--   
--   &gt;&gt;&gt; dType t
--   Float
--   </pre>
dType :: forall gradient layout device shape. SGetDataType dataType => Tensor gradient layout device dataType shape -> DType
data DataTypeError
DataTypeError :: DType -> DType -> DataTypeError
[dtExpected] :: DataTypeError -> DType
[dtActual] :: DataTypeError -> DType

-- | Checks whether or not the input tensor has the data type
--   <tt>dataType</tt> and returns a statically annotated copy of it
--   wrapped in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the data
--   type <tt>dataType</tt>. If it does not have it, then the result will
--   be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t =
--   sOnes (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU)
--   (SUncheckedDataType Float) (SShape $ SName <tt>"batch" :&amp;: SSize
--   </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|: SNil)
--   &gt;&gt;&gt; t' &lt;- checkedDataType <tt>('DataType 'Float) t
--   &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout
--   'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name
--   "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt;
--   t' &lt;- checkedDataType </tt>('DataType 'Double) t *** Exception:
--   DataTypeError {dtExpected = Double, dtActual = Float}
sCheckedDataType :: forall dataType' m gradient layout device dataType shape. (SGetDataType dataType, MonadThrow m) => SDataType dataType' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device (Seq (dataType <+> dataType') dataType') shape)
checkedDataType :: forall dataType' m gradient layout device dataType shape. (SingI dataType', SGetDataType dataType, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device (Seq (dataType <+> dataType') dataType') shape)

-- | Returns the input tensor but with <a>UncheckedDataType</a> as
--   data-type type annotation. Any static information about the tensor's
--   data type is thus erased. However, the tensor's underlying data
--   structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDataType t
--   uncheckedDataType t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          'UncheckedDataType
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedDataType :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device 'UncheckedDataType shape
class SGetShape (shape :: Shape [Dim (Name Symbol) (Size Nat)])

-- | Returns the gradually typed shape of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' = sOnes (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; t = sOnes' . SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil
--   
--   &gt;&gt;&gt; sShape t
--   SShape (SCons (SDim {sDimName = SName, sDimSize = SSize}) (SCons (SDim {sDimName = SName, sDimSize = SSize}) SNil))
--   
--   &gt;&gt;&gt; t = sOnes' . SUncheckedShape $ [Dim "batch" 32, Dim "feature" 8]
--   
--   &gt;&gt;&gt; sShape t
--   SUncheckedShape [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   
--   &gt;&gt;&gt; t = sOnes' . SShape $ SUncheckedName "batch" :&amp;: SUncheckedSize 32 :|: SUncheckedName "feature" :&amp;: SSize @32 :|: SNil
--   
--   &gt;&gt;&gt; sShape t
--   SShape (SCons (SDim {sDimName = SUncheckedName "batch", sDimSize = SUncheckedSize 32}) (SCons (SDim {sDimName = SUncheckedName "feature", sDimSize = SSize}) SNil))
--   
--   &gt;&gt;&gt; t = sOnes' . SShape $ SName @"batch" :&amp;: SUncheckedSize 32 :|: SName @"feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; sShape t
--   SShape (SCons (SDim {sDimName = SName, sDimSize = SUncheckedSize 32}) (SCons (SDim {sDimName = SName, sDimSize = SUncheckedSize 8}) SNil))
--   </pre>
sShape :: forall gradient layout device dataType. SGetShape shape => Tensor gradient layout device dataType shape -> SShape shape

-- | Returns the untyped shape of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' = sOnes (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; t = sOnes' . SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil
--   
--   &gt;&gt;&gt; dims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   
--   &gt;&gt;&gt; t = sOnes' . SUncheckedShape $ [Dim "batch" 32, Dim "feature" 8]
--   
--   &gt;&gt;&gt; dims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   
--   &gt;&gt;&gt; t = sOnes' . SShape $ SUncheckedName "batch" :&amp;: SUncheckedSize 32 :|: SUncheckedName "feature" :&amp;: SSize @32 :|: SNil
--   
--   &gt;&gt;&gt; dims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 32}]
--   
--   &gt;&gt;&gt; t = sOnes' . SShape $ SName @"batch" :&amp;: SUncheckedSize 32 :|: SName @"feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; dims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   </pre>
dims :: forall gradient layout device dataType. SGetShape shape => Tensor gradient layout device dataType shape -> [Dim String Integer]
class SGetDims (dims :: [Dim (Name Symbol) (Size Nat)])
sDims :: SGetDims dims => [String] -> [Integer] -> SList dims
dimsError :: forall a. a
dimNameError :: forall a. String -> String -> a
dimSizeError :: forall a b. Show a => a -> a -> b
dimNameSizeError :: forall a b. Show a => String -> String -> a -> a -> b
class SGetDim (dim :: Dim (Name Symbol) (Size Nat))
sDim :: SGetDim dim => String -> Integer -> SDim dim
data ShapeError
ShapeError :: [Dim String Integer] -> [Dim String Integer] -> ShapeError
[seExpected] :: ShapeError -> [Dim String Integer]
[seActual] :: ShapeError -> [Dim String Integer]

-- | Checks whether or not the input tensor has the shape <tt>shape</tt>
--   and returns a statically annotated copy of it wrapped in a
--   <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the shape
--   <tt>shape</tt>. If it is not, then the result will be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t =
--   sOnes (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU)
--   (SDataType SFloat) (SUncheckedShape [Dim "batch" 32, Dim "feature" 8])
--   &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SName <tt>"batch"
--   :&amp;: SSize </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|:
--   SNil) t &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient 'WithGradient)
--   ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim
--   ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SUncheckedName "batch"
--   :&amp;: SSize <tt>32 :|: SName </tt>"feature" :&amp;: SUncheckedSize 8
--   :|: SNil) t &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient
--   'WithGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float)
--   ('Shape '[ 'Dim 'UncheckedName ('Size 32), 'Dim ('Name "feature")
--   'UncheckedSize]) &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SName
--   <tt>"batch" :&amp;: SSize </tt>32 :|: SName @"feature" :&amp;:
--   SUncheckedSize 32 :|: SNil) t *** Exception: ShapeError {seExpected =
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature",
--   dimSize = 32}], seActual = [Dim {dimName = "batch", dimSize = 32},Dim
--   {dimName = "feature", dimSize = 8}]}
sCheckedShape :: forall shape' m gradient layout device dataType shape. (SGetShape shape, MonadThrow m) => SShape shape' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType (Seq (shape <+> shape') shape'))
checkedShape :: forall shape' m gradient layout device dataType shape. (SingI shape', SGetShape shape, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType (Seq (shape <+> shape') shape'))

-- | Returns the input tensor but with the selected dimension replaces with
--   <tt>UncheckedDim</tt> as dimension type annotation. The static
--   information about the selected tensor dimension is thus erased.
--   However, the tensor's underlying data structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDim @('SelectDim ('ByName "batch")) t
--   uncheckedDim @('SelectDim ('ByName "batch")) t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim 'UncheckedName 'UncheckedSize,
--                'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDim @('SelectDim ('ByIndex 1)) t
--   uncheckedDim @('SelectDim ('ByIndex 1)) t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim 'UncheckedName 'UncheckedSize])
--   </pre>
uncheckedDim :: forall selectDim gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (ReplaceDimF selectDim shape ('Dim 'UncheckedName 'UncheckedSize))

-- | Returns the input tensor but with <a>UncheckedShape</a> as shape type
--   annotation. Any static information about the tensor's shape is thus
--   erased. However, the tensor's underlying data structure is not
--   changed.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedShape t
--   uncheckedShape t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   </pre>
uncheckedShape :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType 'UncheckedShape
gitHubErrorMsg :: String
isContiguous :: Tensor gradient layout device dataType shape -> Bool
contiguous :: Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
withTensor :: Tensor gradient layout device dataType shape -> (Ptr () -> IO a) -> IO a
class TensorLikeRaw a

-- | Guesses outer dim.
--   
--   <pre>
--   &gt;&gt;&gt; guessDim @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]
--   Just 3
--   </pre>
guessDim :: TensorLikeRaw a => Maybe a -> Maybe Int

-- | Guesses inner dims.
--   
--   <pre>
--   &gt;&gt;&gt; guessInnerDims @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]
--   [2]
--   </pre>
guessInnerDims :: (TensorLikeRaw a, MonadThrow m) => Maybe a -> m [Int]

-- | Reads a value from a tensor.
tensorPeekElemOff :: TensorLikeRaw a => Ptr () -> Int -> [Int] -> IO a

-- | Writes a value to a tensor.
tensorPokeElemOff :: TensorLikeRaw a => Ptr () -> Int -> [Int] -> a -> IO ()

-- | Guesses dims: concatenates <a>guessDim</a> with <a>guessInnerDims</a>.
--   
--   <pre>
--   &gt;&gt;&gt; guessDims @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]
--   [3,2]
--   </pre>
guessDims :: forall a m. (TensorLikeRaw a, MonadThrow m) => Maybe a -> m [Int]
unexpectedDimsError :: forall a m b. (TensorLikeRaw a, MonadThrow m) => [Int] -> Maybe a -> m b
class TensorLike a (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]) | a -> dims, a -> dType

-- | Creates a tensor from a <a>TensorLike</a> value.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- sToTensor (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) ([(1, 2), (3, 4), (5, 6)] :: [(Int, Int)])
--   
--   &gt;&gt;&gt; t
--   Tensor Int64 [3,2] [[ 1,  2],
--                       [ 3,  4],
--                       [ 5,  6]]
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Int64)
--          ('Shape
--             '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") ('Size 2)])
--   </pre>
sToTensor :: forall gradient layout device m. (TensorLike a dType dims, MonadThrow m) => SGradient gradient -> SLayout layout -> SDevice device -> a -> m (Tensor gradient layout device ('DataType dType) ('Shape dims))

-- | Creates a <a>TensorLike</a> from a tensor.
fromTensor :: forall gradient layout device. TensorLike a dType dims => Tensor gradient layout device ('DataType dType) ('Shape dims) -> a

-- | Non-singleton version of <a>sToTensor</a>.
toTensor :: forall gradient layout device a dType dims m. (TensorLike a dType dims, SingI gradient, SingI layout, SingI device, MonadThrow m) => a -> m (Tensor gradient layout device ('DataType dType) ('Shape dims))
sToTensorRaw :: forall gradient layout device a dType dims m. (TensorLike a dType dims, TensorLikeRaw a, KnownDType dType, MonadThrow m) => SGradient gradient -> SLayout layout -> SDevice device -> a -> m (Tensor gradient layout device ('DataType dType) ('Shape dims))
fromTensorRaw :: forall gradient layout device a dType dims. (TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) => Tensor gradient layout device ('DataType dType) ('Shape dims) -> a
data DimMismatchError
DimMismatchError :: [Int] -> [Int] -> DimMismatchError
[dmeFirst] :: DimMismatchError -> [Int]
[dmeOther] :: DimMismatchError -> [Int]
checkDims :: MonadThrow m => [Int] -> [Int] -> m ()
sChangeTensorOptions :: forall gradient layout device dataType gradientFrom layoutFrom deviceFrom dataTypeFrom shape. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape -> Tensor gradient layout device dataType shape
changeTensorOptions :: forall gradient layout device dataType gradientFrom layoutFrom deviceFrom dataTypeFrom shape. (SingI gradient, SingI layout, SingI device, SingI dataType) => Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape -> Tensor gradient layout device dataType shape
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.GradientError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.LayoutError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.DeviceError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.DataTypeError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.ShapeError
instance GHC.Classes.Eq Torch.GraduallyTyped.Tensor.Type.DimMismatchError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.DimMismatchError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.DimMismatchError
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Bool 'Torch.GraduallyTyped.DType.Bool '[]
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Int 'Torch.GraduallyTyped.DType.Int64 '[]
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Float 'Torch.GraduallyTyped.DType.Float '[]
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Double 'Torch.GraduallyTyped.DType.Double '[]
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLike b dType dims', Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw b, Torch.GraduallyTyped.DType.KnownDType dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape (dims Torch.GraduallyTyped.Unify.<+> dims')) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size 2))) => Torch.GraduallyTyped.Tensor.Type.TensorLike (a, b) dType dimsOut
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.DType.KnownDType dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape dims) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)) => Torch.GraduallyTyped.Tensor.Type.TensorLike [a] dType dimsOut
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.DType.KnownDType dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape dims) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)) => Torch.GraduallyTyped.Tensor.Type.TensorLike (Data.Vector.Vector a) dType dimsOut
instance (GHC.TypeNats.KnownNat n, Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.DType.KnownDType dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape dims) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size n))) => Torch.GraduallyTyped.Tensor.Type.TensorLike (Data.Vector.Sized.Vector n a) dType dimsOut
instance (Data.Singletons.Internal.SingI gradient, Data.Singletons.Internal.SingI layout, Data.Singletons.Internal.SingI device, Data.Singletons.Internal.SingI dType) => Torch.GraduallyTyped.Tensor.Type.TensorLike (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device ('Torch.GraduallyTyped.DType.DataType dType) ('Torch.GraduallyTyped.Shape.Type.Shape dims)) dType dims
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Bool
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Int
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Float
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Double
instance (Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw b) => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (a, b)
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw [a]
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (Data.Vector.Vector a)
instance (GHC.TypeNats.KnownNat n, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a) => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (Data.Vector.Sized.Vector n a)
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.ShapeError
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim dim, Torch.GraduallyTyped.Tensor.Type.SGetDims dims) => Torch.GraduallyTyped.Tensor.Type.SGetDims (dim : dims)
instance Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim 'Torch.GraduallyTyped.Shape.Type.UncheckedName 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)
instance GHC.TypeLits.KnownSymbol name => Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name name) 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)
instance GHC.TypeNats.KnownNat size => Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim 'Torch.GraduallyTyped.Shape.Type.UncheckedName ('Torch.GraduallyTyped.Shape.Type.Size size))
instance (GHC.TypeLits.KnownSymbol name, GHC.TypeNats.KnownNat size) => Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name name) ('Torch.GraduallyTyped.Shape.Type.Size size))
instance Torch.GraduallyTyped.Tensor.Type.SGetDims dims => Torch.GraduallyTyped.Tensor.Type.SGetShape ('Torch.GraduallyTyped.Shape.Type.Shape dims)
instance Torch.GraduallyTyped.Tensor.Type.SGetDims '[]
instance Torch.GraduallyTyped.Tensor.Type.SGetShape 'Torch.GraduallyTyped.Shape.Type.UncheckedShape
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.DataTypeError
instance Torch.GraduallyTyped.Tensor.Type.SGetDataType 'Torch.GraduallyTyped.DType.UncheckedDataType
instance Data.Singletons.Internal.SingI dType => Torch.GraduallyTyped.Tensor.Type.SGetDataType ('Torch.GraduallyTyped.DType.DataType dType)
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.DeviceError
instance Torch.GraduallyTyped.Tensor.Type.SGetDevice 'Torch.GraduallyTyped.Device.UncheckedDevice
instance Torch.GraduallyTyped.Tensor.Type.SGetDevice ('Torch.GraduallyTyped.Device.Device 'Torch.GraduallyTyped.Device.CPU)
instance GHC.TypeNats.KnownNat deviceIndex => Torch.GraduallyTyped.Tensor.Type.SGetDevice ('Torch.GraduallyTyped.Device.Device ('Torch.GraduallyTyped.Device.CUDA deviceIndex))
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.LayoutError
instance Torch.GraduallyTyped.Tensor.Type.SGetLayout 'Torch.GraduallyTyped.Layout.UncheckedLayout
instance Torch.GraduallyTyped.Tensor.Type.SGetLayout ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Sparse)
instance Torch.GraduallyTyped.Tensor.Type.SGetLayout ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense)
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.GradientError
instance Torch.GraduallyTyped.Tensor.Type.SGetGradient 'Torch.GraduallyTyped.RequiresGradient.UncheckedGradient
instance Torch.GraduallyTyped.Tensor.Type.SGetGradient ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithGradient)
instance Torch.GraduallyTyped.Tensor.Type.SGetGradient ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient)
instance GHC.Show.Show (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)
instance GHC.Num.Num (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)
instance Torch.Internal.Class.Castable (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor)
instance Torch.Internal.Class.Castable [Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)
instance Torch.Internal.Class.Castable (Torch.HList.HList tensors) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor] => Torch.Internal.Class.Castable (Torch.HList.HList (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape : tensors)) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor]
instance Torch.Internal.Class.Castable (Torch.HList.HList '[]) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor]
instance forall k (l :: [k]). Torch.Internal.Class.Castable (Torch.HList.HList l) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor] => Torch.Internal.Class.Castable (Torch.HList.HList l) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)

module Torch.GraduallyTyped.NN.Class
class HasForward model input generator output generatorOutput | model input generator -> output, model input generator -> generatorOutput

-- | <tt>forward m i g</tt> for a model <tt>m</tt>, an input <tt>i</tt>,
--   and a generator <tt>g</tt> returns the tuple <tt>(o, g')</tt> where
--   <tt>o</tt> is the output of the model applied to the input and
--   <tt>g'</tt> is the updated generator. <tt>forward m i g</tt> may throw
--   an exception if the input <tt>i</tt> or the generator <tt>g</tt> are
--   not compatible with the model <tt>m</tt>.
forward :: forall m. (HasForward model input generator output generatorOutput, MonadThrow m) => model -> input -> generator -> m (output, generatorOutput)
class HasInitialize model input generator generatorOutput | model -> input, model generator -> generatorOutput
initialize :: HasInitialize model input generator generatorOutput => input -> generator -> (model, generatorOutput)
type StateDictKey = String
type StateDict = Map StateDictKey (ForeignPtr Tensor)
newtype FromStateDictError
FromStateDictKeyNotFoundError :: StateDictKey -> FromStateDictError
[fsdeExpectedKey] :: FromStateDictError -> StateDictKey
newtype ToStateDictError
ToStateDictKeyAlreadyInUseError :: StateDictKey -> ToStateDictError
[fsdeTakenKey] :: ToStateDictError -> StateDictKey
class HasStateDict model input | model -> input
fromStateDict :: forall m. (HasStateDict model input, MonadThrow m, MonadState StateDict m) => input -> StateDictKey -> m model
toStateDict :: forall m. (HasStateDict model input, MonadThrow m, MonadState StateDict m) => StateDictKey -> model -> m ()
stateDictFromPretrained :: FilePath -> IO StateDict
instance GHC.Show.Show Torch.GraduallyTyped.NN.Class.FromStateDictError
instance GHC.Show.Show Torch.GraduallyTyped.NN.Class.ToStateDictError
instance Torch.GraduallyTyped.NN.Class.HasStateDict () ()
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Layout.SLayout layout, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape shape)
instance (GHC.TypeNats.KnownNat n, Torch.GraduallyTyped.NN.Class.HasStateDict a input) => Torch.GraduallyTyped.NN.Class.HasStateDict (Data.Vector.Sized.Vector n a) input
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.NN.Class.ToStateDictError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.NN.Class.FromStateDictError
instance Torch.GraduallyTyped.NN.Class.HasInitialize () () generator generator
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a input generator generatorOutput', Torch.GraduallyTyped.NN.Class.HasInitialize b input generatorOutput' generatorOutput) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b) input generator generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a input generator generatorOutput, Torch.GraduallyTyped.NN.Class.HasInitialize a input generatorOutput generatorOutput, GHC.TypeNats.KnownNat n, n' GHC.Types.~ (n GHC.TypeNats.+ 1)) => Torch.GraduallyTyped.NN.Class.HasInitialize (Data.Vector.Sized.Vector n' a) input generator generatorOutput
instance Torch.GraduallyTyped.NN.Class.HasForward () input generator input generator
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generator output' generatorOutput', Torch.GraduallyTyped.NN.Class.HasForward b output' generatorOutput' output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (a, b) input generator output generatorOutput
instance Torch.GraduallyTyped.NN.Class.HasForward (Data.Vector.Sized.Vector 0 a) input generator input generator
instance Torch.GraduallyTyped.NN.Class.HasForward a input generator output generatorOutput => Torch.GraduallyTyped.NN.Class.HasForward (Data.Vector.Sized.Vector 1 a) input generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generator output generatorOutput, Torch.GraduallyTyped.NN.Class.HasForward a output generatorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Data.Vector.Sized.Vector n a) input generator output generatorOutput

module Torch.GraduallyTyped.Tensor.Other

-- | triu
triu :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | tril
tril :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | masked fill
maskedFill :: forall gradient layout device dataType shape value gradient' layout' device' dataType' shape'. Scalar value => Tensor gradient layout device dataType shape -> value -> Tensor gradient' layout' device' dataType' shape' -> Tensor (Seq (gradient <+> 'Gradient 'WithoutGradient) gradient') (layout <+> (layout' <+> 'Layout 'Dense)) (device <+> device') (Seq (dataType <+> 'DataType 'Bool) dataType') (BroadcastShapesF shape shape')

module Torch.GraduallyTyped.Tensor.MathOperations.Reduction
allT :: forall requiresGradient layout device shape. Tensor requiresGradient layout device ('DataType 'Bool) shape -> Tensor requiresGradient layout device ('DataType 'Bool) ('Shape '[])
type MeanErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply mean on the dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family MeanCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family MeanSelectDimsF (bys :: [By Symbol Nat]) (dims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family MeanF (selectDims :: SelectDims [By Symbol Nat]) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
sMeanDim :: forall selectDims gradient layout device dataType shape. SSelectDims selectDims -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (MeanF selectDims shape)
meanDim :: forall selectDims gradient layout device dataType shape. SingI selectDims => Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (MeanF selectDims shape)

module Torch.GraduallyTyped.Tensor.MathOperations.Comparison
gt :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
lt :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
ge :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
le :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
eq :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
ne :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
(>.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
(<.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
(>=.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
(<=.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
(==.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
(/=.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') (Seq (dataType <+> dataType') ('DataType 'Bool)) (BroadcastShapesF shape shape')
data Order
Ascending :: Order
Descending :: Order
data Sorted gradient layout device dataType shape
[Sorted] :: forall gradient layout device dataType shape. {sorted :: Tensor gradient layout device dataType shape, indices :: Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int64) shape} -> Sorted gradient layout device dataType shape
type SortErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply sort on the dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family SortCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe (Dim (Name Symbol) (Size Nat))) :: [Dim (Name Symbol) (Size Nat)]
type family SortF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
sSort :: forall selectDim gradient layout device dataType shape. SSelectDim selectDim -> Order -> Tensor gradient layout device dataType shape -> Sorted gradient layout device dataType (SortF selectDim shape)
sort :: forall selectDim gradient layout device dataType shape. SingI selectDim => Order -> Tensor gradient layout device dataType shape -> Sorted gradient layout device dataType (SortF selectDim shape)
instance GHC.Generics.Generic Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Classes.Ord Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Classes.Eq Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order

module Torch.GraduallyTyped.Tensor.IndexingSlicingJoining
class HasCat (selectDim :: SelectDim (By Symbol Nat)) k (c :: k -> Type) (a :: k) where {
    type family CatF selectDim a c :: Type;
}

-- | Concatenates the given sequence of seq tensors in the given dimension.
--   All tensors must either have the same shape (except in the
--   concatenating dimension) or be empty.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type cat @('SelectDim ('ByName "feature")) [t]
--   cat @('SelectDim ('ByName "feature")) [t]
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim 'UncheckedName 'UncheckedSize])
--   
--   &gt;&gt;&gt; :type cat @('SelectDim ( 'ByIndex 0)) [t]
--   cat @('SelectDim ( 'ByIndex 0)) [t]
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim 'UncheckedName 'UncheckedSize,
--                'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type sCat (SUncheckedSelectDim (ByIndex 0)) [t]
--   sCat (SUncheckedSelectDim (ByIndex 0)) [t]
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   </pre>
sCat :: HasCat selectDim k c a => SSelectDim selectDim -> c a -> CatF selectDim a c
cat :: (HasCat selectDim k c a, SingI selectDim) => c a -> CatF selectDim a c
type family CatListImplF (selectDim :: SelectDim (By Symbol Nat)) (tensor :: Type) :: Maybe Type
type CheckSpellingMessage = "Check the spelling of named dimensions, and make sure the number of dimensions is correct."
type family CatListCheckF (selectDim :: SelectDim (By Symbol Nat)) (tensor :: Type) (result :: Maybe Type) :: Type
type CatListF selectDim tensor = CatListCheckF selectDim tensor (CatListImplF selectDim tensor)
type family CatHListImplF (selectDim :: SelectDim (By Symbol Nat)) (tensors :: [Type]) (acc :: Maybe (Gradient RequiresGradient, Layout LayoutType, Device (DeviceType Nat), DataType DType, Shape [Dim (Name Symbol) (Size Nat)])) :: Type
type CatHListF selectDim tensors = CatHListImplF selectDim tensors 'Nothing
type ReshapeNumelMismatchMessage (numel :: Nat) (numel' :: Nat) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (shape' :: Shape [Dim (Name Symbol) (Size Nat)]) = "Cannot reshape the tensor. The original shape," % "" % "    '" <> shape <> "'," % "" % "and the new shape," % "" % "    '" <> shape' <> "'," % "" % "have different total numbers of elements," % "" % "    '" <> numel <> "' versus '" <> numel' <> "'," % "" % "respectively."
type family ReshapeImplF (numel :: Maybe Nat) (numel' :: Maybe Nat) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (shape' :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family ReshapeF (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (shape' :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]

-- | Returns a tensor with the same data and number of elements as the
--   input tensor, but with the specified shape:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) = sRandn (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; output = sReshape (SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @2 :|: SNil) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 2)])
--   </pre>
--   
--   At the value level, a single dimension may be '-1', in which case it
--   is inferred from the remaining dimensions and the number of elements
--   in the input:
--   
--   <pre>
--   &gt;&gt;&gt; output' = sReshape (SShape $ SUncheckedName "*" :&amp;: SUncheckedSize (-1) :|: SNil) output
--   
--   &gt;&gt;&gt; :type output'
--   output'
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   
--   &gt;&gt;&gt; dims output'
--   [Dim {dimName = "*", dimSize = 4}]
--   </pre>
sReshape :: forall shape' gradient layout device dataType shape shape''. shape'' ~ ReshapeF shape shape' => SShape shape' -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape''
reshape :: forall shape' gradient layout device dataType shape shape''. (shape'' ~ ReshapeF shape shape', SingI shape') => Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape''
type TransposeBy0Message (by0 :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot transpose the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "because the specified source dimension" % "" % "    '" <> by0 <> "'" % "" % "could not be found."
type TransposeBy1Message (by1 :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot transpose the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "because the specified target dimension" % "" % "    '" <> by1 <> "'" % "" % "could not be found."

-- | Compute transposed shapes.
--   
--   <pre>
--   &gt;&gt;&gt; type SelectBatch = 'SelectDim ('ByName "batch" :: By Symbol Nat)
--   
--   &gt;&gt;&gt; type SelectFeature = 'SelectDim ('ByName "feature" :: By Symbol Nat)
--   
--   &gt;&gt;&gt; type Dims = '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "feature") ('Size 8), 'Dim ('Name "anotherFeature") ('Size 12)]
--   
--   &gt;&gt;&gt; :kind! TransposeF SelectBatch SelectFeature ('Shape Dims)
--   TransposeF SelectBatch SelectFeature ('Shape Dims) :: Shape
--                                                           [Dim (Name Symbol) (Size Nat)]
--   = 'Shape
--       '[ 'Dim ('Name "feature") ('Size 8),
--          'Dim ('Name "batch") ('Size 10),
--          'Dim ('Name "anotherFeature") ('Size 12)]
--   
--   &gt;&gt;&gt; :kind! TransposeF SelectFeature SelectBatch ('Shape Dims)
--   TransposeF SelectFeature SelectBatch ('Shape Dims) :: Shape
--                                                           [Dim (Name Symbol) (Size Nat)]
--   = 'Shape
--       '[ 'Dim ('Name "feature") ('Size 8),
--          'Dim ('Name "batch") ('Size 10),
--          'Dim ('Name "anotherFeature") ('Size 12)]
--   </pre>
type family TransposeF (selectDim0 :: SelectDim (By Symbol Nat)) (selectDim1 :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family TransposeIndexIndexDimsF (index0 :: Nat) (index1 :: Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]

-- | Returns a tensor that is a transposed version of <tt>input</tt>. The
--   selected dimensions <tt>selectDim0</tt> and <tt>selectDim1</tt> are
--   swapped.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- mkGenerator @('Device 'CPU) 0
--   
--   &gt;&gt;&gt; (input, _) = randn @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "feature") ('Size 5)]) g
--   
--   &gt;&gt;&gt; output &lt;- transpose @('SelectDim ('ByName "batch")) @('SelectDim ('ByName "feature")) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "feature") ('Size 5),
--                'Dim ('Name "batch") ('Size 10)])
--   
--   &gt;&gt;&gt; output &lt;- sTranspose (SUncheckedSelectDim (ByIndex 0)) (SSelectDim (SByIndex @1)) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   
--   &gt;&gt;&gt; dims output
--   [W TensorImpl.h:934] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())
--   [Dim {dimName = "feature", dimSize = 5},Dim {dimName = "batch", dimSize = 10}]
--   </pre>
sTranspose :: forall selectDim0 selectDim1 gradient layout device dataType shape shape' m. (shape' ~ TransposeF selectDim0 selectDim1 shape, MonadThrow m) => SSelectDim selectDim0 -> SSelectDim selectDim1 -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
data TransposeError
TransposeMixedSelectorsError :: By String Integer -> By String Integer -> TransposeError
[teBy0] :: TransposeError -> By String Integer
[teBy1] :: TransposeError -> By String Integer
transpose :: forall selectDim0 selectDim1 gradient layout device dataType shape shape' m. (shape' ~ TransposeF selectDim0 selectDim1 shape, SingI selectDim0, SingI selectDim1, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
type UnsqueezeByMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot unsqueeze the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "because the specified source dimension" % "" % "    '" <> by <> "'" % "" % "could not be found."
type family UnsqueezeF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family UnsqueezeIndexDimsF (index :: Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
sUnsqueeze :: forall selectDim gradient layout device dataType shape shape'. shape' ~ UnsqueezeF selectDim shape => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape'
unsqueeze :: forall selectDim gradient layout device dataType shape shape'. (shape' ~ UnsqueezeF selectDim shape, SingI selectDim) => Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape'
sExpand :: forall shape' gradient layout device dataType shape input output. (input ~ Tensor gradient layout device dataType shape, output ~ Tensor gradient layout device dataType (BroadcastShapesF shape shape')) => SShape shape' -> input -> output
expand :: forall shape' gradient layout device dataType shape input output. (SingI shape', input ~ Tensor gradient layout device dataType shape, output ~ Tensor gradient layout device dataType (BroadcastShapesF shape shape')) => input -> output

-- | Slices the self tensor along the selected dimension at the given
--   index. This function returns a view of the original tensor with the
--   given dimension removed.
--   
--   <pre>
--   &gt;&gt;&gt; nats = sArangeNaturals (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt32) (SSize @8)
--   
--   &gt;&gt;&gt; input = sReshape (SShape $ SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @2 :|: SNil) nats
--   
--   &gt;&gt;&gt; input
--   Tensor Int32 [4,2] [[ 0,  1],
--                       [ 2,  3],
--                       [ 4,  5],
--                       [ 6,  7]]
--   </pre>
--   
--   <tt>index</tt> can be provided at compile-time: &gt;&gt;&gt; sSelect
--   (SSelectDim (SByIndex <tt>0)) (SIndex </tt>1) input Tensor Int32 [2] [
--   2, 3]
--   
--   <tt>index</tt> can also be provided at runtime: &gt;&gt;&gt; sSelect
--   (SSelectDim (SByIndex @0)) (SUncheckedIndex 1) input Tensor Int32 [2]
--   [ 2, 3]
--   
--   It produces a runtime error if the <tt>index</tt> is too large:
--   &gt;&gt;&gt; sSelect (SSelectDim (SByIndex @0)) (SUncheckedIndex 10)
--   input *** Exception: IndexOutOfBoundError {ioobeIndex = 10, ioobeDim =
--   Dim {dimName = "*", dimSize = 4}}
sSelect :: forall selectDim index gradient layout device dataType shapeIn shapeOut m. (index `InRangeF` GetDimF selectDim shapeIn, shapeOut ~ RemoveDimF selectDim shapeIn, SGetShape shapeIn, MonadThrow m) => SSelectDim selectDim -> SIndex index -> Tensor gradient layout device dataType shapeIn -> m (Tensor gradient layout device dataType shapeOut)
data IndexOutOfBoundError
IndexOutOfBoundError :: Natural -> Dim String Integer -> IndexOutOfBoundError
[ioobeIndex] :: IndexOutOfBoundError -> Natural
[ioobeDim] :: IndexOutOfBoundError -> Dim String Integer
select :: forall selectDim index gradient layout device dataType shapeIn shapeOut m. (SingI selectDim, SingI index, index `InRangeF` GetDimF selectDim shapeIn, shapeOut ~ RemoveDimF selectDim shapeIn, SGetShape shapeIn, MonadThrow m) => Tensor gradient layout device dataType shapeIn -> m (Tensor gradient layout device dataType shapeOut)
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.IndexOutOfBoundError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.IndexOutOfBoundError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeError
instance (Torch.Internal.Class.Castable (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.CatHListF selectDim tensors) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor), Torch.Internal.Class.Castable (Torch.HList.HList tensors) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)) => Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.HasCat selectDim [*] Torch.HList.HList tensors
instance Torch.Internal.Class.Castable (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.CatListF selectDim (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor) => Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.HasCat selectDim * [] (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)

module Torch.GraduallyTyped.Shape

module Torch.GraduallyTyped.Tensor.MathOperations.Pointwise

-- | Computes the element-wise absolute value of the given <tt>input</tt>
--   tensor: &lt;math&gt; The result is returned as a new tensor.
abs :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Alias for <a>abs</a>.
absolute :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arccosine of the elements of
--   <tt>input</tt>: &lt;math&gt;
acos :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arccosine of the elements of
--   <tt>input</tt>: &lt;math&gt;
--   
--   Note that the domain of the inverse hyperbolic cosine is &lt;math&gt;,
--   and values outside this range will be mapped to &lt;math&gt;, except
--   for &lt;math&gt; for which the output is mapped to &lt;math&gt;.
acosh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise addition of one tensor and another: &lt;math&gt; The
--   result is returned as a new tensor.
--   
--   The shape of <tt>other</tt> must be broadcastable with the shape of
--   <tt>input</tt>. See <a>addScalar</a> for a version of this function
--   where the <tt>other</tt> input is a scalar.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; sRandn' = sRandn (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; (a, g') = sRandn' (SShape $ SName @"feature" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (b, _) = sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @1 :|: SNil) g'
--   
--   &gt;&gt;&gt; :type a `add` b
--   a `add` b
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "*") ('Size 4), 'Dim ('Name "feature") ('Size 4)])
--   </pre>
add :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (BroadcastShapesF shape shape')

-- | Adds a scalar <tt>other</tt> to a tensor <tt>input</tt>: &lt;math&gt;
--   The result is returned as a new tensor. See <a>add</a> for a version
--   of this function where the second argument is a tensor.
--   
--   TODO: add data type unification of <tt>other</tt> and
--   <tt>dataType</tt>.
addScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Performs the element-wise division of <tt>tensor1</tt> by
--   <tt>tensor2</tt>, multiply the result by a scalar <tt>value</tt> and
--   add it to <tt>input</tt>: &lt;math&gt;
--   
--   See <a>addcmul</a> for a version of this function where
--   <tt>tensor1</tt> and <tt>tensor2</tt> are multiplied rather than
--   divided.
--   
--   Note further that for inputs of type <a>Float</a> or <a>Double</a>,
--   <tt>value</tt> must be a real number, otherwise it must be an integer.
addcdiv :: forall value gradient layout device dataType shape. Scalar value => value -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Performs the element-wise multiplication of <tt>tensor1</tt> by
--   <tt>tensor2</tt>, multiply the result by the scalar <tt>value</tt> and
--   add it to <tt>input</tt>: &lt;math&gt;
--   
--   See <a>addcdiv</a> for a version of this function where
--   <tt>tensor1</tt> and <tt>tensor2</tt> are divided rather than
--   multiplied.
--   
--   Note further that for inputs of type <a>Float</a> or <a>Double</a>,
--   <tt>value</tt> must be a real number, otherwise it must be an integer.
addcmul :: forall scalar gradient layout device dataType shape. Scalar scalar => scalar -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arcsine of the elements of
--   <tt>input</tt>: &lt;math&gt;
asin :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the inverse hyperbolic sine of the elements
--   of <tt>input</tt>: &lt;math&gt;
asinh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arctangent of the elements of
--   <tt>input</tt>: &lt;math&gt;
atan :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the inverse hyperbolic tangent of the
--   elements of <tt>input</tt>: &lt;math&gt;
--   
--   Note that the domain of the inverse hyperbolic tangent is
--   &lt;math&gt;, and values outside this range will be mapped to
--   &lt;math&gt;, except for the values &lt;math&gt; and &lt;math&gt; for
--   which the output is mapped to &lt;math&gt; respectively.
atanh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise arctangent of <tt>input</tt> and <tt>other</tt> with
--   consideration of the quadrant. Returns a new tensor where each element
--   is the signed angle in radians between the vectors &lt;math&gt; and
--   &lt;math&gt;. Here $mathrm{other}_i$, the &lt;math&gt;-th element of
--   the second argument of this function, is the x coordinate while
--   $mathrm{input}_i$, the &lt;math&gt;-th element of the first argument,
--   is the y coordinate.
--   
--   Note that the shapes of <tt>input</tt> and <tt>other</tt> must be
--   broadcastable.
atan2 :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (BroadcastShapesF shape shape')

-- | Computes the bitwise NOT of the given <tt>input</tt> tensor. The data
--   type of the <tt>input</tt> tensor must be <a>Bool</a> or an integral
--   data type. For <a>Bool</a> tensors, the function computes the logical
--   NOT.
bitwiseNot :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device ('DataType 'Bool) shape

-- | Computes the bitwise AND of the <tt>input</tt> and the <tt>other</tt>
--   tensor. The data type of the tensors must be <a>Bool</a> or an
--   integral data type. For <a>Bool</a> tensors, the function computes the
--   logical AND.
--   
--   See <a>bitwiseAndScalar</a> for a version of this function where
--   <tt>other</tt> is a scalar.
bitwiseAnd :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the bitwise AND of the tensor <tt>input</tt> and the scalar
--   <tt>other</tt>. The data type of the inputs must be <a>Bool</a> or an
--   integral data type. If the data type is <a>Bool</a>, then the function
--   computes the logical AND.
--   
--   See <a>bitwiseAnd</a> for a version of this function where
--   <tt>other</tt> is a tensor.
bitwiseAndScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Computes the bitwise OR of the <tt>input</tt> and the <tt>other</tt>
--   tensor. The data type of the tensors must be <a>Bool</a> or an
--   integral data type. For <a>Bool</a> tensors, the function computes the
--   logical OR.
--   
--   See <a>bitwiseOrScalar</a> for a version of this function where
--   <tt>other</tt> is a scalar.
bitwiseOr :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the bitwise OR of the tensor <tt>input</tt> and the scalar
--   <tt>other</tt>. The data type of the inputs must be <a>Bool</a> or an
--   integral data type. If the data type is <a>Bool</a>, then the function
--   computes the logical OR.
--   
--   See <a>bitwiseOr</a> for a version of this function where
--   <tt>other</tt> is a tensor.
bitwiseOrScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Computes the bitwise XOR of the <tt>input</tt> and the <tt>other</tt>
--   tensor. The data type of the tensors must be <a>Bool</a> or an
--   integral data type. For <a>Bool</a> tensors, the function computes the
--   logical XOR.
--   
--   See <a>bitwiseXorScalar</a> for a version of this function where
--   <tt>other</tt> is a scalar.
bitwiseXor :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the bitwise XOR of the tensor <tt>input</tt> and the scalar
--   <tt>other</tt>. The data type of the inputs must be <a>Bool</a> or an
--   integral data type. If the data type is <a>Bool</a>, then the function
--   computes the logical XOR.
--   
--   See <a>bitwiseXor</a> for a version of this function where
--   <tt>other</tt> is a tensor.
bitwiseXorScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the ceil of the elements of <tt>input</tt>,
--   that is, the smallest integer greater than or equal to each element:
--   &lt;math&gt; where &lt;math&gt; is the floor of the &lt;math&gt;-th
--   element of <tt>input</tt> which can be computed with <a>floor</a>.
ceil :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Clamp all elements in input into the range [ min, max ] and return the
--   result as a new tensor.
clamp :: forall min max gradient layout device dataType shape. (Scalar min, Scalar max) => min -> max -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
cos :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
cosh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
deg2rad :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise division of the first input tensor, the
--   <tt>dividend</tt>, by the second input tensor, the <tt>divisor</tt>.
--   &lt;math&gt; The result is returned as a new tensor.
--   
--   See <a>divScalar</a> for a version of this function where the
--   <tt>divisor</tt> is a scalar.
--   
--   Note further that "true divisions" can be computed with
--   <a>trueDivide</a> or <a>trueDivideScalar</a> which can come in handy
--   when both the <tt>dividend</tt> and the <tt>divisor</tt> have
--   <a>Bool</a> or integer data types.
div :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (BroadcastShapesF shape shape')

-- | Element-wise division of the first input, the <tt>dividend</tt>
--   tensor, by the second input, the <tt>divisor</tt> scalar. &lt;math&gt;
--   The result is returned as a new tensor.
--   
--   See <a>div</a> for a version of this function where the divisor is a
--   tensor.
--   
--   Note further that "true divisions" can be computed with
--   <a>trueDivide</a> or <a>trueDivideScalar</a> which can come in handy
--   when both the dividend and the divisor have <a>Bool</a> or integer
--   data types.
divScalar :: forall divisor gradient layout device dataType shape. Scalar divisor => Tensor gradient layout device dataType shape -> divisor -> Tensor gradient layout device dataType shape

-- | Computes the logarithmic derivative of the gamma function on
--   <tt>input</tt>: &lt;math&gt;
digamma :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes and returns the error function of each element of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>erfc</a> that computes the complementary error function to
--   high numerical accuracy and <a>erfinv</a> that computes the inverse of
--   the error function.
erf :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the complementary error function of each element of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>erf</a> that computes the error function and <a>erfinv</a>
--   that computes the inverse of the error function.
erfc :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the inverse error function of each element of <tt>input</tt>:
--   &lt;math&gt; where &lt;math&gt; for &lt;math&gt;. <a>erfinv</a> is not
--   defined outside this interval.
--   
--   See also <a>erf</a> that computes the error function and <a>erfc</a>
--   that computes the complementary error function.
erfinv :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the exponential of the elements of the input
--   tensor <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>expm1</a> for a high-accuracy calculation of the
--   exponential of the elements of <tt>input</tt> minus 1.
exp :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the exponential of the elements minus 1 of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>exp</a> for the exponential function.
expm1 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the floor of the elements of <tt>input</tt>,
--   that is, the largest integer less than or equal to each element.:
--   &lt;math&gt; where &lt;math&gt; is the ceil of the &lt;math&gt;-th
--   element of <tt>input</tt> which can be computed with <a>ceil</a>.
floor :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Return the element-wise division of the tensor <tt>dividend</tt> by
--   the tensor <tt>divisor</tt> rounded down to the nearest integer:
--   &lt;math&gt;
--   
--   See <a>floorDivideScalar</a> for a version of this function where
--   <tt>divisor</tt> is a scalar.
floorDivide :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Return the division of the tensor <tt>dividend</tt> by the scalar
--   <tt>divisor</tt> rounded down to the nearest integer: &lt;math&gt;
--   
--   See <a>floorDivide</a> for a version of this function where
--   <tt>divisor</tt> is a tensor.
floorDivideScalar :: forall divisor gradient layout device dataType shape. Scalar divisor => Tensor gradient layout device dataType shape -> divisor -> Tensor gradient layout device dataType shape

-- | Computes the element-wise remainder of the division of the tensor
--   <tt>dividend</tt> by the tensor <tt>divisor</tt>. The dividend and
--   divisor may contain both for integer and floating point numbers. The
--   remainder has the same sign as the <tt>dividend</tt> input.
--   
--   See <a>fmodScalar</a> for a version of this function where
--   <tt>divisor</tt> is a scalar.
fmod :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the element-wise remainder of the division of the tensor
--   <tt>dividend</tt> by the scalar <tt>divisor</tt>. The dividend and
--   divisor may contain both for integer and floating point numbers. The
--   remainder has the same sign as the <tt>dividend</tt> input.
--   
--   See <a>fmodScalar</a> for a version of this function where
--   <tt>divisor</tt> is a scalar.
fmodScalar :: forall divisor gradient layout device dataType shape. Scalar divisor => divisor -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the fractional portion of each element in <tt>input</tt>:
--   &lt;math&gt;
frac :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Linear interpolation of two tensors, <tt>start</tt> and <tt>end</tt>,
--   based on a tensor <tt>weight</tt>. For linear interpolations based on
--   a scalar see <a>lerpScalar</a>.
--   
--   Returned is the result of the following computation as a tensor:
--   &lt;math&gt;
--   
--   Note that the shapes of <tt>start</tt>, <tt>end</tt>, and also
--   <tt>weight</tt> must be broadcastable.
lerp :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Linear interpolation of two tensors, <tt>start</tt> and <tt>end</tt>,
--   based on a scalar <tt>weight</tt>. For linear interpolations based on
--   a tensor see <a>lerp</a>.
--   
--   Returned is the result of the following computation as a tensor:
--   &lt;math&gt;
--   
--   Note that the shapes of <tt>start</tt> and <tt>end</tt> must be
--   broadcastable.
lerpScalar :: forall weight gradient layout device dataType shape. Scalar weight => weight -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the logarithm of the gamma function on <tt>input</tt>:
--   &lt;math&gt;
lgamma :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the natural logarithm of the elements of
--   <tt>input</tt>: &lt;math&gt;
log :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the decimal logarithm of the elements of
--   <tt>input</tt>: &lt;math&gt;
log10 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the natural logarithm of &lt;math&gt;:
--   &lt;math&gt;
--   
--   Consider using this function over a literal implementation using
--   <a>log</a>. <a>log1p</a> is much more accurate for small values of
--   <tt>input</tt>.
log1p :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the logarithm to the base 2 of the elements
--   of <tt>input</tt>: &lt;math&gt;
log2 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Logarithm of the sum of exponentiations of the inputs. Calculates
--   pointwise the function &lt;math&gt;.
--   
--   This function is useful in statistics where the calculated
--   probabilities of events may be so small as to exceed the range of
--   normal floating point numbers. In such cases the logarithm of the
--   calculated probability is stored. This function allows adding
--   probabilities stored in such a fashion.
--   
--   <a>logaddexp</a> must not be confused with <tt>logsumexp</tt> which
--   performs a reduction on a single tensor.
logaddexp :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Logarithm of the sum of exponentiations of the inputs in base-2.
--   Calculates pointwise the function &lt;math&gt;.
--   
--   See <a>logaddexp</a> for further details.
logaddexp2 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the element-wise logical AND of the given input tensors. The
--   output tensor will have the <a>Bool</a> data type. If the input
--   tensors are not a bool tensors, then zeros are treated as <a>False</a>
--   and nonzeros are treated as <a>True</a>.
logicalAnd :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device ('DataType 'Bool) shape

-- | Computes the element-wise logical NOT of the given input tensor. The
--   output tensor will have the <a>Bool</a> data type. If the input tensor
--   is not a bool tensor, zeros are treated as <a>False</a> and non-zeros
--   are treated as <a>True</a>.
logicalNot :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape

-- | Computes the element-wise logical OR of the given input tensors. The
--   output tensor will have the <a>Bool</a> data type. If the input
--   tensors are not a bool tensors, then zeros are treated as <a>False</a>
--   and nonzeros are treated as <a>True</a>.
logicalOr :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) (BroadcastShapesF shape shape')

-- | Computes the element-wise logical XOR of the given input tensors. The
--   output tensor will have the <a>Bool</a> data type. If the input
--   tensors are not a bool tensors, then zeros are treated as <a>False</a>
--   and nonzeros are treated as <a>True</a>.
logicalXor :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device ('DataType 'Bool) shape

-- | Element-wise multiplication of two tensors: &lt;math&gt; The result is
--   returned as a new tensor.
--   
--   The shape of <tt>other</tt> must be broadcastable with the shape of
--   <tt>input</tt>. See <a>mulScalar</a> for a version of this function
--   where the <tt>other</tt> input is a scalar.
mul :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (BroadcastShapesF shape shape')
mulScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Computes the multivariate log-gamma function with dimension <tt>p</tt>
--   element-wise, given by &lt;math&gt; where &lt;math&gt; and
--   &lt;math&gt; is the gamma function.
--   
--   All elements of the input tensor must be greater than &lt;math&gt;.
--   Otherwise, the computation is halted and an exception is thrown.
mvlgamma :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the negative of the elements of
--   <tt>input</tt>: &lt;math&gt;
neg :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the &lt;math&gt;-th derivative of the digamma function
--   &lt;math&gt; on the <tt>input</tt>, where &lt;math&gt;. &lt;math&gt;
--   is called the order of the polygamma function &lt;math&gt; that is
--   defined as: &lt;math&gt; where &lt;math&gt;.
polygamma :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Takes the power of each element in the tensor <tt>input</tt> with the
--   corresponding element in the tensor <a>exponent</a> and returns a
--   tensor with the result.
--   
--   Note that the <a>exponent</a> and the <tt>input</tt> must be tensors
--   with broadcastable shapes. See <a>powScalar</a> for a version that
--   takes a scalar <a>exponent</a> as argument and <a>powTensor</a> for a
--   version where the <tt>input</tt> is a scalar and the <a>exponent</a> a
--   tensor.
--   
--   The following operation is applied: &lt;math&gt;
pow :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient' layout' device' dataType' shape' -> Tensor gradient layout device dataType shape -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (BroadcastShapesF shape shape')

-- | Takes the power of each element in the tensor <tt>input</tt> with the
--   scalar <a>exponent</a> and returns a tensor with the result.
--   
--   Note that the <a>exponent</a> is a scalar. See <a>pow</a> for a
--   version that takes a tensor <a>exponent</a> as argument and
--   <a>powTensor</a> for a version where the <tt>input</tt> is a scalar
--   and the <a>exponent</a> a tensor.
--   
--   The following operation is applied: &lt;math&gt;
powScalar :: forall exponent gradient layout device dataType shape. Scalar exponent => Tensor gradient layout device dataType shape -> exponent -> Tensor gradient layout device dataType shape

-- | Takes the power of the scalar <tt>input</tt> with each element in the
--   tensor <a>exponent</a> and returns a tensor with the result.
--   
--   Note that the <a>exponent</a> is a tensor while the <tt>input</tt> is
--   a scalar. See <a>pow</a> for a version that takes a tensor
--   <tt>input</tt> as argument and <a>powScalar</a> for a version where
--   the <tt>input</tt> is a tensor and the <a>exponent</a> a scalar.
--   
--   The following operation is applied: &lt;math&gt;
powTensor :: forall input gradient layout device dataType shape. Scalar input => input -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with each of the elements of <tt>input</tt>
--   converted from angles in radians to degrees.
rad2deg :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the reciprocal of the elements of
--   <tt>input</tt>: &lt;math&gt;
reciprocal :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the element-wise remainder of division.
--   
--   The dividend and divisor may contain integer and floating point
--   numbers. The remainder has the same sign as the divisor other.
--   
--   When other is a tensor, the shapes of input and other must be
--   broadcastable.
remainder :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with each of the elements of <tt>input</tt>
--   rounded to the closest integer. Note that the data type is unchanged.
round :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the reciprocal of the square-root of each of
--   the elements of <tt>input</tt>: &lt;math&gt;
rsqrt :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the sigmoid of the elements of
--   <tt>input</tt>: &lt;math&gt;
sigmoid :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the signs of the elements of <tt>input</tt>:
--   &lt;math&gt;
sign :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the sine of the elements of <tt>input</tt>:
--   &lt;math&gt;
sin :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the hyperbolic sine of the elements of
--   <tt>input</tt>: &lt;math&gt;
sinh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise subtraction of one tensor from another: &lt;math&gt; The
--   result is returned as a new tensor.
--   
--   The shape of <tt>other</tt> must be broadcastable with the shape of
--   <tt>input</tt>. See <a>subScalar</a> for a version of this function
--   where the <tt>other</tt> input is a scalar.
sub :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient) (layout <+> layout') (device <+> device') (dataType <+> dataType') (BroadcastShapesF shape shape')

-- | Subtracts a scalar <tt>other</tt> from a tensor <tt>input</tt>:
--   &lt;math&gt; The result is returned as a new tensor. See <a>sub</a>
--   for a version of this function where the second argument is a tensor.
subScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the square-root of the elements of
--   <tt>input</tt>: &lt;math&gt;
sqrt :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the square of the elements of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See <a>pow</a>, <a>powScalar</a>, or <a>powTensor</a> for
--   exponentiation with respect to arbitrary exponents.
square :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the tangent of the elements of
--   <tt>input</tt>: &lt;math&gt;
tan :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the hyperbolic tangent of the elements of
--   <tt>input</tt>: &lt;math&gt;
tanh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Performs true division that always computes the division in floating
--   point: &lt;math&gt;
--   
--   <a>trueDivide</a> is completely equivalent to division using
--   <a>div</a> except when both inputs have <a>Bool</a> or integer data
--   types, in which case the inputs are converted to floating data types
--   before performing the division.
--   
--   See <a>trueDivideScalar</a> for a version of this function where the
--   divisor is a scalar.
trueDivide :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (shape <+> shape')

-- | Performs true division that always computes the division in floating
--   point: &lt;math&gt;
--   
--   <a>trueDivideScalar</a> is completely equivalent to division using
--   <a>divScalar</a> except when both inputs have <a>Bool</a> or integer
--   data types, in which case the inputs are converted to floating data
--   types before performing the division.
--   
--   See <a>trueDivide</a> for a version of this function where the divisor
--   is a tensor.
trueDivideScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape
trunc :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

module Torch.GraduallyTyped.NN.Functional.Activation

-- | Thresholds each element of the input Tensor.
threshold :: forall threshold value gradient layout device dataType shape. (Scalar threshold, Scalar value) => threshold -> value -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the rectified linear unit function element-wise, that is,
--   &lt;math&gt;
relu :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the gaussian error linear unit function element-wise.
gelu :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the gaussian error linear unit function element-wise.
--   
--   This is the implementation of the GELU activation function from
--   Google's BERT repo (and coincidentally also from OpenAI's GPT). See
--   also <a>https://arxiv.org/abs/1606.08415</a>.
--   
--   <pre>
--   &gt;&gt;&gt; geluNew $ sFull (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNil) 0.5
--   Tensor Float []  0.3457
--   </pre>
geluNew :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the HardTanh function element-wise.
hardtanh :: forall minValue maxValue gradient layout device dataType shape. (Scalar minValue, Scalar maxValue) => minValue -> maxValue -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the hardswish function element-wise.
hardswish :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the exponential linear unit function element-wise, with alpha
--   input, &lt;math&gt;
elu :: forall alpha gradient layout device dataType shape. Scalar alpha => alpha -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the scaled exponential linear unit function element-wise, that
--   is, &lt;math&gt; with &lt;math&gt; and &lt;math&gt;.
selu :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the continuously differentiable exponential linear unit
--   function element-wise, that is, &lt;math&gt;
celu :: forall alpha gradient layout device dataType shape. Scalar alpha => alpha -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the element-wise function: &lt;math&gt; the the angle of the
--   negative slope can be controlled. A typical value for it is 0.01.
leakyRelu :: forall negativeSlope gradient layout device dataType shape. Scalar negativeSlope => negativeSlope -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the parameterized rectified linear unit function element-wise,
--   that is, &lt;math&gt; The weight parameter is typically learnable.
prelu :: forall gradient' gradient layout device dataType shape. Tensor gradient' layout device dataType shape -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

module Torch.GraduallyTyped.Tensor.MathOperations.BlasLapack
type family MatmulDimsImplF (reversedDims :: [Dim (Name Symbol) (Size Nat)]) (reversedDims' :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family MatmulDimsCheckF (dims :: [Dim (Name Symbol) (Size Nat)]) (dims' :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type MatmulDimsF dims dims' = MatmulDimsCheckF dims dims' (MatmulDimsImplF (Reverse dims) (Reverse dims'))
type family MatmulF (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (shape' :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]

-- | Matrix product of two tensors.
--   
--   The following code serves the examples of <tt>matmul</tt> below:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; sRandn' = sRandn (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   </pre>
--   
--   In order to understand the behavior of <tt>matmul</tt>, consider the
--   following cases:
--   
--   <ol>
--   <li>If both tensors are 1-dimensional, the dot product (scalar) is
--   returned:<pre>&gt;&gt;&gt; (tensor1, g') = sRandn' (SShape $ SName
--   @"*" :&amp;: SSize @3 :|: SNil) g &gt;&gt;&gt; (tensor2, g'') =
--   sRandn' (SShape $ SName @"*" :&amp;: SSize @3 :|: SNil) g'
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2 &gt;&gt;&gt; :type
--   result result :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense)
--   ('Device 'CPU) ('DataType 'Float) ('Shape '[]) </pre></li>
--   <li>If both arguments are 2-dimensional, the matrix-matrix product is
--   returned:<pre>&gt;&gt;&gt; (tensor1, g') = sRandn' (SShape $ SName
--   @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   &gt;&gt;&gt; (tensor2, g'') = sRandn' (SShape $ SName @"*" :&amp;:
--   SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g' &gt;&gt;&gt;
--   result = tensor1 `matmul` tensor2 &gt;&gt;&gt; :type result result ::
--   Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU)
--   ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name
--   "*") ('Size 7)]) </pre></li>
--   <li>If the first argument is 1-dimensional and the second argument is
--   2-dimensional, a 1 is prepended to its dimension for the purpose of
--   the matrix multiply. After the matrix multiply, the prepended
--   dimension is removed:<pre>&gt;&gt;&gt; (tensor1, g') = sRandn' (SShape
--   $ SName @"*" :&amp;: SSize @4 :|: SNil) g &gt;&gt;&gt; (tensor2, g'')
--   = sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;:
--   SSize @7 :|: SNil) g' &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   &gt;&gt;&gt; :type result result :: Tensor ('Gradient 'WithGradient)
--   ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim
--   ('Name "*") ('Size 7)]) </pre></li>
--   <li>If the first argument is 2-dimensional and the second argument is
--   1-dimensional, the matrix-vector product is returned:<pre>&gt;&gt;&gt;
--   (tensor1, g') = sRandn' (SShape $ SName @"*" :&amp;: SSize @3 :|:
--   SName @"*" :&amp;: SSize @4 :|: SNil) g &gt;&gt;&gt; (tensor2, g'') =
--   sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g'
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2 &gt;&gt;&gt; :type
--   result result :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense)
--   ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size
--   3)]) </pre></li>
--   <li>If both arguments are at least 1-dimensional and at least one
--   argument is &lt;math&gt;-dimensional (where &lt;math&gt;), then a
--   batched matrix multiply is returned.</li>
--   </ol>
--   
--   The following is an example of a batched matrix multiplication:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') = sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') = sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 3),
--                'Dim ('Name "*") ('Size 7)])
--   </pre>
--   
--   If the first argument is 1-dimensional, a 1 is prepended to its
--   dimension for the purpose of the batched matrix multiply and removed
--   after:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') = sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') = sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 7)])
--   </pre>
--   
--   If the second argument is 1-dimensional, a 1 is appended to its
--   dimension for the purpose of the batched matrix multiply and removed
--   after:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') = sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') = sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 3)])
--   </pre>
--   
--   The non-matrix (i.e. batch) dimensions are broadcasted (and thus must
--   be broadcastable). For example, if <tt>input</tt> is a &lt;math&gt;
--   tensor and <tt>other</tt> is a &lt;math&gt; tensor, <tt>output</tt>
--   will be a &lt;math&gt; tensor:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') = sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @1 :|: SName @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') = sRandn' (SShape $ SName @"*" :&amp;: SSize @5 :|: SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 5),
--                'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7)])
--   </pre>
matmul :: forall gradient layout layout' device device' dataType dataType' shape shape'. Tensor gradient layout device dataType shape -> Tensor gradient layout' device' dataType' shape' -> Tensor gradient (layout <+> layout') (device <+> device') (dataType <+> dataType') (MatmulF shape shape')

module Torch.GraduallyTyped.Tensor.MathOperations

module Torch.GraduallyTyped.NN.Functional.NonLinearActivation
type SoftMaxErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply softmax on the dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family SoftmaxCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe (Dim (Name Symbol) (Size Nat))) :: [Dim (Name Symbol) (Size Nat)]
type family SoftmaxF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]

-- | Applies the softmax function that is defined as:
--   
--   &lt;math&gt;
--   
--   Softmax is applied to all slices along <tt>selectDim</tt>, and will
--   re-scale them so that the elements lie in the range &lt;math&gt; and
--   sum to &lt;math&gt;:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) = sRandn (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil) g
--   
--   &gt;&gt;&gt; result = softmax (SSelectDim (SByName @"feature")) input
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
softmax :: forall selectDim gradient layout device dataType shape. SSelectDim selectDim -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (SoftmaxF selectDim shape)

-- | Applies the softmax function that is defined as:
--   
--   &lt;math&gt;
--   
--   Softmax is applied to all slices along <tt>selectDim</tt>, and will
--   re-scale them so that the elements lie in the range &lt;math&gt; and
--   sum to &lt;math&gt;:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) = sRandn (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil) g
--   
--   &gt;&gt;&gt; result = softmax (SSelectDim (SByName @"feature")) input
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
logSoftmax :: forall selectDim gradient layout device dataType shape. SSelectDim selectDim -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (SoftmaxF selectDim shape)

module Torch.GraduallyTyped.NN.Activation
data Softmax (selectDim :: SelectDim (By Symbol Nat))
[Softmax] :: forall selectDim. {softmaxSelectDim :: SSelectDim selectDim} -> Softmax selectDim
data Relu
[Relu] :: Relu
data Gelu
[Gelu] :: Gelu
data GeluNew
[GeluNew] :: GeluNew
data Tanh
[Tanh] :: Tanh
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Activation.Softmax selectDim)
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.Tanh () generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.Tanh ()
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.Tanh (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.GeluNew () generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.GeluNew ()
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.GeluNew (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.Gelu () generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.Gelu ()
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.Gelu (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.Relu () generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.Relu ()
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.Relu (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Activation.Softmax selectDim) (Torch.GraduallyTyped.Shape.Type.SSelectDim selectDim) generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Activation.Softmax selectDim) (Torch.GraduallyTyped.Shape.Type.SSelectDim selectDim)
instance (output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType (Torch.GraduallyTyped.NN.Functional.NonLinearActivation.SoftmaxF selectDim shape)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Activation.Softmax selectDim) (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator output generator

module Torch.GraduallyTyped.Random
data Generator (device :: Device (DeviceType Nat))
mkGenerator :: forall device. SingI device => Word64 -> IO (Generator device)
sMkGenerator :: forall device. SDevice device -> Word64 -> IO (Generator device)
withGenerator :: forall requiresGradient layout device dataType shape device'. (ForeignPtr Generator -> IO (Tensor requiresGradient layout device dataType shape)) -> Tensor requiresGradient layout device dataType shape -> Generator device' -> (Tensor requiresGradient layout device dataType shape, Generator (device <+> device'))

module Torch.GraduallyTyped.Tensor.Creation

-- | Create a gradually typed tensor of ones.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @32 :|: SUncheckedName "feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; :type sOnes (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--   sOnes (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Int64)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim 'UncheckedName 'UncheckedSize])
--   </pre>
sOnes :: forall gradient layout device dataType shape. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> Tensor gradient layout device dataType shape

-- | Create a typed tensor of ones.
--   
--   <pre>
--   &gt;&gt;&gt; ones :: CPUParameter ('DataType 'Float) ('Shape '[])
--   Tensor Float []  1.0000
--   
--   &gt;&gt;&gt; ones :: CPUTensor ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1)])
--   Tensor Int64 [1] [ 1]
--   </pre>
ones :: forall gradient layout device dataType shape. (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape) => Tensor gradient layout device dataType shape

-- | Create a gradually typed tensor of zeros.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @32 :|: SUncheckedName "feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; :type sZeros (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--   sZeros (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Int64)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim 'UncheckedName 'UncheckedSize])
--   </pre>
sZeros :: forall gradient layout device dataType shape. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> Tensor gradient layout device dataType shape

-- | Create a typed tensor of zeros.
--   
--   <pre>
--   &gt;&gt;&gt; zeros :: CPUParameter ('DataType 'Float) ('Shape '[])
--   Tensor Float []  0.0000
--   
--   &gt;&gt;&gt; zeros :: CPUTensor ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1)])
--   Tensor Int64 [1] [ 0]
--   </pre>
zeros :: forall gradient layout device dataType shape. (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape) => Tensor gradient layout device dataType shape

-- | Create a gradually typed tensor filled with a given scalar value.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @32 :|: SUncheckedName "feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; input = -1
--   
--   &gt;&gt;&gt; :type sFull (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape input
--   sFull (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape input
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Int64)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim 'UncheckedName 'UncheckedSize])
--   </pre>
sFull :: forall gradient layout device dataType shape input. Scalar input => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> input -> Tensor gradient layout device dataType shape

-- | Create a typed tensor filled with a given scalar value.
--   
--   <pre>
--   &gt;&gt;&gt; full (-1) :: CPUParameter ('DataType 'Float) ('Shape '[])
--   Tensor Float [] -1.0000
--   
--   &gt;&gt;&gt; full (-1) :: CPUTensor ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1)])
--   Tensor Int64 [1] [-1]
--   </pre>
full :: forall gradient layout device dataType shape input. (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape, Scalar input) => input -> Tensor gradient layout device dataType shape

-- | Create a gradually typed random tensor.
sRandn :: forall gradient layout device dataType shape device'. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> Generator device' -> (Tensor gradient layout device dataType shape, Generator (device <+> device'))

-- | Create typed random tensor.
randn :: forall gradient layout device dataType shape device'. (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape) => Generator device' -> (Tensor gradient layout device dataType shape, Generator (device <+> device'))

-- | Create a gradually typed one-dimensional tensor of the numbers
--   <tt>0</tt> to <tt>size -1</tt>.
sArangeNaturals :: forall gradient layout device dataType size shape. shape ~ 'Shape '[ 'Dim ('Name "*") size] => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SSize size -> Tensor gradient layout device dataType shape

-- | Create a typed one-dimensional tensor of the numbers <tt>0</tt> to
--   <tt>size -1</tt>.
arangeNaturals :: forall gradient layout device dataType size shape. (shape ~ 'Shape '[ 'Dim ('Name "*") size], SingI gradient, SingI layout, SingI device, SingI dataType, SingI size) => Tensor gradient layout device dataType shape

-- | Create a gradually typed rectangular tensor with ones on the diagonal
--   and zeros elsewhere.
sEye :: forall gradient layout device dataType rows cols shape. shape ~ 'Shape '[ 'Dim ('Name "*") rows, 'Dim ('Name "*") cols] => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SSize rows -> SSize cols -> Tensor gradient layout device dataType shape

-- | Create a typed rectangular tensor with ones on the diagonal and zeros
--   elsewhere.
eye :: forall gradient layout device dataType rows cols shape. (shape ~ 'Shape '[ 'Dim ('Name "*") rows, 'Dim ('Name "*") cols], SingI gradient, SingI layout, SingI device, SingI dataType, SingI rows, SingI cols) => Tensor gradient layout device dataType shape

-- | Create a gradually typed square tensor with ones on the diagonal and
--   zeros elsewhere.
sEyeSquare :: forall gradient layout device dataType size shape. shape ~ 'Shape '[ 'Dim ('Name "*") size, 'Dim ('Name "*") size] => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SSize size -> Tensor gradient layout device dataType shape

-- | Create a typed square tensor with ones on the diagonal and zeros
--   elsewhere.
eyeSquare :: forall gradient layout device dataType size shape. (shape ~ 'Shape '[ 'Dim ('Name "*") size, 'Dim ('Name "*") size], SingI gradient, SingI layout, SingI device, SingI dataType, SingI size) => Tensor gradient layout device dataType shape

module Torch.GraduallyTyped.Tensor

module Torch.GraduallyTyped.Autograd
class HasGrad parameters where {
    type family Gradients parameters :: Type;
    type family Loss parameters :: Type;
}

-- | calculate gradients of a zero-dimensional tensor with respect to a
--   list of independent tensor parameters
grad :: HasGrad parameters => Loss parameters -> parameters -> Gradients parameters
instance Torch.GraduallyTyped.Autograd.HasGrad (Torch.GraduallyTyped.Tensor.Type.Tensor ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithGradient) layout device dataType shape)

module Torch.GraduallyTyped.NN.Transformer.Type
data TransformerStyle
T5 :: TransformerStyle
ByT5 :: TransformerStyle
BART :: TransformerStyle
MBART :: TransformerStyle
Pegasus :: TransformerStyle
BERT :: TransformerStyle
RoBERTa :: TransformerStyle
GPT2 :: TransformerStyle
type T5Sym0 = 'T5 :: TransformerStyle
type ByT5Sym0 = 'ByT5 :: TransformerStyle
type BARTSym0 = 'BART :: TransformerStyle
type MBARTSym0 = 'MBART :: TransformerStyle
type PegasusSym0 = 'Pegasus :: TransformerStyle
type BERTSym0 = 'BERT :: TransformerStyle
type RoBERTaSym0 = 'RoBERTa :: TransformerStyle
type GPT2Sym0 = 'GPT2 :: TransformerStyle
data STransformerStyle z_a1nvb
[ST5] :: STransformerStyle ('T5 :: TransformerStyle)
[SByT5] :: STransformerStyle ('ByT5 :: TransformerStyle)
[SBART] :: STransformerStyle ('BART :: TransformerStyle)
[SMBART] :: STransformerStyle ('MBART :: TransformerStyle)
[SPegasus] :: STransformerStyle ('Pegasus :: TransformerStyle)
[SBERT] :: STransformerStyle ('BERT :: TransformerStyle)
[SRoBERTa] :: STransformerStyle ('RoBERTa :: TransformerStyle)
[SGPT2] :: STransformerStyle ('GPT2 :: TransformerStyle)
data TransformerHead
WithoutHead :: TransformerHead
WithLMHead :: TransformerHead
WithMLMHead :: TransformerHead
type WithoutHeadSym0 = 'WithoutHead :: TransformerHead
type WithLMHeadSym0 = 'WithLMHead :: TransformerHead
type WithMLMHeadSym0 = 'WithMLMHead :: TransformerHead
data STransformerHead z_a1nBP
[SWithoutHead] :: STransformerHead ('WithoutHead :: TransformerHead)
[SWithLMHead] :: STransformerHead ('WithLMHead :: TransformerHead)
[SWithMLMHead] :: STransformerHead ('WithMLMHead :: TransformerHead)
padded :: Integral n => n -> a -> [a] -> [a]
mkTransformerInput :: forall batchDim seqDim m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, 'Shape '[batchDim, seqDim] ~ Seq ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]) ('Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => Int -> SDim batchDim -> SDim seqDim -> [[Int]] -> m output
type MkPosC device shape seqDim seqName seqSize output = (SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), seqDim ~ 'Dim seqName seqSize, output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") seqSize]))
mkPos :: forall m gradient layout device dataType shape seqDim seqName seqSize output. (MonadThrow m, MkPosC device shape seqDim seqName seqSize output) => Tensor gradient layout device dataType shape -> m output
type MkTransformerPaddingMaskC layout device dataType shape output = (output ~ Tensor ('Gradient 'WithoutGradient) (layout <+> 'Layout 'Dense) (device <+> 'Device 'CPU) (Seq (dataType <+> 'DataType 'Int64) ('DataType 'Bool)) (BroadcastShapesF shape ('Shape '[ 'Dim ('Name "*") ('Size 1)])))
mkTransformerPaddingMask :: forall gradient layout device dataType shape output. MkTransformerPaddingMaskC layout device dataType shape output => Int -> Tensor gradient layout device dataType shape -> output
type MkTransformerAttentionMaskC transformerDataType gradient layout device dataType shape seqDim output = (SGetLayout layout, SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), output ~ Tensor (Seq (gradient <+> 'Gradient 'WithoutGradient) ('Gradient 'WithoutGradient)) (layout <+> 'Layout 'Dense) device (Seq (dataType <+> 'DataType 'Bool) transformerDataType) (BroadcastShapesF (UnsqueezeF ('SelectDim ('ByIndex 1)) shape) ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim])))
mkTransformerAttentionMask :: forall m transformerDataType gradient layout device dataType shape seqDim output. (MonadThrow m, MkTransformerAttentionMaskC transformerDataType gradient layout device dataType shape seqDim output) => SDataType transformerDataType -> Double -> Tensor gradient layout device dataType shape -> m output
type MkTransformerDecoderAttentionMaskC transformerDataType layout device shape seqDim output = (SGetLayout layout, SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), output ~ Tensor ('Gradient 'WithoutGradient) (layout <+> 'Layout 'Dense) device transformerDataType (BroadcastShapesF (BroadcastShapesF ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim]) (UnsqueezeF ('SelectDim ('ByIndex 1)) shape)) ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim])))
mkTransformerDecoderAttentionMask :: forall m transformerDataType gradient layout device dataType shape seqDim output. (MonadThrow m, MkTransformerDecoderAttentionMaskC transformerDataType layout device shape seqDim output) => SDataType transformerDataType -> Double -> Tensor gradient layout device dataType shape -> m output
type MkTransformerCrossAttentionMaskC transformerDataType decoderInputShape decoderInputSeqDim gradient layout device dataType shape seqDim output = (SGetLayout layout, SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), SGetShape decoderInputShape, decoderInputSeqDim ~ (decoderInputShape ! 1), output ~ Tensor (Seq (gradient <+> 'Gradient 'WithoutGradient) ('Gradient 'WithoutGradient)) (layout <+> 'Layout 'Dense) device (Seq (dataType <+> 'DataType 'Bool) transformerDataType) (BroadcastShapesF (UnsqueezeF ('SelectDim ('ByIndex 1)) shape) ('Shape '[ 'Dim ('Name "*") ('Size 1), decoderInputSeqDim, seqDim])))
mkTransformerCrossAttentionMask :: forall m transformerDataType decoderInputShape decoderInputSeqDim gradient layout device dataType shape seqDim output. (MonadThrow m, MkTransformerCrossAttentionMaskC transformerDataType decoderInputShape decoderInputSeqDim gradient layout device dataType shape seqDim output) => SDataType transformerDataType -> SShape decoderInputShape -> Double -> Tensor gradient layout device dataType shape -> m output
data ShiftRight fillValue
[ShiftRight] :: forall fillValue. fillValue -> ShiftRight fillValue
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue) fillValue generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue) fillValue
instance (input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.SGetLayout inputLayout, Torch.GraduallyTyped.Tensor.Type.SGetDevice inputDevice, Torch.GraduallyTyped.Tensor.Type.SGetDataType inputDataType, Torch.GraduallyTyped.Tensor.Type.SGetShape inputShape, inputBatchDim GHC.Types.~ (inputShape Torch.GraduallyTyped.Shape.Class.! 0), inputSeqDim GHC.Types.~ (inputShape Torch.GraduallyTyped.Shape.Class.! 1), Torch.GraduallyTyped.Scalar.Scalar fillValue, rightShiftedInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> 'Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient) inputLayout inputDevice inputDataType (Torch.GraduallyTyped.Shape.Class.ReplaceDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) (inputShape Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.Shape.Type.Shape '[inputBatchDim, inputSeqDim]) (Torch.GraduallyTyped.Shape.Class.AddDimF inputSeqDim ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size 1))))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue) input generator rightShiftedInput generator
instance Data.Singletons.Internal.SingKind Torch.GraduallyTyped.NN.Transformer.Type.TransformerHead
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.WithoutHead
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.WithLMHead
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.WithMLMHead
instance Data.Singletons.Internal.SingKind Torch.GraduallyTyped.NN.Transformer.Type.TransformerStyle
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.T5
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.BART
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.MBART
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.BERT
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa
instance Data.Singletons.Internal.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.GPT2
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.Type.TransformerStyle
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.Type.TransformerStyle

module Torch.GraduallyTyped.NN.Initialization

-- | Note: Identity = linear w/o activation
data ForNonLinearity
ForIdentity :: ForNonLinearity
ForSigmoid :: ForNonLinearity
ForTanh :: ForNonLinearity
ForRelu :: ForNonLinearity
ForLeakyRelu :: Float -> ForNonLinearity
data FanMode
FanIn :: FanMode
FanOut :: FanMode
errorPrefix :: String

-- | Gain scaling value for He initialization
calculateGain :: ForNonLinearity -> Float

-- | Fan-in / Fan-out scaling calculation
calculateFan :: [Dim String Integer] -> (Integer, Integer)

-- | Xavier uniform initialization
sXavierUniform :: forall gradient layout device dataType shape gain device'. (Num gain, Floating gain, Scalar gain) => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> gain -> Generator device' -> (Tensor gradient layout device dataType shape, Generator (device <+> device'))

-- | Xavier normal initialization
sXavierNormal :: forall gradient layout device dataType shape gain device'. (Num gain, Floating gain, Scalar gain) => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> gain -> Generator device' -> (Tensor gradient layout device dataType shape, Generator (device <+> device'))

-- | Get fan in or fan out value depending on selected fan mode, used by
--   Kaiming
getter :: forall a. FanMode -> (a, a) -> a

-- | Kaiming uniform initialization
sKaimingUniform :: forall gradient layout device dataType shape device'. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> FanMode -> ForNonLinearity -> Generator device' -> (Tensor gradient layout device dataType shape, Generator (device <+> device'))

-- | Kaiming normal initialization
sKaimingNormal :: forall gradient layout device dataType shape device'. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> FanMode -> ForNonLinearity -> Generator device' -> (Tensor gradient layout device dataType shape, Generator (device <+> device'))
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Show.Show Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Initialization.FanMode
instance GHC.Show.Show Torch.GraduallyTyped.NN.Initialization.FanMode
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Initialization.FanMode
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Initialization.FanMode

module Torch.GraduallyTyped.NN.Functional.Sparse
type EmbedDimsErrorMessage (embedDims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply the embedding." % "The embedding weight tensor must have exactly two dimensions," % "but the following dimensions were found:" % "" % "    " <> embedDims <> "." % ""
type family EmbeddingF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
embedding :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. SGetLayout layout => Maybe Natural -> Bool -> Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (Seq (dataType' <+> 'DataType 'Int64) dataType) (EmbeddingF shape shape')

module Torch.GraduallyTyped.NN.Sparse
data Embedding (gradient :: Gradient RequiresGradient) (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (embedNumDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (paddingIdx :: Maybe Nat)
[Embedding] :: forall gradient layout device dataType embedNumDim embedDim paddingIdx. {embeddingWeight :: Tensor gradient layout device dataType ('Shape '[embedNumDim, embedDim])} -> Embedding gradient layout device dataType embedNumDim embedDim paddingIdx
instance (device'' GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> device')) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim 'GHC.Maybe.Nothing) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Layout.SLayout layout, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim embedNumDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim) (Torch.GraduallyTyped.Random.Generator device') (Torch.GraduallyTyped.Random.Generator device'')
instance (device'' GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> device')) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim ('GHC.Maybe.Just paddingIdx)) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Layout.SLayout layout, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim embedNumDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim) (Torch.GraduallyTyped.Random.Generator device') (Torch.GraduallyTyped.Random.Generator device'')
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim paddingIdx) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Layout.SLayout layout, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim embedNumDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim)
instance (Torch.GraduallyTyped.Tensor.Type.SGetLayout layout, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') (layout Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (Torch.GraduallyTyped.Prelude.Seq (dataType' Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[embedNumDim, embedDim]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim 'GHC.Maybe.Nothing) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generator output generator
instance (Torch.GraduallyTyped.Tensor.Type.SGetLayout layout, GHC.TypeNats.KnownNat paddingIdx, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') (layout Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (Torch.GraduallyTyped.Prelude.Seq (dataType' Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[embedNumDim, embedDim]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim ('GHC.Maybe.Just paddingIdx)) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generator output generator

module Torch.GraduallyTyped.NN.Functional.Normalization
type family LayerNormImplF (reverseNormalizedDims :: [Dim (Name Symbol) (Size Nat)]) (reverseInputDims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type LayerNormShapeErrorMessage = "Cannot apply the layer norm. " % "The normalized shape exceeds the input shape."
type family LayerNormWithBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (biasShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
layerNormWithBias :: forall gradient gradient' gradient'' layout layout' layout'' device device' device'' dataType dataType' dataType'' shape shape' shape''. SGetShape shape => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Double -> Tensor gradient'' layout'' device'' dataType'' shape'' -> Tensor (gradient' <|> (gradient' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) (LayerNormWithBiasF shape shape' shape'')
type family LayerNormWithoutBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family LayerNormWithoutBiasSelectDimsF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: SelectDims [By Symbol Nat]
type family LayerNormWithoutBiasBysF (weightDims :: [Dim (Name Symbol) (Size Nat)]) (inputDims :: [Dim (Name Symbol) (Size Nat)]) (inputDimsLength :: Nat) (counter :: Nat) :: [By Symbol Nat]

-- | T5-style layer norm
layerNormWithoutBias :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. (SGetShape shape, SGetShape shape') => Tensor gradient layout device dataType shape -> Double -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (LayerNormWithoutBiasF shape shape')

module Torch.GraduallyTyped.NN.Normalization
data LayerNorm (hasBias :: HasBias) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (normalizedShape :: Shape [Dim (Name Symbol) (Size Nat)])
[LayerNormWithBias] :: forall gradient device dataType normalizedShape. {layerNormWithBiasWeight :: Tensor gradient ('Layout 'Dense) device dataType normalizedShape, layerNormBias :: Tensor gradient ('Layout 'Dense) device dataType normalizedShape, layerNormWithBiasEps :: Double} -> LayerNorm 'WithBias gradient device dataType normalizedShape
[LayerNormWithoutBias] :: forall gradient device dataType normalizedShape. {layerNormWithoutBiasWeight :: Tensor gradient ('Layout 'Dense) device dataType normalizedShape, layerNormWithoutBiasEps :: Double} -> LayerNorm 'WithoutBias gradient device dataType normalizedShape
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape normalizedShape, GHC.Types.Double) generator generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape normalizedShape, GHC.Types.Double) generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape normalizedShape, GHC.Types.Double)
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape normalizedShape, GHC.Types.Double)
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape normalizedShape, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF normalizedShape normalizedShape shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generator output generator
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape normalizedShape, Torch.GraduallyTyped.Tensor.Type.SGetShape shape', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithoutBiasF normalizedShape shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generator output generator

module Torch.GraduallyTyped.NN.Functional.Linear

-- | Compute the output shape of a linear transformation.
--   
--   <pre>
--   &gt;&gt;&gt; type InputDim = 'Dim ('Name "input") ('Size 5)
--   
--   &gt;&gt;&gt; type OutputDim = 'Dim ('Name "output") ('Size 10)
--   
--   &gt;&gt;&gt; type BatchDim = 'Dim ('Name "batch") ('Size 20)
--   
--   &gt;&gt;&gt; type WeightShape = 'Shape '[OutputDim, InputDim]
--   
--   &gt;&gt;&gt; type BiasShape = 'Shape '[OutputDim]
--   
--   &gt;&gt;&gt; type InputShape = 'Shape '[BatchDim, InputDim]
--   
--   &gt;&gt;&gt; :kind! LinearWithBiasF WeightShape BiasShape InputShape
--   LinearWithBiasF WeightShape BiasShape InputShape :: Shape
--                                                         [Dim (Name Symbol) (Size Nat)]
--   = 'Shape
--       '[ 'Dim ('Name "batch") ('Size 20),
--          'Dim ('Name "output") ('Size 10)]
--   </pre>
type family LinearWithBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (biasShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family LinearWithBiasDimsF (weightDims :: [Dim (Name Symbol) (Size Nat)]) (biasDims :: [Dim (Name Symbol) (Size Nat)]) (reversedInputDims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type LinearInputDimsErrorMessage = "Cannot apply the linear transformation." % "The input tensor does not have the minimum required number of dimensions." % "At least one dimension is needed, but none were found."
type LinearBiasDimsErrorMessage (biasDims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply the linear transformation." % "The bias tensor must have exactly one dimension," % "but the following dimensions were found:" % "" % "    " <> biasDims <> "." % ""
type LinearWeightDimsErrorMessage (weightDims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply the linear transformation." % "The weight tensor must have exactly two dimensions," % "but the following dimensions were found:" % "" % "    " <> weightDims <> "." % ""

-- | Applies a linear transformation to the incoming data: &lt;math&gt;
--   
--   Supported shapes:
--   
--   <ul>
--   <li><tt>input</tt>: &lt;math&gt;, where &lt;math&gt; is the batch
--   size, &lt;math&gt; means any number of additional dimensions and
--   &lt;math&gt; are the input features.</li>
--   <li><tt>weight</tt>: &lt;math&gt;</li>
--   <li><tt>bias</tt>: &lt;math&gt;</li>
--   <li><tt>output</tt>: &lt;math&gt;</li>
--   </ul>
--   
--   Examples:
--   
--   <pre>
--   &gt;&gt;&gt; inputDim = SName @"input" :&amp;: SSize @5
--   
--   &gt;&gt;&gt; outputDim = SName @"output" :&amp;: SSize @10
--   
--   &gt;&gt;&gt; batchDim = SName @"batch" :&amp;: SSize @20
--   
--   &gt;&gt;&gt; weightShape = SShape $ outputDim :|: inputDim :|: SNil
--   
--   &gt;&gt;&gt; biasShape = SShape $ outputDim :|: SNil
--   
--   &gt;&gt;&gt; inputShape = SShape $ batchDim :|: inputDim :|: SNil
--   
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; sRandn' = sRandn (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; (weight, g') = sRandn' weightShape g
--   
--   &gt;&gt;&gt; (bias, g'') = sRandn' biasShape g'
--   
--   &gt;&gt;&gt; (input, _) = sRandn' inputShape g''
--   
--   &gt;&gt;&gt; result = linearWithBias weight bias input
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 20),
--                'Dim ('Name "output") ('Size 10)])
--   </pre>
linearWithBias :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' gradient'' layout'' device'' dataType'' shape''. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor gradient'' layout'' device'' dataType'' shape'' -> Tensor (gradient' <|> (gradient'' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) (LinearWithBiasF shape shape' shape'')
type family LinearWithoutBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family LinearWithoutBiasDimsF (weightDims :: [Dim (Name Symbol) (Size Nat)]) (reversedInputDims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
linearWithoutBias :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (LinearWithoutBiasF shape shape')
testLinearWithoutBias :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense) 'UncheckedDevice ('DataType 'Float) ('Shape '[ 'Dim ('Name "output") ('Size 2)])

module Torch.GraduallyTyped.NN.Linear
data Linear (hasBias :: HasBias) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputDim :: Dim (Name Symbol) (Size Nat)) (outputDim :: Dim (Name Symbol) (Size Nat))
[LinearWithBias] :: forall gradient device dataType inputDim outputDim. {linearWithBiasWeight :: Tensor gradient ('Layout 'Dense) device dataType ('Shape '[outputDim, inputDim]), linearBias :: Tensor gradient ('Layout 'Dense) device dataType ('Shape '[outputDim])} -> Linear 'WithBias gradient device dataType inputDim outputDim
[LinearWithoutBias] :: forall gradient device dataType inputDim outputDim. {linearWithoutBiasWeight :: Tensor gradient ('Layout 'Dense) device dataType ('Shape '[outputDim, inputDim])} -> Linear 'WithoutBias gradient device dataType inputDim outputDim
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Linear.Linear hasBias gradient device dataType inputDim outputDim)
instance (generator GHC.Types.~ Torch.GraduallyTyped.Random.Generator device', generator' GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> device')) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Linear.Linear 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType inputDim outputDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim inputDim, Torch.GraduallyTyped.Shape.Type.SDim outputDim) generator generator'
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Linear.Linear 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType inputDim outputDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim inputDim, Torch.GraduallyTyped.Shape.Type.SDim outputDim)
instance (output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Linear.LinearWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[outputFeatures, inputFeatures]) ('Torch.GraduallyTyped.Shape.Type.Shape '[outputFeatures]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Linear.Linear 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType inputFeatures outputFeatures) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generator output generator
instance (generator GHC.Types.~ Torch.GraduallyTyped.Random.Generator device', generator' GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> device')) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Linear.Linear 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType inputDim outputDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim inputDim, Torch.GraduallyTyped.Shape.Type.SDim outputDim) generator generator'
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Linear.Linear 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType inputDim outputDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim inputDim, Torch.GraduallyTyped.Shape.Type.SDim outputDim)
instance (output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Linear.LinearWithoutBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[outputFeatures, inputFeatures]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Linear.Linear 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType inputFeatures outputFeatures) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generator output generator

module Torch.GraduallyTyped.NN.Transformer.Pooler
data GPooler (dense :: Type) (activation :: Type)
[GPooler] :: forall dense activation. {poolerDense :: dense, poolerActivation :: activation} -> GPooler dense activation
newtype Pooler (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat))
[Pooler] :: forall style gradient device dataType inputEmbedDim. GPooler (PoolerDenseF style gradient device dataType inputEmbedDim) (PoolerActivationF style) -> Pooler style gradient device dataType inputEmbedDim
type family PoolerDenseF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family PoolerActivationF (style :: TransformerStyle) :: Type
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Pooler.PoolerDenseF style gradient device dataType inputEmbedDim) input generator denseOutput denseGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Pooler.PoolerActivationF style) denseOutput denseGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Pooler.Pooler style gradient device dataType inputEmbedDim) input generator output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.LMHead
data GLMHead (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (dense :: Type) (activation :: Type) (layerNorm :: Type) (decoder :: Type) (bias :: Type)
[GLMHead] :: forall inputEmbedDim dense activation layerNorm decoder bias. {lmHeadInputEmbedDim :: SDim inputEmbedDim, lmHeadDense :: dense, lmHeadActivation :: activation, lmHeadLayerNorm :: layerNorm, lmHeadDecoder :: decoder, lmHeadBias :: bias} -> GLMHead inputEmbedDim dense activation layerNorm decoder bias

-- | Language modelling head for transformer encoders and decoders.
newtype LMHead (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat))
[LMHead] :: forall style gradient device dataType inputEmbedDim vocabDim. GLMHead inputEmbedDim (LMHeadDenseF style gradient device dataType inputEmbedDim) (LMHeadActivationF style) (LMHeadLayerNormF style gradient device dataType inputEmbedDim) (LMHeadDecoderF style gradient device dataType inputEmbedDim vocabDim) (LMHeadBiasF style gradient device dataType vocabDim) -> LMHead style gradient device dataType inputEmbedDim vocabDim
type family LMHeadDenseF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family LMHeadActivationF (style :: TransformerStyle) :: Type
type family LMHeadLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family LMHeadDecoderF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family LMHeadBiasF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeLMHeadDenseInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeLMHeadActivationInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeLMHeadLayerNormInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeLMHeadDecoderInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family LMHeadOutputF (style :: TransformerStyle) (decoderOutput :: Type) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
instance (Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadDenseF style gradient device dataType inputEmbedDim) input generator denseOutput denseGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadActivationF style) denseOutput denseGeneratorOutput activationOutput activationGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadLayerNormF style gradient device dataType inputEmbedDim) activationOutput activationGeneratorOutput layerNormOutput layerNormGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadDecoderF style gradient device dataType inputEmbedDim vocabDim) layerNormOutput layerNormGeneratorOutput decoderOutput generatorOutput, decoderOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape', output GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadOutputF style decoderOutput gradient device dataType vocabDim) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHead style gradient device dataType inputEmbedDim vocabDim) input generator output generatorOutput
instance (Data.Singletons.Internal.SingI style, dense GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadDenseF style gradient device dataType inputEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize dense (Torch.GraduallyTyped.NN.Transformer.LMHead.HasInitializeLMHeadDenseInputF style gradient device dataType inputEmbedDim vocabDim) generator generator', activation GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadActivationF style, Torch.GraduallyTyped.NN.Class.HasInitialize activation (Torch.GraduallyTyped.NN.Transformer.LMHead.HasInitializeLMHeadActivationInputF style gradient device dataType inputEmbedDim vocabDim) generator' generator'', layerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadLayerNormF style gradient device dataType inputEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm (Torch.GraduallyTyped.NN.Transformer.LMHead.HasInitializeLMHeadLayerNormInputF style gradient device dataType inputEmbedDim vocabDim) generator'' generator''', decoder GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadDecoderF style gradient device dataType inputEmbedDim vocabDim, Torch.GraduallyTyped.NN.Class.HasInitialize decoder (Torch.GraduallyTyped.NN.Transformer.LMHead.HasInitializeLMHeadDecoderInputF style gradient device dataType inputEmbedDim vocabDim) generator''' generator'''', bias GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.LMHead.LMHeadBiasF style gradient device dataType vocabDim) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHead style gradient device dataType inputEmbedDim vocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, GHC.Types.Double) generator generator''''
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.LMHead.LMHead style gradient device dataType inputEmbedDim vocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, GHC.Types.Double)

module Torch.GraduallyTyped.NN.Functional

module Torch.GraduallyTyped.NN.Dropout

-- | Given a random generator, randomly zeroes some of the elements of the
--   input tensor with probability <tt>p</tt> using samples from a
--   Bernoulli distribution. Each channel will be zeroed out independently
--   on every <a>forward</a> call.
newtype Dropout (p :: Type)
[Dropout] :: forall p. p -> Dropout p
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Dropout.Dropout p)
instance GHC.Show.Show p => GHC.Show.Show (Torch.GraduallyTyped.NN.Dropout.Dropout p)
instance Torch.GraduallyTyped.Scalar.Scalar p => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Dropout.Dropout p) p generator generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Dropout.Dropout p) p
instance (Torch.GraduallyTyped.Scalar.Scalar p, input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape, generator GHC.Types.~ Torch.GraduallyTyped.Random.Generator generatorDevice, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout (device Torch.GraduallyTyped.Unify.<+> generatorDevice) dataType shape, generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> generatorDevice)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Dropout.Dropout p) input generator output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention

-- | Generic multi-headed attention layer. Needs to be specialized to a
--   given transformer type, e.g. <a>T5</a>. See <a>MultiHeadAttention</a>.
data GMultiHeadAttention (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (qInProj :: Type) (kInProj :: Type) (vInProj :: Type) (outProj :: Type) (dropout :: Type)
[GMultiHeadAttention] :: forall headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout. {mhaHeadDim :: SDim headDim " head dim", mhaHeadEmbedDim :: SDim headEmbedDim " head embed dim", mhaEmbedDim :: SDim embedDim " embed dim", mhaQInProj :: qInProj " in-projection for query", mhaKInProj :: kInProj " in-projection for key", mhaVInProj :: vInProj " in-projection for value", mhaOutProj :: outProj " out-projection", mhaDropout :: dropout " dropout"} -> GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout

-- | Multi-headed attention layer.
newtype MultiHeadAttention (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (valueEmbedDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[MultiHeadAttention] :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim dropoutP. GMultiHeadAttention headDim headEmbedDim embedDim (QInProjF style gradient device dataType queryEmbedDim embedDim) (KInProjF style gradient device dataType keyEmbedDim embedDim) (VInProjF style gradient device dataType valueEmbedDim embedDim) (OutProjF style gradient device dataType embedDim queryEmbedDim) (DropoutF style dropoutP) -> MultiHeadAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim dropoutP
type family QInProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family KInProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family VInProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (valueEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family OutProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family DropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type

-- | Whether or not scaling is applied in the multi-headed attention layer.
data Scaling

-- | Scaling is not done.
NoScaling :: Scaling

-- | Scaling is applied to the query after in the in-projection.
QueryScaling :: Double -> Scaling

-- | Scaling is applied to the attention weights.
WeightScaling :: Double -> Scaling

-- | Whether or not out-projection is applied in the multi-headed attention
--   layer.
data OutProj

-- | Out-projection is absent.
NoOutProj :: OutProj

-- | Out-projection is applied.
OutProj :: OutProj
type BatchDim queryShape keyShape valueShape = (queryShape ! 0) <+> (keyShape ! 0) <+> (valueShape ! 0)
getBatchDim :: forall m queryShape keyShape valueShape batchDim. (MonadThrow m, batchDim ~ BatchDim queryShape keyShape valueShape) => SShape queryShape -> SShape keyShape -> SShape valueShape -> m (SDim batchDim)
type QuerySeqDim queryShape = queryShape ! 1
getQuerySeqDim :: forall m queryShape querySeqDim. (MonadThrow m, querySeqDim ~ QuerySeqDim queryShape) => SShape queryShape -> m (SDim querySeqDim)
type KeySeqDim keyShape valueShape = (keyShape ! 1) <+> (valueShape ! 1)
getKeySeqDim :: forall m keyShape valueShape keySeqDim. (MonadThrow m, keySeqDim ~ KeySeqDim keyShape valueShape) => SShape keyShape -> SShape valueShape -> m (SDim keySeqDim)
testMHA :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") ('Size 3)]))
instance (GHC.Show.Show qInProj, GHC.Show.Show kInProj, GHC.Show.Show vInProj, GHC.Show.Show outProj, GHC.Show.Show dropout) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout)
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.Scaling
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.Scaling
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.Scaling
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.Scaling
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProj
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProj
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProj
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProj
instance (GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.QInProjF style gradient device dataType queryEmbedDim embedDim), GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.KInProjF style gradient device dataType keyEmbedDim embedDim), GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.VInProjF style gradient device dataType valueEmbedDim embedDim), GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProjF style gradient device dataType embedDim queryEmbedDim), GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.DropoutF style dropoutP)) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim dropoutP)
instance (Torch.GraduallyTyped.NN.Class.HasForward qInProj (Torch.GraduallyTyped.Tensor.Type.Tensor queryRequiresGradient queryLayout queryDevice queryDataType queryShape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor qRequiresGradient qLayout qDevice qDataType qShape0) qGeneratorOutput, qShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF qShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, querySeqDim, headDim, headEmbedDim])), Torch.GraduallyTyped.NN.Class.HasForward kInProj (Torch.GraduallyTyped.Tensor.Type.Tensor keyRequiresGradient keyLayout keyDevice keyDataType keyShape) qGeneratorOutput (Torch.GraduallyTyped.Tensor.Type.Tensor qRequiresGradient kLayout kDevice kDataType kShape0) kGeneratorOutput, weightsShape0 GHC.Types.~ Torch.GraduallyTyped.NN.Functional.NonLinearActivation.SoftmaxF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF (Torch.GraduallyTyped.Tensor.MathOperations.BlasLapack.MatmulF qShape (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF kShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, keySeqDim, headDim, headEmbedDim]))))) attentionBiasShape), Torch.GraduallyTyped.NN.Class.HasForward dropout (Torch.GraduallyTyped.Tensor.Type.Tensor (qRequiresGradient Torch.GraduallyTyped.Unify.<|> attentionBiasRequiresGradient) (qLayout Torch.GraduallyTyped.Unify.<+> (kLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (qDevice Torch.GraduallyTyped.Unify.<+> (kDevice Torch.GraduallyTyped.Unify.<+> attentionBiasDevice)) (qDataType Torch.GraduallyTyped.Unify.<+> (kDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) weightsShape0) kGeneratorOutput (Torch.GraduallyTyped.Tensor.Type.Tensor weightsRequiresGradient weightsLayout weightsDevice weightsDataType weightsShape) weightsGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward vInProj (Torch.GraduallyTyped.Tensor.Type.Tensor valueRequiresGradient valueLayout valueDevice valueDataType valueShape) weightsGeneratorOutput (Torch.GraduallyTyped.Tensor.Type.Tensor weightsRequiresGradient vLayout vDevice vDataType vShape0) vGeneratorOutput, outputQueryShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.MathOperations.BlasLapack.MatmulF weightsShape (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF vShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, keySeqDim, headDim, headEmbedDim])))), Torch.GraduallyTyped.NN.Class.HasForward outProj (Torch.GraduallyTyped.Tensor.Type.Tensor weightsRequiresGradient (weightsLayout Torch.GraduallyTyped.Unify.<+> vLayout) (weightsDevice Torch.GraduallyTyped.Unify.<+> vDevice) (weightsDataType Torch.GraduallyTyped.Unify.<+> vDataType) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF outputQueryShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, querySeqDim, embedDim]))) vGeneratorOutput output generatorOutput, Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetShape keyShape, Torch.GraduallyTyped.Tensor.Type.SGetShape valueShape, batchDim GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.BatchDim queryShape keyShape valueShape, querySeqDim GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.QuerySeqDim queryShape, keySeqDim GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.KeySeqDim keyShape valueShape) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout) (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.Scaling, Torch.GraduallyTyped.Tensor.Type.Tensor queryRequiresGradient queryLayout queryDevice queryDataType queryShape, Torch.GraduallyTyped.Tensor.Type.Tensor keyRequiresGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor valueRequiresGradient valueLayout valueDevice valueDataType valueShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasRequiresGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) generator output generatorOutput
instance (Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.QInProjF style gradient device dataType queryEmbedDim embedDim) (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.KInProjF style gradient device dataType keyEmbedDim embedDim) (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.VInProjF style gradient device dataType valueEmbedDim embedDim) (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProjF style gradient device dataType embedDim queryEmbedDim) (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.DropoutF style dropoutP)) (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.Scaling, Torch.GraduallyTyped.Tensor.Type.Tensor queryRequiresGradient queryLayout queryDevice queryDataType queryShape, Torch.GraduallyTyped.Tensor.Type.Tensor keyRequiresGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor valueRequiresGradient valueLayout valueDevice valueDataType valueShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasRequiresGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> (keyLayout Torch.GraduallyTyped.Unify.<+> (attentionBiasLayout Torch.GraduallyTyped.Unify.<+> valueLayout)))) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> (generatorDevice Torch.GraduallyTyped.Unify.<+> valueDevice))))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> (keyDataType Torch.GraduallyTyped.Unify.<+> (attentionBiasDataType Torch.GraduallyTyped.Unify.<+> valueDataType)))) outputShape, generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryRequiresGradient queryLayout queryDevice queryDataType queryShape, Torch.GraduallyTyped.Tensor.Type.Tensor keyRequiresGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor valueRequiresGradient valueLayout valueDevice valueDataType valueShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasRequiresGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (qInProj GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.QInProjF style gradient device dataType queryEmbedDim embedDim, Torch.GraduallyTyped.NN.Class.HasInitialize qInProj (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim) generator generator', kInProj GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.KInProjF style gradient device dataType keyEmbedDim embedDim, Torch.GraduallyTyped.NN.Class.HasInitialize kInProj (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim) generator' generator'', vInProj GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.VInProjF style gradient device dataType valueEmbedDim embedDim, Torch.GraduallyTyped.NN.Class.HasInitialize vInProj (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim valueEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim) generator'' generator''', outProj GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.OutProjF style gradient device dataType embedDim queryEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize outProj (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim) generator''' generator'''', dropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.DropoutF style dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize dropout dropoutP generator'''' generator'''') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim valueEmbedDim, dropoutP) generator generator''''
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim valueEmbedDim, dropoutP)

module Torch.GraduallyTyped.NN.Transformer.SelfAttention

-- | Generic self-attention layer. Needs to be specialized to a given
--   transformer type, e.g. <a>T5</a>. See <a>SelfAttention</a>.
data GSelfAttention (mha :: Type) (layerNorm :: Type) (dropout :: Type)
[GSelfAttention] :: forall mha layerNorm dropout. {saMultiHeadAttention :: mha " self-attention", saLayerNorm :: layerNorm " layer norm", saDropout :: dropout " dropout"} -> GSelfAttention mha layerNorm dropout

-- | Self-attention layer.
newtype SelfAttention (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[SelfAttention] :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP. GSelfAttention (SAMultiheadAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (SALayerNormF style gradient device dataType queryEmbedDim) (SADropoutF style dropoutP) -> SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP
type family SAMultiheadAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type) :: Type
type family SALayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family SADropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type
testSA :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") ('Size 4), 'Dim ('Name "*") ('Size 512)]))
instance (Torch.GraduallyTyped.Scalar.Scalar dropoutP, multiHeadAttention GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.SelfAttention.SAMultiheadAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize multiHeadAttention (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, dropoutP) generator generator', layerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.SelfAttention.SALayerNormF style gradient device dataType queryEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]), GHC.Types.Double) generator' generator', dropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.SelfAttention.SADropoutF style dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize dropout dropoutP generator' generator') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, dropoutP, GHC.Types.Double) generator generator'
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, dropoutP, GHC.Types.Double)
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SALayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType queryEmbedDim) query generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SAMultiheadAttentionF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (layerNormOutput, layerNormOutput, layerNormOutput, attentionBias) generator mhaOutput mhaGeneratorOutput, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient0 layout0 device0 dataType0 shape0, mhaOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient1 layout1 device1 dataType1 shape1, mhaGeneratorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator device2, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient0 Torch.GraduallyTyped.Unify.<|> gradient1) (layout0 Torch.GraduallyTyped.Unify.<+> layout1) (device0 Torch.GraduallyTyped.Unify.<+> (device1 Torch.GraduallyTyped.Unify.<+> device2)) (dataType0 Torch.GraduallyTyped.Unify.<+> dataType1) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape0 shape1), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device1 Torch.GraduallyTyped.Unify.<+> device2)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, attentionBias) generator output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SALayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType queryEmbedDim) query generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SAMultiheadAttentionF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (layerNormOutput, layerNormOutput, layerNormOutput, attentionBias) generator mhaOutput mhaGeneratorOutput, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient0 layout0 device0 dataType0 shape0, mhaOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient1 layout1 device1 dataType1 shape1, mhaGeneratorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator device2, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient0 Torch.GraduallyTyped.Unify.<|> gradient1) (layout0 Torch.GraduallyTyped.Unify.<+> layout1) (device0 Torch.GraduallyTyped.Unify.<+> (device1 Torch.GraduallyTyped.Unify.<+> device2)) (dataType0 Torch.GraduallyTyped.Unify.<+> dataType1) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape0 shape1), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device1 Torch.GraduallyTyped.Unify.<+> device2)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, attentionBias) generator output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, attentionBias GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType headDim headEmbedDim embedDim queryEmbedDim queryEmbedDim queryEmbedDim dropoutP) (query, query, query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) mhaOutputShape) (Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape mhaOutputShape)), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, attentionBias GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention 'Torch.GraduallyTyped.NN.Transformer.Type.BERT gradient device dataType headDim headEmbedDim embedDim queryEmbedDim queryEmbedDim queryEmbedDim dropoutP) (query, query, query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) mhaOutputShape) (Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape mhaOutputShape)), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention 'Torch.GraduallyTyped.NN.Transformer.Type.BERT gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, attentionBias GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa gradient device dataType headDim headEmbedDim embedDim queryEmbedDim queryEmbedDim queryEmbedDim dropoutP) (query, query, query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) mhaOutputShape) (Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape mhaOutputShape)), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, attentionBias GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape, normedQueryGradient GHC.Types.~ (gradient Torch.GraduallyTyped.Unify.<|> queryGradient), normedQueryLayout GHC.Types.~ ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> queryLayout), normedQueryDevice GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> queryDevice), normedQueryDataType GHC.Types.~ (dataType Torch.GraduallyTyped.Unify.<+> queryDataType), normedQueryShape GHC.Types.~ Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) queryShape, normedQuery GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor normedQueryGradient normedQueryLayout normedQueryDevice normedQueryDataType normedQueryShape, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType headDim headEmbedDim embedDim queryEmbedDim queryEmbedDim queryEmbedDim dropoutP) (normedQuery, normedQuery, normedQuery, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) mhaOutputShape) (Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient)) (queryLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) (queryDataType Torch.GraduallyTyped.Unify.<+> (dataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape mhaOutputShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork

-- | Generic transformer feed-forward network. Needs to be specialized to a
--   given transformer type, e.g. <a>T5</a>. See
--   <a>TransformerFeedForwardNetwork</a>.
data GTransformerFeedForwardNetwork (inputWeight1 :: Type) (inputWeight2 :: Type) (outputWeight :: Type) (activation :: Type) (activationDropout :: Type) (layerNorm :: Type) (dropout :: Type)
[GTransformerFeedForwardNetwork] :: forall inputWeight1 inputWeight2 outputWeight activation activationDropout layerNorm dropout. {ffnInputWeight1 :: inputWeight1 " first input weight", ffnInputWeight2 :: inputWeight2 " second input weight", ffnOutputWeight :: outputWeight " output weight", ffnActivation :: activation " activation", ffnActivationDropout :: activationDropout " activation dropout", ffnLayerNorm :: layerNorm " feed-forward layer norm", ffnDropout :: dropout " feed-forward dropout"} -> GTransformerFeedForwardNetwork inputWeight1 inputWeight2 outputWeight activation activationDropout layerNorm dropout

-- | Transformer feed-forward network.
data TransformerFeedForwardNetwork (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerFeedForwardNetwork] :: forall style gradient device dataType queryEmbedDim ffnDim dropoutP. GTransformerFeedForwardNetwork (FFNInputWeight1F style gradient device dataType queryEmbedDim ffnDim) (FFNInputWeight2F style gradient device dataType queryEmbedDim ffnDim) (FFNOutputWeightF style gradient device dataType queryEmbedDim ffnDim) (FFNActivationF style) (FFNActivationDropoutF style dropoutP) (FFNLayerNormF style gradient device dataType queryEmbedDim) (FFNDropoutF style dropoutP) -> TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP
type family FFNInputWeight1F (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family FFNInputWeight2F (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family FFNOutputWeightF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family FFNActivationF (style :: TransformerStyle) :: Type
type family FFNActivationDropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type
type family FFNLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family FFNDropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type
type family HasInitializeFFNInputWeight2InputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeFFNActivationDropoutInputF (style :: TransformerStyle) (dropoutP :: Type) :: Type
type family FeedForwardNetworkOutputShape (style :: TransformerStyle) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (queryShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> gradient) (queryLayout Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> generatorDevice)) (queryDataType Torch.GraduallyTyped.Unify.<+> dataType) (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FeedForwardNetworkOutputShape 'Torch.GraduallyTyped.NN.Transformer.Type.T5 queryEmbedDim ffnDim queryShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> gradient) (queryLayout Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> generatorDevice)) (queryDataType Torch.GraduallyTyped.Unify.<+> dataType) (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FeedForwardNetworkOutputShape 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 queryEmbedDim ffnDim queryShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> queryGradient) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> queryLayout) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)) (dataType Torch.GraduallyTyped.Unify.<+> queryDataType) (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FeedForwardNetworkOutputShape 'Torch.GraduallyTyped.NN.Transformer.Type.BART queryEmbedDim ffnDim queryShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator ((device Torch.GraduallyTyped.Unify.<+> ((device Torch.GraduallyTyped.Unify.<+> queryDevice) Torch.GraduallyTyped.Unify.<+> generatorDevice)) Torch.GraduallyTyped.Unify.<+> ((device Torch.GraduallyTyped.Unify.<+> queryDevice) Torch.GraduallyTyped.Unify.<+> generatorDevice))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> queryGradient) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> queryLayout) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)) (dataType Torch.GraduallyTyped.Unify.<+> queryDataType) (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FeedForwardNetworkOutputShape 'Torch.GraduallyTyped.NN.Transformer.Type.BERT queryEmbedDim ffnDim queryShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator ((device Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> queryDevice)) Torch.GraduallyTyped.Unify.<+> generatorDevice)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork 'Torch.GraduallyTyped.NN.Transformer.Type.BERT gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> queryGradient) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> queryLayout) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)) (dataType Torch.GraduallyTyped.Unify.<+> queryDataType) (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FeedForwardNetworkOutputShape 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa queryEmbedDim ffnDim queryShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator ((device Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> queryDevice)) Torch.GraduallyTyped.Unify.<+> generatorDevice)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> gradient) (queryLayout Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> generatorDevice)) (queryDataType Torch.GraduallyTyped.Unify.<+> dataType) (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FeedForwardNetworkOutputShape 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus queryEmbedDim ffnDim queryShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Data.Singletons.Internal.SingI style, inputWeight1 GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNInputWeight1F style gradient device dataType queryEmbedDim ffnDim, Torch.GraduallyTyped.NN.Class.HasInitialize inputWeight1 (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim) generator generator', inputWeight2 GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNInputWeight2F style gradient device dataType queryEmbedDim ffnDim, Torch.GraduallyTyped.NN.Class.HasInitialize inputWeight2 (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.HasInitializeFFNInputWeight2InputF style gradient device dataType queryEmbedDim ffnDim) generator' generator'', outputWeight GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNOutputWeightF style gradient device dataType queryEmbedDim ffnDim, Torch.GraduallyTyped.NN.Class.HasInitialize outputWeight (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim) generator'' generator''', activation GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNActivationF style, Torch.GraduallyTyped.NN.Class.HasInitialize activation () generator''' generator''', activationDropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNActivationDropoutF style dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize activationDropout (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.HasInitializeFFNActivationDropoutInputF style dropoutP) generator''' generator''', layerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNLayerNormF style gradient device dataType queryEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]), GHC.Types.Double) generator''' generator''', dropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.FFNDropoutF style dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize dropout dropoutP generator''' generator''') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator generator'''
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double)

module Torch.GraduallyTyped.NN.Transformer.Block

-- | Transformer encoder block consisting of self-attention and a
--   feed-forward network.
--   
--   TODO: Some transformers use LayerDrop, see
--   <a>https://arxiv.org/abs/1909.11556</a>, during training. To support
--   this, we will need a layer wrapper that is either the identity
--   function or the wrapped layer based on a uniformly random draw from a
--   supplied generator. Complications will arise due to the gradual
--   typing...
data TransformerBlock (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerBlock] :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP. {tbSelfAttention :: SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP " self-attention layer", tbFeedForwardNetwork :: TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP " feed-forward network"} -> TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP
testBlock :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 17), 'Dim ('Name "*") ('Size 512)]))
instance (Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, dropoutP, GHC.Types.Double) generator generator', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator' generator'') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator generator''
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double)
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) input (Torch.GraduallyTyped.Random.Generator generatorDevice) selfAttentionOutput selfAttentionGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP) selfAttentionOutput selfAttentionGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) input (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.Stack

-- | Transformer encoder stack.
newtype TransformerStack (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerStack] :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP. Vector numLayers (TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) -> TransformerStack style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP
testStack :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 17), 'Dim ('Name "*") ('Size 512)]))
instance (GHC.TypeNats.KnownNat numLayers, Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) input generator generator', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) input generator' generator', numLayers' GHC.Types.~ (numLayers GHC.TypeNats.+ 1)) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Stack.TransformerStack style numLayers' gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) input generator generator'
instance (GHC.TypeNats.KnownNat numLayers, Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) input) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Stack.TransformerStack style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) input
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Stack.TransformerStack style 0 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (query, attentionBias) generator query generator
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (query, attentionBias) generator output generatorOutput => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Stack.TransformerStack style 1 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (query, attentionBias) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (query, attentionBias) generator output generatorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Block.TransformerBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (output, attentionBias) generatorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Stack.TransformerStack style n gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim dropoutP) (query, attentionBias) generator output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.Encoder

-- | Generic transformer encoder. Needs to be specialized to a given
--   transformer type, e.g. <a>T5</a>. See <a>TransformerEncoder</a>.
data GTransformerEncoder (stack :: Type) (embedLayerNorm :: Type) (layerNorm :: Type) (dropout :: Type) (posEnc :: Type)
[GTransformerEncoder] :: forall stack embedLayerNorm layerNorm dropout posEnc. {teStack :: stack " encoder layer stack", teEmbedLayerNorm :: embedLayerNorm " encoder embedding layer norm", teLayerNorm :: layerNorm " encoder layer norm", teDropout :: dropout " encoder dropout", tePosEnc :: posEnc " positional encoding"} -> GTransformerEncoder stack embedLayerNorm layerNorm dropout posEnc

-- | Transformer encoder.
newtype TransformerEncoder (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerEncoder] :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP. GTransformerEncoder (TEStackF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (TEEmbedLayerNormF style gradient device dataType inputEmbedDim) (TELayerNormF style gradient device dataType inputEmbedDim) (TEDropoutF style dropoutP) (TEPosEncF style gradient device dataType headDim inputEmbedDim posEncDim) -> TransformerEncoder style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP
type family TEStackF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type) :: Type
type family TEEmbedLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family TELayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family TEDropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type
type family TEPosEncF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeTEEmbedLayerNormInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeTELayerNormInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeTEPosEncInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type
testEncoder :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)]))
instance (Data.Singletons.Internal.SingI style, stack GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize stack (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator generator', embedLayerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Encoder.TEEmbedLayerNormF style gradient device dataType inputEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize embedLayerNorm (Torch.GraduallyTyped.NN.Transformer.Encoder.HasInitializeTEEmbedLayerNormInputF style gradient device dataType inputEmbedDim) generator' generator'', layerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Encoder.TELayerNormF style gradient device dataType inputEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm (Torch.GraduallyTyped.NN.Transformer.Encoder.HasInitializeTELayerNormInputF style gradient device dataType inputEmbedDim) generator'' generator''', dropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF style dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize dropout dropoutP generator''' generator''', posEnc GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Encoder.TEPosEncF style gradient device dataType headDim inputEmbedDim posEncDim, Torch.GraduallyTyped.NN.Class.HasInitialize posEnc (Torch.GraduallyTyped.NN.Transformer.Encoder.HasInitializeTEPosEncInputF style gradient device dataType headDim inputEmbedDim posEncDim) generator''' generator'''') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, dropoutP, GHC.Types.Double) generator generator''''
instance (Data.Singletons.Internal.SingI style, GHC.TypeNats.KnownNat numLayers) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, dropoutP, GHC.Types.Double)
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 dropoutP) input generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (dropoutOutput, Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (relPosGradient Torch.GraduallyTyped.Unify.<|> attentionMaskGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (relPosLayout Torch.GraduallyTyped.Unify.<+> attentionMaskLayout)) (device Torch.GraduallyTyped.Unify.<+> (relPosDevice Torch.GraduallyTyped.Unify.<+> attentionMaskDevice)) (Torch.GraduallyTyped.Prelude.Seq (relPosDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType Torch.GraduallyTyped.Unify.<+> attentionMaskDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, headDim]) relPosShape))) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape))) dropoutGeneratorOutput stackOutput stackGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TELayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType inputEmbedDim) stackOutput stackGeneratorOutput layerNormOutput layerNormGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 dropoutP) layerNormOutput layerNormGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder 'Torch.GraduallyTyped.NN.Transformer.Type.T5 numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (input, Torch.GraduallyTyped.Tensor.Type.Tensor relPosGradient relPosLayout relPosDevice relPosDataType relPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 dropoutP) input generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (dropoutOutput, Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (relPosGradient Torch.GraduallyTyped.Unify.<|> attentionMaskGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (relPosLayout Torch.GraduallyTyped.Unify.<+> attentionMaskLayout)) (device Torch.GraduallyTyped.Unify.<+> (relPosDevice Torch.GraduallyTyped.Unify.<+> attentionMaskDevice)) (Torch.GraduallyTyped.Prelude.Seq (relPosDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType Torch.GraduallyTyped.Unify.<+> attentionMaskDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, headDim]) relPosShape))) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape))) dropoutGeneratorOutput stackOutput stackGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TELayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType inputEmbedDim) stackOutput stackGeneratorOutput layerNormOutput layerNormGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 dropoutP) layerNormOutput layerNormGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (input, Torch.GraduallyTyped.Tensor.Type.Tensor relPosGradient relPosLayout relPosDevice relPosDataType relPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEEmbedLayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType inputEmbedDim) (Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> posGradient)) (inputLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> posLayout)) (inputDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> posDevice)) (inputDataType Torch.GraduallyTyped.Unify.<+> Torch.GraduallyTyped.Prelude.Seq (posDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF inputShape (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, inputEmbedDim]) posShape))) generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.BART dropoutP) layerNormOutput generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF 'Torch.GraduallyTyped.NN.Transformer.Type.BART numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (dropoutOutput, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape)) dropoutGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder 'Torch.GraduallyTyped.NN.Transformer.Type.BART numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.Tensor posGradient posLayout posDevice posDataType posShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEEmbedLayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.BERT gradient device dataType inputEmbedDim) (Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> posGradient)) (inputLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> posLayout)) (inputDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> posDevice)) (inputDataType Torch.GraduallyTyped.Unify.<+> Torch.GraduallyTyped.Prelude.Seq (posDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF inputShape (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, inputEmbedDim]) posShape))) generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.BERT dropoutP) layerNormOutput generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF 'Torch.GraduallyTyped.NN.Transformer.Type.BERT numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (dropoutOutput, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape)) dropoutGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder 'Torch.GraduallyTyped.NN.Transformer.Type.BERT numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.Tensor posGradient posLayout posDevice posDataType posShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEEmbedLayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa gradient device dataType inputEmbedDim) (Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> posGradient)) (inputLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> posLayout)) (inputDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> posDevice)) (inputDataType Torch.GraduallyTyped.Unify.<+> Torch.GraduallyTyped.Prelude.Seq (posDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF inputShape (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, inputEmbedDim]) posShape))) generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa dropoutP) layerNormOutput generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (dropoutOutput, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape)) dropoutGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.Tensor posGradient posLayout posDevice posDataType posShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> posGradient)) (inputLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> posLayout)) (inputDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> posDevice)) (inputDataType Torch.GraduallyTyped.Unify.<+> Torch.GraduallyTyped.Prelude.Seq (posDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF inputShape (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, inputEmbedDim]) posShape))) generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TEStackF 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim dropoutP) (dropoutOutput, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape)) dropoutGeneratorOutput stackOutput generatorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TELayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType inputEmbedDim) stackOutput generatorOutput output generatorOutput, GHC.Show.Show output) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.Tensor posGradient posLayout posDevice posDataType posShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generator output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.EncoderOnly

-- | Generic encoder-only transformer model.
data GEncoderOnlyTransformer (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoder :: Type) (encoderEmbedding :: Type) (encoderTypeEmbedding :: Type)
[GEncoderOnlyTransformer] :: forall inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding. {eoInputEmbedDim :: SDim inputEmbedDim " input embedding dim for scaling", eoEncoder :: encoder " encoder", eoEmbedding :: encoderEmbedding " encoder embedding", eoTypeEmbedding :: encoderTypeEmbedding " encoder type embedding"} -> GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding

-- | Encoder-only transformer model.
data EncoderOnlyTransformer (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[EncoderOnlyTransformer] :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP. GEncoderOnlyTransformer inputEmbedDim (EOEncoderF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (EOEmbeddingF style gradient device dataType inputEmbedDim vocabDim) (EOTypeEmbeddingF style gradient device dataType inputEmbedDim typeVocabDim) -> EncoderOnlyTransformer style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP
type family EOEncoderF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type) :: Type
type family EOEmbeddingF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family EOTypeEmbeddingF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
data GEncoderOnlyTransformerWithLMHead (transformer :: Type) (lmHead :: Type)
[GEncoderOnlyTransformerWithLMHead] :: forall transformer lmHead. {eoTransformer :: transformer " encoder-only transformer", eoLMHead :: lmHead " language modelling head"} -> GEncoderOnlyTransformerWithLMHead transformer lmHead

-- | Encoder-only transformer model with language modelling head.
data EncoderOnlyTransformerWithLMHead (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[EncoderOnlyTransformerWithLMHead] :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP. GEncoderOnlyTransformerWithLMHead (EOTransformerF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP) (EOLMHeadF style gradient device dataType inputEmbedDim vocabDim) -> EncoderOnlyTransformerWithLMHead style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP
type family EOTransformerF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type) :: Type
type family EOLMHeadF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Input data type for use with an encoder-only transformer.
data EncoderOnlyTransformerInput input inputType pos attentionMask
[EncoderOnlyTransformerInput] :: forall input inputType pos attentionMask. {eoInput :: input, eoInputType :: inputType, eoPos :: pos, eoAttentionMask :: attentionMask} -> EncoderOnlyTransformerInput input inputType pos attentionMask

-- | Output data type for use with an encoder-only transformer.
data EncoderOnlyTransformerOutput encoderOutput
[EncoderOnlyTransformerOutput] :: forall encoderOutput. {eoEncoderOutput :: encoderOutput} -> EncoderOnlyTransformerOutput encoderOutput
instance (GHC.Show.Show input, GHC.Show.Show inputType, GHC.Show.Show pos, GHC.Show.Show attentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask)
instance GHC.Show.Show encoderOutput => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerOutput encoderOutput)
instance (Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EOEmbeddingF style gradient device dataType inputEmbedDim vocabDim) input generator embeddingOutput embeddingGeneratorOutput, embeddingOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EOTypeEmbeddingF style gradient device dataType inputEmbedDim typeVocabDim) inputType embeddingGeneratorOutput typeEmbeddingOutput typeEmbeddingGeneratorOutput, typeEmbeddingOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient'' layout'' device'' dataType'' shape'', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EOEncoderF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient' Torch.GraduallyTyped.Unify.<|> gradient'') (layout' Torch.GraduallyTyped.Unify.<+> layout'') (device' Torch.GraduallyTyped.Unify.<+> device'') (dataType' Torch.GraduallyTyped.Unify.<+> dataType'') (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape' shape''), pos, attentionMask) typeEmbeddingGeneratorOutput encoderOutput generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformer style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP) (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask) generator (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerOutput encoderOutput) generatorOutput
instance (Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EOTransformerF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP) input generator eoOutput eoGeneratorOutput, eoOutput GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerOutput encoderOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EOLMHeadF style gradient device dataType inputEmbedDim vocabDim) encoderOutput eoGeneratorOutput lmHeadOutput generatorOutput, output GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerOutput lmHeadOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerWithLMHead style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP) input generator output generatorOutput
instance (Data.Singletons.Internal.SingI style, GHC.TypeNats.KnownNat numLayers) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformerWithLMHead style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.Shape.Type.SDim typeVocabDim, dropoutP, GHC.Types.Double)
instance (Data.Singletons.Internal.SingI style, GHC.TypeNats.KnownNat numLayers) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.EncoderOnly.EncoderOnlyTransformer style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.Shape.Type.SDim typeVocabDim, dropoutP, GHC.Types.Double)

module Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common

-- | RoBERTa dType.
type RoBERTaDType = 'Float

-- | RoBERTa dType singleton.
robertaDType :: SDType RoBERTaDType

-- | RoBERTa data type.
type RoBERTaDataType = 'DataType RoBERTaDType

-- | RoBERTa data type singleton.
robertaDataType :: SDataType RoBERTaDataType

-- | RoBERTa dropout probability type.
type RoBERTaDropoutP = Float

-- | RoBERTa dropout rate. 'dropout_rate = 0.1'
robertaDropoutP :: RoBERTaDropoutP

-- | RoBERTa positional encoding dimension.
--   
--   Note the two extra dimensions.
type RoBERTaPosEncDim = 'Dim ('Name "*") ('Size 514)

-- | RoBERTa positional encoding dimension singleton.
robertaPosEncDim :: SDim RoBERTaPosEncDim

-- | RoBERTa layer-norm epsilon. 'layer_norm_epsilon = 1e-5'
robertaEps :: Double

-- | RoBERTa maximum number of position embeddings.
--   'max_position_embeddings = 514'
robertaMaxPositionEmbeddings :: Int

-- | RoBERTa padding token id. 'pad_token_id = 1'
robertaPadTokenId :: Int

-- | RoBERTa begin-of-sentence token id. 'bos_token_id = 0'
robertaBOSTokenId :: Int

-- | RoBERTa end-of-sentence token id. 'eos_token_id = 0'
robertaEOSTokenId :: Int

-- | RoBERTa attention mask bias
robertaAttentionMaskBias :: Double
data GRoBERTaModel (robertaModel :: Type)
[GRoBERTaModel] :: forall robertaModel. {robertaModel :: robertaModel} -> GRoBERTaModel robertaModel

-- | RoBERTa model.
newtype RoBERTaModel (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat))
[RoBERTaModel] :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim. GRoBERTaModel (RoBERTaModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim) -> RoBERTaModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim
type family RoBERTaModelF (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
mkRoBERTaInput :: forall batchDim seqDim m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, 'Shape '[batchDim, seqDim] ~ Seq ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]) ('Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> [[Int]] -> m output
mkRoBERTaPaddingMask :: forall gradient layout device dataType shape output. MkTransformerPaddingMaskC layout device dataType shape output => Tensor gradient layout device dataType shape -> output
data RoBERTaInput input
[RoBERTaInput] :: forall input. {robertaInput :: input} -> RoBERTaInput input
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim)
instance GHC.Show.Show input => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaInput input)
instance (Data.Singletons.Internal.SingI headDim, Data.Singletons.Internal.SingI headEmbedDim, Data.Singletons.Internal.SingI embedDim, Data.Singletons.Internal.SingI inputEmbedDim, Data.Singletons.Internal.SingI ffnDim, Data.Singletons.Internal.SingI vocabDim, Data.Singletons.Internal.SingI typeVocabDim, GHC.TypeNats.KnownNat numLayers, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaDataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaPosEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.Shape.Type.SDim typeVocabDim, Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaDropoutP, GHC.Types.Double)) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common.RoBERTaModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device)

module Torch.GraduallyTyped.NN.Transformer.RoBERTa.Base

-- | RoBERTa-Base number of layers. 'num_hidden_layers = 12'
type RoBERTaBaseNumLayers = 12

-- | RoBERTa-Base number of attention heads. 'num_attention_heads = 12'
type RoBERTaBaseHeadDim = 'Dim ('Name "*") ('Size 12)

-- | RoBERTa-Base head embedding dimension. 'd_kv = 64'
type RoBERTaBaseHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | RoBERTa-Base embedding dimension. 'hidden_size = n_heads * d_kv = 768'
type RoBERTaBaseEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | RoBERTa-Base model dimension. 'hidden_size = 768'
type RoBERTaBaseInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | RoBERTa-Base feed-forward network dimension. 'intermediate_size =
--   3072'
type RoBERTaBaseFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | RoBERTa-Base vocabulary dimension. 'vocab_size = 50265'
type RoBERTaBaseVocabDim = 'Dim ('Name "*") ('Size 50265)

-- | RoBERTa-Base type vocabulary dimension. 'type_vocab_size = 1'
type RoBERTaBaseTypeVocabDim = 'Dim ('Name "*") ('Size 1)

-- | RoBERTa-Base model.
type RoBERTaBase (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = RoBERTaModel transformerHead RoBERTaBaseNumLayers gradient device RoBERTaBaseHeadDim RoBERTaBaseHeadEmbedDim RoBERTaBaseEmbedDim RoBERTaBaseInputEmbedDim RoBERTaBaseFFNDim RoBERTaBaseVocabDim RoBERTaBaseTypeVocabDim

module Torch.GraduallyTyped.NN.Transformer.RoBERTa
testForwardRoBERTaBase :: IO ()
testRoBERTaInput :: IO _

module Torch.GraduallyTyped.NN.Transformer.BERT.Common

-- | BERT dType.
type BERTDType = 'Float

-- | BERT dType singleton.
bertDType :: SDType BERTDType

-- | BERT data type.
type BERTDataType = 'DataType BERTDType

-- | BERT data type singleton.
bertDataType :: SDataType BERTDataType

-- | BERT dropout probability type.
type BERTDropoutP = Float

-- | BERT dropout rate. 'dropout_rate = 0.1'
bertDropoutP :: BERTDropoutP

-- | BERT positional encoding dimension.
type BERTPosEncDim = 'Dim ('Name "*") ('Size 512)

-- | BERT positional encoding dimension singleton.
bertPosEncDim :: SDim BERTPosEncDim

-- | BERT layer-norm epsilon. 'layer_norm_epsilon = 1e-12'
bertEps :: Double

-- | BERT maximum number of position embeddings. 'max_position_embeddings =
--   512'
bertMaxPositionEmbeddings :: Int

-- | BERT padding token id. 'pad_token_id = 0'
bertPadTokenId :: Int

-- | BERT attention mask bias
bertAttentionMaskBias :: Double
data GBERTModel (bertModel :: Type)
[GBERTModel] :: forall bertModel. {bertModel :: bertModel} -> GBERTModel bertModel

-- | BERT model.
newtype BERTModel (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat))
[BERTModel] :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim. GBERTModel (BERTModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim) -> BERTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim
type family BERTModelF (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
mkBERTInput :: forall batchDim seqDim m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, 'Shape '[batchDim, seqDim] ~ Seq ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]) ('Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> [[Int]] -> m output
mkBERTPaddingMask :: forall gradient layout device dataType shape output. MkTransformerPaddingMaskC layout device dataType shape output => Tensor gradient layout device dataType shape -> output
data BERTInput input
[BERTInput] :: forall input. {bertInput :: input} -> BERTInput input
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim)
instance GHC.Show.Show input => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTInput input)
instance (Data.Singletons.Internal.SingI headDim, Data.Singletons.Internal.SingI headEmbedDim, Data.Singletons.Internal.SingI embedDim, Data.Singletons.Internal.SingI inputEmbedDim, Data.Singletons.Internal.SingI ffnDim, Data.Singletons.Internal.SingI vocabDim, Data.Singletons.Internal.SingI typeVocabDim, GHC.TypeNats.KnownNat numLayers, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTDataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTPosEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.Shape.Type.SDim typeVocabDim, Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTDropoutP, GHC.Types.Double)) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.BERT.Common.BERTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device)

module Torch.GraduallyTyped.NN.Transformer.BERT.BaseUncased

-- | BERT-Base-Uncased number of layers. 'num_hidden_layers = 12'
type BERTBaseUncasedNumLayers = 12

-- | BERT-Base-Uncased number of attention heads. 'num_attention_heads =
--   12'
type BERTBaseUncasedHeadDim = 'Dim ('Name "*") ('Size 12)

-- | BERT-Base-Uncased number of attention heads singleton.
bertBaseUncasedHeadDim :: SDim BERTBaseUncasedHeadDim

-- | BERT-Base-Uncased head embedding dimension. 'd_kv = 64'
type BERTBaseUncasedHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | BERT-Base-Uncased head embedding dimension singleton.
bertBaseUncasedHeadEmbedDim :: SDim BERTBaseUncasedHeadEmbedDim

-- | BERT-Base-Uncased embedding dimension. 'hidden_size = n_heads * d_kv =
--   768'
type BERTBaseUncasedEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BERT-Base-Uncased embedding dimension singleton.
bertBaseUncasedEmbedDim :: SDim BERTBaseUncasedEmbedDim

-- | BERT-Base-Uncased model dimension. 'hidden_size = 768'
type BERTBaseUncasedInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BERT-Base-Uncased model dimension singleton.
bertBaseUncasedInputEmbedDim :: SDim BERTBaseUncasedInputEmbedDim

-- | BERT-Base-Uncased feed-forward network dimension. 'intermediate_size =
--   3072'
type BERTBaseUncasedFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | BERT-Base-Uncased feed-forward network dimension singleton.
bertBaseUncasedFFNDim :: SDim BERTBaseUncasedFFNDim

-- | BERT-Base-Uncased vocabulary dimension. 'vocab_size = 30522'
type BERTBaseUncasedVocabDim = 'Dim ('Name "*") ('Size 30522)

-- | BERT-Base-Uncased vocabulary dimension singleton.
bertBaseUncasedVocabDim :: SDim BERTBaseUncasedVocabDim

-- | BERT-Base-Uncased type vocabulary dimension. 'type_vocab_size = 2'
type BERTBaseUncasedTypeVocabDim = 'Dim ('Name "*") ('Size 2)

-- | BERT-Base-Uncased type vocabulary dimension singleton.
bertBaseUncasedTypeVocabDim :: SDim BERTBaseUncasedTypeVocabDim

-- | BERT-Base-Uncased model.
type BERTBaseUncased (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = BERTModel transformerHead BERTBaseUncasedNumLayers gradient device BERTBaseUncasedHeadDim BERTBaseUncasedHeadEmbedDim BERTBaseUncasedEmbedDim BERTBaseUncasedInputEmbedDim BERTBaseUncasedFFNDim BERTBaseUncasedVocabDim BERTBaseUncasedTypeVocabDim

module Torch.GraduallyTyped.NN.Transformer.BERT
testForwardBERTBaseUncased :: IO ()

module Torch.GraduallyTyped.NN.Transformer.CrossAttention

-- | Generic cross-attention layer. Needs to be specialized to a given
--   transformer type, e.g. <a>T5</a>. See <a>CrossAttention</a>.
data GCrossAttention (mha :: Type) (layerNorm :: Type) (dropout :: Type)
[GCrossAttention] :: forall mha layerNorm dropout. {caMultiHeadAttention :: mha " cross-attention", caLayerNorm :: layerNorm " layer norm", caDropout :: dropout " dropout"} -> GCrossAttention mha layerNorm dropout

-- | Cross-attention layer.
newtype CrossAttention (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[CrossAttention] :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP. GCrossAttention (CAMultiheadAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (CALayerNormF style gradient device dataType queryEmbedDim) (CADropoutF style dropoutP) -> CrossAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP
type family CAMultiheadAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type) :: Type
type family CALayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family CADropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type
testCA :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 4), 'Dim ('Name "*") ('Size 512)]))
instance (Torch.GraduallyTyped.Scalar.Scalar dropoutP, multiHeadAttention GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.CrossAttention.CAMultiheadAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize multiHeadAttention (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, dropoutP) generator generator', layerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.CrossAttention.CALayerNormF style gradient device dataType queryEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SShape ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]), GHC.Types.Double) generator' generator', dropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.CrossAttention.CADropoutF style dropoutP) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, dropoutP, GHC.Types.Double) generator generator'
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, dropoutP, GHC.Types.Double)
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CALayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType queryEmbedDim) query generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CAMultiheadAttentionF 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (layerNormOutput, key, key, attentionBias) generator mhaOutput mhaGeneratorOutput, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient0 layout0 device0 dataType0 shape0, mhaOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient1 layout1 device1 dataType1 shape1, mhaGeneratorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator device2, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient0 Torch.GraduallyTyped.Unify.<|> gradient1) (layout0 Torch.GraduallyTyped.Unify.<+> layout1) (device0 Torch.GraduallyTyped.Unify.<+> (device1 Torch.GraduallyTyped.Unify.<+> device2)) (dataType0 Torch.GraduallyTyped.Unify.<+> dataType1) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape0 shape1), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device1 Torch.GraduallyTyped.Unify.<+> device2)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention 'Torch.GraduallyTyped.NN.Transformer.Type.T5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (query, key, attentionBias) generator output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CALayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType queryEmbedDim) query generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CAMultiheadAttentionF 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (layerNormOutput, key, key, attentionBias) generator mhaOutput mhaGeneratorOutput, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient0 layout0 device0 dataType0 shape0, mhaOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient1 layout1 device1 dataType1 shape1, mhaGeneratorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator device2, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient0 Torch.GraduallyTyped.Unify.<|> gradient1) (layout0 Torch.GraduallyTyped.Unify.<+> layout1) (device0 Torch.GraduallyTyped.Unify.<+> (device1 Torch.GraduallyTyped.Unify.<+> device2)) (dataType0 Torch.GraduallyTyped.Unify.<+> dataType1) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape0 shape1), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device1 Torch.GraduallyTyped.Unify.<+> device2)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (query, key, attentionBias) generator output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, key GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor keyGradient keyLayout keyDevice keyDataType keyShape, attentionBias GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim keyEmbedDim dropoutP) (query, key, key, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> (keyGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient))) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> (keyLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout))) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> (keyDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType))) mhaOutputShape) (Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))))), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (queryGradient Torch.GraduallyTyped.Unify.<|> (keyGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient))) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (queryLayout Torch.GraduallyTyped.Unify.<+> (keyLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout))) (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) (dataType Torch.GraduallyTyped.Unify.<+> (queryDataType Torch.GraduallyTyped.Unify.<+> (keyDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType))) (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape mhaOutputShape)), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (device Torch.GraduallyTyped.Unify.<+> (queryDevice Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (query, key, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim queryEmbedDim, Torch.GraduallyTyped.Scalar.Scalar dropoutP, query GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, key GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor keyGradient keyLayout keyDevice keyDataType keyShape, attentionBias GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape, normedQueryGradient GHC.Types.~ (gradient Torch.GraduallyTyped.Unify.<|> queryGradient), normedQueryLayout GHC.Types.~ ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> queryLayout), normedQueryDevice GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> queryDevice), normedQueryDataType GHC.Types.~ (dataType Torch.GraduallyTyped.Unify.<+> queryDataType), normedQueryShape GHC.Types.~ Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[queryEmbedDim]) queryShape, normedQuery GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor normedQueryGradient normedQueryLayout normedQueryDevice normedQueryDataType normedQueryShape, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.MultiHeadAttention.MultiHeadAttention 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim keyEmbedDim dropoutP) (normedQuery, key, key, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) (Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> (keyGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient))) (queryLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (keyLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout))) (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) (queryDataType Torch.GraduallyTyped.Unify.<+> (dataType Torch.GraduallyTyped.Unify.<+> (keyDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType))) mhaOutputShape) (Torch.GraduallyTyped.Random.Generator (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))))), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> (keyGradient Torch.GraduallyTyped.Unify.<|> attentionBiasGradient))) (queryLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (keyLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout))) (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice)))) (queryDataType Torch.GraduallyTyped.Unify.<+> (dataType Torch.GraduallyTyped.Unify.<+> (keyDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType))) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape mhaOutputShape), generatorOutput GHC.Types.~ Torch.GraduallyTyped.Random.Generator (queryDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> (keyDevice Torch.GraduallyTyped.Unify.<+> (attentionBiasDevice Torch.GraduallyTyped.Unify.<+> generatorDevice))))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (query, key, attentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.DecoderBlock

-- | Transformer decoder block consisting of self-attention,
--   cross-attention, and a feed-forward network.
data TransformerDecoderBlock (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerDecoderBlock] :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP. {tdbSelfAttention :: SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP " self-attention layer", tdbCrossAttention :: CrossAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP " cross-attention layer", tdbFeedForwardNetwork :: TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP " feed-forward network"} -> TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP
testDecoderBlock :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)]))
instance (Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, dropoutP, GHC.Types.Double) generator generator', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, dropoutP, GHC.Types.Double) generator' generator'', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator'' generator''') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator generator'''
instance Data.Singletons.Internal.SingI style => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim queryEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim keyEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double)
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SelfAttention.SelfAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim dropoutP) (query, decoderAttentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) selfAttentionOutput selfAttentionGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.CrossAttention.CrossAttention style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim dropoutP) (selfAttentionOutput, key, crossAttentionBias) selfAttentionGeneratorOutput crossAttentionOutput crossAttentionGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.FeedForwardNetwork.TransformerFeedForwardNetwork style gradient device dataType queryEmbedDim ffnDim dropoutP) crossAttentionOutput crossAttentionGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (query, key, decoderAttentionBias, crossAttentionBias) (Torch.GraduallyTyped.Random.Generator generatorDevice) output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.DecoderStack

-- | Transformer decoder stack.
data TransformerDecoderStack (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerDecoderStack] :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP. Vector numLayers (TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) -> TransformerDecoderStack style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP
testDecoderStack :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)]))
instance (GHC.TypeNats.KnownNat numLayers, Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) input generator generator', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) input generator' generator', numLayers' GHC.Types.~ (numLayers GHC.TypeNats.+ 1)) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack style numLayers' gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) input generator generator'
instance (GHC.TypeNats.KnownNat numLayers, Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) input) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) input
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack style 0 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (query, key, decoderAttentionBias, crossAttentionBias) generator query generator
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (query, key, decoderAttentionBias, crossAttentionBias) generator output generatorOutput => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack style 1 gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (query, key, decoderAttentionBias, crossAttentionBias) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (query, key, decoderAttentionBias, crossAttentionBias) generator output generatorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderBlock.TransformerDecoderBlock style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (output, key, decoderAttentionBias, crossAttentionBias) generatorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim dropoutP) (query, key, decoderAttentionBias, crossAttentionBias) generator output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.Decoder

-- | Generic transformer decoder. Needs to be specialized to a given
--   transformer type, e.g. <a>T5</a>. See <a>TransformerDecoder</a>.
data GTransformerDecoder (stack :: Type) (embedLayerNorm :: Type) (layerNorm :: Type) (dropout :: Type) (posEnc :: Type)
[GTransformerDecoder] :: forall stack embedLayerNorm layerNorm dropout posEnc. {tdStack :: stack " decoder layer stack", tdEmbedLayerNorm :: embedLayerNorm " decoder embedding layer norm", tdLayerNorm :: layerNorm " decoder layer norm", tdDropout :: dropout " decoder dropout", tdPosEnc :: posEnc " positional encoding"} -> GTransformerDecoder stack embedLayerNorm layerNorm dropout posEnc

-- | Transformer decoder.
newtype TransformerDecoder (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoderOutputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[TransformerDecoder] :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP. GTransformerDecoder (TDStackF style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim dropoutP) (TDEmbedLayerNormF style gradient device dataType decoderInputEmbedDim) (TDLayerNormF style gradient device dataType decoderInputEmbedDim) (TDDropoutF style dropoutP) (TDPosEncF style gradient device dataType headDim decoderInputEmbedDim posEncDim) -> TransformerDecoder style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP
type family TDStackF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoderOutputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type) :: Type
type family TDEmbedLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family TDLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family TDDropoutF (style :: TransformerStyle) (dropoutP :: Type) :: Type
type family TDPosEncF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeTDEmbedLayerNormInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeTDLayerNormInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeTDPosEncInputF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type
testDecoder :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7), 'Dim ('Name "*") ('Size 512)]))
instance (Data.Singletons.Internal.SingI style, stack GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Decoder.TDStackF style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize stack (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim decoderInputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim encoderOutputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, dropoutP, GHC.Types.Double) generator generator', embedLayerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Decoder.TDEmbedLayerNormF style gradient device dataType decoderInputEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize embedLayerNorm (Torch.GraduallyTyped.NN.Transformer.Decoder.HasInitializeTDEmbedLayerNormInputF style gradient device dataType decoderInputEmbedDim) generator' generator'', layerNorm GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Decoder.TDLayerNormF style gradient device dataType decoderInputEmbedDim, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm (Torch.GraduallyTyped.NN.Transformer.Decoder.HasInitializeTDLayerNormInputF style gradient device dataType decoderInputEmbedDim) generator'' generator''', dropout GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Decoder.TDDropoutF style dropoutP, Torch.GraduallyTyped.NN.Class.HasInitialize dropout dropoutP generator''' generator''', posEnc GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.Decoder.TDPosEncF style gradient device dataType headDim decoderInputEmbedDim posEncDim, Torch.GraduallyTyped.NN.Class.HasInitialize posEnc (Torch.GraduallyTyped.NN.Transformer.Decoder.HasInitializeTDPosEncInputF style gradient device dataType headDim decoderInputEmbedDim posEncDim) generator''' generator'''') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim decoderInputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim encoderOutputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, dropoutP, GHC.Types.Double) generator generator''''
instance (Data.Singletons.Internal.SingI style, GHC.TypeNats.KnownNat numLayers) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim decoderInputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim encoderOutputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, dropoutP, GHC.Types.Double)
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Dropout.Dropout dropoutP) decoderInput generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack 'Torch.GraduallyTyped.NN.Transformer.Type.T5 numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim dropoutP) (dropoutOutput, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (decoderRelPosGradient Torch.GraduallyTyped.Unify.<|> decoderAttentionMaskGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (decoderRelPosLayout Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskLayout)) (device Torch.GraduallyTyped.Unify.<+> (decoderRelPosDevice Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskDevice)) (Torch.GraduallyTyped.Prelude.Seq (decoderRelPosDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, headDim]) decoderRelPosShape))) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape)), Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape)) dropoutGeneratorOutput stackOutput stackGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[decoderInputEmbedDim])) stackOutput stackGeneratorOutput layerNormOutput layerNormGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Dropout.Dropout dropoutP) layerNormOutput layerNormGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder 'Torch.GraduallyTyped.NN.Transformer.Type.T5 numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP) (decoderInput, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor decoderRelPosGradient decoderRelPosLayout decoderRelPosDevice decoderRelPosDataType decoderRelPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType decoderAttentionMaskShape, Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType crossAttentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Dropout.Dropout dropoutP) decoderInput generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.DecoderStack.TransformerDecoderStack 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim dropoutP) (dropoutOutput, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> (decoderRelPosGradient Torch.GraduallyTyped.Unify.<|> decoderAttentionMaskGradient)) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> (decoderRelPosLayout Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskLayout)) (device Torch.GraduallyTyped.Unify.<+> (decoderRelPosDevice Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskDevice)) (Torch.GraduallyTyped.Prelude.Seq (decoderRelPosDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, headDim]) decoderRelPosShape))) (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape)), Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape)) dropoutGeneratorOutput stackOutput stackGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[decoderInputEmbedDim])) stackOutput stackGeneratorOutput layerNormOutput layerNormGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Dropout.Dropout dropoutP) layerNormOutput layerNormGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5 numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP) (decoderInput, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor decoderRelPosGradient decoderRelPosLayout decoderRelPosDevice decoderRelPosDataType decoderRelPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType decoderAttentionMaskShape, Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType crossAttentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TDEmbedLayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.BART gradient device dataType decoderInputEmbedDim) (Torch.GraduallyTyped.Tensor.Type.Tensor (decoderInputGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> decoderPosGradient)) (decoderInputLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> decoderPosLayout)) (decoderInputDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> decoderPosDevice)) (decoderInputDataType Torch.GraduallyTyped.Unify.<+> Torch.GraduallyTyped.Prelude.Seq (decoderPosDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF decoderInputShape (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, decoderInputEmbedDim]) decoderPosShape))) generator layerNormOutput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TDDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.BART dropoutP) layerNormOutput generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TDStackF 'Torch.GraduallyTyped.NN.Transformer.Type.BART numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim dropoutP) (dropoutOutput, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape), Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape)) dropoutGeneratorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder 'Torch.GraduallyTyped.NN.Transformer.Type.BART numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor decoderPosGradient decoderPosLayout decoderPosDevice decoderPosDataType decoderPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType decoderAttentionMaskShape, Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType crossAttentionMaskShape) generator output generatorOutput
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TDDropoutF 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor (decoderInputGradient Torch.GraduallyTyped.Unify.<|> (gradient Torch.GraduallyTyped.Unify.<|> decoderPosGradient)) (decoderInputLayout Torch.GraduallyTyped.Unify.<+> ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> decoderPosLayout)) (decoderInputDevice Torch.GraduallyTyped.Unify.<+> (device Torch.GraduallyTyped.Unify.<+> decoderPosDevice)) (decoderInputDataType Torch.GraduallyTyped.Unify.<+> Torch.GraduallyTyped.Prelude.Seq (decoderPosDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) dataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF decoderInputShape (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[posEncDim, decoderInputEmbedDim]) decoderPosShape))) generator dropoutOutput dropoutGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TDStackF 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim dropoutP) (dropoutOutput, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape), Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape)) dropoutGeneratorOutput stackOutput generatorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TDLayerNormF 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus gradient device dataType decoderInputEmbedDim) stackOutput generatorOutput output generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, encoderOutput, Torch.GraduallyTyped.Tensor.Type.Tensor decoderPosGradient decoderPosLayout decoderPosDevice decoderPosDataType decoderPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType decoderAttentionMaskShape, Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType crossAttentionMaskShape) generator output generatorOutput

module Torch.GraduallyTyped.NN.Transformer.SequenceToSequence
data GSequenceToSequenceTransformer (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoder :: Type) (decoder :: Type) (embedding :: Type) (head :: Type)
[GSequenceToSequenceTransformer] :: forall inputEmbedDim encoder decoder embedding head. {seqToSeqInputEmbedDim :: SDim inputEmbedDim " input embedding dim for scaling", seqToSeqEncoder :: encoder " encoder", seqToSeqDecoder :: decoder " decoder", seqToSeqEmbedding :: embedding " shared embedding", seqToSeqHead :: head " transformer head"} -> GSequenceToSequenceTransformer inputEmbedDim encoder decoder embedding head

-- | Sequence-to-sequence transformer model.
newtype SequenceToSequenceTransformer (style :: TransformerStyle) (transformerHead :: TransformerHead) (numEncoderLayers :: Nat) (numDecoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
[SequenceToSequenceTransformer] :: forall style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim dropoutP. GSequenceToSequenceTransformer inputEmbedDim (SequenceToSequenceEncoderF style numEncoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (SequenceToSequenceDecoderF style numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (SequenceToSequenceEmbeddingF style gradient device dataType inputEmbedDim vocabDim) (SequenceToSequenceHeadF style transformerHead gradient device dataType inputEmbedDim vocabDim) -> SequenceToSequenceTransformer style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim dropoutP
type family SequenceToSequenceEncoderF (style :: TransformerStyle) (numEncoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
type family SequenceToSequenceDecoderF (style :: TransformerStyle) (numDecoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (dropoutP :: Type)
type family SequenceToSequenceEmbeddingF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat))
type family SequenceToSequenceHeadF (style :: TransformerStyle) (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family HasInitializeSequenceToSequenceHeadInputF (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Input data type for use with a sequence-to-sequence transformer. Use
--   this for training.
data SequenceToSequenceTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask
[SequenceToSequenceTransformerInput] :: forall input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask. {input :: input, decoderInput :: decoderInput, pos :: pos, decoderPos :: decoderPos, attentionMask :: attentionMask, decoderAttentionMask :: decoderAttentionMask, crossAttentionMask :: crossAttentionMask} -> SequenceToSequenceTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask

-- | Output data type for use with a sequence-to-sequence transformer.
data SequenceToSequenceTransformerOutput decoderOutput encoderOutput
[SequenceToSequenceTransformerOutput] :: forall decoderOutput encoderOutput. {decoderOutput :: decoderOutput, encoderOutput :: encoderOutput} -> SequenceToSequenceTransformerOutput decoderOutput encoderOutput

-- | Input data type for use with a sequence-to-sequence transformer. Use
--   this for inference.
data SequenceToSequenceTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask
[SequenceToSequenceTransformerGenerationInput] :: forall decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask. {generationDecoderInput :: decoderInput, generationEncoderOutput :: encoderOutput, generationDecoderPos :: decoderPos, generationDecoderAttentionMask :: decoderAttentionMask, generationCrossAttentionMask :: crossAttentionMask} -> SequenceToSequenceTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask
testSeqToSeq :: IO ((SequenceToSequenceTransformerOutput (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)])), SequenceToSequenceTransformerOutput (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)]))), Generator ('Device ('CPU :: DeviceType Nat)))
instance (GHC.Show.Show input, GHC.Show.Show decoderInput, GHC.Show.Show pos, GHC.Show.Show decoderPos, GHC.Show.Show attentionMask, GHC.Show.Show decoderAttentionMask, GHC.Show.Show crossAttentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask)
instance (GHC.Show.Show decoderOutput, GHC.Show.Show encoderOutput) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput)
instance (GHC.Show.Show decoderInput, GHC.Show.Show encoderOutput, GHC.Show.Show decoderPos, GHC.Show.Show decoderAttentionMask, GHC.Show.Show crossAttentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask)
instance (Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceEmbeddingF style gradient device dataType inputEmbedDim vocabDim) decoderInput generator embeddingOutput' embeddingGeneratorOutput', embeddingOutput' GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient'' layout'' device'' dataType'' shape'', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceDecoderF style numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (embeddingOutput', encoderOutput, decoderPos, decoderAttentionMask, crossAttentionMask) embeddingGeneratorOutput' decoderOutput decoderGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceHeadF style transformerHead gradient device dataType inputEmbedDim vocabDim) decoderOutput decoderGeneratorOutput headOutput generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim dropoutP) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput headOutput encoderOutput) generatorOutput
instance (Data.Singletons.Internal.SingI style, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceEmbeddingF style gradient device dataType inputEmbedDim vocabDim) input generator embeddingOutput embeddingGeneratorOutput, embeddingOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient' layout' device' dataType' shape', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceEncoderF style numEncoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (embeddingOutput, pos, attentionMask) embeddingGeneratorOutput encoderOutput encoderGeneratorOutput, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceEmbeddingF style gradient device dataType inputEmbedDim vocabDim) decoderInput encoderGeneratorOutput embeddingOutput' embeddingGeneratorOutput', embeddingOutput' GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient'' layout'' device'' dataType'' shape'', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceDecoderF style numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (embeddingOutput', encoderOutput, decoderPos, decoderAttentionMask, crossAttentionMask) embeddingGeneratorOutput' decoderOutput generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim dropoutP) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput
instance (Data.Singletons.Internal.SingI transformerHead, Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Encoder.TransformerEncoder style numEncoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, dropoutP, GHC.Types.Double) generator generator', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Decoder.TransformerDecoder style numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim inputEmbedDim ffnDim posEncDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, dropoutP, GHC.Types.Double) generator' generator'', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Sparse.Embedding gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType vocabDim inputEmbedDim 'GHC.Maybe.Nothing) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Layout.SLayout ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense), Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim) generator'' generator''', Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceHeadF style transformerHead gradient device dataType inputEmbedDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.HasInitializeSequenceToSequenceHeadInputF transformerHead gradient device dataType inputEmbedDim vocabDim) generator''' generator'''') => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, dropoutP, GHC.Types.Double) generator generator''''
instance (Data.Singletons.Internal.SingI style, Data.Singletons.Internal.SingI transformerHead, GHC.TypeNats.KnownNat numEncoderLayers, GHC.TypeNats.KnownNat numDecoderLayers) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim dropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType dataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim posEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, dropoutP, GHC.Types.Double)

module Torch.GraduallyTyped.NN.Transformer.T5.Common

-- | T5 dType.
type T5DType = 'Float

-- | T5 dType singleton.
t5DType :: SDType T5DType

-- | T5 data type.
type T5DataType = 'DataType T5DType

-- | T5 data type singleton.
t5DataType :: SDataType T5DataType

-- | T5 dropout probability type.
type T5DropoutP = Float

-- | T5 dropout rate. 'dropout_rate = 0.1'
t5DropoutP :: T5DropoutP

-- | T5 relative positional encoding bucket dimension.
--   'relative_attention_num_buckets = 32'
type T5RelPosEncBucketDim = 'Dim ('Name "*") ('Size 32)

-- | T5 relative positional encoding bucket dimension singleton.
t5RelPosEncBucketDim :: SDim T5RelPosEncBucketDim

-- | T5 layer-norm epsilon. 'layer_norm_epsilon = 1e-06'
t5Eps :: Double

-- | T5 maximum distance for relative positional encoding.
t5MaxDistance :: Int

-- | T5 padding token id. 'pad_token_id = 0'
t5PadTokenId :: Int

-- | T5 begin-of-sentence token id.
t5BOSTokenId :: Int

-- | T5 end-of-sentence token id. 'eos_token_id = 1'
t5EOSTokenId :: Int

-- | T5 attention mask bias
t5AttentionMaskBias :: Double
data GT5Model (t5Model :: Type)
[GT5Model] :: forall t5Model. {t5Model :: t5Model, t5ShiftRightDecoderInput :: ShiftRight Int, t5ShiftRightPaddingMask :: ShiftRight Int} -> GT5Model t5Model

-- | T5 model.
data T5Model (style :: TransformerStyle) (transformerHead :: TransformerHead) (numEncoderLayers :: Nat) (numDecoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat))
[T5Model] :: forall style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim. GT5Model (T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) -> T5Model style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim
type family T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim = r | r -> style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim
mkT5Input :: forall batchDim seqDim m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, 'Shape '[batchDim, seqDim] ~ Seq ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]) ('Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> [[Int]] -> m output
mkT5PaddingMask :: forall gradient layout device dataType shape output. MkTransformerPaddingMaskC layout device dataType shape output => Tensor gradient layout device dataType shape -> output
mkT5RelPos' :: Int -> Int -> Int -> Int -> [[Int]]
type MkT5RelPosC device shape seqDim seqName seqSize output = (SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), seqDim ~ 'Dim seqName seqSize, 'Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") seqSize, 'Dim ('Name "*") seqSize] ~ Seq ('[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> '[ 'Dim ('Name "*") seqSize, 'Dim ('Name "*") seqSize]) ('Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") seqSize, 'Dim ('Name "*") seqSize]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") seqSize, 'Dim ('Name "*") seqSize]))
mkT5RelPos :: forall m gradient layout device dataType shape seqDim seqName seqSize output. (MonadThrow m, SingI device, MkT5RelPosC device shape seqDim seqName seqSize output) => Tensor gradient layout device dataType shape -> m output
mkT5DecoderRelPos' :: Int -> Int -> Int -> Int -> [[Int]]
mkT5DecoderRelPos :: forall m gradient layout device dataType shape seqDim seqName seqSize output. (MonadThrow m, SingI device, MkT5RelPosC device shape seqDim seqName seqSize output) => Tensor gradient layout device dataType shape -> m output
data T5Input input decoderInput
[T5Input] :: forall input decoderInput. {t5Input :: input, t5DecoderInput :: decoderInput} -> T5Input input decoderInput
data T5Output decoderOutput encoderOutput inputPaddingMask
[T5Output] :: forall decoderOutput encoderOutput inputPaddingMask. {t5DecoderOutput :: decoderOutput, t5EncoderOutput :: encoderOutput, t5InputPaddingMask :: inputPaddingMask} -> T5Output decoderOutput encoderOutput inputPaddingMask
data T5GenerationInput decoderInput encoderOutput inputPaddingMask
[T5GenerationInput] :: forall decoderInput encoderOutput inputPaddingMask. {t5GenerationDecoderInput :: decoderInput, t5GenerationEncoderOutput :: encoderOutput, t5GenerationInputPaddingMask :: inputPaddingMask} -> T5GenerationInput decoderInput encoderOutput inputPaddingMask
testT5 :: IO ((SequenceToSequenceTransformerOutput (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType T5DType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType T5DType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)])), T5Output (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType T5DType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 8), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType T5DType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Bool) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13)]))), Generator ('Device ('CPU :: DeviceType Nat)))
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Model style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim)
instance (GHC.Show.Show input, GHC.Show.Show decoderInput) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Input input decoderInput)
instance (GHC.Show.Show decoderOutput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Output decoderOutput encoderOutput inputPaddingMask)
instance (GHC.Show.Show decoderInput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5GenerationInput decoderInput encoderOutput inputPaddingMask)
instance (inputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, rightShiftedDecoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputGradient rightShiftedDecoderInputLayout rightShiftedDecoderInputDevice rightShiftedDecoderInputDataType rightShiftedDecoderInputShape, Data.Singletons.Internal.SingI rightShiftedDecoderInputDevice, Torch.GraduallyTyped.NN.Transformer.T5.Common.MkT5RelPosC rightShiftedDecoderInputDevice rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim rightShiftedDecoderInputSeqName rightShiftedDecoderInputSeqSize decoderPos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape decoderInputPaddingMask, rightShiftedDecoderInputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputPaddingMaskGradient rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskDataType rightShiftedDecoderInputPaddingMaskShape, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim attentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim crossAttentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskShape rightShiftedDecoderInputPaddingMaskSeqDim decoderAttentionMask, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generator rightShiftedDecoderInput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generator rightShiftedDecoderInputPaddingMask generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerGenerationInput rightShiftedDecoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Model style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5GenerationInput decoderInput encoderOutput inputPaddingMask) generator (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Output decoderOutput encoderOutput inputPaddingMask) generatorOutput
instance (input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Data.Singletons.Internal.SingI inputDevice, Data.Singletons.Internal.SingI rightShiftedDecoderInputDevice, Torch.GraduallyTyped.NN.Transformer.T5.Common.MkT5RelPosC inputDevice inputShape inputSeqDim inputSeqName inputSeqSize pos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC inputLayout inputDevice inputDataType inputShape inputPaddingMask, inputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, rightShiftedDecoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputGradient rightShiftedDecoderInputLayout rightShiftedDecoderInputDevice rightShiftedDecoderInputDataType rightShiftedDecoderInputShape, Torch.GraduallyTyped.NN.Transformer.T5.Common.MkT5RelPosC rightShiftedDecoderInputDevice rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim rightShiftedDecoderInputSeqName rightShiftedDecoderInputSeqSize decoderPos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape decoderInputPaddingMask, rightShiftedDecoderInputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputPaddingMaskGradient rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskDataType rightShiftedDecoderInputPaddingMaskShape, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim attentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim crossAttentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskShape rightShiftedDecoderInputPaddingMaskSeqDim decoderAttentionMask, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generator rightShiftedDecoderInput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generator rightShiftedDecoderInputPaddingMask generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerInput input rightShiftedDecoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Model style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Input input decoderInput) generator (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Output decoderOutput encoderOutput inputPaddingMask) generatorOutput
instance (Data.Singletons.Internal.SingI headDim, Data.Singletons.Internal.SingI headEmbedDim, Data.Singletons.Internal.SingI embedDim, Data.Singletons.Internal.SingI inputEmbedDim, Data.Singletons.Internal.SingI ffnDim, Data.Singletons.Internal.SingI vocabDim, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim Torch.GraduallyTyped.NN.Transformer.T5.Common.T5RelPosEncBucketDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.NN.Transformer.T5.Common.T5DropoutP, GHC.Types.Double)) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.T5.Common.T5Model style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device)

module Torch.GraduallyTyped.NN.Transformer.T5.ThreeB

-- | T5-3B number of layers. 'num_layers = 24'
type T5ThreeBNumLayers = 24

-- | T5-3B number of attention heads. 'n_heads = 32'
type T5ThreeBHeadDim = 'Dim ('Name "*") ('Size 32)

-- | T5-3B head embedding dimension. 'd_kv = 128'
type T5ThreeBHeadEmbedDim = 'Dim ('Name "*") ('Size 128)

-- | T5-3B embedding dimension. 'inner_dim = n_heads * d_kv = 4096'
type T5ThreeBEmbedDim = 'Dim ('Name "*") ('Size 4096)

-- | T5-3B model dimension. 'd_model = 1024'
type T5ThreeBInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-3B feed-forward network dimension. 'd_ff = 16384'
type T5ThreeBFFNDim = 'Dim ('Name "*") ('Size 16384)

-- | T5-3B vocabulary dimension. 'vocab_size = 32128'
type T5ThreeBVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-3B model.
type T5ThreeB (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = T5Model 'T5 transformerHead T5ThreeBNumLayers T5ThreeBNumLayers gradient device T5ThreeBHeadDim T5ThreeBHeadEmbedDim T5ThreeBEmbedDim T5ThreeBInputEmbedDim T5ThreeBFFNDim T5ThreeBVocabDim

module Torch.GraduallyTyped.NN.Transformer.T5.Small

-- | T5-Small number of layers. 'num_layers = 6'
type T5SmallNumLayers = 6

-- | T5-Small number of attention heads. 'n_heads = 8'
type T5SmallHeadDim = 'Dim ('Name "*") ('Size 8)

-- | T5-Small head embedding dimension. 'd_kv = 64'
type T5SmallHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | T5-Small embedding dimension. 'inner_dim = n_heads * d_kv = 512'
type T5SmallEmbedDim = 'Dim ('Name "*") ('Size 512)

-- | T5-Small model dimension. 'd_model = 512'
type T5SmallInputEmbedDim = 'Dim ('Name "*") ('Size 512)

-- | T5-Small feed-forward network dimension. 'd_ff = 2048'
type T5SmallFFNDim = 'Dim ('Name "*") ('Size 2048)

-- | T5-Small vocabulary dimension. 'vocab_size = 32128'
type T5SmallVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-Small model.
type T5Small (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = T5Model 'T5 transformerHead T5SmallNumLayers T5SmallNumLayers gradient device T5SmallHeadDim T5SmallHeadEmbedDim T5SmallEmbedDim T5SmallInputEmbedDim T5SmallFFNDim T5SmallVocabDim

-- | ByT5-Small number of encoder layers. 'num_layers = 12'
type ByT5SmallNumEncoderLayers = 12

-- | ByT5-Small number of decoder layers. 'num_decoder_layers = 4'
type ByT5SmallNumDecoderLayers = 4

-- | ByT5-Small number of attention heads. 'n_heads = 6'
type ByT5SmallHeadDim = 'Dim ('Name "*") ('Size 6)

-- | ByT5-Small head embedding dimension. 'd_kv = 64'
type ByT5SmallHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | ByT5-Small embedding dimension. 'inner_dim = n_heads * d_kv = 384'
type ByT5SmallEmbedDim = 'Dim ('Name "*") ('Size 384)

-- | ByT5-Small model dimension. 'd_model = 1472'
type ByT5SmallInputEmbedDim = 'Dim ('Name "*") ('Size 1472)

-- | T5-Small feed-forward network dimension. 'd_ff = 3584'
type ByT5SmallFFNDim = 'Dim ('Name "*") ('Size 3584)

-- | T5-Small vocabulary dimension. 'vocab_size = 384'
type ByT5SmallVocabDim = 'Dim ('Name "*") ('Size 384)

-- | ByT5-Small model.
type ByT5Small (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = T5Model 'ByT5 transformerHead ByT5SmallNumEncoderLayers ByT5SmallNumDecoderLayers gradient device ByT5SmallHeadDim ByT5SmallHeadEmbedDim ByT5SmallEmbedDim ByT5SmallInputEmbedDim ByT5SmallFFNDim ByT5SmallVocabDim

module Torch.GraduallyTyped.NN.Transformer.T5.Large

-- | T5-Large number of layers. 'num_layers = 24'
type T5LargeNumLayers = 24

-- | T5-Large number of attention heads. 'n_heads = 16'
type T5LargeHeadDim = 'Dim ('Name "*") ('Size 16)

-- | T5-Large head embedding dimension. 'd_kv = 64'
type T5LargeHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | T5-Large embedding dimension. 'inner_dim = n_heads * d_kv = 1024'
type T5LargeEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-Large model dimension. 'd_model = 1024'
type T5LargeInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-Large feed-forward network dimension. 'd_ff = 4096'
type T5LargeFFNDim = 'Dim ('Name "*") ('Size 4096)

-- | T5-Large vocabulary dimension. 'vocab_size = 32128'
type T5LargeVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-Large model.
type T5Large (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = T5Model 'T5 transformerHead T5LargeNumLayers T5LargeNumLayers gradient device T5LargeHeadDim T5LargeHeadEmbedDim T5LargeEmbedDim T5LargeInputEmbedDim T5LargeFFNDim T5LargeVocabDim

module Torch.GraduallyTyped.NN.Transformer.T5.ElevenB

-- | T5-11B number of layers. 'num_layers = 24'
type T5ElevenBNumLayers = 24

-- | T5-11B number of attention heads. 'n_heads = 128'
type T5ElevenBHeadDim = 'Dim ('Name "*") ('Size 128)

-- | T5-11B head embedding dimension. 'd_kv = 128'
type T5ElevenBHeadEmbedDim = 'Dim ('Name "*") ('Size 128)

-- | T5-11B embedding dimension. 'inner_dim = n_heads * d_kv = 16384'
type T5ElevenBEmbedDim = 'Dim ('Name "*") ('Size 16384)

-- | T5-11B model dimension. 'd_model = 1024'
type T5ElevenBInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-11B feed-forward network dimension. 'd_ff = 65536'
type T5ElevenBFFNDim = 'Dim ('Name "*") ('Size 65536)

-- | T5-11B vocabulary dimension. 'vocab_size = 32128'
type T5ElevenBVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-11B model.
type T5ElevenB (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = T5Model 'T5 transformerHead T5ElevenBNumLayers T5ElevenBNumLayers gradient device T5ElevenBHeadDim T5ElevenBHeadEmbedDim T5ElevenBEmbedDim T5ElevenBInputEmbedDim T5ElevenBFFNDim T5ElevenBVocabDim

module Torch.GraduallyTyped.NN.Transformer.T5.Base

-- | T5-Base number of layers. 'num_layers = 12'
type T5BaseNumLayers = 12

-- | T5-Base number of attention heads. 'n_heads = 12'
type T5BaseHeadDim = 'Dim ('Name "*") ('Size 12)

-- | T5-Base head embedding dimension. 'd_kv = 64'
type T5BaseHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | T5-Base embedding dimension. 'inner_dim = n_heads * d_kv = 768'
type T5BaseEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | T5-Base model dimension. 'd_model = 768'
type T5BaseInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | T5-Base feed-forward network dimension. 'd_ff = 3072'
type T5BaseFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | T5-Base vocabulary dimension. 'vocab_size = 32128'
type T5BaseVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-Base model.
type T5Base (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = T5Model 'T5 transformerHead T5BaseNumLayers T5BaseNumLayers gradient device T5BaseHeadDim T5BaseHeadEmbedDim T5BaseEmbedDim T5BaseInputEmbedDim T5BaseFFNDim T5BaseVocabDim

module Torch.GraduallyTyped.NN.Transformer.Pegasus.Common

-- | Pegasus dType.
type PegasusDType = 'Float

-- | Pegasus dType singleton.
pegasusDType :: SDType PegasusDType

-- | Pegasus data type.
type PegasusDataType = 'DataType PegasusDType

-- | Pegasus data type singleton.
pegasusDataType :: SDataType PegasusDataType

-- | Pegasus dropout probability type.
type PegasusDropoutP = Float

-- | Pegasus dropout rate. 'dropout_rate = 0.1'
pegasusDropoutP :: PegasusDropoutP

-- | Pegasus positional encoding dimension.
type PegasusPosEncDim = 'Dim ('Name "*") ('Size 512)

-- | Pegasus positional encoding dimension singleton.
pegasusPosEncDim :: SDim PegasusPosEncDim

-- | Pegasus layer-norm epsilon.
pegasusEps :: Double

-- | Pegasus maximum number of position embeddings.
--   'max_position_embeddings = 512'
pegasusMaxPositionEmbeddings :: Int

-- | Pegasus padding token id. 'pad_token_id = 0'
pegasusPadTokenId :: Int

-- | Pegasus begin-of-sentence token id. 'bos_token_id = 0'
pegasusBOSTokenId :: Int

-- | Pegasus end-of-sentence token id. 'eos_token_id = 0'
pegasusEOSTokenId :: Int

-- | Pegasus attention mask bias
pegasusAttentionMaskBias :: Double
data GPegasusModel (pegasusModel :: Type)
[GPegasusModel] :: forall pegasusModel. {pegasusModel :: pegasusModel, pegasusShiftRightDecoderInput :: ShiftRight Int, pegasusShiftRightPaddingMask :: ShiftRight Int} -> GPegasusModel pegasusModel

-- | Pegasus model.
data PegasusModel (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat))
[PegasusModel] :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim. GPegasusModel (SequenceToSequenceTransformer 'Pegasus transformerHead numLayers numLayers gradient device PegasusDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim PegasusPosEncDim vocabDim PegasusDropoutP) -> PegasusModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim
mkPegasusInput :: forall batchDim seqDim m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, 'Shape '[batchDim, seqDim] ~ Seq ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]) ('Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> [[Int]] -> m output
mkPegasusPaddingMask :: forall gradient layout device dataType shape output. MkTransformerPaddingMaskC layout device dataType shape output => Tensor gradient layout device dataType shape -> output
data PegasusInput input decoderInput
[PegasusInput] :: forall input decoderInput. {pegasusInput :: input, pegasusDecoderInput :: decoderInput} -> PegasusInput input decoderInput
data PegasusOutput decoderOutput encoderOutput inputPaddingMask
[PegasusOutput] :: forall decoderOutput encoderOutput inputPaddingMask. {pegasusDecoderOutput :: decoderOutput, pegasusEncoderOutput :: encoderOutput, pegasusInputPaddingMask :: inputPaddingMask} -> PegasusOutput decoderOutput encoderOutput inputPaddingMask
data PegasusGenerationInput decoderInput encoderOutput inputPaddingMask
[PegasusGenerationInput] :: forall decoderInput encoderOutput inputPaddingMask. {pegasusGenerationDecoderInput :: decoderInput, pegasusGenerationEncoderOutput :: encoderOutput, pegasusGenerationInputPaddingMask :: inputPaddingMask} -> PegasusGenerationInput decoderInput encoderOutput inputPaddingMask
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim)
instance (GHC.Show.Show input, GHC.Show.Show decoderInput) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusInput input decoderInput)
instance (GHC.Show.Show decoderOutput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusOutput decoderOutput encoderOutput inputPaddingMask)
instance (GHC.Show.Show decoderInput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusGenerationInput decoderInput encoderOutput inputPaddingMask)
instance (inputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, rightShiftedDecoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputGradient rightShiftedDecoderInputLayout rightShiftedDecoderInputDevice rightShiftedDecoderInputDataType rightShiftedDecoderInputShape, Torch.GraduallyTyped.NN.Transformer.Type.MkPosC rightShiftedDecoderInputDevice rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim rightShiftedDecoderInputSeqName rightShiftedDecoderInputSeqSize decoderPos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape decoderInputPaddingMask, rightShiftedDecoderInputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputPaddingMaskGradient rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskDataType rightShiftedDecoderInputPaddingMaskShape, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim attentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim crossAttentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskShape rightShiftedDecoderInputPaddingMaskSeqDim decoderAttentionMask, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generator rightShiftedDecoderInput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generator rightShiftedDecoderInputPaddingMask generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus transformerHead numLayers numLayers gradient device Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusPosEncDim vocabDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDropoutP) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerGenerationInput rightShiftedDecoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusGenerationInput decoderInput encoderOutput inputPaddingMask) generator (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusOutput decoderOutput encoderOutput inputPaddingMask) generatorOutput
instance (input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.NN.Transformer.Type.MkPosC inputDevice inputShape inputSeqDim inputSeqName inputSeqSize pos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC inputLayout inputDevice inputDataType inputShape inputPaddingMask, inputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, rightShiftedDecoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputGradient rightShiftedDecoderInputLayout rightShiftedDecoderInputDevice rightShiftedDecoderInputDataType rightShiftedDecoderInputShape, Torch.GraduallyTyped.NN.Transformer.Type.MkPosC rightShiftedDecoderInputDevice rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim rightShiftedDecoderInputSeqName rightShiftedDecoderInputSeqSize decoderPos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape decoderInputPaddingMask, rightShiftedDecoderInputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputPaddingMaskGradient rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskDataType rightShiftedDecoderInputPaddingMaskShape, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim attentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim crossAttentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskShape rightShiftedDecoderInputPaddingMaskSeqDim decoderAttentionMask, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generator rightShiftedDecoderInput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generator rightShiftedDecoderInputPaddingMask generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus transformerHead numLayers numLayers gradient device Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusPosEncDim vocabDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDropoutP) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerInput input rightShiftedDecoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusInput input decoderInput) generator (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusOutput decoderOutput encoderOutput inputPaddingMask) generatorOutput
instance (Data.Singletons.Internal.SingI headDim, Data.Singletons.Internal.SingI headEmbedDim, Data.Singletons.Internal.SingI embedDim, Data.Singletons.Internal.SingI inputEmbedDim, Data.Singletons.Internal.SingI ffnDim, Data.Singletons.Internal.SingI vocabDim, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus transformerHead numLayers numLayers gradient device Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusPosEncDim vocabDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusPosEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusDropoutP, GHC.Types.Double)) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Pegasus.Common.PegasusModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device)

module Torch.GraduallyTyped.NN.Transformer.Pegasus.XSum

-- | Pegasus-XSum number of layers. 'encoder_layers = 16' 'decoder_layers =
--   16'
type PegasusXSumNumLayers = 16

-- | Pegasus-XSum number of attention heads. 'encoder_attention_heads = 16'
--   'decoder_attention_heads = 16'
type PegasusXSumHeadDim = 'Dim ('Name "*") ('Size 16)

-- | Pegasus-XSum head embedding dimension. 'd_kv = 64'
type PegasusXSumHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | Pegasus-XSum embedding dimension. 'hidden_size = n_heads * d_kv =
--   1024'
type PegasusXSumEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | Pegasus-XSum model dimension. 'd_model = 1024'
type PegasusXSumInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | Pegasus-XSum feed-forward network dimension. 'encoder_ffn_dim = 4096'
--   'decoder_ffn_dim = 4096'
type PegasusXSumFFNDim = 'Dim ('Name "*") ('Size 4096)

-- | Pegasus-XSum vocabulary dimension. 'vocab_size = 96103'
type PegasusXSumVocabDim = 'Dim ('Name "*") ('Size 96103)

-- | Pegasus-XSum model.
type PegasusXSum (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = PegasusModel transformerHead PegasusXSumNumLayers gradient device PegasusXSumHeadDim PegasusXSumHeadEmbedDim PegasusXSumEmbedDim PegasusXSumInputEmbedDim PegasusXSumFFNDim PegasusXSumVocabDim

module Torch.GraduallyTyped.NN.Transformer.Pegasus
testForwardPegasusXSum :: IO (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) PegasusDataType ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 158), 'Dim ('Name "*") ('Size 1024)]))
testPegasusInput :: IO _
testPegasusDecoderInput :: IO _

module Torch.GraduallyTyped.NN.Transformer.BART.Common

-- | BART dType.
type BARTDType = 'Float

-- | BART dType singleton.
bartDType :: SDType BARTDType

-- | BART data type.
type BARTDataType = 'DataType BARTDType

-- | BART data type singleton.
bartDataType :: SDataType BARTDataType

-- | BART dropout probability type.
type BARTDropoutP = Float

-- | BART dropout rate. 'dropout_rate = 0.1'
bartDropoutP :: BARTDropoutP

-- | BART positional encoding dimension.
type BARTPosEncDim = 'Dim ('Name "*") ('Size 1026)

-- | BART positional encoding dimension singleton.
bartPosEncDim :: SDim BARTPosEncDim

-- | BART layer-norm epsilon.
bartEps :: Double

-- | BART maximum number of position embeddings. 'max_position_embeddings =
--   1024'
bartMaxPositionEmbeddings :: Int

-- | BART padding token id. 'pad_token_id = 1'
bartPadTokenId :: Int

-- | BART begin-of-sentence token id. 'bos_token_id = 0'
bartBOSTokenId :: Int

-- | BART end-of-sentence token id. 'eos_token_id = 2'
bartEOSTokenId :: Int

-- | BART attention mask bias
bartAttentionMaskBias :: Double
data GBARTModel (bartModel :: Type)
[GBARTModel] :: forall bartModel. {bartModel :: bartModel, bartShiftRightDecoderInput :: ShiftRight Int, bartShiftRightPaddingMask :: ShiftRight Int} -> GBARTModel bartModel

-- | BART model.
data BARTModel (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat))
[BARTModel] :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim. GBARTModel (SequenceToSequenceTransformer 'BART transformerHead numLayers numLayers gradient device BARTDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim BARTPosEncDim vocabDim BARTDropoutP) -> BARTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim
mkBARTInput :: forall batchDim seqDim m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, 'Shape '[batchDim, seqDim] ~ Seq ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]) ('Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> [[Int]] -> m output
mkBARTPaddingMask :: forall gradient layout device dataType shape output. MkTransformerPaddingMaskC layout device dataType shape output => Tensor gradient layout device dataType shape -> output
data BARTInput input decoderInput
[BARTInput] :: forall input decoderInput. {bartInput :: input, bartDecoderInput :: decoderInput} -> BARTInput input decoderInput
data BARTOutput decoderOutput encoderOutput inputPaddingMask
[BARTOutput] :: forall decoderOutput encoderOutput inputPaddingMask. {bartDecoderOutput :: decoderOutput, bartEncoderOutput :: encoderOutput, bartInputPaddingMask :: inputPaddingMask} -> BARTOutput decoderOutput encoderOutput inputPaddingMask
data BARTGenerationInput decoderInput encoderOutput inputPaddingMask
[BARTGenerationInput] :: forall decoderInput encoderOutput inputPaddingMask. {bartGenerationDecoderInput :: decoderInput, bartGenerationEncoderOutput :: encoderOutput, bartGenerationInputPaddingMask :: inputPaddingMask} -> BARTGenerationInput decoderInput encoderOutput inputPaddingMask
testBart :: IO ((SequenceToSequenceTransformerOutput (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType BARTDType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType BARTDType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)])), BARTOutput (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType BARTDType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 8), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType BARTDType) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13), 'Dim ('Name "*") ('Size 512)])) (Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device ('CPU :: DeviceType Nat)) ('DataType 'Bool) ('Shape '[ 'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 13)]))), Generator ('Device ('CPU :: DeviceType Nat)))
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim)
instance (GHC.Show.Show input, GHC.Show.Show decoderInput) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTInput input decoderInput)
instance (GHC.Show.Show decoderOutput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTOutput decoderOutput encoderOutput inputPaddingMask)
instance (GHC.Show.Show decoderInput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTGenerationInput decoderInput encoderOutput inputPaddingMask)
instance (inputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, rightShiftedDecoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputGradient rightShiftedDecoderInputLayout rightShiftedDecoderInputDevice rightShiftedDecoderInputDataType rightShiftedDecoderInputShape, Torch.GraduallyTyped.NN.Transformer.Type.MkPosC rightShiftedDecoderInputDevice rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim rightShiftedDecoderInputSeqName rightShiftedDecoderInputSeqSize decoderPos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape decoderInputPaddingMask, rightShiftedDecoderInputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputPaddingMaskGradient rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskDataType rightShiftedDecoderInputPaddingMaskShape, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim attentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim crossAttentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskShape rightShiftedDecoderInputPaddingMaskSeqDim decoderAttentionMask, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generator rightShiftedDecoderInput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generator rightShiftedDecoderInputPaddingMask generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer 'Torch.GraduallyTyped.NN.Transformer.Type.BART transformerHead numLayers numLayers gradient device Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTPosEncDim vocabDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDropoutP) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerGenerationInput rightShiftedDecoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTGenerationInput decoderInput encoderOutput inputPaddingMask) generator (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTOutput decoderOutput encoderOutput inputPaddingMask) generatorOutput
instance (input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.NN.Transformer.Type.MkPosC inputDevice inputShape inputSeqDim inputSeqName inputSeqSize pos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC inputLayout inputDevice inputDataType inputShape inputPaddingMask, inputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, rightShiftedDecoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputGradient rightShiftedDecoderInputLayout rightShiftedDecoderInputDevice rightShiftedDecoderInputDataType rightShiftedDecoderInputShape, Torch.GraduallyTyped.NN.Transformer.Type.MkPosC rightShiftedDecoderInputDevice rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim rightShiftedDecoderInputSeqName rightShiftedDecoderInputSeqSize decoderPos, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape decoderInputPaddingMask, rightShiftedDecoderInputPaddingMask GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor rightShiftedDecoderInputPaddingMaskGradient rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskDataType rightShiftedDecoderInputPaddingMaskShape, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim attentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType rightShiftedDecoderInputShape rightShiftedDecoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaskDataType inputPaddingMaskShape inputPaddingMaskSeqDim crossAttentionMask, Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType rightShiftedDecoderInputPaddingMaskLayout rightShiftedDecoderInputPaddingMaskDevice rightShiftedDecoderInputPaddingMaskShape rightShiftedDecoderInputPaddingMaskSeqDim decoderAttentionMask, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generator rightShiftedDecoderInput generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generator rightShiftedDecoderInputPaddingMask generator, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer 'Torch.GraduallyTyped.NN.Transformer.Type.BART transformerHead numLayers numLayers gradient device Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTPosEncDim vocabDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDropoutP) (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerInput input rightShiftedDecoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask) generator (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformerOutput decoderOutput encoderOutput) generatorOutput) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTInput input decoderInput) generator (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTOutput decoderOutput encoderOutput inputPaddingMask) generatorOutput
instance (Data.Singletons.Internal.SingI headDim, Data.Singletons.Internal.SingI headEmbedDim, Data.Singletons.Internal.SingI embedDim, Data.Singletons.Internal.SingI inputEmbedDim, Data.Singletons.Internal.SingI ffnDim, Data.Singletons.Internal.SingI vocabDim, Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.SequenceToSequence.SequenceToSequenceTransformer 'Torch.GraduallyTyped.NN.Transformer.Type.BART transformerHead numLayers numLayers gradient device Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType headDim headEmbedDim embedDim inputEmbedDim ffnDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTPosEncDim vocabDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDropoutP) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device, Torch.GraduallyTyped.DType.SDataType Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDataType, Torch.GraduallyTyped.Shape.Type.SDim headDim, Torch.GraduallyTyped.Shape.Type.SDim headEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim embedDim, Torch.GraduallyTyped.Shape.Type.SDim inputEmbedDim, Torch.GraduallyTyped.Shape.Type.SDim ffnDim, Torch.GraduallyTyped.Shape.Type.SDim Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTPosEncDim, Torch.GraduallyTyped.Shape.Type.SDim vocabDim, Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTDropoutP, GHC.Types.Double)) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.BART.Common.BARTModel transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim) (Torch.GraduallyTyped.RequiresGradient.SGradient gradient, Torch.GraduallyTyped.Device.SDevice device)

module Torch.GraduallyTyped.NN.Transformer.BART.Base

-- | BART-Base number of layers. 'encoder_layers = 6' 'decoder_layers = 6'
type BARTBaseNumLayers = 6

-- | BART-Base number of attention heads. 'encoder_attention_heads = 12'
--   'decoder_attention_heads = 12'
type BARTBaseHeadDim = 'Dim ('Name "*") ('Size 12)

-- | BART-Base head embedding dimension. 'd_kv = 64'
type BARTBaseHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | BART-Base embedding dimension. 'hidden_size = n_heads * d_kv = 768'
type BARTBaseEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BART-Base model dimension. 'd_model = 768'
type BARTBaseInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BART-Base feed-forward network dimension. 'encoder_ffn_dim = 3072'
--   'decoder_ffn_dim = 3072'
type BARTBaseFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | BART-Base vocabulary dimension. 'vocab_size = 50265'
type BARTBaseVocabDim = 'Dim ('Name "*") ('Size 50265)

-- | BART-Base model.
type BARTBase (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) = BARTModel transformerHead BARTBaseNumLayers gradient device BARTBaseHeadDim BARTBaseHeadEmbedDim BARTBaseEmbedDim BARTBaseInputEmbedDim BARTBaseFFNDim BARTBaseVocabDim

module Torch.GraduallyTyped.NN.Transformer.BART
testForwardBARTBase :: IO ()
testBARTInput :: IO _
testBARTDecoderInput :: IO _


-- | This module contains bits and pieces to work with the Spider SQL
--   language.
--   
--   TODO: * don't accept reserved keywords in names without quoting *
--   aliases have to be defined in scope, otherwise fall back to table id *
--   don't define an alias twice in the same scope * optionally (?) use the
--   schema to constrain column and table names * test the parser(s) on
--   more examples * pretty printing * random generation of
--   <a>SpiderSQL</a> values
module Torch.Language.SpiderSQL
data SpiderSQL
SpiderSQL :: Select -> From -> Maybe Cond -> [ColUnit] -> Maybe OrderBy -> Maybe Cond -> Maybe Int -> Maybe SpiderSQL -> Maybe SpiderSQL -> Maybe SpiderSQL -> SpiderSQL
[spiderSQLSelect] :: SpiderSQL -> Select
[spiderSQLFrom] :: SpiderSQL -> From
[spiderSQLWhere] :: SpiderSQL -> Maybe Cond
[spiderSQLGroupBy] :: SpiderSQL -> [ColUnit]
[spiderSQLOrderBy] :: SpiderSQL -> Maybe OrderBy
[spiderSQLHaving] :: SpiderSQL -> Maybe Cond
[spiderSQLLimit] :: SpiderSQL -> Maybe Int
[spiderSQLIntersect] :: SpiderSQL -> Maybe SpiderSQL
[spiderSQLExcept] :: SpiderSQL -> Maybe SpiderSQL
[spiderSQLUnion] :: SpiderSQL -> Maybe SpiderSQL
data Select
Select :: [Agg] -> Select
SelectDistinct :: [Agg] -> Select
data From
From :: [TableUnit] -> Maybe Cond -> From
[fromTableUnits] :: From -> [TableUnit]
[fromCond] :: From -> Maybe Cond
data Cond
And :: Cond -> Cond -> Cond
Or :: Cond -> Cond -> Cond
Not :: Cond -> Cond
Between :: ValUnit -> Val -> Val -> Cond
Eq :: ValUnit -> Val -> Cond
Gt :: ValUnit -> Val -> Cond
Lt :: ValUnit -> Val -> Cond
Ge :: ValUnit -> Val -> Cond
Le :: ValUnit -> Val -> Cond
Ne :: ValUnit -> Val -> Cond
In :: ValUnit -> Val -> Cond
Like :: ValUnit -> Val -> Cond
data ColUnit
ColUnit :: AggType -> Maybe (Either Alias TableId) -> ColumnId -> ColUnit
[colUnitAggId] :: ColUnit -> AggType
[colUnitTable] :: ColUnit -> Maybe (Either Alias TableId)
[colUnitColId] :: ColUnit -> ColumnId
DistinctColUnit :: AggType -> Maybe (Either Alias TableId) -> ColumnId -> ColUnit
[distinctColUnitAggId] :: ColUnit -> AggType
[distinctColUnitTable] :: ColUnit -> Maybe (Either Alias TableId)
[distinctColUnitColdId] :: ColUnit -> ColumnId
data OrderBy
OrderBy :: OrderByOrder -> [ValUnit] -> OrderBy
data OrderByOrder
Asc :: OrderByOrder
Desc :: OrderByOrder
data Agg
Agg :: AggType -> ValUnit -> Agg
data TableUnit
TableUnitSQL :: SpiderSQL -> Maybe Alias -> TableUnit
Table :: TableId -> Maybe Alias -> TableUnit
data ValUnit
Column :: ColUnit -> ValUnit
Minus :: ColUnit -> ColUnit -> ValUnit
Plus :: ColUnit -> ColUnit -> ValUnit
Times :: ColUnit -> ColUnit -> ValUnit
Divide :: ColUnit -> ColUnit -> ValUnit
data Val
ValColUnit :: ColUnit -> Val
Number :: Double -> Val
ValString :: String -> Val
ValSQL :: SpiderSQL -> Val
Terminal :: Val
data AggType
NoneAggOp :: AggType
Max :: AggType
Min :: AggType
Count :: AggType
Sum :: AggType
Avg :: AggType
data ColumnId
Star :: ColumnId
ColumnId :: String -> ColumnId
newtype TableId
TableId :: String -> TableId
newtype Alias
Alias :: String -> Alias

-- | <tt>keyword k</tt> is a parser that consumes <a>Char</a> tokens and
--   yields them if and only if they assemble the <a>String</a> <tt>s</tt>.
--   The parser is not sensitive to letter casing.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (isKeyword "mykeyword") "MYKEYWORD"
--   ("MYKEYWORD","")
--   </pre>
isKeyword :: CharParsing m => String -> m String
isSelect :: CharParsing m => m String
isDistinct :: CharParsing m => m String
isStar :: CharParsing m => m String
isComma :: CharParsing m => m String
isDot :: CharParsing m => m String
isSemicolon :: CharParsing m => m String
isEq :: CharParsing m => m String
isGt :: CharParsing m => m String
isLt :: CharParsing m => m String
isGe :: CharParsing m => m String
isLe :: CharParsing m => m String
isNe :: CharParsing m => m String
isIn :: CharParsing m => m String
isLike :: CharParsing m => m String
isBetween :: CharParsing m => m String
isAnd :: CharParsing m => m String
isOr :: CharParsing m => m String
isNot :: CharParsing m => m String
isMinus :: CharParsing m => m String
isPlus :: CharParsing m => m String
isTimes :: CharParsing m => m String
isDivide :: CharParsing m => m String
isMax :: CharParsing m => m String
isMin :: CharParsing m => m String
isCount :: CharParsing m => m String
isSum :: CharParsing m => m String
isAvg :: CharParsing m => m String
isFrom :: CharParsing m => m String
isJoin :: CharParsing m => m String
isAs :: CharParsing m => m String
isOn :: CharParsing m => m String
isWhere :: CharParsing m => m String
isGroupBy :: CharParsing m => m String
isOrderBy :: CharParsing m => m String
isAsc :: CharParsing m => m String
isDesc :: CharParsing m => m String
isHaving :: CharParsing m => m String
isLimit :: CharParsing m => m String
isIntersect :: CharParsing m => m String
isExcept :: CharParsing m => m String
isUnion :: CharParsing m => m String
betweenParentheses :: CharParsing m => m a -> m a
betweenOptionalParentheses :: CharParsing m => m a -> m a

-- | <a>Select</a> parser
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] select "select count table.*"
--   (Select [Agg Count (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "table")), colUnitColId = Star}))],"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] select "SELECT COUNT (DISTINCT t5.title)"
--   (Select [Agg Count (Column (DistinctColUnit {distinctColUnitAggId = NoneAggOp, distinctColUnitTable = Just (Left (Alias "t5")), distinctColUnitColdId = ColumnId "title"}))],"")
--   </pre>
select :: (TokenParsing m, Monad m) => m Select

-- | <a>Agg</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] agg "count table.id"
--   (Agg Count (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "table")), colUnitColId = ColumnId "id"})),"")
--   </pre>
agg :: (TokenParsing m, Monad m) => m Agg

-- | <a>AggType</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] aggType ""
--   (NoneAggOp,"")
--   </pre>
aggType :: CharParsing m => m AggType

-- | <a>ValUnit</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] valUnit "t1.stadium_id"
--   (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"}),"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head . filter (null . snd) $ parseString @[] valUnit "t1.stadium_length * t1.stadium_width"
--   (Times (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_length"}) (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_width"}),"")
--   </pre>
valUnit :: (TokenParsing m, Monad m) => m ValUnit

-- | <a>ColUnit</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] colUnit "count ( distinct my_table.* )"
--   (DistinctColUnit {distinctColUnitAggId = Count, distinctColUnitTable = Just (Left (Alias "my_table")), distinctColUnitColdId = Star},"")
--   </pre>
colUnit :: (TokenParsing m, Monad m) => m ColUnit

-- | <a>TableId</a> parser.
tableId :: CharParsing m => m TableId

-- | <a>Alias</a> parser.
alias :: CharParsing m => m Alias

-- | <a>ColumnId</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; parseString @[] columnId "*"
--   [(Star,"")]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; parseString @[] columnId "c"
--   [(ColumnId "c","")]
--   </pre>
columnId :: CharParsing m => m ColumnId
tableUnitAlias :: TableUnit -> Maybe Alias
tableUnitTableId :: TableUnit -> Maybe TableId
condAliases :: Cond -> [Alias]
condTableIds :: Cond -> [TableId]
valUnitAliases :: ValUnit -> [Alias]
valUnitTableIds :: ValUnit -> [TableId]
colUnitAlias :: ColUnit -> Maybe Alias
colUnitTableId :: ColUnit -> Maybe TableId
valAlias :: Val -> Maybe Alias
valTableId :: Val -> Maybe TableId

-- | <a>From</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] from "FROM people AS t1 JOIN pets AS t2 ON t1.pet_id = t2.pet_id"
--   (From {fromTableUnits = [Table (TableId "people") (Just (Alias "t1")),Table (TableId "pets") (Just (Alias "t2"))], fromCond = Just (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "pet_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "pet_id"})))},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] from "FROM organization AS t3 JOIN author AS t1 ON t3.oid = t1.oid JOIN writes AS t4 ON t4.aid = t1.aid JOIN publication AS t5 ON t4.pid = t5.pid JOIN conference AS t2 ON t5.cid = t2.cid"
--   (From {fromTableUnits = [Table (TableId "organization") (Just (Alias "t3")),Table (TableId "author") (Just (Alias "t1")),Table (TableId "writes") (Just (Alias "t4")),Table (TableId "publication") (Just (Alias "t5")),Table (TableId "conference") (Just (Alias "t2"))], fromCond = Just (And (And (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "oid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "oid"}))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "aid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "aid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "pid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "pid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "cid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "cid"}))))},"")
--   </pre>
from :: forall m. (TokenParsing m, Monad m) => m From

-- | <a>TableUnit</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] tableUnit "people as t1"
--   (Table (TableId "people") (Just (Alias "t1")),"")
--   </pre>
tableUnit :: (TokenParsing m, Monad m) => m TableUnit

-- | <a>Cond</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] cond "t1.stadium_id = t2.stadium_id"
--   (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "stadium_id"})),"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (cond &lt;* isToken ';') "t2.name = \"VLDB\" AND t3.name = \"University of Michigan\";"
--   (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "name"})) (ValString "VLDB")) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "name"})) (ValString "University of Michigan")),"")
--   </pre>
cond :: (TokenParsing m, Monad m) => m Cond

-- | <a>Val</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] val "count t1.stadium_id"
--   (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = ColumnId "count"})," t1.stadium_id")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] val "(select *)"
--   (ValSQL (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [], fromCond = Nothing}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing}),"")
--   </pre>
val :: (TokenParsing m, Monad m) => m Val

-- | Parser for quoted strings.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] quotedString "\"hello world\""
--   ("hello world","")
--   </pre>
quotedString :: CharParsing m => m String

-- | Parser for where clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] whereCond "where t1.id = t2.id"
--   (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "id"})),"")
--   </pre>
whereCond :: (TokenParsing m, Monad m) => m Cond

-- | Parser for group-by clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] groupBy "group by count t1.id, t2.id"
--   ([ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = ColumnId "count"}]," t1.id, t2.id")
--   </pre>
groupBy :: (TokenParsing m, Monad m) => m [ColUnit]

-- | <a>OrderBy</a> Parser.
--   
--   <pre>
--   &gt;&gt;&gt; head . filter (null . snd) $ parseString @[] orderBy "order by t1.stadium_id, t2.pet_id desc"
--   (OrderBy Desc [Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"}),Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "pet_id"})],"")
--   </pre>
orderBy :: (TokenParsing m, Monad m) => m OrderBy

-- | Parser for having clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] havingCond "having count(t1.customer_id) = 10"
--   (Eq (Column (ColUnit {colUnitAggId = Count, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "customer_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = ColumnId "10"})),"")
--   </pre>
havingCond :: (TokenParsing m, Monad m) => m Cond

-- | Parser for limit clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] limit "limit 10"
--   (10,"")
--   </pre>
limit :: (TokenParsing m, Monad m) => m Int

-- | <a>SpiderSQL</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "select * ;"
--   (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [], fromCond = Nothing}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "select * from concert;"
--   (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [Table (TableId "concert") Nothing], fromCond = Nothing}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "select T2.name, count(*) from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id group by t1.stadium_id;"
--   (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "T2")), colUnitColId = ColumnId "name"})),Agg NoneAggOp (Column (ColUnit {colUnitAggId = Count, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [Table (TableId "concert") (Just (Alias "t1")),Table (TableId "stadium") (Just (Alias "t2"))], fromCond = Just (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "stadium_id"})))}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"}], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "SELECT COUNT ( DISTINCT t5.title ) FROM organization AS t3 JOIN author AS t1 ON t3.oid = t1.oid JOIN writes AS t4 ON t4.aid = t1.aid JOIN publication AS t5 ON t4.pid = t5.pid JOIN conference AS t2 ON t5.cid = t2.cid WHERE t2.name = \"VLDB\" AND t3.name = \"University of Michigan\";"
--   (SpiderSQL {spiderSQLSelect = Select [Agg Count (Column (DistinctColUnit {distinctColUnitAggId = NoneAggOp, distinctColUnitTable = Just (Left (Alias "t5")), distinctColUnitColdId = ColumnId "title"}))], spiderSQLFrom = From {fromTableUnits = [Table (TableId "organization") (Just (Alias "t3")),Table (TableId "author") (Just (Alias "t1")),Table (TableId "writes") (Just (Alias "t4")),Table (TableId "publication") (Just (Alias "t5")),Table (TableId "conference") (Just (Alias "t2"))], fromCond = Just (And (And (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "oid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "oid"}))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "aid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "aid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "pid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "pid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "cid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "cid"}))))}, spiderSQLWhere = Just (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "name"})) (ValString "VLDB")) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "name"})) (ValString "University of Michigan"))), spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
spiderSQL :: (TokenParsing m, Monad m) => m SpiderSQL

-- | Auxiliary parser for table names, column names, and aliases.
name :: CharParsing m => m String
instance GHC.Show.Show Torch.Language.SpiderSQL.OrderByOrder
instance GHC.Classes.Eq Torch.Language.SpiderSQL.OrderByOrder
instance GHC.Show.Show Torch.Language.SpiderSQL.AggType
instance GHC.Classes.Eq Torch.Language.SpiderSQL.AggType
instance GHC.Show.Show Torch.Language.SpiderSQL.ColumnId
instance GHC.Classes.Eq Torch.Language.SpiderSQL.ColumnId
instance GHC.Show.Show Torch.Language.SpiderSQL.TableId
instance GHC.Classes.Eq Torch.Language.SpiderSQL.TableId
instance GHC.Show.Show Torch.Language.SpiderSQL.Alias
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Alias
instance GHC.Show.Show Torch.Language.SpiderSQL.ColUnit
instance GHC.Classes.Eq Torch.Language.SpiderSQL.ColUnit
instance GHC.Show.Show Torch.Language.SpiderSQL.ValUnit
instance GHC.Classes.Eq Torch.Language.SpiderSQL.ValUnit
instance GHC.Show.Show Torch.Language.SpiderSQL.Agg
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Agg
instance GHC.Show.Show Torch.Language.SpiderSQL.Select
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Select
instance GHC.Show.Show Torch.Language.SpiderSQL.OrderBy
instance GHC.Classes.Eq Torch.Language.SpiderSQL.OrderBy
instance GHC.Show.Show Torch.Language.SpiderSQL.TableUnit
instance GHC.Classes.Eq Torch.Language.SpiderSQL.TableUnit
instance GHC.Show.Show Torch.Language.SpiderSQL.From
instance GHC.Classes.Eq Torch.Language.SpiderSQL.From
instance GHC.Show.Show Torch.Language.SpiderSQL.Val
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Val
instance GHC.Show.Show Torch.Language.SpiderSQL.Cond
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Cond
instance GHC.Show.Show Torch.Language.SpiderSQL.SpiderSQL
instance GHC.Classes.Eq Torch.Language.SpiderSQL.SpiderSQL

module Torch.Language

module Torch.GraduallyTyped.NN.Transformer.T5.Generation
data IsFinished
Finished :: IsFinished
Unfinished :: IsFinished
data Beams a b
[Beams] :: forall a b. {finished :: [Hypothesis 'Finished a b], unfinished :: [Hypothesis 'Unfinished a b]} -> Beams a b
data Hypothesis (isFinished :: IsFinished) a b
[InitialHypothesis] :: forall a b. Hypothesis 'Unfinished a b
[UnfinishedHypothesis] :: forall a b. {currentToken :: a, currentScore :: Float, previousHypothesis :: Hypothesis 'Unfinished a b} -> Hypothesis 'Unfinished a b
[FinishedHypothesis] :: forall a b. {finalToken :: a, finalScore :: Float, penultimateHypothesis :: Hypothesis 'Unfinished a b, finalValue :: b} -> Hypothesis 'Finished a b
getTokens :: forall a b. Hypothesis 'Unfinished a b -> [a]
getScore :: forall a b. Hypothesis 'Unfinished a b -> Float
data SomeHypothesis a b
SomeHypothesis :: Hypothesis isFinished a b -> SomeHypothesis a b
[unSomeHypothesis] :: SomeHypothesis a b -> Hypothesis isFinished a b
beamSearch :: forall a b m. Monad m => Int -> Int -> ([Hypothesis 'Unfinished a b] -> m [SomeHypothesis a b]) -> m [Beams a b]
beamSearchStep :: forall a b m. Functor m => ([Hypothesis 'Unfinished a b] -> m [SomeHypothesis a b]) -> Beams a b -> m (Beams a b)
runBeamSearch :: forall model input decoderInput encoderOutput encoderOutputShape encoderOutput' inputPaddingMask decoderOutput generator. (HasForward model (T5Input input decoderInput) generator (T5Output decoderOutput encoderOutput inputPaddingMask) generator, encoderOutput ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType encoderOutputShape, 'UncheckedShape ~ BroadcastShapesF encoderOutputShape 'UncheckedShape, SGetShape encoderOutputShape, HasForward model (T5GenerationInput decoderInput encoderOutput' inputPaddingMask) generator (T5Output decoderOutput encoderOutput' inputPaddingMask) generator, encoderOutput' ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType 'UncheckedShape, decoderInput ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize]), decoderOutput ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType 'UncheckedShape, generator ~ Generator ('Device 'CPU)) => Int -> Int -> model -> input -> generator -> IO [Beams Int [Int]]
testBeamSearch :: IO ()
next :: forall t b i a. (i ~ Int, Show i, MonadTrans t, Monad (t (StateT [i] b)), Alternative (t (StateT [i] b)), Monad b, Foldable b) => t (StateT [i] b) i -> Parser (StateT [i] b) i a -> (Parser (StateT [i] b) i a -> t (StateT [i] b) a) -> t (StateT [i] b) a
notNull :: (Monad b, Foldable b) => Parser (StateT [i] b) i a -> StateT [i] b Bool
hasFree :: (Monad b, Foldable b) => Parser (StateT [i] b) i a -> StateT [i] b Bool

-- | <tt>transParser vocab p</tt> transforms a parser <tt>p</tt> over
--   characters <a>Char</a> into a parser over token indices <a>Int</a>
--   using the vocabulary <tt>vocab</tt>.
transParser :: MonadPlus b => Map Int String -> Parser b Char a -> Parser b Int a

-- | Get continuations from model
getIs :: forall model input generator b decoderInput encoderOutput decoderOutput inputPaddingMask s. (Alternative b, MonadThrow b, s ~ (Maybe (encoderOutput, inputPaddingMask), generator), decoderInput ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") 'UncheckedSize]), decoderOutput ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType 'UncheckedShape, HasForward model (T5Input input decoderInput) generator (T5Output decoderOutput encoderOutput inputPaddingMask) generator, HasForward model (T5GenerationInput decoderInput encoderOutput inputPaddingMask) generator (T5Output decoderOutput encoderOutput inputPaddingMask) generator) => Int -> model -> input -> StateT s (StateT [Int] b) Int
runParser :: forall model input generator b a. _ => Int -> model -> input -> generator -> Parser (StateT [Int] b) Int a -> b (a, [Int])

-- | <tt>t5Text</tt> parses a <a>Char</a> sequence delimited by
--   <tt><a>/s</a></tt> as a <a>String</a>.
t5Text :: MonadPlus b => Parser b Char String
t5Test :: MonadPlus b => Parser b Char String

-- | <tt>t5Sql</tt> parses a <a>Char</a> sequence starting with <tt>"</tt>
--   and ending with <tt>" <a>/s</a></tt> as <a>SpiderSQL</a>.
t5Sql :: (TokenParsing (FreeT ((->) Char) b), MonadPlus b) => Parser b Char SpiderSQL
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Beams a b)
instance (GHC.Classes.Eq a, GHC.Classes.Eq b) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Unfinished a b)
instance (GHC.Classes.Eq a, GHC.Classes.Eq b) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Finished a b)
instance (GHC.Classes.Ord a, GHC.Classes.Ord b) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Unfinished a b)
instance (GHC.Classes.Ord a, GHC.Classes.Ord b) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Finished a b)
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Unfinished a b)
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Finished a b)
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Combinators.Parsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Int] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Char.CharParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Int] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Token.TokenParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Int] b))

module Torch.GraduallyTyped.NN.Transformer.T5
testForwardT5Small :: IO ()
testForwardByT5Small :: IO ()

module Torch.GraduallyTyped.NN.Transformer

module Torch.GraduallyTyped.NN

module Torch.GraduallyTyped
class HasGrad parameters where {
    type family Gradients parameters :: Type;
    type family Loss parameters :: Type;
}

-- | calculate gradients of a zero-dimensional tensor with respect to a
--   list of independent tensor parameters
grad :: HasGrad parameters => Loss parameters -> parameters -> Gradients parameters
