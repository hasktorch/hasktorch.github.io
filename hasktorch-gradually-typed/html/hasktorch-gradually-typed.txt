-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | experimental project for hasktorch
--   
--   experimental project for hasktorch
@package hasktorch-gradually-typed
@version 0.2.0.0

module Control.Monad.Fresh
newtype Successor a
Successor :: (a -> a) -> Successor a
[suc] :: Successor a -> a -> a

-- | The monad transformer for generating fresh values.
newtype FreshT e m a
FreshT :: ReaderT (Successor e) (StateT e m) a -> FreshT e m a
[unFreshT] :: FreshT e m a -> ReaderT (Successor e) (StateT e m) a
type Fresh e = FreshT e Identity
successor :: forall e. (e -> e) -> Successor e
enumSucc :: forall e. Enum e => Successor e

-- | Run a <tt>FreshT</tt> computation starting from the value <tt>toEnum
--   0</tt>
runFreshT :: forall e m a. (Enum e, Monad m) => FreshT e m a -> m a

-- | Run a <tt>Fresh</tt> computation starting from the value <tt>toEnum
--   0</tt>
runFresh :: forall e a. Enum e => Fresh e a -> a

-- | Run a <tt>FreshT</tt> computation starting from a specific value
--   <tt>e</tt>.
runFreshTFrom :: forall e m a. (Monad m, Enum e) => e -> FreshT e m a -> m a

-- | Run a <tt>Fresh</tt> computation starting from a specific value
--   <tt>e</tt>.
runFreshFrom :: forall e a. Enum e => e -> Fresh e a -> a

-- | Run a <tt>FreshT</tt> computation starting from a specific value
--   <tt>e</tt> with a the next fresh value determined by <tt>Successor
--   e</tt>.
runFreshTWith :: forall e m a. Monad m => Successor e -> e -> FreshT e m a -> m a

-- | Run a <tt>FreshT</tt> computation starting from a specific value
--   <tt>e</tt> with a the next fresh value determined by <tt>Successor
--   e</tt>.
runFreshWith :: forall e a. Successor e -> e -> Fresh e a -> a

-- | The MTL style class for generating fresh values
class Monad m => MonadFresh e m | m -> e

-- | Generate a fresh value <tt>e</tt>, <tt>fresh</tt> should never produce
--   the same value within a monadic computation.
fresh :: MonadFresh e m => m e
instance GHC.Base.Functor m => GHC.Base.Functor (Control.Monad.Fresh.FreshT e m)
instance GHC.Base.Monad m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.Fresh.MonadFresh e m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.Identity.IdentityT m)
instance Control.Monad.Fresh.MonadFresh e m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.State.Lazy.StateT s m)
instance Control.Monad.Fresh.MonadFresh e m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.Reader.ReaderT s m)
instance (Control.Monad.Fresh.MonadFresh e m, GHC.Base.Monoid s) => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.Writer.Lazy.WriterT s m)
instance Control.Monad.Fresh.MonadFresh e m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.Maybe.MaybeT m)
instance Control.Monad.Fresh.MonadFresh e m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.Cont.ContT r m)
instance (GHC.Base.Monoid w, Control.Monad.Fresh.MonadFresh e m) => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.RWS.Lazy.RWST r w s m)
instance Control.Monad.Fresh.MonadFresh e m => Control.Monad.Fresh.MonadFresh e (Control.Monad.Trans.Except.ExceptT e' m)
instance GHC.Base.Monad m => GHC.Base.Monad (Control.Monad.Fresh.FreshT e m)
instance GHC.Base.MonadPlus m => GHC.Base.MonadPlus (Control.Monad.Fresh.FreshT e m)
instance (GHC.Base.Functor f, GHC.Base.Monad f) => GHC.Base.Applicative (Control.Monad.Fresh.FreshT e f)
instance (GHC.Base.Monad m, GHC.Base.Functor m, GHC.Base.MonadPlus m) => GHC.Base.Alternative (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.Trans.Class.MonadTrans (Control.Monad.Fresh.FreshT e)
instance Control.Monad.Reader.Class.MonadReader r m => Control.Monad.Reader.Class.MonadReader r (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.State.Class.MonadState s m => Control.Monad.State.Class.MonadState s (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.Writer.Class.MonadWriter w m => Control.Monad.Writer.Class.MonadWriter w (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.Fix.MonadFix m => Control.Monad.Fix.MonadFix (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.IO.Class.MonadIO m => Control.Monad.IO.Class.MonadIO (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.Cont.Class.MonadCont m => Control.Monad.Cont.Class.MonadCont (Control.Monad.Fresh.FreshT e m)
instance Control.Monad.Error.Class.MonadError e m => Control.Monad.Error.Class.MonadError e (Control.Monad.Fresh.FreshT e' m)
instance Control.Monad.Morph.MFunctor (Control.Monad.Fresh.FreshT e)
instance Hedgehog.Internal.Gen.MonadGen m => Hedgehog.Internal.Gen.MonadGen (Control.Monad.Fresh.FreshT e m)

module Torch.Data.Parser

-- | <tt>Parser b i a</tt> is a parser that consumes a stream of <tt>i</tt>
--   tokens and as a result yields a value of type <tt>a</tt>, while
--   operating under the <tt>b</tt> non-determinism monad.
--   
--   For most purposes, the non-determinism monad <tt>b</tt> should be a
--   <a>MonadPlus</a>. Useful examples include <tt>[]</tt> or
--   <tt>Logic</tt> if you want backtracking, and <a>Maybe</a> if you want
--   no backtracking.
--   
--   Use <tt><a>StateT</a> s []</tt> if you want to maintain a state
--   <tt>s</tt> that is automatically reverted when backtracking via
--   <tt>[]</tt>.
--   
--   <tt>hoistFreeT</tt> can be used to change the backtracking monad.
--   
--   <a>FreeT</a> provides instances for <a>Functor</a>,
--   <a>Applicative</a>, <a>Monad</a>, <a>Alternative</a> and
--   <a>MonadPlus</a>.
type Parser (b :: Type -> Type) (i :: Type) (a :: Type) = FreeT ((->) i) b a

-- | Recurse over a parser.
--   
--   Tears down the free monad transformer over the '(-&gt;) i' functor
--   using iteration: <tt> recurse next parser = next parser (parser' -&gt;
--   next parser' (parser'' -&gt; next parser'' (parser''' -&gt; next
--   parser''' (...)))) </tt>
recurse :: forall t b i a. (Parser b i a -> (Parser b i a -> t b a) -> t b a) -> Parser b i a -> t b a
parseStream :: forall s b i a. Monad b => (s -> b (i, s)) -> Parser (StateT s b) i a -> s -> b (a, s)
parseString :: forall b i a. MonadPlus b => Parser (StateT [i] b) i a -> [i] -> b (a, [i])

-- | <tt>token</tt> is trivial parser that consumes a single token
--   <tt>i</tt> and yields it.
--   
--   Other parsers can be derived from this one using methods of the
--   <a>Functor</a>, <a>Applicative</a>, <a>Monad</a>, <a>Alternative</a>,
--   and <a>MonadPlus</a> typeclasses and the parser combinators in this
--   module.
token :: forall b i. Monad b => Parser b i i
eof :: Alternative b => Parser (StateT [i] b) i ()
notFollowedBy :: forall b i a. (Alternative b, Foldable b, MonadPlus b) => Parser (StateT [i] b) i a -> Parser (StateT [i] b) i ()

-- | <tt>satisfy p</tt> is a simple parser that consumes a single token
--   <tt>i</tt> and yields it if and only if <tt>p i</tt> evaluates to
--   <a>True</a>. Otherwise, the parser fails.
satisfy :: forall b i. MonadPlus b => (i -> Bool) -> Parser b i i

-- | <tt>is i</tt> is a simple parser that consumes a single token and
--   yields it if and only if it is equal to <tt>i</tt>. Otherwise, the
--   parser fails.
isToken :: forall b i. (MonadPlus b, Eq i) => i -> Parser b i i

-- | <tt>isNot i</tt> is a simple parser that consumes a single token and
--   yields it if and only if it is not equal to <tt>i</tt>. If the token
--   is equal to <tt>i</tt>, the parser fails.
isNotToken :: forall b i. (MonadPlus b, Eq i) => i -> Parser b i i

-- | Stateful scanner.
--   
--   <pre>
--   &gt;&gt;&gt; :{
--     f s a | "ell" `isInfixOf` (s ++ [a]) = Nothing
--           | otherwise                    = Just (s ++ [a])
--   :}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (scan f "" token) "hello 123"
--   ("hel","lo 123")
--   </pre>
scan :: (Alternative m, Monad m) => (s -> a -> Maybe s) -> s -> m a -> m [a]

-- | <tt>atMost n p</tt> applies the parser <tt>p</tt> at most <tt>n</tt>
--   times and returns every parsing result. If parsing of <tt>p</tt>
--   succeeds less the <tt>n</tt> times, <tt>repeatP n p</tt> succeeds as
--   well.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (atMost 2 (isToken 'a')) "aaaaaab"
--   ("aa","aaaab")
--   </pre>
atMost :: (Alternative m, Monad m) => Int -> m a -> m [a]

-- | <tt>eitherP p p'</tt> combines the two alternatives <tt>p</tt> and
--   <tt>p'</tt>.
eitherP :: Alternative f => f a -> f b -> f (Either a b)

-- | <tt>void p</tt> applies the parser <tt>p</tt> and discards its result.
void :: Functor f => f a -> f ()

-- | <tt>combine p p'</tt> merges the results of <tt>p</tt> and <tt>p'</tt>
--   using the <a>Semigroup</a> instance.
combine :: (Applicative f, Semigroup a) => f a -> f a -> f a

-- | <tt>combines ps</tt> merges the results of the parsers <tt>ps</tt>
--   using the <a>Monoid</a> instance.
combines :: (Applicative f, Monoid a) => [f a] -> f a

-- | <tt>isString s</tt> is a simple parser that consumes <a>Char</a>
--   tokens and yields them if and only if they assemble the <a>String</a>
--   <tt>s</tt>. Otherwise, the parser fails.
isString :: (Traversable t, MonadPlus b, Eq i) => t i -> Parser b i (t i)

-- | <tt>string</tt> matches any string
--   
--   <pre>
--   &gt;&gt;&gt; parseString @[] string "a string"
--   [("a string",""),("a strin","g"),("a stri","ng"),("a str","ing"),("a st","ring"),("a s","tring"),("a ","string"),("a"," string"),("","a string")]
--   </pre>
--   
--   <ul>
--   <li>- &gt;&gt;&gt; p = string @[] &gt;&gt;= s -&gt; (guard ("dog"
--   <a>isInfixOf</a> s) &gt;&gt; pure s)</li>
--   <li>- &gt;&gt;&gt; head $ parseString p "this is a string with a
--   dog"</li>
--   <li>- ("this is a string with a dog","")</li>
--   <li>- &gt;&gt;&gt; p = string @[] &gt;&gt;= s -&gt; (guard (not $
--   "dog" <a>isInfixOf</a> s) &gt;&gt; pure s)</li>
--   <li>- &gt;&gt;&gt; head $ parseString p "this is also string with a
--   dog"</li>
--   <li>- ("this is also string with a do","g")</li>
--   </ul>
string :: MonadPlus b => Parser b i [i]
intP :: (CharParsing m, Monad m) => m Int
doubleP :: (CharParsing m, Monad m) => m Double
instance (GHC.Base.Applicative f, Control.Monad.Logic.Class.MonadLogic b, GHC.Base.MonadPlus b) => Control.Monad.Logic.Class.MonadLogic (Control.Monad.Trans.Free.FreeT f b)
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Combinators.Parsing (Control.Monad.Trans.Free.FreeT ((->) i) (Control.Monad.Trans.State.Lazy.StateT [i] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Char.CharParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Char] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Token.TokenParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Char] b))

module Torch.GraduallyTyped.Internal.Vector
uncons :: Vector a -> Maybe (a, Vector a)

module Torch.GraduallyTyped.Internal.Void
data Void

module Torch.GraduallyTyped.LearningRateSchedules

-- | Single-cycle learning rate schedule. See, for instance,
--   <a>https://arxiv.org/abs/1803.09820</a>.
--   
--   This is a simple schedule that is a stepwise linear interpolation
--   between the initial, maximum, and final learning rates. The initial
--   learning rate is zero.
singleCycleLearningRateSchedule :: Double -> Double -> Int -> Int -> Int -> Int -> Double

module Torch.GraduallyTyped.NN.Type
data HasBias
WithBias :: HasBias
WithoutBias :: HasBias
type family WithBiasSym0 :: HasBias
type family WithoutBiasSym0 :: HasBias
data SHasBias :: HasBias -> Type
[SWithBias] :: SHasBias ('WithBias :: HasBias)
[SWithoutBias] :: SHasBias ('WithoutBias :: HasBias)
data HasDropout
WithDropout :: HasDropout
WithoutDropout :: HasDropout
type family WithDropoutSym0 :: HasDropout
type family WithoutDropoutSym0 :: HasDropout
data SHasDropout :: HasDropout -> Type
[SWithDropout] :: SHasDropout ('WithDropout :: HasDropout)
[SWithoutDropout] :: SHasDropout ('WithoutDropout :: HasDropout)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Type.SHasDropout hasDropout)
instance Data.Singletons.SingKind Torch.GraduallyTyped.NN.Type.HasDropout
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Type.WithDropout
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Type.WithoutDropout
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Type.HasDropout
instance GHC.Show.Show Torch.GraduallyTyped.NN.Type.HasDropout
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Type.HasDropout
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Type.HasDropout
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Type.SHasBias hasBias)
instance Data.Singletons.SingKind Torch.GraduallyTyped.NN.Type.HasBias
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Type.WithBias
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Type.WithoutBias
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Type.HasBias
instance GHC.Show.Show Torch.GraduallyTyped.NN.Type.HasBias
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Type.HasBias
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Type.HasBias

module Torch.GraduallyTyped.Prelude.Bool

module Torch.GraduallyTyped.Prelude.List

module Torch.GraduallyTyped.Prelude.Maybe

module Torch.GraduallyTyped.Prelude.TypeLits

module Torch.GraduallyTyped.Prelude

-- | The kind of types with lifted values. For example <tt>Int ::
--   Type</tt>.
type Type = TYPE LiftedRep

-- | The kind of constraints, like <tt>Show a</tt>
data Constraint

-- | <a>Proxy</a> is a type that holds no data, but has a phantom parameter
--   of arbitrary type (or even kind). Its use is to provide type
--   information, even though there is no value available of that type (or
--   it may be too costly to create one).
--   
--   Historically, <tt><a>Proxy</a> :: <a>Proxy</a> a</tt> is a safer
--   alternative to the <tt><a>undefined</a> :: a</tt> idiom.
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy (Void, Int -&gt; Int)
--   Proxy
--   </pre>
--   
--   Proxy can even hold types of higher kinds,
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Either
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy Functor
--   Proxy
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; Proxy :: Proxy complicatedStructure
--   Proxy
--   </pre>
data Proxy (t :: k)
Proxy :: Proxy (t :: k)

-- | Type-level "or"
type family (a :: Bool) || (b :: Bool) :: Bool
infixr 2 ||

-- | Type-level <a>If</a>. <tt>If True a b</tt> ==&gt; <tt>a</tt>; <tt>If
--   False a b</tt> ==&gt; <tt>b</tt>
type family If (cond :: Bool) (tru :: k) (fls :: k) :: k

-- | The type-level equivalent of <a>error</a>.
--   
--   The polymorphic kind of this type allows it to be used in several
--   settings. For instance, it can be used as a constraint, e.g. to
--   provide a better error message for a non-existent instance,
--   
--   <pre>
--   -- in a context
--   instance TypeError (Text "Cannot <a>Show</a> functions." :$$:
--                       Text "Perhaps there is a missing argument?")
--         =&gt; Show (a -&gt; b) where
--       showsPrec = error "unreachable"
--   </pre>
--   
--   It can also be placed on the right-hand side of a type-level function
--   to provide an error for an invalid case,
--   
--   <pre>
--   type family ByteSize x where
--      ByteSize Word16   = 2
--      ByteSize Word8    = 1
--      ByteSize a        = TypeError (Text "The type " :&lt;&gt;: ShowType a :&lt;&gt;:
--                                     Text " is not exportable.")
--   </pre>
type family TypeError (a :: ErrorMessage) :: b

-- | Addition of type-level naturals.
type family (a :: Natural) + (b :: Natural) :: Natural
infixl 6 +

-- | Multiplication of type-level naturals.
type family (a :: Natural) * (b :: Natural) :: Natural
infixl 7 *

-- | Comparison of type-level naturals, as a function.
type family CmpNat (a :: Natural) (b :: Natural) :: Ordering

-- | A description of a custom type error.
data ErrorMessage

-- | Show the text as is.
Text :: Symbol -> ErrorMessage

-- | Pretty print the type. <tt>ShowType :: k -&gt; ErrorMessage</tt>
ShowType :: t -> ErrorMessage

-- | Put two pieces of error message next to each other.
(:<>:) :: ErrorMessage -> ErrorMessage -> ErrorMessage

-- | Stack two pieces of error message on top of each other.
(:$$:) :: ErrorMessage -> ErrorMessage -> ErrorMessage
infixl 5 :$$:
infixl 6 :<>:

-- | A type synonym for <a>Natural</a>.
--   
--   Prevously, this was an opaque data type, but it was changed to a type
--   synonym.
type Nat = Natural
data SList (a1 :: [a])
[SNil] :: forall a. SList ('[] :: [a])
[SCons] :: forall a (n1 :: a) (n2 :: [a]). Sing n1 -> Sing n2 -> SList (n1 : n2)
infixr 5 `SCons`
data IsChecked a
Checked :: a -> IsChecked a
Unchecked :: a -> IsChecked a
pattern IsChecked :: a -> IsChecked a
pattern Demoted :: SingKind k => Demote k -> Sing (a :: k)
pattern Demoted' :: (SingKind k, Demote k ~ IsChecked t) => t -> Sing (a :: k)
forgetIsChecked :: IsChecked a -> a
pattern (:|:) :: forall a as. Sing a -> SList as -> SList (a : as)
infixr 8 :|:
type family All (c :: k -> Constraint) (xs :: [k]) :: Constraint
class KnownElem k x where {
    type ElemValF k :: Type;
}
elemVal :: KnownElem k x => ElemValF k
class KnownList k (xs :: [k])
listVal :: KnownList k xs => [ElemValF k]

-- | Can be used to report stuck type families, see
--   <a>https://kcsongor.github.io/report-stuck-families/</a>. This family
--   is able to check whether its argument <tt>a</tt> is stuck and report
--   an error <tt>err</tt> in that case.
type family Assert err a

-- | Approximates a normal form on the type level. <a>Catch</a> forces its
--   argument <tt>a</tt> and returns an empty <a>Constraint</a> if and only
--   if the argument does not produce a <a>TypeError</a>.
--   
--   The first equation will recursively force the kind of the argument
--   until it reaches <a>Type</a> or a <a>TypeError</a>. In the former
--   case, it falls over to the second equation which will produce the
--   empty constraint. In the latter case, it gets stuck with 'Catch
--   (TypeError ...)', and the compiler will report the error message.
--   
--   Thanks to <a>https://kcsongor.github.io/kcsongor</a> for the
--   suggestion.
type family Catch a
type family Seq (a :: k) (b :: k') :: k'

-- | Returns the first element of a type-level tuple with the kind <tt>(k,
--   k')</tt> marked by a prefix quote.
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Fst '(Int, String)
--   Fst '(Int, String) :: *
--   = Int
--   
--   &gt;&gt;&gt; :kind! Fst '(Functor, String)
--   Fst '(Functor, String) :: (* -&gt; *) -&gt; Constraint
--   = Functor
--   </pre>
type family Fst (t :: (k, k')) :: k

-- | Returns the second element of a type-level tuple with the kind <tt>(k,
--   k')</tt> marked by a prefix quote.
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Snd '(Int, String)
--   Snd '(Int, String) :: *
--   = [Char]
--   
--   &gt;&gt;&gt; :kind! Snd '(Int, Monad)
--   Snd '(Int, Monad) :: (* -&gt; *) -&gt; Constraint
--   = Monad
--   </pre>
type family Snd (t :: (k, k')) :: k'

-- | Check that a given type is an element of a type-level list:
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Elem String '[]
--   Elem String '[] :: Bool
--   = 'False
--   
--   &gt;&gt;&gt; :kind! Elem String '[Int, String]
--   Elem String '[Int, String] :: Bool
--   = 'True
--   
--   &gt;&gt;&gt; :kind! Elem String '[Int, Bool]
--   Elem String '[Int, Bool] :: Bool
--   = 'False
--   </pre>
type family Elem (e :: t) (es :: [t]) :: Bool
type family Head (xs :: [a]) :: Maybe a
type family Tail (xs :: [a]) :: Maybe [a]
type family Length (xs :: [a]) :: Nat

-- | Test whether or not a given type contains another:
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Contains (Either Int String) Int
--   Contains (Either Int String) Int :: Bool
--   = 'True
--   
--   &gt;&gt;&gt; :kind! Contains (Either Int String) Bool
--   Contains (Either Int String) Bool :: Bool
--   = 'False
--   
--   &gt;&gt;&gt; :kind! Contains (Either Int String) Either
--   Contains (Either Int String) Either :: Bool
--   = 'True
--   </pre>
type family Contains (f :: k) (a :: k') :: Bool

-- | Extract all occurrences of a given type from another:
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Extract (Either Int String) Int
--   Extract (Either Int String) Int :: [*]
--   = '[Int]
--   
--   &gt;&gt;&gt; :kind! Extract (Either Int String) Bool
--   Extract (Either Int String) Bool :: [*]
--   = '[]
--   
--   &gt;&gt;&gt; :kind! Extract (Either Int String) Either
--   Extract (Either Int String) Either :: [* -&gt; * -&gt; *]
--   = '[Either]
--   </pre>
type family Extract (f :: k) (a :: k') :: [k']
type family FromMaybe (d :: k) (x :: Maybe k) :: k
type family MaybeF (d :: k') (f :: k -> k') (x :: Maybe k) :: k'
type family FstMaybe (t :: Maybe (k, k')) :: Maybe k
type family SndMaybe (t :: Maybe (k, k')) :: Maybe k'
type family PrependMaybe (h :: Maybe a) (t :: Maybe [a]) :: Maybe [a]
type family MapMaybe (f :: k -> k') (a :: Maybe k) :: Maybe k'
type family BindMaybe (f :: k -> Maybe k') (a :: Maybe k) :: Maybe k'
type family JoinMaybe (a :: Maybe (Maybe k)) :: Maybe k
type family LiftM2Maybe (f :: k -> k' -> k'') (a :: Maybe k) (b :: Maybe k') :: Maybe k''
type family LiftTimesMaybe (a :: Maybe Nat) (b :: Maybe Nat) :: Maybe Nat
type family LiftTypeEqMaybe (a :: Maybe k) (b :: Maybe k') :: Constraint
type family Concat (xs :: [k]) (ys :: [k]) :: [k]
type Reverse xs = ReverseImplF xs '[]
type family (<?) (a :: Nat) (b :: Nat)
type family When (cond :: Bool) (constraint :: Constraint) :: Constraint
whenM :: Monad m => m Bool -> m () -> m ()
unlessM :: Monad m => m Bool -> m () -> m ()
ifM :: Monad m => m Bool -> m a -> m a -> m a
guardM :: MonadPlus m => m Bool -> m ()

-- | The <a>&amp;&amp;</a> operator lifted to a monad. If the first
--   argument evaluates to <a>False</a> the second argument will not be
--   evaluated.
(&&^) :: Monad m => m Bool -> m Bool -> m Bool
infixr 3 &&^

-- | The <a>||</a> operator lifted to a monad. If the first argument
--   evaluates to <a>True</a> the second argument will not be evaluated.
(||^) :: Monad m => m Bool -> m Bool -> m Bool
infixr 2 ||^

-- | <a>&amp;&amp;</a> lifted to an Applicative. Unlike <a>&amp;&amp;^</a>
--   the operator is <b>not</b> short-circuiting.
(<&&>) :: Applicative a => a Bool -> a Bool -> a Bool
infixr 3 <&&>

-- | <a>||</a> lifted to an Applicative. Unlike <a>||^</a> the operator is
--   <b>not</b> short-circuiting.
(<||>) :: Applicative a => a Bool -> a Bool -> a Bool
infixr 2 <||>
instance GHC.Base.Functor Torch.GraduallyTyped.Prelude.IsChecked
instance GHC.Generics.Generic (Torch.GraduallyTyped.Prelude.IsChecked a)
instance GHC.Show.Show a => GHC.Show.Show (Torch.GraduallyTyped.Prelude.IsChecked a)
instance GHC.Classes.Ord a => GHC.Classes.Ord (Torch.GraduallyTyped.Prelude.IsChecked a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Torch.GraduallyTyped.Prelude.IsChecked a)
instance Torch.GraduallyTyped.Prelude.KnownList k '[]
instance forall k (x :: k) (xs :: [k]). (Torch.GraduallyTyped.Prelude.KnownElem k x, Torch.GraduallyTyped.Prelude.KnownList k xs) => Torch.GraduallyTyped.Prelude.KnownList k (x : xs)

module Torch.GraduallyTyped.Layout

-- | Data type that represents the memory layout of a tensor.
data LayoutType

-- | The memory layout of the tensor is dense (strided).
Dense :: LayoutType

-- | The memory layout of the tensor is sparse.
Sparse :: LayoutType
type family DenseSym0 :: LayoutType
type family SparseSym0 :: LayoutType
data SLayoutType :: LayoutType -> Type
[SDense] :: SLayoutType ('Dense :: LayoutType)
[SSparse] :: SLayoutType ('Sparse :: LayoutType)
class KnownLayoutType (layoutType :: LayoutType)
layoutTypeVal :: KnownLayoutType layoutType => LayoutType

-- | Data type to represent whether or not the tensor's memory layout is
--   checked, that is, known to the compiler.
data Layout (layoutType :: Type)

-- | The tensor's memory layout is unknown to the compiler.
[UncheckedLayout] :: forall layoutType. Layout layoutType

-- | The tensor's memory layout is known to the compiler.
[Layout] :: forall layoutType. layoutType -> Layout layoutType
data SLayout (layout :: Layout LayoutType)
[SUncheckedLayout] :: LayoutType -> SLayout 'UncheckedLayout
[SLayout] :: forall layoutType. SLayoutType layoutType -> SLayout ('Layout layoutType)
class KnownLayout (layout :: Layout LayoutType)
layoutVal :: KnownLayout layout => Layout LayoutType
type family GetLayouts f
instance GHC.Show.Show layoutType => GHC.Show.Show (Torch.GraduallyTyped.Layout.Layout layoutType)
instance GHC.Show.Show (Torch.GraduallyTyped.Layout.SLayoutType layoutType)
instance GHC.Show.Show (Torch.GraduallyTyped.Layout.SLayout layout)
instance Torch.GraduallyTyped.Layout.KnownLayout 'Torch.GraduallyTyped.Layout.UncheckedLayout
instance Torch.GraduallyTyped.Layout.KnownLayoutType layoutType => Torch.GraduallyTyped.Layout.KnownLayout ('Torch.GraduallyTyped.Layout.Layout layoutType)
instance Data.Singletons.SingI layoutType => Data.Singletons.SingI ('Torch.GraduallyTyped.Layout.Layout layoutType)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Layout.Layout Torch.GraduallyTyped.Layout.LayoutType)
instance Torch.GraduallyTyped.Layout.KnownLayoutType 'Torch.GraduallyTyped.Layout.Dense
instance Torch.GraduallyTyped.Layout.KnownLayoutType 'Torch.GraduallyTyped.Layout.Sparse
instance Data.Singletons.SingKind Torch.GraduallyTyped.Layout.LayoutType
instance Data.Singletons.SingI 'Torch.GraduallyTyped.Layout.Dense
instance Data.Singletons.SingI 'Torch.GraduallyTyped.Layout.Sparse
instance Torch.Internal.Class.Castable Torch.GraduallyTyped.Layout.LayoutType Torch.Internal.Type.Layout
instance GHC.Classes.Eq Torch.GraduallyTyped.Layout.LayoutType
instance GHC.Show.Show Torch.GraduallyTyped.Layout.LayoutType

module Torch.GraduallyTyped.Index.Type
data Index (index :: Type)
[UncheckedIndex] :: forall index. Index index
[Index] :: forall index. index -> Index index
[NegativeIndex] :: forall index. index -> Index index
data SIndex (index :: Index Nat)
[SUncheckedIndex] :: Integer -> SIndex 'UncheckedIndex
[SIndex] :: forall index. KnownNat index => SIndex ('Index index)
[SNegativeIndex] :: forall index. KnownNat index => SIndex ('NegativeIndex index)
type family IndexF (index :: Index Nat) :: Nat
newtype DemotedIndex
DemotedIndex :: Integer -> DemotedIndex
instance GHC.Show.Show index => GHC.Show.Show (Torch.GraduallyTyped.Index.Type.Index index)
instance GHC.Show.Show (Torch.GraduallyTyped.Index.Type.SIndex index)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Index.Type.Index GHC.TypeNats.Nat)
instance GHC.TypeNats.KnownNat index => Data.Singletons.SingI ('Torch.GraduallyTyped.Index.Type.Index index)
instance GHC.TypeNats.KnownNat index => Data.Singletons.SingI ('Torch.GraduallyTyped.Index.Type.NegativeIndex index)

module Torch.GraduallyTyped.Device

-- | Data type to represent compute devices.
data DeviceType (deviceId :: Type)

-- | The tensor is stored in the CPU's memory.
[CPU] :: forall deviceId. DeviceType deviceId

-- | The tensor is stored the memory of the GPU with ID <tt>deviceId</tt>.
[CUDA] :: forall deviceId. deviceId -> DeviceType deviceId
data SDeviceType (deviceType :: DeviceType Nat)
[SCPU] :: SDeviceType 'CPU
[SCUDA] :: forall deviceId. KnownNat deviceId => SDeviceType ('CUDA deviceId)
type family CUDAF (deviceType :: DeviceType Nat) :: Nat
class KnownDeviceType (deviceType :: DeviceType Nat)
deviceTypeVal :: KnownDeviceType deviceType => DeviceType Int16

-- | Data type to represent whether or not the compute device is checked,
--   that is, known to the compiler.
data Device (deviceType :: Type)

-- | The compute device is unknown to the compiler.
[UncheckedDevice] :: forall deviceType. Device deviceType

-- | The compute device is known to the compiler.
[Device] :: forall deviceType. deviceType -> Device deviceType
data SDevice (deviceType :: Device (DeviceType Nat))
[SUncheckedDevice] :: DeviceType Int16 -> SDevice 'UncheckedDevice
[SDevice] :: forall deviceType. SDeviceType deviceType -> SDevice ('Device deviceType)
class KnownDevice (device :: Device (DeviceType Nat))
deviceVal :: KnownDevice device => Device (DeviceType Int16)
type family GetDevices f
instance GHC.Show.Show deviceId => GHC.Show.Show (Torch.GraduallyTyped.Device.DeviceType deviceId)
instance GHC.Classes.Ord deviceId => GHC.Classes.Ord (Torch.GraduallyTyped.Device.DeviceType deviceId)
instance GHC.Classes.Eq deviceId => GHC.Classes.Eq (Torch.GraduallyTyped.Device.DeviceType deviceId)
instance GHC.Show.Show deviceType => GHC.Show.Show (Torch.GraduallyTyped.Device.Device deviceType)
instance GHC.Show.Show (Torch.GraduallyTyped.Device.SDeviceType deviceType)
instance GHC.Show.Show (Torch.GraduallyTyped.Device.SDevice device)
instance Torch.GraduallyTyped.Device.KnownDevice 'Torch.GraduallyTyped.Device.UncheckedDevice
instance Torch.GraduallyTyped.Device.KnownDeviceType deviceType => Torch.GraduallyTyped.Device.KnownDevice ('Torch.GraduallyTyped.Device.Device deviceType)
instance Data.Singletons.SingI deviceType => Data.Singletons.SingI ('Torch.GraduallyTyped.Device.Device deviceType)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Device.Device (Torch.GraduallyTyped.Device.DeviceType GHC.TypeNats.Nat))
instance Torch.GraduallyTyped.Device.KnownDeviceType 'Torch.GraduallyTyped.Device.CPU
instance GHC.TypeNats.KnownNat deviceId => Torch.GraduallyTyped.Device.KnownDeviceType ('Torch.GraduallyTyped.Device.CUDA deviceId)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Device.DeviceType GHC.TypeNats.Nat)
instance Data.Singletons.SingI 'Torch.GraduallyTyped.Device.CPU
instance GHC.TypeNats.KnownNat deviceId => Data.Singletons.SingI ('Torch.GraduallyTyped.Device.CUDA deviceId)

module Torch.GraduallyTyped.DType
data DType

-- | Bool
Bool :: DType

-- | Byte
UInt8 :: DType

-- | Char
Int8 :: DType

-- | Short
Int16 :: DType

-- | Int
Int32 :: DType

-- | Long
Int64 :: DType

-- | Half
Half :: DType

-- | Float
Float :: DType

-- | Double
Double :: DType

-- | ComplexHalf
ComplexHalf :: DType

-- | ComplexFloat
ComplexFloat :: DType

-- | ComplexDouble
ComplexDouble :: DType

-- | QInt8
QInt8 :: DType

-- | QUInt8
QUInt8 :: DType

-- | QInt32
QInt32 :: DType

-- | BFloat16
BFloat16 :: DType
type family BoolSym0 :: DType
type family UInt8Sym0 :: DType
type family Int8Sym0 :: DType
type family Int16Sym0 :: DType
type family Int32Sym0 :: DType
type family Int64Sym0 :: DType
type family HalfSym0 :: DType
type family FloatSym0 :: DType
type family DoubleSym0 :: DType
type family ComplexHalfSym0 :: DType
type family ComplexFloatSym0 :: DType
type family ComplexDoubleSym0 :: DType
type family QInt8Sym0 :: DType
type family QUInt8Sym0 :: DType
type family QInt32Sym0 :: DType
type family BFloat16Sym0 :: DType
data SDType :: DType -> Type
[SBool] :: SDType ('Bool :: DType)
[SUInt8] :: SDType ('UInt8 :: DType)
[SInt8] :: SDType ('Int8 :: DType)
[SInt16] :: SDType ('Int16 :: DType)
[SInt32] :: SDType ('Int32 :: DType)
[SInt64] :: SDType ('Int64 :: DType)
[SHalf] :: SDType ('Half :: DType)
[SFloat] :: SDType ('Float :: DType)
[SDouble] :: SDType ('Double :: DType)
[SComplexHalf] :: SDType ('ComplexHalf :: DType)
[SComplexFloat] :: SDType ('ComplexFloat :: DType)
[SComplexDouble] :: SDType ('ComplexDouble :: DType)
[SQInt8] :: SDType ('QInt8 :: DType)
[SQUInt8] :: SDType ('QUInt8 :: DType)
[SQInt32] :: SDType ('QInt32 :: DType)
[SBFloat16] :: SDType ('BFloat16 :: DType)
class KnownDType (dType :: DType)
dTypeVal :: KnownDType dType => DType

-- | Data type to represent whether or not the tensor data type is checked,
--   that is, known to the compiler.
data DataType (dType :: Type)

-- | The tensor data type is unknown to the compiler.
[UncheckedDataType] :: forall dType. DataType dType

-- | The tensor data type is known to the compiler.
[DataType] :: forall dType. dType -> DataType dType
data SDataType (dataType :: DataType DType)
[SUncheckedDataType] :: DType -> SDataType 'UncheckedDataType
[SDataType] :: forall dType. SDType dType -> SDataType ('DataType dType)
class KnownDataType (dataType :: DataType DType)
dataTypeVal :: KnownDataType dataType => DataType DType
type family GetDataTypes f
instance GHC.Show.Show dType => GHC.Show.Show (Torch.GraduallyTyped.DType.DataType dType)
instance GHC.Show.Show (Torch.GraduallyTyped.DType.SDType dType)
instance GHC.Show.Show (Torch.GraduallyTyped.DType.SDataType dataType)
instance Torch.GraduallyTyped.DType.KnownDataType 'Torch.GraduallyTyped.DType.UncheckedDataType
instance Torch.GraduallyTyped.DType.KnownDType dType => Torch.GraduallyTyped.DType.KnownDataType ('Torch.GraduallyTyped.DType.DataType dType)
instance Data.Singletons.SingI dType => Data.Singletons.SingI ('Torch.GraduallyTyped.DType.DataType dType)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.DType.DataType Torch.GraduallyTyped.DType.DType)
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Bool
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.UInt8
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int8
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int16
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int32
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Int64
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Half
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Float
instance Torch.GraduallyTyped.DType.KnownDType 'Torch.GraduallyTyped.DType.Double
instance Data.Singletons.SingKind Torch.GraduallyTyped.DType.DType
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Bool
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.UInt8
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Int8
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Int16
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Int32
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Int64
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Half
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Float
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.Double
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.ComplexHalf
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.ComplexFloat
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.ComplexDouble
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.QInt8
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.QUInt8
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.QInt32
instance Data.Singletons.SingI 'Torch.GraduallyTyped.DType.BFloat16
instance Torch.Internal.Class.Castable Torch.GraduallyTyped.DType.DType Torch.Internal.Type.ScalarType
instance GHC.Read.Read Torch.GraduallyTyped.DType.DType
instance GHC.Show.Show Torch.GraduallyTyped.DType.DType
instance GHC.Classes.Eq Torch.GraduallyTyped.DType.DType

module Torch.GraduallyTyped.RequiresGradient

-- | Data type to represent whether or not the tensor requires gradient
--   computations.
data RequiresGradient

-- | The tensor requires gradient computations.
WithGradient :: RequiresGradient

-- | Gradient computations for this tensor are disabled.
WithoutGradient :: RequiresGradient
type family WithGradientSym0 :: RequiresGradient
type family WithoutGradientSym0 :: RequiresGradient
data SRequiresGradient :: RequiresGradient -> Type
[SWithGradient] :: SRequiresGradient ('WithGradient :: RequiresGradient)
[SWithoutGradient] :: SRequiresGradient ('WithoutGradient :: RequiresGradient)
class KnownRequiresGradient (requiresGradient :: RequiresGradient)
requiresGradientVal :: KnownRequiresGradient requiresGradient => RequiresGradient

-- | Data type to represent whether or not it is known by the compiler if
--   the tensor requires gradient computations.
data Gradient (requiresGradient :: Type)

-- | Whether or not the tensor requires gradient computations is unknown to
--   the compiler.
[UncheckedGradient] :: forall requiresGradient. Gradient requiresGradient

-- | Whether or not the tensor requires gradient computations is known to
--   the compiler.
[Gradient] :: forall requiresGradient. requiresGradient -> Gradient requiresGradient
data SGradient (gradient :: Gradient RequiresGradient)
[SUncheckedGradient] :: RequiresGradient -> SGradient 'UncheckedGradient
[SGradient] :: forall requiresGradient. SRequiresGradient requiresGradient -> SGradient ('Gradient requiresGradient)
class KnownGradient (gradient :: Gradient RequiresGradient)
gradientVal :: KnownGradient gradient => Gradient RequiresGradient
type family GetGradients f
instance GHC.Show.Show requiresGradient => GHC.Show.Show (Torch.GraduallyTyped.RequiresGradient.Gradient requiresGradient)
instance GHC.Show.Show (Torch.GraduallyTyped.RequiresGradient.SRequiresGradient requiresGradient)
instance GHC.Show.Show (Torch.GraduallyTyped.RequiresGradient.SGradient requiresGradient)
instance Torch.GraduallyTyped.RequiresGradient.KnownGradient 'Torch.GraduallyTyped.RequiresGradient.UncheckedGradient
instance Torch.GraduallyTyped.RequiresGradient.KnownRequiresGradient requiresGradient => Torch.GraduallyTyped.RequiresGradient.KnownGradient ('Torch.GraduallyTyped.RequiresGradient.Gradient requiresGradient)
instance Data.Singletons.SingI requiresGradient => Data.Singletons.SingI ('Torch.GraduallyTyped.RequiresGradient.Gradient requiresGradient)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.RequiresGradient.Gradient Torch.GraduallyTyped.RequiresGradient.RequiresGradient)
instance Torch.GraduallyTyped.RequiresGradient.KnownRequiresGradient 'Torch.GraduallyTyped.RequiresGradient.WithGradient
instance Torch.GraduallyTyped.RequiresGradient.KnownRequiresGradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient
instance Data.Singletons.SingKind Torch.GraduallyTyped.RequiresGradient.RequiresGradient
instance Data.Singletons.SingI 'Torch.GraduallyTyped.RequiresGradient.WithGradient
instance Data.Singletons.SingI 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient
instance GHC.Classes.Eq Torch.GraduallyTyped.RequiresGradient.RequiresGradient
instance GHC.Show.Show Torch.GraduallyTyped.RequiresGradient.RequiresGradient

module Torch.GraduallyTyped.Scalar
class (Castable a (ForeignPtr Scalar)) => Scalar a
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Types.Float
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Types.Double
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Types.Int
instance Torch.GraduallyTyped.Scalar.Scalar GHC.Num.Integer.Integer
instance Torch.Internal.Class.Castable GHC.Types.Float (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Double (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Int (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Num.Integer.Integer (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)

module Torch.GraduallyTyped.Shape.Type
data Size (size :: Type)
[UncheckedSize] :: forall size. Size size
[Size] :: forall size. size -> Size size
data SSize (size :: Size Nat)
[SUncheckedSize] :: Integer -> SSize 'UncheckedSize
[SSize] :: forall size. KnownNat size => SSize ('Size size)
type family SizeF (size :: Size Nat) :: Nat
class KnownSize (size :: Size Nat)
sizeVal :: KnownSize size => Size Integer
data Name (name :: Type)
[UncheckedName] :: forall name. Name name
[Name] :: forall name. name -> Name name
data SName (name :: Name Symbol)
[SUncheckedName] :: String -> SName 'UncheckedName
[SName] :: forall name. KnownSymbol name => SName ('Name name)
pattern SNoName :: SName ('Name "*")
type family NameF (name :: Name Symbol) :: Symbol
class KnownName (name :: Name Symbol)
nameVal :: KnownName name => Name String
data Dim (name :: Type) (size :: Type)
[Dim] :: forall name size. name -> size -> Dim name size
data SDim (dim :: Dim (Name Symbol) (Size Nat))
[SDim] :: forall name size. SName name -> SSize size -> SDim ('Dim name size)
pattern (:&:) :: forall (name :: Name Symbol) (size :: Size Nat). SName name -> SSize size -> SDim ('Dim name size)
infix 9 :&:
class KnownDim (dim :: Dim (Name Symbol) (Size Nat))
dimVal :: KnownDim dim => Dim (Name String) (Size Integer)

-- | Data type to select dimensions by name or by index.
data By (name :: Type) (index :: Type)

-- | Select a dimension by name.
[ByName] :: forall name index. name -> By name index

-- | Select a dimension by index. Counting starts at zero for the first
--   dimension.
[ByIndex] :: forall name index. index -> By name index
data SBy (by :: By Symbol Nat)
[SByName] :: forall name. KnownSymbol name => SBy ('ByName name)
[SByIndex] :: forall index. KnownNat index => SBy ('ByIndex index)
type family ByNameF (by :: By Symbol Nat) :: Symbol
type family ByIndexF (by :: By Symbol Nat) :: Nat
class KnownBy (by :: By Symbol Nat)
byVal :: KnownBy by => By String Integer
data SelectDim (by :: Type)

-- | Unknown method of dimension selection.
[UncheckedSelectDim] :: forall by. SelectDim by

-- | Known method of dimension selection, that is, either by name or by
--   index.
[SelectDim] :: forall by. by -> SelectDim by
data SSelectDim (selectDim :: SelectDim (By Symbol Nat))
[SUncheckedSelectDim] :: By String Integer -> SSelectDim 'UncheckedSelectDim
[SSelectDim] :: forall by. SBy by -> SSelectDim ('SelectDim by)
class KnownSelectDim (selectDim :: SelectDim (By Symbol Nat))
selectDimVal :: KnownSelectDim selectDim => SelectDim (By String Integer)
data SelectDims (selectDims :: Type)
[UncheckedSelectDims] :: forall selectDims. SelectDims selectDims
[SelectDims] :: forall selectDims. selectDims -> SelectDims selectDims
data SSelectDims (selectDims :: SelectDims [By Symbol Nat])
[SUncheckedSelectDims] :: [By String Integer] -> SSelectDims 'UncheckedSelectDims
[SSelectDims] :: forall bys. SList bys -> SSelectDims ('SelectDims bys)
class KnownSelectDims (selectDims :: SelectDims [By Symbol Nat])
selectDimsVal :: KnownSelectDims selectDims => SelectDims [By String Integer]

-- | Data type to represent tensor shapes, that is, lists of dimensions.
data Shape (dims :: Type)

-- | The shape is fully unchecked. Neither the number of the dimensions nor
--   any dimension properties are known to the compiler.
[UncheckedShape] :: forall dims. Shape dims

-- | The shape is partially known to the compiler. The list of dimensions
--   has a known length, but may contain <tt>UncheckedDim</tt>, that is,
--   unknown dimensions.
[Shape] :: forall dims. dims -> Shape dims
data SShape (shape :: Shape [Dim (Name Symbol) (Size Nat)])
[SUncheckedShape] :: [Dim String Integer] -> SShape 'UncheckedShape
[SShape] :: forall dims. SList dims -> SShape ('Shape dims)
class KnownShape (shape :: Shape [Dim (Name Symbol) (Size Nat)])
shapeVal :: KnownShape shape => Shape [Dim (Name String) (Size Integer)]
type family GetShapes f
instance GHC.Show.Show size => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Size size)
instance GHC.Show.Show name => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Name name)
instance (GHC.Show.Show name, GHC.Show.Show size) => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.Classes.Ord name, GHC.Classes.Ord size) => GHC.Classes.Ord (Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.Classes.Eq name, GHC.Classes.Eq size) => GHC.Classes.Eq (Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.Classes.Ord name, GHC.Classes.Ord index) => GHC.Classes.Ord (Torch.GraduallyTyped.Shape.Type.By name index)
instance (GHC.Classes.Eq name, GHC.Classes.Eq index) => GHC.Classes.Eq (Torch.GraduallyTyped.Shape.Type.By name index)
instance (GHC.Show.Show name, GHC.Show.Show index) => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.By name index)
instance GHC.Show.Show dims => GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.Shape dims)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SSize size)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SName name)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SDim dim)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SBy by)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SSelectDim selectDim)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SSelectDims selectDims)
instance GHC.Show.Show (Torch.GraduallyTyped.Shape.Type.SShape shape)
instance Torch.GraduallyTyped.Shape.Type.KnownShape 'Torch.GraduallyTyped.Shape.Type.UncheckedShape
instance Torch.GraduallyTyped.Shape.Type.KnownShape ('Torch.GraduallyTyped.Shape.Type.Shape '[])
instance (Torch.GraduallyTyped.Shape.Type.KnownShape ('Torch.GraduallyTyped.Shape.Type.Shape dims), Torch.GraduallyTyped.Shape.Type.KnownDim dim) => Torch.GraduallyTyped.Shape.Type.KnownShape ('Torch.GraduallyTyped.Shape.Type.Shape (dim : dims))
instance Data.Singletons.SingI dims => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.Shape dims)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.Shape [Torch.GraduallyTyped.Shape.Type.Dim (Torch.GraduallyTyped.Shape.Type.Name GHC.Types.Symbol) (Torch.GraduallyTyped.Shape.Type.Size GHC.TypeNats.Nat)])
instance Torch.GraduallyTyped.Shape.Type.KnownSelectDims 'Torch.GraduallyTyped.Shape.Type.UncheckedSelectDims
instance Torch.GraduallyTyped.Shape.Type.KnownSelectDims ('Torch.GraduallyTyped.Shape.Type.SelectDims '[])
instance (Torch.GraduallyTyped.Shape.Type.KnownBy by, Torch.GraduallyTyped.Shape.Type.KnownSelectDims ('Torch.GraduallyTyped.Shape.Type.SelectDims bys)) => Torch.GraduallyTyped.Shape.Type.KnownSelectDims ('Torch.GraduallyTyped.Shape.Type.SelectDims (by : bys))
instance Data.Singletons.SingI bys => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.SelectDims bys)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.SelectDims [Torch.GraduallyTyped.Shape.Type.By GHC.Types.Symbol GHC.TypeNats.Nat])
instance Torch.GraduallyTyped.Shape.Type.KnownSelectDim 'Torch.GraduallyTyped.Shape.Type.UncheckedSelectDim
instance Torch.GraduallyTyped.Shape.Type.KnownBy by => Torch.GraduallyTyped.Shape.Type.KnownSelectDim ('Torch.GraduallyTyped.Shape.Type.SelectDim by)
instance Data.Singletons.SingI by => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.SelectDim by)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.SelectDim (Torch.GraduallyTyped.Shape.Type.By GHC.Types.Symbol GHC.TypeNats.Nat))
instance GHC.TypeLits.KnownSymbol name => Torch.GraduallyTyped.Shape.Type.KnownBy ('Torch.GraduallyTyped.Shape.Type.ByName name)
instance GHC.TypeNats.KnownNat index => Torch.GraduallyTyped.Shape.Type.KnownBy ('Torch.GraduallyTyped.Shape.Type.ByIndex index)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.By GHC.Types.Symbol GHC.TypeNats.Nat)
instance GHC.TypeLits.KnownSymbol name => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.ByName name)
instance GHC.TypeNats.KnownNat index => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.ByIndex index)
instance (Torch.GraduallyTyped.Shape.Type.KnownName name, Torch.GraduallyTyped.Shape.Type.KnownSize size) => Torch.GraduallyTyped.Shape.Type.KnownDim ('Torch.GraduallyTyped.Shape.Type.Dim name size)
instance (GHC.TypeLits.KnownSymbol name, GHC.TypeNats.KnownNat size) => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name name) ('Torch.GraduallyTyped.Shape.Type.Size size))
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.Dim (Torch.GraduallyTyped.Shape.Type.Name GHC.Types.Symbol) (Torch.GraduallyTyped.Shape.Type.Size GHC.TypeNats.Nat))
instance Data.Bifunctor.Bifunctor Torch.GraduallyTyped.Shape.Type.Dim
instance Torch.GraduallyTyped.Shape.Type.KnownName 'Torch.GraduallyTyped.Shape.Type.UncheckedName
instance GHC.TypeLits.KnownSymbol name => Torch.GraduallyTyped.Shape.Type.KnownName ('Torch.GraduallyTyped.Shape.Type.Name name)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.Name GHC.Types.Symbol)
instance GHC.TypeLits.KnownSymbol name => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.Name name)
instance Torch.GraduallyTyped.Shape.Type.KnownSize 'Torch.GraduallyTyped.Shape.Type.UncheckedSize
instance GHC.TypeNats.KnownNat size => Torch.GraduallyTyped.Shape.Type.KnownSize ('Torch.GraduallyTyped.Shape.Type.Size size)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Shape.Type.Size GHC.TypeNats.Nat)
instance GHC.TypeNats.KnownNat size => Data.Singletons.SingI ('Torch.GraduallyTyped.Shape.Type.Size size)
instance Torch.Internal.Class.Castable GHC.Base.String (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Dimname)
instance Torch.Internal.Class.Castable [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Dimname] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.DimnameList)
instance Torch.Internal.Class.Castable [GHC.Base.String] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.DimnameList)
instance Torch.Internal.Class.Castable [GHC.Num.Integer.Integer] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)

module Torch.GraduallyTyped.Internal.TensorOptions
newtype TensorOptions
TensorOptions :: ForeignPtr TensorOptions -> TensorOptions
tensorOptions :: forall gradient layout device dataType. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> TensorOptions
tensorDims :: forall shape. SShape shape -> [Dim String Integer]
instance Torch.Internal.Class.Castable Torch.GraduallyTyped.Internal.TensorOptions.TensorOptions (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorOptions)

module Torch.GraduallyTyped.Index.Class
type family IndexOutOfBound (idx :: Nat) (dim :: Dim (Name Symbol) (Size Nat))
type family InRangeImplF (idx :: Index Nat) (dim :: Dim (Name Symbol) (Size Nat)) :: Bool
type family InRangeCheckF (idx :: Index Nat) (dim :: Dim (Name Symbol) (Size Nat)) (ok :: Bool) :: Constraint
type InRangeF idx dim = InRangeCheckF idx dim (InRangeImplF idx dim)

module Torch.GraduallyTyped.Index

module Torch.GraduallyTyped.Tensor.MathOperations.Other

module Torch.GraduallyTyped.Tensor.MathOperations.Spectral

module Torch.GraduallyTyped.Unify

-- | <tt>a <a>+</a> b</tt> unifies <tt>a</tt> and <tt>b</tt>. Think of it
--   as a kind-level monoid.
type family (<+>) a b
infixr 8 <+>

-- | Desugared kind unification.
--   
--   TODO: add data type unification of scalar (Haskell) data types and
--   those of kind <tt>DataType</tt>. Perhaps convert the scalar (Haskell)
--   data type first to a <tt>DataType</tt> so that the kinds are aligned.
type family Unify k a b
type family UnifyCheck k a b
type UnifyRequiresGradientMessage (requiresGradient :: RequiresGradient) (requiresGradient' :: RequiresGradient) = "The supplied tensors must all either require or disable gradient calculation," % "but different gradient settings were found:" % "" % "    " <> requiresGradient <> " and " <> requiresGradient' <> "." % ""
type UnifyLayoutErrorMessage (layoutType :: k) (layoutType' :: k') = "The supplied tensors must have the same memory layout," % "but different layouts were found:" % "" % "    " <> layoutType <> " and " <> layoutType' <> "." % ""
type UnifyDeviceErrorMessage (deviceType :: k) (deviceType' :: k') = "The supplied tensors must be on the same device, " % "but different device locations were found:" % "" % "    " <> deviceType <> " and " <> deviceType' <> "." % ""
type UnifyDataTypeErrorMessage (dType :: k) (dType' :: k') = "The supplied tensors must have the same data type, " % "but different data types were found:" % "" % "    " <> dType <> " and " <> dType' <> "." % ""
type UnifyDimsErrorMessage (dims :: k) (dims' :: k') = "The supplied tensors must have shapes with identical number of dimensions," % "but dimension lists of different lengths were found." % "Here are the tails of both dimension lists:" % "" % "    " <> dims <> " and " <> dims' <> "." % "" % "Try extending, (un-)squeezing, or broadcasting the tensor(s)."
type UnifyNameErrorMessage (name :: k) (name' :: k') = "The supplied dimensions must be the same," % "but dimensions with different names were found:" % "" % "    " <> name <> " and " <> name' <> "." % "" % "Check spelling and whether or not this is really what you want." % "If you are certain, consider dropping or changing the names."
type UnifySizeErrorMessage (size :: k) (size' :: k') = "The supplied dimensions must be the same," % "but dimensions with different sizes were found:" % "" % "    " <> size <> " and " <> size' <> "." % "" % "Check whether or not this is really what you want." % "If you are certain, adjust the sizes such that they match."
type UnifyRightAssociativeL k a b c = Unify k (Unify k a b) c ~ Unify k a (Unify k b c)
type UnifyIdempotenceL2 k a b = Unify k a (Unify k a b) ~ Unify k a b
type UnifyIdempotenceL2C k a b = Unify k a (Unify k b a) ~ Unify k a b
type UnifyIdempotenceL3 k a b c = Unify k a (Unify k b (Unify k a c)) ~ Unify k a (Unify k b c)
type UnifyIdempotenceL3C k a b c = Unify k a (Unify k b (Unify k c a)) ~ Unify k a (Unify k b c)
type UnifyIdempotenceL4 k a b c d = Unify k a (Unify k b (Unify k c (Unify k a d))) ~ Unify k a (Unify k b (Unify k c d))
type UnifyIdempotenceL4C k a b c d = Unify k a (Unify k b (Unify k c (Unify k d a))) ~ Unify k a (Unify k b (Unify k c d))
type UnifyIdempotenceL5 k a b c d e = Unify k a (Unify k b (Unify k c (Unify k d (Unify k a e)))) ~ Unify k a (Unify k b (Unify k c (Unify k d e)))
type UnifyIdempotenceL5C k a b c d e = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e a)))) ~ Unify k a (Unify k b (Unify k c (Unify k d e)))
type UnifyIdempotenceL6 k a b c d e f = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k a f))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e f))))
type UnifyIdempotenceL6C k a b c d e f = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f a))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e f))))
type UnifyIdempotenceL7 k a b c d e f g = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k a g)))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f g)))))
type UnifyIdempotenceL7C k a b c d e f g = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g a)))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f g)))))
type UnifyIdempotenceL8 k a b c d e f g h = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k a h))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g h))))))
type UnifyIdempotenceL8C k a b c d e f g h = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h a))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g h))))))
type UnifyIdempotenceL9 k a b c d e f g h i = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h (Unify k a i)))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h i)))))))
type UnifyIdempotenceL9C k a b c d e f g h i = Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h (Unify k i a)))))))) ~ Unify k a (Unify k b (Unify k c (Unify k d (Unify k e (Unify k f (Unify k g (Unify k h i)))))))
type family (<|>) a b
infixr 8 <|>
type family Or k a b
type OrRightAssociativeL k a b c = Or k (Or k a b) c ~ Or k a (Or k b c)
type OrIdempotenceL2 k a b = Or k a (Or k a b) ~ Or k a b
type OrIdempotenceL2C k a b = Or k a (Or k b a) ~ Or k a b
type OrIdempotenceL3 k a b c = Or k a (Or k b (Or k a c)) ~ Or k a (Or k b c)
type OrIdempotenceL3C k a b c = Or k a (Or k b (Or k c a)) ~ Or k a (Or k b c)
type OrIdempotenceL4 k a b c d = Or k a (Or k b (Or k c (Or k a d))) ~ Or k a (Or k b (Or k c d))
type OrIdempotenceL4C k a b c d = Or k a (Or k b (Or k c (Or k d a))) ~ Or k a (Or k b (Or k c d))
type OrIdempotenceL5 k a b c d e = Or k a (Or k b (Or k c (Or k d (Or k a e)))) ~ Or k a (Or k b (Or k c (Or k d e)))
type OrIdempotenceL5C k a b c d e = Or k a (Or k b (Or k c (Or k d (Or k e a)))) ~ Or k a (Or k b (Or k c (Or k d e)))
type OrIdempotenceL6 k a b c d e f = Or k a (Or k b (Or k c (Or k d (Or k e (Or k a f))))) ~ Or k a (Or k b (Or k c (Or k d (Or k e f))))
type OrIdempotenceL6C k a b c d e f = Or k a (Or k b (Or k c (Or k d (Or k e (Or k f a))))) ~ Or k a (Or k b (Or k c (Or k d (Or k e f))))

module Torch.GraduallyTyped.Shape.Class
type family AddSizeF (size :: Size Nat) (size' :: Size Nat) :: Size Nat
type family AddDimF (dim :: Dim (Name Symbol) (Size Nat)) (dim' :: Dim (Name Symbol) (Size Nat)) :: Dim (Name Symbol) (Size Nat)
type family BroadcastSizeF (size :: Size Nat) (size' :: Size Nat) :: Maybe (Size Nat)
type family BroadcastDimF (dim :: Dim (Name Symbol) (Size Nat)) (dim' :: Dim (Name Symbol) (Size Nat)) :: Maybe (Dim (Name Symbol) (Size Nat))
type family NumelDimF (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe Nat
type family BroadcastDimsCheckF (dims :: [Dim (Name Symbol) (Size Nat)]) (dims' :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family BroadcastDimsImplF (reversedDims :: [Dim (Name Symbol) (Size Nat)]) (reversedDims' :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type BroadcastDimsF dims dims' = BroadcastDimsCheckF dims dims' (BroadcastDimsImplF (Reverse dims) (Reverse dims'))
type family BroadcastShapesF (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (shape' :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family NumelDimsF (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe Nat
type family NumelF (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Maybe Nat
type family GetDimAndIndexByNameF (index :: Nat) (result :: (Maybe (Dim (Name Symbol) (Size Nat)), Maybe Nat)) (name :: Symbol) (dims :: [Dim (Name Symbol) (Size Nat)]) :: (Maybe (Dim (Name Symbol) (Size Nat)), Maybe Nat)
type family GetDimByNameF (name :: Symbol) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe (Dim (Name Symbol) (Size Nat))
type family GetIndexByNameF (name :: Symbol) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe Nat
type family GetDimByIndexF (index :: Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe (Dim (Name Symbol) (Size Nat))
type family GetDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe (Dim (Name Symbol) (Size Nat))
type GetDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot return the first dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family GetDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe (Dim (Name Symbol) (Size Nat))) :: Dim (Name Symbol) (Size Nat)
type family GetDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Dim (Name Symbol) (Size Nat)
type family (!) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (_k :: k) :: Dim (Name Symbol) (Size Nat)

-- | Get dimension by index or by name from a shape.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @8 :|: SUncheckedName "feature" :&amp;: SSize @2 :|: SNil
--   
--   &gt;&gt;&gt; dim = sGetDimFromShape (SSelectDim $ SByName @"batch") shape
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim ('Name "batch") ('Size 8)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Checked "batch", dimSize = Checked 8}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dim = sGetDimFromShape (SSelectDim $ SByName @"feature") shape
--   
--   &gt;&gt;&gt; :type dim
--   dim
--     :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName 'UncheckedSize))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Unchecked "feature", dimSize = Checked 2}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dim = sGetDimFromShape (SSelectDim $ SByName @"sequence") shape
--   
--   &gt;&gt;&gt; :type dim
--   dim
--     :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName 'UncheckedSize))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   *** Exception: GetDimError {gdeBy = ByName "sequence"}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dim = sGetDimFromShape (SSelectDim $ SByIndex @0) shape
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim ('Name "batch") ('Size 8)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Checked "batch", dimSize = Checked 8}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; :type sGetDimFromShape (SSelectDim $ SByIndex @2) shape
--   sGetDimFromShape (SSelectDim $ SByIndex @2) shape
--     :: MonadThrow m =&gt; m (SDim (TypeError ...))
--   </pre>
sGetDimFromShape :: forall selectDim shape dim m. (dim ~ GetDimF selectDim shape, MonadThrow m) => SSelectDim selectDim -> SShape shape -> m (SDim dim)
data GetDimError
GetDimError :: By String Integer -> GetDimError
[gdeBy] :: GetDimError -> By String Integer
GetDimErrorWithDims :: By String Integer -> [Dim String Integer] -> GetDimError
[gdewdBy] :: GetDimError -> By String Integer
[gdewdDims] :: GetDimError -> [Dim String Integer]
type family ReplaceDimByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimNameByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (name :: Name Symbol) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimNameImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (name' :: Name Symbol) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimSizeByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (size' :: Size Nat) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimSizeImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (size' :: Size Nat) :: Maybe [Dim (Name Symbol) (Size Nat)]
type ReplaceDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) = "Cannot replace the first dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'" % "" % "with" % "" % "    '" <> dim <> "'." % ""
type family ReplaceDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family ReplaceDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Shape [Dim (Name Symbol) (Size Nat)]
type family InsertDimByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family InsertDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Maybe [Dim (Name Symbol) (Size Nat)]
type InsertDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) = "Cannot insert the dimension" % "" % "    '" <> dim <> "'" % "" % "before the first dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family InsertDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family InsertDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) (dim :: Dim (Name Symbol) (Size Nat)) :: Shape [Dim (Name Symbol) (Size Nat)]
type family PrependDimF (dim :: Dim (Name Symbol) (Size Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family RemoveDimByIndexF (index :: Maybe Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type family RemoveDimImplF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: Maybe [Dim (Name Symbol) (Size Nat)]
type RemoveDimErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot remove the dimension by" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family RemoveDimCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family RemoveDimF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
data UnifyNameError
UnifyNameError :: String -> String -> UnifyNameError
[uneExpect] :: UnifyNameError -> String
[uneActual] :: UnifyNameError -> String
sUnifyName :: forall m name name'. MonadThrow m => SName name -> SName name' -> m (SName (name <+> name'))
data UnifySizeError
UnifySizeError :: Integer -> Integer -> UnifySizeError
[useExpect] :: UnifySizeError -> Integer
[useActual] :: UnifySizeError -> Integer
sUnifySize :: forall m size size'. MonadThrow m => SSize size -> SSize size' -> m (SSize (size <+> size'))

-- | Unify two dimensions.
--   
--   <pre>
--   &gt;&gt;&gt; dimA = SName @"*" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dimB = SName @"batch" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dim = sUnifyDim dimA dimB
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim ('Name "batch") ('Size 0)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Checked "batch", dimSize = Checked 0}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dimC = SName @"feature" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; :type sUnifyDim dimB dimC
--   sUnifyDim dimB dimC
--     :: MonadThrow m =&gt; m (SDim ('Dim (TypeError ...) ('Size 0)))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dimD = SUncheckedName "batch" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dim = sUnifyDim dimA dimD
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName ('Size 0)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   Dim {dimName = Unchecked "batch", dimSize = Checked 0}
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dimE = SUncheckedName "feature" :&amp;: SSize @0
--   
--   &gt;&gt;&gt; dim = sUnifyDim dimB dimE
--   
--   &gt;&gt;&gt; :type dim
--   dim :: MonadThrow m =&gt; m (SDim ('Dim 'UncheckedName ('Size 0)))
--   
--   &gt;&gt;&gt; fromSing &lt;$&gt; dim
--   *** Exception: UnifyNameError {uneExpect = "batch", uneActual = "feature"}
--   </pre>
sUnifyDim :: forall m dim dim'. MonadThrow m => SDim dim -> SDim dim' -> m (SDim (dim <+> dim'))
instance GHC.Show.Show Torch.GraduallyTyped.Shape.Class.GetDimError
instance GHC.Show.Show Torch.GraduallyTyped.Shape.Class.UnifyNameError
instance GHC.Show.Show Torch.GraduallyTyped.Shape.Class.UnifySizeError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Shape.Class.UnifySizeError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Shape.Class.UnifyNameError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Shape.Class.GetDimError

module Torch.GraduallyTyped.Tensor.Type

-- | A gradually typed tensor.
--   
--   <pre>
--                             Compute device, e.g. `'Device <a>CPU</a>
--                            
--                                            List of dimensions, e.g. `'Shape '[ 'Dim 'UncheckedName ('Size 8), 'Dim 'UncheckedName ('Size 1) ]`
--                                           
--   Tensor gradient layout device dataType shape
--                                  
--                                   Data type, e.g. `'DataType <a>DType</a>
--                    
--                     Memory layout, e.g. `'Layout <a>Dense</a>
--             
--              Whether or not the tensor requires a gradient, e.g. `'Gradient <a>WithGradient</a> for one that does
--   </pre>
newtype Tensor (gradient :: Gradient RequiresGradient) (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (shape :: Shape [Dim (Name Symbol) (Size Nat)])

-- | Unsafe constructor for tensors. Do not call this constructor directly,
--   use smart constructors like <tt>ones</tt> or <tt>randn</tt> instead.
[UnsafeTensor] :: forall gradient layout device dataType shape. ForeignPtr Tensor -> Tensor gradient layout device dataType shape
data TensorSpec (gradient :: Gradient RequiresGradient) (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (shape :: Shape [Dim (Name Symbol) (Size Nat)])
[TensorSpec] :: forall gradient layout device dataType shape. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SShape shape -> TensorSpec gradient layout device dataType shape

-- | Alias for an untyped tensor without gradients.
type UncheckedTensor = Tensor 'UncheckedGradient 'UncheckedLayout 'UncheckedDevice 'UncheckedDataType 'UncheckedShape

-- | Alias for an untyped tensor with gradients.
type UncheckedParameter = Tensor ('Gradient 'WithGradient) 'UncheckedLayout 'UncheckedDevice 'UncheckedDataType 'UncheckedShape

-- | Alias for a tensor on CPU memory without gradients.
type CPUTensor = Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU)

-- | Alias for a tensor on CPU memory with gradients.
type CPUParameter = Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU)

-- | Alias for a sparse tensor on CPU memory without gradients.
type SparseCPUTensor = Tensor ('Gradient 'WithoutGradient) ('Layout 'Sparse) ('Device 'CPU)

-- | Alias for a sparse tensor on CPU memory with gradients.
type SparseCPUParameter = Tensor ('Gradient 'WithGradient) ('Layout 'Sparse) ('Device 'CPU)

-- | Alias for a tensor on CUDA memory without gradients.
type CUDATensor deviceId = Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device ('CUDA deviceId))

-- | Alias for a tensor on CUDA memory with gradients.
type CUDAParameter deviceId = Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device ('CUDA deviceId))

-- | Alias for a sparse tensor on CUDA memory without gradients.
type SparseCUDATensor deviceId = Tensor ('Gradient 'WithoutGradient) ('Layout 'Sparse) ('Device ('CUDA deviceId))

-- | Alias for a sparse tensor on CUDA memory with gradients.
type SparseCUDAParameter deviceId = Tensor ('Gradient 'WithGradient) ('Layout 'Sparse) ('Device ('CUDA deviceId))

-- | Takes a tensor that may or may not require gradient computations and
--   returns a copy that does not require them.
withoutGradient :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> IO (Tensor ('Gradient 'WithoutGradient) layout device dataType shape)

-- | Takes a tensor that does not requires gradient computations and
--   returns a copy that requires them.
withGradient :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> IO (Tensor ('Gradient 'WithGradient) layout device dataType shape)

-- | Turn gradient computations off or on for a tensor.
sSetGradient :: forall gradient gradient' layout device dataType shape. SGradient gradient -> Tensor gradient' layout device dataType shape -> IO (Tensor gradient layout device dataType shape)
class SGetGradient (gradient :: Gradient RequiresGradient)

-- | Returns the gradually typed information for whether or not gradient
--   computations for the tensor are turned on.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' gradient = sOnes $ TensorSpec gradient (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SGradient SWithGradient
--   
--   &gt;&gt;&gt; sGetGradient t
--   SGradient SWithGradient
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedGradient WithoutGradient
--   
--   &gt;&gt;&gt; sGetGradient t
--   SUncheckedGradient WithoutGradient
--   </pre>
sGetGradient :: forall layout device dataType shape. SGetGradient gradient => Tensor gradient layout device dataType shape -> SGradient gradient

-- | Returns the untyped memory layout of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' gradient = sOnes $ TensorSpec gradient (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SGradient SWithGradient
--   
--   &gt;&gt;&gt; getRequiresGradient t
--   WithGradient
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedGradient WithoutGradient
--   
--   &gt;&gt;&gt; getRequiresGradient t
--   WithoutGradient
--   </pre>
getRequiresGradient :: forall layout device dataType shape. SGetGradient gradient => Tensor gradient layout device dataType shape -> RequiresGradient
data GradientError
GradientError :: RequiresGradient -> RequiresGradient -> GradientError
[geExpected] :: GradientError -> RequiresGradient
[geActual] :: GradientError -> RequiresGradient

-- | Checks whether or not gradient computations are required for a tensor
--   and returns a statically annotated copy of it wrapped in a
--   <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if gradients are computed for the
--   tensor according to the argument <tt>gradient</tt>. If gradients are
--   expected but none are computed, then the result will be
--   <a>Nothing</a>. If gradients are not expected but are computed, then
--   the result will be <a>Nothing</a> as well.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t
--   &lt;- sOnes $ TensorSpec (SUncheckedGradient WithGradient) (SLayout
--   SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName <tt>"batch"
--   :&amp;: SSize </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|:
--   SNil) &gt;&gt;&gt; t' &lt;- sCheckedGradient (SGradient SWithGradient)
--   t &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient 'WithGradient)
--   ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim
--   ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   &gt;&gt;&gt; t' &lt;- sCheckedGradient (SGradient SWithoutGradient) t
--   *** Exception: GradientError {geExpected = WithoutGradient, geActual =
--   WithGradient}
sCheckedGradient :: forall gradient' m gradient layout device dataType shape. (SGetGradient gradient, MonadThrow m, Catch (gradient <+> gradient')) => SGradient gradient' -> Tensor gradient layout device dataType shape -> m (Tensor gradient' layout device dataType shape)
checkedGradient :: forall gradient' m gradient layout device dataType shape. (SingI gradient', SGetGradient gradient, MonadThrow m, Catch (gradient <+> gradient')) => Tensor gradient layout device dataType shape -> m (Tensor gradient' layout device dataType shape)

-- | Returns the input tensor but with <tt>UncheckedGradeint</tt> as
--   gradient type annotation. Any static information about whether or not
--   the gradient computation is required for the tensor is lost. However,
--   the tensor's underlying data structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedGradient t
--   uncheckedGradient t
--     :: Tensor
--          'UncheckedGradient
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedGradient :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor 'UncheckedGradient layout device dataType shape

-- | Returns a dense copy of the tensor.
toDense :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient ('Layout 'Dense) device dataType shape)

-- | Returns a sparse copy of the tensor.
toSparse :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient ('Layout 'Sparse) device dataType shape)

-- | Set the memory layout of a tensor to a given layout.
sSetLayout :: forall m gradient layout layout' device dataType shape. MonadThrow m => SLayout layout -> Tensor gradient layout' device dataType shape -> m (Tensor gradient layout device dataType shape)
class SGetLayout (layout :: Layout LayoutType)

-- | Returns the gradually typed memory layout of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' layout = sOnes $ TensorSpec (SGradient SWithGradient) layout (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SLayout SDense
--   
--   &gt;&gt;&gt; sGetLayout t
--   SLayout SDense
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedLayout Dense
--   
--   &gt;&gt;&gt; sGetLayout t
--   SUncheckedLayout Dense
--   </pre>
sGetLayout :: forall gradient device dataType shape. SGetLayout layout => Tensor gradient layout device dataType shape -> SLayout layout

-- | Returns the untyped memory layout of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' layout = sOnes $ TensorSpec (SGradient SWithGradient) layout (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SLayout SDense
--   
--   &gt;&gt;&gt; getLayoutType t
--   Dense
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedLayout Dense
--   
--   &gt;&gt;&gt; getLayoutType t
--   Dense
--   </pre>
getLayoutType :: forall gradient device dataType shape. SGetLayout layout => Tensor gradient layout device dataType shape -> LayoutType
data LayoutError
LayoutError :: LayoutType -> LayoutType -> LayoutError
[leExpected] :: LayoutError -> LayoutType
[leActual] :: LayoutError -> LayoutType

-- | Checks whether or not the input tensor has the memory layout
--   <tt>layout</tt> and returns a statically annotated copy of it wrapped
--   in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the memory
--   layout <tt>layout</tt>. If it does not have it, then the result will
--   be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t
--   &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SUncheckedLayout
--   Dense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName <tt>"batch"
--   :&amp;: SSize </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|:
--   SNil) &gt;&gt;&gt; t' &lt;- sCheckedLayout (SLayout SDense) t
--   &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout
--   'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name
--   "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt;
--   t' &lt;- sCheckedLayout (SLayout SSparse) t *** Exception: LayoutError
--   {leExpected = Sparse, leActual = Dense}
sCheckedLayout :: forall layout' m gradient layout device dataType shape. (SGetLayout layout, MonadThrow m, Catch (layout <+> layout')) => SLayout layout' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout' device dataType shape)
checkedLayout :: forall layout' m gradient layout device dataType shape. (SingI layout', SGetLayout layout, MonadThrow m, Catch (layout <+> layout')) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout' device dataType shape)

-- | Returns the input tensor but with <a>UncheckedLayout</a> as memory
--   layout type annotation. Any static information about the tensor's
--   memory layout is thus erased. However, the tensor's underlying data
--   structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedLayout t
--   uncheckedLayout t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          'UncheckedLayout
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedLayout :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient 'UncheckedLayout device dataType shape

-- | Returns a copy of the tensor in CPU memory.
cpu :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout ('Device 'CPU) dataType shape)

-- | Returns a copy of the tensor in CUDA memory.
cuda :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout ('Device ('CUDA 0)) dataType shape)

-- | Reallocates a tensor on the specified device.
sSetDevice :: forall m gradient layout device device' dataType shape. MonadThrow m => SDevice device -> Tensor gradient layout device' dataType shape -> m (Tensor gradient layout device dataType shape)
class SGetDevice (device :: Device (DeviceType Nat))

-- | Returns the gradually typed compute device of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; ones' device = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) device (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- ones' $ SDevice SCPU
--   
--   &gt;&gt;&gt; sGetDevice t
--   SDevice SCPU
--   
--   &gt;&gt;&gt; t &lt;- ones' $ SUncheckedDevice CPU
--   
--   &gt;&gt;&gt; sGetDevice t
--   SUncheckedDevice CPU
--   </pre>
sGetDevice :: forall gradient layout dataType shape. SGetDevice device => Tensor gradient layout device dataType shape -> SDevice device

-- | Returns the untyped compute device of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; ones' device = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) device (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- ones' $ SDevice SCPU
--   
--   &gt;&gt;&gt; getDeviceType t
--   CPU
--   
--   &gt;&gt;&gt; t &lt;- ones' $ SUncheckedDevice CPU
--   
--   &gt;&gt;&gt; getDeviceType t
--   CPU
--   </pre>
getDeviceType :: forall gradient layout dataType shape. SGetDevice device => Tensor gradient layout device dataType shape -> DeviceType Int16
data DeviceError
DeviceError :: DeviceType Int16 -> DeviceType Int16 -> DeviceError
[deExpected] :: DeviceError -> DeviceType Int16
[deActual] :: DeviceError -> DeviceType Int16

-- | Checks whether or not the input tensor is in the memory of
--   <tt>device</tt> and returns a statically annotated copy of it wrapped
--   in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor is indeed on
--   <tt>device</tt>. If it is not, then the result will be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t
--   &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense)
--   (SUncheckedDevice CPU) (SDataType SFloat) (SShape $ SName <tt>"batch"
--   :&amp;: SSize </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|:
--   SNil) &gt;&gt;&gt; t' &lt;- sCheckedDevice (SDevice SCPU) t
--   &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout
--   'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name
--   "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt;
--   t' &lt;- sCheckedDevice (SDevice (SCUDA @0)) t *** Exception:
--   DeviceError {deExpected = CUDA 0, deActual = CPU}
sCheckedDevice :: forall device' m gradient layout device dataType shape. (SGetDevice device, MonadThrow m, Catch (device <+> device')) => SDevice device' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device' dataType shape)
checkedDevice :: forall device' m gradient layout device dataType shape. (SingI device', SGetDevice device, MonadThrow m, Catch (device <+> device')) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device' dataType shape)

-- | Returns the input tensor but with <a>UncheckedDevice</a> as device
--   type annotation. Any static information about the tensor's device is
--   thus erased. However, the tensor's underlying data structure is not
--   changed.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDevice t
--   uncheckedDevice t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          'UncheckedDevice
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedDevice :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout 'UncheckedDevice dataType shape

-- | Returns a copy of the tensor converted to <a>DType</a>.
bool :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape)

-- | Returns a copy of the tensor converted to <a>UInt8</a>.
byte :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'UInt8) shape)

-- | Returns a copy of the tensor converted to <a>Int8</a>.
char :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int8) shape)

-- | Returns a copy of the tensor converted to <a>DType</a>.
short :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int16) shape)

-- | Returns a copy of the tensor converted to <a>Int32</a>.
int :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int32) shape)

-- | Returns a copy of the tensor converted to <a>Int64</a>.
long :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int64) shape)

-- | Returns a copy of the tensor converted to the 16-bit floating point
--   format <a>Half</a>.
half :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Half) shape)

-- | Returns a copy of the tensor converted to the 32-bit floating point
--   format <a>DType</a>.
float :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Float) shape)

-- | Returns a copy of the tensor converted to the 32-bit floating point
--   format <a>DType</a>.
double :: forall m gradient layout device dataType shape. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Double) shape)

-- | Set the data type of a tensor to the specified data type.
sSetDataType :: forall m gradient layout device dataType dataType' shape. MonadThrow m => SDataType dataType -> Tensor gradient layout device dataType' shape -> m (Tensor gradient layout device dataType shape)
class SGetDataType (dataType :: DataType DType)

-- | Returns the gradually typed compute data type of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' dataType = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) dataType (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SDataType SFloat
--   
--   &gt;&gt;&gt; sGetDataType t
--   SDataType SFloat
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedDataType Float
--   
--   &gt;&gt;&gt; sGetDataType t
--   SUncheckedDataType Float
--   </pre>
sGetDataType :: forall gradient layout device shape. SGetDataType dataType => Tensor gradient layout device dataType shape -> SDataType dataType

-- | Returns the untyped compute data type of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' dataType = sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) dataType (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SDataType SFloat
--   
--   &gt;&gt;&gt; getDType t
--   Float
--   
--   &gt;&gt;&gt; t &lt;- sOnes' $ SUncheckedDataType Float
--   
--   &gt;&gt;&gt; getDType t
--   Float
--   </pre>
getDType :: forall gradient layout device shape. SGetDataType dataType => Tensor gradient layout device dataType shape -> DType
data DataTypeError
DataTypeError :: DType -> DType -> DataTypeError
[dtExpected] :: DataTypeError -> DType
[dtActual] :: DataTypeError -> DType

-- | Checks whether or not the input tensor has the data type
--   <tt>dataType</tt> and returns a statically annotated copy of it
--   wrapped in a <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the data
--   type <tt>dataType</tt>. If it does not have it, then the result will
--   be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t
--   &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense)
--   (SDevice SCPU) (SUncheckedDataType Float) (SShape $ SName <tt>"batch"
--   :&amp;: SSize </tt>32 :|: SName <tt>"feature" :&amp;: SSize </tt>8 :|:
--   SNil) &gt;&gt;&gt; t' &lt;- checkedDataType <tt>('DataType 'Float) t
--   &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient 'WithGradient) ('Layout
--   'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name
--   "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)]) &gt;&gt;&gt;
--   t' &lt;- checkedDataType </tt>('DataType 'Double) t *** Exception:
--   DataTypeError {dtExpected = Double, dtActual = Float}
sCheckedDataType :: forall dataType' m gradient layout device dataType shape. (SGetDataType dataType, MonadThrow m, Catch (dataType <+> dataType')) => SDataType dataType' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType' shape)
checkedDataType :: forall dataType' m gradient layout device dataType shape. (SingI dataType', SGetDataType dataType, MonadThrow m, Catch (dataType <+> dataType')) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType' shape)

-- | Returns the input tensor but with <a>UncheckedDataType</a> as
--   data-type type annotation. Any static information about the tensor's
--   data type is thus erased. However, the tensor's underlying data
--   structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDataType t
--   uncheckedDataType t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          'UncheckedDataType
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
uncheckedDataType :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device 'UncheckedDataType shape
class SGetShape (shape :: Shape [Dim (Name Symbol) (Size Nat)])

-- | Returns the gradually typed shape of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' = sOnes . TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil
--   
--   &gt;&gt;&gt; sGetShape t
--   SShape (SCons (SDim {sDimName = SName, sDimSize = SSize}) (SCons (SDim {sDimName = SName, sDimSize = SSize}) SNil))
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SUncheckedShape $ [Dim "batch" 32, Dim "feature" 8]
--   
--   &gt;&gt;&gt; sGetShape t
--   SUncheckedShape [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SUncheckedName "batch" :&amp;: SUncheckedSize 32 :|: SUncheckedName "feature" :&amp;: SSize @32 :|: SNil
--   
--   &gt;&gt;&gt; sGetShape t
--   SShape (SCons (SDim {sDimName = SUncheckedName "batch", sDimSize = SUncheckedSize 32}) (SCons (SDim {sDimName = SUncheckedName "feature", sDimSize = SSize}) SNil))
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @"batch" :&amp;: SUncheckedSize 32 :|: SName @"feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; sGetShape t
--   SShape (SCons (SDim {sDimName = SName, sDimSize = SUncheckedSize 32}) (SCons (SDim {sDimName = SName, sDimSize = SUncheckedSize 8}) SNil))
--   </pre>
sGetShape :: forall gradient layout device dataType. SGetShape shape => Tensor gradient layout device dataType shape -> SShape shape

-- | Returns the untyped shape of the input tensor.
--   
--   <pre>
--   &gt;&gt;&gt; sOnes' = sOnes . TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil
--   
--   &gt;&gt;&gt; getDims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SUncheckedShape $ [Dim "batch" 32, Dim "feature" 8]
--   
--   &gt;&gt;&gt; getDims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SUncheckedName "batch" :&amp;: SUncheckedSize 32 :|: SUncheckedName "feature" :&amp;: SSize @32 :|: SNil
--   
--   &gt;&gt;&gt; getDims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 32}]
--   
--   &gt;&gt;&gt; t &lt;- sOnes' . SShape $ SName @"batch" :&amp;: SUncheckedSize 32 :|: SName @"feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; getDims t
--   [Dim {dimName = "batch", dimSize = 32},Dim {dimName = "feature", dimSize = 8}]
--   </pre>
getDims :: forall gradient layout device dataType. SGetShape shape => Tensor gradient layout device dataType shape -> [Dim String Integer]
class SGetDims (dims :: [Dim (Name Symbol) (Size Nat)])
sGetDims :: SGetDims dims => [String] -> [Integer] -> SList dims
dimsError :: forall a. a
dimNameError :: forall a. String -> String -> a
dimSizeError :: forall a b. Show a => a -> a -> b
dimNameSizeError :: forall a b. Show a => String -> String -> a -> a -> b
class SGetDim (dim :: Dim (Name Symbol) (Size Nat))
sGetDim :: SGetDim dim => String -> Integer -> SDim dim
data ShapeError
ShapeError :: [Dim String Integer] -> [Dim String Integer] -> ShapeError
[seExpected] :: ShapeError -> [Dim String Integer]
[seActual] :: ShapeError -> [Dim String Integer]

-- | Checks whether or not the input tensor has the shape <tt>shape</tt>
--   and returns a statically annotated copy of it wrapped in a
--   <a>MonadThrow</a> <tt>m</tt>.
--   
--   For instance, if <tt>m</tt> is <a>Maybe</a>, then the result will be
--   wrapped in <a>Just</a> if and only if the tensor has indeed the shape
--   <tt>shape</tt>. If it is not, then the result will be <a>Nothing</a>.
--   
--   In the REPL, <tt>m</tt> will default to <a>IO</a>: &gt;&gt;&gt; t
--   &lt;- sOnes $ TensorSpec (SGradient SWithGradient) (SLayout SDense)
--   (SDevice SCPU) (SDataType SFloat) (SUncheckedShape [Dim "batch" 32,
--   Dim "feature" 8]) &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $ SName
--   <tt>"batch" :&amp;: SSize </tt>32 :|: SName <tt>"feature" :&amp;:
--   SSize </tt>8 :|: SNil) t &gt;&gt;&gt; :type t' t' :: Tensor ('Gradient
--   'WithGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float)
--   ('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature")
--   ('Size 8)]) &gt;&gt;&gt; t' &lt;- sCheckedShape (SShape $
--   SUncheckedName "batch" :&amp;: SSize <tt>32 :|: SName </tt>"feature"
--   :&amp;: SUncheckedSize 8 :|: SNil) t &gt;&gt;&gt; :type t' t' ::
--   Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU)
--   ('DataType 'Float) ('Shape '[ 'Dim 'UncheckedName ('Size 32), 'Dim
--   ('Name "feature") 'UncheckedSize]) &gt;&gt;&gt; t' &lt;- sCheckedShape
--   (SShape $ SName <tt>"batch" :&amp;: SSize </tt>32 :|: SName @"feature"
--   :&amp;: SUncheckedSize 32 :|: SNil) t *** Exception: ShapeError
--   {seExpected = [Dim {dimName = "batch", dimSize = 32},Dim {dimName =
--   "feature", dimSize = 32}], seActual = [Dim {dimName = "batch", dimSize
--   = 32},Dim {dimName = "feature", dimSize = 8}]}
sCheckedShape :: forall shape' m gradient layout device dataType shape. (SGetShape shape, MonadThrow m, Catch (shape <+> shape')) => SShape shape' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
checkedShape :: forall shape' m gradient layout device dataType shape. (SingI shape', SGetShape shape, MonadThrow m, Catch (shape <+> shape')) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')

-- | Returns the input tensor but with the selected dimension replaces with
--   <tt>UncheckedDim</tt> as dimension type annotation. The static
--   information about the selected tensor dimension is thus erased.
--   However, the tensor's underlying data structure is not changed.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDim @('SelectDim ('ByName "batch")) t
--   uncheckedDim @('SelectDim ('ByName "batch")) t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim 'UncheckedName 'UncheckedSize,
--                'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedDim @('SelectDim ('ByIndex 1)) t
--   uncheckedDim @('SelectDim ('ByIndex 1)) t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim 'UncheckedName 'UncheckedSize])
--   </pre>
uncheckedDim :: forall selectDim gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (ReplaceDimF selectDim shape ('Dim 'UncheckedName 'UncheckedSize))

-- | Returns the input tensor but with <a>UncheckedShape</a> as shape type
--   annotation. Any static information about the tensor's shape is thus
--   erased. However, the tensor's underlying data structure is not
--   changed.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type uncheckedShape t
--   uncheckedShape t
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   </pre>
uncheckedShape :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType 'UncheckedShape
gitHubErrorMsg :: String
isContiguous :: Tensor gradient layout device dataType shape -> Bool
contiguous :: Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
withTensor :: Tensor gradient layout device dataType shape -> (Ptr () -> IO a) -> IO a
class TensorLikeRaw a

-- | Guesses outer dim.
--   
--   <pre>
--   &gt;&gt;&gt; guessDim @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]
--   Just 3
--   </pre>
guessDim :: TensorLikeRaw a => Maybe a -> Maybe Int

-- | Guesses inner dims.
--   
--   <pre>
--   &gt;&gt;&gt; guessInnerDims @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]
--   [2]
--   </pre>
guessInnerDims :: (TensorLikeRaw a, MonadThrow m) => Maybe a -> m [Int]

-- | Reads a value from a tensor.
tensorPeekElemOff :: TensorLikeRaw a => Ptr () -> Int -> [Int] -> IO a

-- | Writes a value to a tensor.
tensorPokeElemOff :: TensorLikeRaw a => Ptr () -> Int -> [Int] -> a -> IO ()

-- | Guesses dims: concatenates <a>guessDim</a> with <a>guessInnerDims</a>.
--   
--   <pre>
--   &gt;&gt;&gt; guessDims @[[Int]] $ pure [[1, 2], [3, 4], [5, 6]]
--   [3,2]
--   </pre>
guessDims :: forall a m. (TensorLikeRaw a, MonadThrow m) => Maybe a -> m [Int]
unexpectedDimsError :: forall a m b. (TensorLikeRaw a, MonadThrow m) => [Int] -> Maybe a -> m b
class TensorLike a (dType :: DType) (dims :: [Dim (Name Symbol) (Size Nat)]) | a -> dims, a -> dType

-- | Creates a tensor from a <a>TensorLike</a> value.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- sToTensor (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) ([(1, 2), (3, 4), (5, 6)] :: [(Int, Int)])
--   
--   &gt;&gt;&gt; t
--   Tensor Int64 [3,2] [[ 1,  2],
--                       [ 3,  4],
--                       [ 5,  6]]
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Int64)
--          ('Shape
--             '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") ('Size 2)])
--   </pre>
sToTensor :: forall gradient layout device m. (TensorLike a dType dims, MonadThrow m) => SGradient gradient -> SLayout layout -> SDevice device -> a -> m (Tensor gradient layout device ('DataType dType) ('Shape dims))

-- | Creates a <a>TensorLike</a> from a tensor.
fromTensor :: forall gradient layout device. TensorLike a dType dims => Tensor gradient layout device ('DataType dType) ('Shape dims) -> a

-- | Non-singleton version of <a>sToTensor</a>.
toTensor :: forall gradient layout device a dType dims m. (TensorLike a dType dims, SingI gradient, SingI layout, SingI device, MonadThrow m) => a -> m (Tensor gradient layout device ('DataType dType) ('Shape dims))
sToTensorRaw :: forall gradient layout device a dType dims m. (TensorLike a dType dims, TensorLikeRaw a, SingI dType, MonadThrow m) => SGradient gradient -> SLayout layout -> SDevice device -> a -> m (Tensor gradient layout device ('DataType dType) ('Shape dims))
fromTensorRaw :: forall gradient layout device a dType dims. (TensorLike a dType dims, TensorLikeRaw a, SGetDims dims) => Tensor gradient layout device ('DataType dType) ('Shape dims) -> a
data DimMismatchError
DimMismatchError :: [Int] -> [Int] -> DimMismatchError
[dmeFirst] :: DimMismatchError -> [Int]
[dmeOther] :: DimMismatchError -> [Int]
checkDims :: MonadThrow m => [Int] -> [Int] -> m ()
unzip3 :: Functor f => f (a, b, c) -> (f a, f b, f c)
sSetTensorOptions :: forall gradient layout device dataType gradientFrom layoutFrom deviceFrom dataTypeFrom shape. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape -> IO (Tensor gradient layout device dataType shape)
setTensorOptions :: forall gradient layout device dataType gradientFrom layoutFrom deviceFrom dataTypeFrom shape. (SingI gradient, SingI layout, SingI device, SingI dataType) => Tensor gradientFrom layoutFrom deviceFrom dataTypeFrom shape -> IO (Tensor gradient layout device dataType shape)
instance GHC.Generics.Generic (Torch.GraduallyTyped.Tensor.Type.TensorSpec gradient layout device dataType shape)
instance GHC.Show.Show (Torch.GraduallyTyped.Tensor.Type.TensorSpec gradient layout device dataType shape)
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.GradientError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.LayoutError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.DeviceError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.DataTypeError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.ShapeError
instance GHC.Classes.Eq Torch.GraduallyTyped.Tensor.Type.DimMismatchError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.Type.DimMismatchError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.DimMismatchError
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Bool 'Torch.GraduallyTyped.DType.Bool '[]
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Int 'Torch.GraduallyTyped.DType.Int64 '[]
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Float 'Torch.GraduallyTyped.DType.Float '[]
instance Torch.GraduallyTyped.Tensor.Type.TensorLike GHC.Types.Double 'Torch.GraduallyTyped.DType.Double '[]
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLike b dType dims', Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw b, Data.Singletons.SingI dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape (dims Torch.GraduallyTyped.Unify.<+> dims')) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size 2))) => Torch.GraduallyTyped.Tensor.Type.TensorLike (a, b) dType dimsOut
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLike b dType dims', Torch.GraduallyTyped.Tensor.Type.TensorLike c dType dims', Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw b, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw c, Data.Singletons.SingI dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape (dims Torch.GraduallyTyped.Unify.<+> dims')) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size 3))) => Torch.GraduallyTyped.Tensor.Type.TensorLike (a, b, c) dType dimsOut
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Data.Singletons.SingI dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape dims) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)) => Torch.GraduallyTyped.Tensor.Type.TensorLike [a] dType dimsOut
instance (Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Data.Singletons.SingI dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape dims) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)) => Torch.GraduallyTyped.Tensor.Type.TensorLike (Data.Vector.Vector a) dType dimsOut
instance (GHC.TypeNats.KnownNat n, Torch.GraduallyTyped.Tensor.Type.TensorLike a dType dims, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Data.Singletons.SingI dType, Torch.GraduallyTyped.Tensor.Type.SGetDims dimsOut, 'Torch.GraduallyTyped.Shape.Type.Shape dimsOut GHC.Types.~ Torch.GraduallyTyped.Shape.Class.InsertDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 0)) ('Torch.GraduallyTyped.Shape.Type.Shape dims) ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size n))) => Torch.GraduallyTyped.Tensor.Type.TensorLike (Data.Vector.Sized.Vector n a) dType dimsOut
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Bool
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Int
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Float
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw GHC.Types.Double
instance (Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw b) => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (a, b)
instance (Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw b, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw c) => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (a, b, c)
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw [a]
instance Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (Data.Vector.Vector a)
instance (GHC.TypeNats.KnownNat n, Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw a) => Torch.GraduallyTyped.Tensor.Type.TensorLikeRaw (Data.Vector.Sized.Vector n a)
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.ShapeError
instance (Torch.GraduallyTyped.Tensor.Type.SGetDim dim, Torch.GraduallyTyped.Tensor.Type.SGetDims dims) => Torch.GraduallyTyped.Tensor.Type.SGetDims (dim : dims)
instance Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim 'Torch.GraduallyTyped.Shape.Type.UncheckedName 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)
instance GHC.TypeLits.KnownSymbol name => Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name name) 'Torch.GraduallyTyped.Shape.Type.UncheckedSize)
instance GHC.TypeNats.KnownNat size => Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim 'Torch.GraduallyTyped.Shape.Type.UncheckedName ('Torch.GraduallyTyped.Shape.Type.Size size))
instance (GHC.TypeLits.KnownSymbol name, GHC.TypeNats.KnownNat size) => Torch.GraduallyTyped.Tensor.Type.SGetDim ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name name) ('Torch.GraduallyTyped.Shape.Type.Size size))
instance Torch.GraduallyTyped.Tensor.Type.SGetDims dims => Torch.GraduallyTyped.Tensor.Type.SGetShape ('Torch.GraduallyTyped.Shape.Type.Shape dims)
instance Torch.GraduallyTyped.Tensor.Type.SGetDims '[]
instance Torch.GraduallyTyped.Tensor.Type.SGetShape 'Torch.GraduallyTyped.Shape.Type.UncheckedShape
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.DataTypeError
instance Torch.GraduallyTyped.Tensor.Type.SGetDataType 'Torch.GraduallyTyped.DType.UncheckedDataType
instance Data.Singletons.SingI dType => Torch.GraduallyTyped.Tensor.Type.SGetDataType ('Torch.GraduallyTyped.DType.DataType dType)
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.DeviceError
instance Torch.GraduallyTyped.Tensor.Type.SGetDevice 'Torch.GraduallyTyped.Device.UncheckedDevice
instance Torch.GraduallyTyped.Tensor.Type.SGetDevice ('Torch.GraduallyTyped.Device.Device 'Torch.GraduallyTyped.Device.CPU)
instance GHC.TypeNats.KnownNat deviceIndex => Torch.GraduallyTyped.Tensor.Type.SGetDevice ('Torch.GraduallyTyped.Device.Device ('Torch.GraduallyTyped.Device.CUDA deviceIndex))
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.LayoutError
instance Torch.GraduallyTyped.Tensor.Type.SGetLayout 'Torch.GraduallyTyped.Layout.UncheckedLayout
instance Torch.GraduallyTyped.Tensor.Type.SGetLayout ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Sparse)
instance Torch.GraduallyTyped.Tensor.Type.SGetLayout ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense)
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.Type.GradientError
instance Torch.GraduallyTyped.Tensor.Type.SGetGradient 'Torch.GraduallyTyped.RequiresGradient.UncheckedGradient
instance Torch.GraduallyTyped.Tensor.Type.SGetGradient ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithGradient)
instance Torch.GraduallyTyped.Tensor.Type.SGetGradient ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient)
instance GHC.Show.Show (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)
instance GHC.Num.Num (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)
instance Torch.Internal.Class.Castable (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor)
instance Torch.Internal.Class.Castable [Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)
instance Torch.Internal.Class.Castable (Torch.HList.HList tensors) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor] => Torch.Internal.Class.Castable (Torch.HList.HList (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape : tensors)) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor]
instance Torch.Internal.Class.Castable (Torch.HList.HList '[]) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor]
instance forall k (l :: [k]). Torch.Internal.Class.Castable (Torch.HList.HList l) [GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor] => Torch.Internal.Class.Castable (Torch.HList.HList l) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)

module Torch.GraduallyTyped.Tensor.Other

-- | triu
triu :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | tril
tril :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | masked fill
maskedFill :: forall gradient layout device dataType shape value gradient' layout' device' dataType' shape' shape'' m. (Scalar value, MonadThrow m, Catch (gradient <+> 'Gradient 'WithoutGradient), Catch (dataType <+> 'DataType 'Bool), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> value -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor gradient' (layout <+> (layout' <+> 'Layout 'Dense)) (device <+> device') dataType' shape'')

module Torch.GraduallyTyped.Tensor.MathOperations.Reduction
type ReductionErrorMessage reduction by dims = "Cannot apply '" <> reduction <> "' on the dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family ReductionCheckF reduction by dims result
type family BoolReductionF reduction selectDim shape

-- | Tests if all element in input evaluates to True.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; shape = SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @4 :|: SNil
--   
--   &gt;&gt;&gt; (t, _) &lt;- sRandn (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) shape) g
--   
--   &gt;&gt;&gt; t' &lt;- all =&lt;&lt; bool t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Bool)
--          ('Shape '[])
--   </pre>
all :: forall requiresGradient layout device dataType shape m. MonadThrow m => Tensor requiresGradient layout device dataType shape -> m (Tensor requiresGradient layout device ('DataType 'Bool) ('Shape '[]))

-- | Reduces each row of the input tensor in the selected dimension to True
--   if all elements in the row evaluate to True and False otherwise. For a
--   version that accepts non-singleton parameters see <a>allDim</a>.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; shape = SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @4 :|: SNil
--   
--   &gt;&gt;&gt; (t, _) &lt;- sRandn (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) shape) g
--   
--   &gt;&gt;&gt; t' &lt;- sAllDim (SSelectDim (SByIndex @1)) =&lt;&lt; bool t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Bool)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1)])
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; sAllDim (SUncheckedSelectDim (ByIndex 3)) t
--   *** Exception: HasktorchException "Exception: Dimension out of range (expected to be in range of [-2, 1], but got 3)...
--   </pre>
sAllDim :: forall selectDim gradient layout device dataType shape shape' m. (MonadThrow m, shape' ~ BoolReductionF "all" selectDim shape, Catch shape') => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Bool) shape')
type AllDimF selectDim shape = BoolReductionF "all" selectDim shape

-- | Reduces each row of the input tensor in the selected dimension to True
--   if all elements in the row evaluate to True and False otherwise. For a
--   version that accepts singleton parameters see <a>sAllDim</a>.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; type Shape' = 'Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 4) ]
--   
--   &gt;&gt;&gt; (t, _) &lt;- randn @('Gradient 'WithoutGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @Shape' g
--   
--   &gt;&gt;&gt; t' &lt;- allDim @('SelectDim ('ByIndex 1)) =&lt;&lt; bool t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Bool)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1)])
--   </pre>
allDim :: forall selectDim gradient layout device dataType shape shape' m. (SingI selectDim, MonadThrow m, shape' ~ AllDimF selectDim shape, Catch shape') => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Bool) shape')

-- | Tests if any element in input evaluates to True.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; shape = SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @4 :|: SNil
--   
--   &gt;&gt;&gt; (t, _) &lt;- sRandn (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) shape) g
--   
--   &gt;&gt;&gt; t' &lt;- any =&lt;&lt; bool t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Bool)
--          ('Shape '[])
--   </pre>
any :: forall requiresGradient layout device dataType shape m. MonadThrow m => Tensor requiresGradient layout device dataType shape -> m (Tensor requiresGradient layout device ('DataType 'Bool) ('Shape '[]))
type AnyDimF selectDim shape = BoolReductionF "any" selectDim shape

-- | Reduces each row of the input tensor in the selected dimension to True
--   if any element in the row evaluates to True and False otherwise. For a
--   version that accepts non-singleton parameters see <a>anyDim</a>.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; shape = SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @4 :|: SNil
--   
--   &gt;&gt;&gt; (t, _) &lt;- sRandn (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) shape) g
--   
--   &gt;&gt;&gt; t' &lt;- sAnyDim (SSelectDim (SByIndex @1)) =&lt;&lt; bool t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Bool)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1)])
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; sAnyDim (SUncheckedSelectDim (ByIndex 3)) t
--   *** Exception: HasktorchException "Exception: Dimension out of range (expected to be in range of [-2, 1], but got 3)...
--   </pre>
sAnyDim :: forall selectDim gradient layout device shape dataType shape' m. (MonadThrow m, shape' ~ AnyDimF selectDim shape, Catch shape') => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Bool) shape')

-- | Reduces each row of the input tensor in the selected dimension to True
--   if any element in the row evaluates to True and False otherwise. For a
--   version that accepts singleton parameters see <a>sAnyDim</a>.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; type Shape' = 'Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 4) ]
--   
--   &gt;&gt;&gt; (t, _) &lt;- randn @('Gradient 'WithoutGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @Shape' g
--   
--   &gt;&gt;&gt; t' &lt;- anyDim @('SelectDim ('ByIndex 1)) =&lt;&lt; bool t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Bool)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1)])
--   </pre>
anyDim :: forall selectDim gradient layout device dataType shape shape' m. (SingI selectDim, MonadThrow m, shape' ~ AnyDimF selectDim shape, Catch shape') => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Bool) shape')
type family MeanSelectDimsF (bys :: [By Symbol Nat]) (dims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type family MeanF (selectDims :: SelectDims [By Symbol Nat]) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]

-- | Reduces the mean value over each row of the input tensor in the
--   dimensions selected by <tt>selectDims</tt>. For a version that accepts
--   non-singleton parameters see <tt>meanDim</tt>.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @8 :|: SName @"width" :&amp;: SSize @224 :|: SName @"height" :&amp;: SSize @224 :|: SNil
--   
--   &gt;&gt;&gt; (t, _) &lt;- sRandn (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) shape) g
--   
--   &gt;&gt;&gt; t' &lt;- sMeanDims (SSelectDims $ SByName @"width" :|: SByName @"height" :|: SNil) t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 8), 'Dim ('Name "width") ('Size 1),
--                'Dim ('Name "height") ('Size 1)])
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; sMeanDims (SUncheckedSelectDims [ByName "feature"]) t
--   *** Exception: HasktorchException "Exception: Name 'feature' not found in Tensor['batch', 'width', 'height']...
--   </pre>
sMeanDims :: forall selectDims gradient layout device dataType shape shape' m. (MonadThrow m, shape' ~ MeanF selectDims shape, Catch shape') => SSelectDims selectDims -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')

-- | Reduce the mean value over each row of the input tensor in the
--   dimensions selected by <tt>selectDims</tt>. For a version that accepts
--   singleton parameters see <tt>sMeanDim</tt>.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; type Shape' = 'Shape '[ 'Dim ('Name "batch") ('Size 8), 'Dim ('Name "feature") ('Size 4) ]
--   
--   &gt;&gt;&gt; (t, _) &lt;- randn @('Gradient 'WithoutGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @Shape' g
--   
--   &gt;&gt;&gt; t' &lt;- meanDims @('SelectDims '[ 'ByName "feature" ]) t
--   
--   &gt;&gt;&gt; :type t'
--   t'
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 8),
--                'Dim ('Name "feature") ('Size 1)])
--   </pre>
meanDims :: forall selectDims gradient layout device dataType shape shape' m. (SingI selectDims, MonadThrow m, shape' ~ MeanF selectDims shape, Catch shape') => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
type DimPositiveMessage reduction dim = "Cannot apply '" <> reduction <> "' because the dimension" % "" % "    '" <> dim <> "'" % "" % "is not positive."
type family DimPositiveF reduction dim
type family AllDimsPositiveImplF reduction dims
type family AllDimsPositiveF reduction shape
type MeanAllCheckF shape = AllDimsPositiveF "meanAll" shape

-- | Reduces a tensor by calculating the mean value over all dimensions.
meanAll :: forall gradient layout device dataType shape. MeanAllCheckF shape => Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType ('Shape '[])
type ArgmaxF selectDim shape = BoolReductionF "argmax" selectDim shape

-- | Argmax of a tensor given a dimension.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; spec = TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNoName :&amp;: SSize @2 :|: SNoName :&amp;: SSize @5 :|: SNil)
--   
--   &gt;&gt;&gt; (t, _) &lt;- sRandn spec g
--   
--   &gt;&gt;&gt; r &lt;- argmax (SSelectDim $ SByIndex @1) t
--   
--   &gt;&gt;&gt; :type r
--   r :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Int64)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1)])
--   
--   &gt;&gt;&gt; r
--   Tensor Int64 [2,1] [[ 0],
--                       [ 2]]
--   </pre>
argmax :: forall selectDims gradient layout device dataType shape shape' m. (MonadThrow m, shape' ~ ArgmaxF selectDims shape, Catch shape') => SSelectDim selectDims -> Tensor gradient layout device dataType shape -> m (Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int64) shape')
type MaxAllCheckF shape = AllDimsPositiveF "maxAll" shape
maxAll :: forall gradient layout device dataType shape. MaxAllCheckF shape => Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType ('Shape '[])

module Torch.GraduallyTyped.Tensor.MathOperations.Comparison
gt :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
lt :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
ge :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
le :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
eq :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
ne :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
(>.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
(<.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
(>=.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
(<=.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
(==.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
(/=.) :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, Catch (dataType <+> dataType'), shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')
data Order
Ascending :: Order
Descending :: Order
data Sorted gradient layout device dataType shape
[Sorted] :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Int64) shape -> Sorted gradient layout device dataType shape
type SortErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply sort on the dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family SortCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe (Dim (Name Symbol) (Size Nat))) :: [Dim (Name Symbol) (Size Nat)]
type family SortF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
sSort :: forall selectDim gradient layout device dataType shape. SSelectDim selectDim -> Order -> Tensor gradient layout device dataType shape -> Sorted gradient layout device dataType (SortF selectDim shape)
sort :: forall selectDim gradient layout device dataType shape. SingI selectDim => Order -> Tensor gradient layout device dataType shape -> Sorted gradient layout device dataType (SortF selectDim shape)
instance GHC.Generics.Generic Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Classes.Ord Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Classes.Eq Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Order
instance GHC.Generics.Generic (Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Sorted gradient layout device dataType shape)
instance GHC.Show.Show (Torch.GraduallyTyped.Tensor.MathOperations.Comparison.Sorted gradient layout device dataType shape)

module Torch.GraduallyTyped.Tensor.IndexingSlicingJoining
class HasCat (selectDim :: SelectDim (By Symbol Nat)) k (c :: k -> Type) (a :: k) where {
    type CatF selectDim a c :: Type;
}

-- | Concatenates the given sequence of seq tensors in the given dimension.
--   All tensors must either have the same shape (except in the
--   concatenating dimension) or be empty.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- ones @('Gradient 'WithGradient) @('Layout 'Dense) @('Device 'CPU) @('DataType 'Float) @('Shape '[ 'Dim ('Name "batch") ('Size 32), 'Dim ('Name "feature") ('Size 8)])
--   
--   &gt;&gt;&gt; :type cat @('SelectDim ('ByName "feature")) [t]
--   cat @('SelectDim ('ByName "feature")) [t]
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 32),
--                   'Dim 'UncheckedName 'UncheckedSize]))
--   
--   &gt;&gt;&gt; :type cat @('SelectDim ( 'ByIndex 0)) [t]
--   cat @('SelectDim ( 'ByIndex 0)) [t]
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             ('Shape
--                '[ 'Dim 'UncheckedName 'UncheckedSize,
--                   'Dim ('Name "feature") ('Size 8)]))
--   
--   &gt;&gt;&gt; :type sCat (SUncheckedSelectDim (ByIndex 0)) [t]
--   sCat (SUncheckedSelectDim (ByIndex 0)) [t]
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             'UncheckedShape)
--   </pre>
sCat :: forall m. (HasCat selectDim k c a, MonadThrow m) => SSelectDim selectDim -> c a -> m (CatF selectDim a c)
cat :: forall m. (HasCat selectDim k c a, SingI selectDim, MonadThrow m) => c a -> m (CatF selectDim a c)
type family CatListImplF (selectDim :: SelectDim (By Symbol Nat)) (tensor :: Type) :: Maybe Type
type CheckSpellingMessage = "Check the spelling of named dimensions, and make sure the number of dimensions is correct."
type family CatListCheckF (selectDim :: SelectDim (By Symbol Nat)) (tensor :: Type) (result :: Maybe Type) :: Type
type CatListF selectDim tensor = CatListCheckF selectDim tensor (CatListImplF selectDim tensor)
type family CatHListImplF selectDim tensors acc
type CatHListF selectDim tensors = CatHListImplF selectDim tensors 'Nothing
type ReshapeNumelMismatchMessage numel numel' shape shape' = "Cannot reshape the tensor. The original shape," % "" % "    '" <> shape <> "'," % "" % "and the new shape," % "" % "    '" <> shape' <> "'," % "" % "have different total numbers of elements," % "" % "    '" <> numel <> "' versus '" <> numel' <> "'," % "" % "respectively."
type family ReshapeImplF numel numel' shape shape'
type family ReshapeF shape shape'

-- | Returns a tensor with the same data and number of elements as the
--   input tensor, but with the specified shape:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) &lt;- sRandn (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil)) g
--   
--   &gt;&gt;&gt; output &lt;- sReshape (SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @2 :|: SNil) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 2)])
--   </pre>
--   
--   At the value level, a single dimension may be '-1', in which case it
--   is inferred from the remaining dimensions and the number of elements
--   in the input:
--   
--   <pre>
--   &gt;&gt;&gt; output' &lt;- sReshape (SShape $ SUncheckedName "*" :&amp;: SUncheckedSize (-1) :|: SNil) output
--   
--   &gt;&gt;&gt; :type output'
--   output'
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   
--   &gt;&gt;&gt; getDims output'
--   [Dim {dimName = "*", dimSize = 4}]
--   </pre>
sReshape :: forall m shape' gradient layout device dataType shape shape''. MonadThrow m => (shape'' ~ ReshapeF shape shape', Catch shape'') => SShape shape' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape'')

-- | Returns a tensor with the same data and number of elements as the
--   input tensor, but with the specified shape:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) &lt;- sRandn (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil)) g
--   
--   &gt;&gt;&gt; output &lt;- sReshape (SShape $ SName @"*" :&amp;: SSize @2 :|: SName @"*" :&amp;: SSize @2 :|: SNil) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 2)])
--   </pre>
--   
--   At the value level, a single dimension may be '-1', in which case it
--   is inferred from the remaining dimensions and the number of elements
--   in the input:
--   
--   <pre>
--   &gt;&gt;&gt; output' &lt;- sReshape (SShape $ SUncheckedName "*" :&amp;: SUncheckedSize (-1) :|: SNil) output
--   
--   &gt;&gt;&gt; :type output'
--   output'
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   
--   &gt;&gt;&gt; getDims output'
--   [Dim {dimName = "*", dimSize = 4}]
--   </pre>
sSetShape :: forall m shape' gradient layout device dataType shape shape''. MonadThrow m => (shape'' ~ ReshapeF shape shape', Catch shape'') => SShape shape' -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape'')
type family AllDimSizesChecked shape
reshape :: forall m shape' gradient layout device dataType shape shape''. (shape'' ~ ReshapeF shape shape', Catch shape'', When (AllDimSizesChecked shape) (shape' ~ shape''), SingI shape', MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape'')
type TransposeBy0Message by0 dims = "Cannot transpose the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "because the specified source dimension" % "" % "    '" <> by0 <> "'" % "" % "could not be found."
type TransposeBy1Message by1 dims = "Cannot transpose the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "because the specified target dimension" % "" % "    '" <> by1 <> "'" % "" % "could not be found."

-- | Compute transposed shapes.
--   
--   <pre>
--   &gt;&gt;&gt; type SelectBatch = 'SelectDim ('ByName "batch" :: By Symbol Nat)
--   
--   &gt;&gt;&gt; type SelectFeature = 'SelectDim ('ByName "feature" :: By Symbol Nat)
--   
--   &gt;&gt;&gt; type Dims = '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "feature") ('Size 8), 'Dim ('Name "anotherFeature") ('Size 12)]
--   
--   &gt;&gt;&gt; :kind! TransposeF SelectBatch SelectFeature ('Shape Dims)
--   TransposeF SelectBatch SelectFeature ('Shape Dims) :: Shape
--                                                           [Dim (Name Symbol) (Size Natural)]
--   = 'Shape
--       '[ 'Dim ('Name "feature") ('Size 8),
--          'Dim ('Name "batch") ('Size 10),
--          'Dim ('Name "anotherFeature") ('Size 12)]
--   
--   &gt;&gt;&gt; :kind! TransposeF SelectFeature SelectBatch ('Shape Dims)
--   TransposeF SelectFeature SelectBatch ('Shape Dims) :: Shape
--                                                           [Dim (Name Symbol) (Size Natural)]
--   = 'Shape
--       '[ 'Dim ('Name "feature") ('Size 8),
--          'Dim ('Name "batch") ('Size 10),
--          'Dim ('Name "anotherFeature") ('Size 12)]
--   </pre>
type family TransposeF selectDim0 selectDim1 shape
type family TransposeIndexIndexDimsF index0 index1 dims

-- | Returns a tensor that is a transposed version of <tt>input</tt>. The
--   selected dimensions <tt>selectDim0</tt> and <tt>selectDim1</tt> are
--   swapped.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) &lt;- sRandn (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"feature" :&amp;: SSize @5 :|: SNil)) g
--   
--   &gt;&gt;&gt; output &lt;- sTranspose (SSelectDim (SByName @"batch")) (SSelectDim (SByName @"feature")) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "feature") ('Size 5),
--                'Dim ('Name "batch") ('Size 10)])
--   
--   &gt;&gt;&gt; output &lt;- sTranspose (SUncheckedSelectDim (ByIndex 0)) (SSelectDim (SByIndex @1)) input
--   
--   &gt;&gt;&gt; :type output
--   output
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          'UncheckedShape
--   
--   &gt;&gt;&gt; getDims output
--   [Dim {dimName = "feature", dimSize = 5},Dim {dimName = "batch", dimSize = 10}]
--   </pre>
sTranspose :: forall selectDim0 selectDim1 gradient layout device dataType shape shape' m. (shape' ~ TransposeF selectDim0 selectDim1 shape, Catch shape', MonadThrow m) => SSelectDim selectDim0 -> SSelectDim selectDim1 -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
data TransposeError
TransposeMixedSelectorsError :: By String Integer -> By String Integer -> TransposeError
[teBy0] :: TransposeError -> By String Integer
[teBy1] :: TransposeError -> By String Integer
transpose :: forall selectDim0 selectDim1 gradient layout device dataType shape shape' m. (shape' ~ TransposeF selectDim0 selectDim1 shape, Catch shape', SingI selectDim0, SingI selectDim1, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
type UnsqueezeByMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot unsqueeze the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "because the specified source dimension" % "" % "    '" <> by <> "'" % "" % "could not be found."
type family UnsqueezeF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family UnsqueezeIndexDimsF (index :: Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]

-- | Unsqueezes a tensor with the specified dimension.
sUnsqueeze :: forall selectDim gradient layout device dataType shape shape' m. (shape' ~ UnsqueezeF selectDim shape, Catch shape', MonadThrow m) => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')

-- | Unsqueezes a tensor with the specified dimension.
unsqueeze :: forall selectDim gradient layout device dataType shape shape' m. (shape' ~ UnsqueezeF selectDim shape, Catch shape', SingI selectDim, MonadThrow m) => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')
type family SqueezeAllShapeF shape
type family SqueezeAllDimsF dims
squeezeAll :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType (SqueezeAllShapeF shape)
type family SqueezeDimByIndexF dimIndex dims
type family SqueezeDimImplF by dims
type family SqueezeDimByNameF dimIndex dims
type SqueezeDimMessage by dims = "Cannot squeeze the tensor with the dimensions" % "" % "    '" <> dims <> "'" % "" % "at the dimension" % "" % "    '" <> by <> "'." % ""
type family SqueezeDimCheckF by dims result

-- | Calculate the output shape of a squeeze along a given dimension
--   
--   <pre>
--   &gt;&gt;&gt; :kind! SqueezeDimF ('SelectDim ('ByIndex 1)) ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") ('Size 2)])
--   ...
--   = 'Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 2)]
--   </pre>
type family SqueezeDimF selectDim shape

-- | Squeeze a particular dimension.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- sOnes $ TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNoName :&amp;: SSize @2 :|: SNoName :&amp;: SSize @1 :|: SNoName :&amp;: SSize @2 :|: SNoName :&amp;: SSize @1 :|: SNoName :&amp;: SSize @2 :|: SNil)
--   
--   &gt;&gt;&gt; result &lt;- sSqueezeDim (SSelectDim $ SByIndex @1) t
--   
--   &gt;&gt;&gt; :t result
--   result
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 2),
--                'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") ('Size 2)])
--   
--   &gt;&gt;&gt; result
--   Tensor Float [2,2,1,2] [[[[ 1.0000   ,  1.0000   ]],
--                            [[ 1.0000   ,  1.0000   ]]],
--                           [[[ 1.0000   ,  1.0000   ]],
--                            [[ 1.0000   ,  1.0000   ]]]]
--   </pre>
sSqueezeDim :: forall selectDim gradient layout device dataType shape shape' m. (MonadThrow m, shape' ~ SqueezeDimF selectDim shape, Catch shape') => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')

-- | Expands a tensor to the specified shape.
sExpand :: forall shape' shape'' gradient layout device dataType shape. (shape'' ~ BroadcastShapesF shape shape', Catch shape'') => SShape shape' -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape''

-- | Expands a tensor to the specified shape.
expand :: forall shape' shape'' gradient layout device dataType shape. (SingI shape', shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape''

-- | Slices the self tensor along the selected dimension at the given
--   index. This function returns a view of the original tensor with the
--   given dimension removed.
--   
--   <pre>
--   &gt;&gt;&gt; nats &lt;- sArangeNaturals (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt32) (SSize @8)
--   
--   &gt;&gt;&gt; input &lt;- sReshape (SShape $ SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @2 :|: SNil) nats
--   
--   &gt;&gt;&gt; input
--   Tensor Int32 [4,2] [[ 0,  1],
--                       [ 2,  3],
--                       [ 4,  5],
--                       [ 6,  7]]
--   </pre>
--   
--   <tt>index</tt> can be provided at compile-time:
--   
--   <pre>
--   &gt;&gt;&gt; sSelect (SSelectDim (SByIndex @0)) (SIndex @1) input
--   Tensor Int32 [2] [ 2,  3]
--   </pre>
--   
--   <tt>index</tt> can also be provided at runtime:
--   
--   <pre>
--   &gt;&gt;&gt; sSelect (SSelectDim (SByIndex @0)) (SUncheckedIndex 1) input
--   Tensor Int32 [2] [ 2,  3]
--   </pre>
--   
--   It produces a runtime error if the <tt>index</tt> is too large:
--   
--   <pre>
--   &gt;&gt;&gt; sSelect (SSelectDim (SByIndex @0)) (SUncheckedIndex 10) input
--   *** Exception: IndexOutOfBoundError {ioobeIndex = 10, ioobeDim = Dim {dimName = "*", dimSize = 4}}
--   </pre>
sSelect :: forall selectDim index gradient layout device dataType shapeIn shapeOut m. (index `InRangeF` GetDimF selectDim shapeIn, shapeOut ~ RemoveDimF selectDim shapeIn, Catch shapeOut, SGetShape shapeIn, MonadThrow m) => SSelectDim selectDim -> SIndex index -> Tensor gradient layout device dataType shapeIn -> m (Tensor gradient layout device dataType shapeOut)
data IndexOutOfBoundError
IndexOutOfBoundError :: Integer -> Dim String Integer -> IndexOutOfBoundError
[ioobeIndex] :: IndexOutOfBoundError -> Integer
[ioobeDim] :: IndexOutOfBoundError -> Dim String Integer
select :: forall selectDim index gradient layout device dataType shapeIn shapeOut m. (SingI selectDim, SingI index, index `InRangeF` GetDimF selectDim shapeIn, shapeOut ~ RemoveDimF selectDim shapeIn, Catch shapeOut, SGetShape shapeIn, MonadThrow m) => Tensor gradient layout device dataType shapeIn -> m (Tensor gradient layout device dataType shapeOut)

-- | <a>GatherDimImplF</a> is a type-level helper function for
--   <a>sGatherDim</a>.
--   
--   <pre>
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 1) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 4), 'Dim ('Name "feature") ('Size 1)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "feature") ('Size 1)]
--   ...
--   = 'Just
--       '[ 'Dim ('Name "batch") ('Size 2),
--          'Dim ('Name "sequence") ('Size 4),
--          'Dim ('Name "feature") ('Size 1)]
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 1) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 4), 'Dim ('Name "feature") ('Size 1)] '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "*") ('Size 1)]
--   ...
--   = 'Just
--       '[ 'Dim ('Name "batch") ('Size 2),
--          'Dim ('Name "sequence") ('Size 4),
--          'Dim ('Name "feature") ('Size 1)]
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 1) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 4), 'Dim ('Name "feature") ('Size 2)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "feature") ('Size 1)]
--   ...
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 1) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 4), 'Dim ('Name "feature") ('Size 1)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "boo") ('Size 1), 'Dim ('Name "feature") ('Size 1)]
--   ...
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 1) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 0), 'Dim ('Name "feature") ('Size 1)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 2), 'Dim ('Name "feature") ('Size 1)]
--   ...
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 1) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "feature") ('Size 1)]
--   ...
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByIndex 2) '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "feature") ('Size 3)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "feature") ('Size 1)]
--   ...
--   = 'Just
--       '[ 'Dim ('Name "batch") ('Size 2),
--          'Dim ('Name "sequence") ('Size 1),
--          'Dim ('Name "feature") ('Size 3)]
--   
--   &gt;&gt;&gt; :kind! GatherDimImplF ('ByName "feature") '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "feature") ('Size 3)] '[ 'Dim ('Name "batch") ('Size 2), 'Dim ('Name "sequence") ('Size 1), 'Dim ('Name "*") ('Size 1)]
--   ...
--   = 'Just
--       '[ 'Dim ('Name "batch") ('Size 2),
--          'Dim ('Name "sequence") ('Size 1),
--          'Dim ('Name "feature") ('Size 3)]
--   </pre>
type family GatherDimImplF by indexDims inputDims
type family GatherDimByIndexF dimIndex indexDims inputDims
type family GatherDimByNameF dimIndex dimIndex' indexDims inputDims
type GatherDimMessage by indexDims inputDims = "Cannot gather the tensor with the dimensions" % "" % "    '" <> inputDims <> "'" % "" % "at the dimension" % "" % "    '" <> by <> "'" % "" % "using an index of shape" % "" % "    '" <> indexDims <> "'." % ""
type family GatherDimCheckF by indexDims inputDims result

-- | Calculate the output shape of a gather operation for a given index
--   shape along a given axis.
--   
--   <pre>
--   &gt;&gt;&gt; :kind! GatherDimF ('SelectDim ('ByIndex 2)) ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") ('Size 3)]) ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") ('Size 1)])
--   ...
--   = 'Shape
--       '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 1),
--          'Dim ('Name "*") ('Size 3)]
--   </pre>
type family GatherDimF selectDim indexShape inputShape

-- | Gather values along an axis for a specified dimension.
--   
--   <pre>
--   &gt;&gt;&gt; sToTensor' = sToTensor (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU)
--   
--   &gt;&gt;&gt; t &lt;- sToTensor' [[1 :: Float, 2], [3, 4]]
--   
--   &gt;&gt;&gt; idx &lt;- sToTensor' [[0 :: Int, 0], [1, 0]]
--   
--   &gt;&gt;&gt; result &lt;- sGatherDim (SSelectDim $ SByIndex @1) idx t
--   
--   &gt;&gt;&gt; :t result
--   result
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "*") 'UncheckedSize,
--                'Dim ('Name "*") 'UncheckedSize])
--   
--   &gt;&gt;&gt; result
--   Tensor Float [2,2] [[ 1.0000   ,  1.0000   ],
--                       [ 4.0000   ,  3.0000   ]]
--   
--   &gt;&gt;&gt; shape = SShape $ SNoName :&amp;: SSize @2 :|: SNoName :&amp;: SSize @2 :|: SNil
--   
--   &gt;&gt;&gt; t' &lt;- sCheckedShape shape t
--   
--   &gt;&gt;&gt; idx' &lt;- sCheckedShape shape idx
--   
--   &gt;&gt;&gt; result &lt;- sGatherDim (SSelectDim $ SByIndex @1) idx' t'
--   
--   &gt;&gt;&gt; :t result
--   result
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape '[ 'Dim ('Name "*") ('Size 2), 'Dim ('Name "*") ('Size 2)])
--   
--   &gt;&gt;&gt; result
--   Tensor Float [2,2] [[ 1.0000   ,  1.0000   ],
--                       [ 4.0000   ,  3.0000   ]]
--   </pre>
sGatherDim :: forall selectDim indexGradient inputGradient indexLayout inputLayout indexDevice inputDevice indexDataType inputDataType indexShape inputShape outputShape m. (MonadThrow m, outputShape ~ GatherDimF selectDim indexShape inputShape, Catch outputShape, Catch (indexDataType <+> 'DataType 'Int64)) => SSelectDim selectDim -> Tensor indexGradient indexLayout indexDevice indexDataType indexShape -> Tensor inputGradient inputLayout inputDevice inputDataType inputShape -> m (Tensor (indexGradient <|> inputGradient) (indexLayout <+> inputLayout) (indexDevice <+> inputDevice) inputDataType outputShape)
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeError
instance GHC.Show.Show Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.IndexOutOfBoundError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.IndexOutOfBoundError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeError
instance (Torch.Internal.Class.Castable (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.CatHListF selectDim tensors) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor), Torch.Internal.Class.Castable (Torch.HList.HList tensors) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)) => Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.HasCat selectDim [*] Torch.HList.HList tensors
instance Torch.Internal.Class.Castable (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.CatListF selectDim (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Tensor) => Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.HasCat selectDim (*) [] (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)

module Torch.GraduallyTyped.Tensor.Indexing
data IndexType a
NewAxis :: IndexType a
Ellipsis :: IndexType a
SliceAll :: IndexType a
SliceAt :: a -> IndexType a
SliceBool :: Bool -> IndexType a
SliceFrom :: a -> IndexType a
SliceUpTo :: a -> IndexType a
SliceWithStep :: a -> IndexType a
SliceFromUpTo :: a -> a -> IndexType a
SliceFromWithStep :: a -> a -> IndexType a
SliceUpToWithStep :: a -> a -> IndexType a
SliceFromUpToWithStep :: a -> a -> a -> IndexType a
data SIndexType :: forall (a_a1rgI :: Type). IndexType a_a1rgI -> Type
[SNewAxis] :: forall (a_a1rgI :: Type). () => SIndexType ('NewAxis :: IndexType (a_a1rgI :: Type))
[SEllipsis] :: forall (a_a1rgI :: Type). () => SIndexType ('Ellipsis :: IndexType (a_a1rgI :: Type))
[SSliceAll] :: forall (a_a1rgI :: Type). () => SIndexType ('SliceAll :: IndexType (a_a1rgI :: Type))
[SSliceAt] :: forall (a_a1rgI :: Type) (n_a1rBm :: a_a1rgI). () => Sing n_a1rBm -> SIndexType ('SliceAt n_a1rBm :: IndexType (a_a1rgI :: Type))
[SSliceBool] :: forall (a_a1rgI :: Type) (n_a1rBo :: Bool). () => Sing n_a1rBo -> SIndexType ('SliceBool n_a1rBo :: IndexType (a_a1rgI :: Type))
[SSliceFrom] :: forall (a_a1rgI :: Type) (n_a1rBq :: a_a1rgI). () => Sing n_a1rBq -> SIndexType ('SliceFrom n_a1rBq :: IndexType (a_a1rgI :: Type))
[SSliceUpTo] :: forall (a_a1rgI :: Type) (n_a1rBs :: a_a1rgI). () => Sing n_a1rBs -> SIndexType ('SliceUpTo n_a1rBs :: IndexType (a_a1rgI :: Type))
[SSliceWithStep] :: forall (a_a1rgI :: Type) (n_a1rBu :: a_a1rgI). () => Sing n_a1rBu -> SIndexType ('SliceWithStep n_a1rBu :: IndexType (a_a1rgI :: Type))
[SSliceFromUpTo] :: forall (a_a1rgI :: Type) (n_a1rBw :: a_a1rgI) (n_a1rBx :: a_a1rgI). () => Sing n_a1rBw -> Sing n_a1rBx -> SIndexType ('SliceFromUpTo n_a1rBw n_a1rBx :: IndexType (a_a1rgI :: Type))
[SSliceFromWithStep] :: forall (a_a1rgI :: Type) (n_a1rBB :: a_a1rgI) (n_a1rBC :: a_a1rgI). () => Sing n_a1rBB -> Sing n_a1rBC -> SIndexType ('SliceFromWithStep n_a1rBB n_a1rBC :: IndexType (a_a1rgI :: Type))
[SSliceUpToWithStep] :: forall (a_a1rgI :: Type) (n_a1rBG :: a_a1rgI) (n_a1rBH :: a_a1rgI). () => Sing n_a1rBG -> Sing n_a1rBH -> SIndexType ('SliceUpToWithStep n_a1rBG n_a1rBH :: IndexType (a_a1rgI :: Type))
[SSliceFromUpToWithStep] :: forall (a_a1rgI :: Type) (n_a1rBL :: a_a1rgI) (n_a1rBM :: a_a1rgI) (n_a1rBN :: a_a1rgI). () => Sing n_a1rBL -> Sing n_a1rBM -> Sing n_a1rBN -> SIndexType ('SliceFromUpToWithStep n_a1rBL n_a1rBM n_a1rBN :: IndexType (a_a1rgI :: Type))
data Indices (indexTypes :: Type)
[UncheckedIndices] :: forall indexTypes. Indices indexTypes
[Indices] :: forall indexTypes. indexTypes -> Indices indexTypes
data SIndices (indices :: Indices [IndexType (Index Nat)])
[SUncheckedIndices] :: [IndexType Integer] -> SIndices 'UncheckedIndices
[SIndices] :: forall indexTypes. SList indexTypes -> SIndices ('Indices indexTypes)
type family IndexDims indices shape
(!) :: forall indices requiresGradient layout device dataType shape m. MonadThrow m => Tensor requiresGradient layout device dataType shape -> SIndices indices -> m (Tensor requiresGradient layout device dataType (IndexDims indices shape))

-- | Generate a slice from a <a>python compatible expression</a>. When you
--   take the odd-numberPed element of tensor with `tensor[1::2]` in
--   python, you can write `tensor ! [slice|1::2|]` in hasktorch.
slice :: QuasiQuoter
instance Data.Singletons.SingI indexTypes => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.Indices indexTypes)
instance Data.Singletons.SingKind (Torch.GraduallyTyped.Tensor.Indexing.Indices [Torch.GraduallyTyped.Tensor.Indexing.IndexType (Torch.GraduallyTyped.Index.Type.Index GHC.TypeNats.Nat)])
instance Data.Singletons.SingKind a => Data.Singletons.SingKind (Torch.GraduallyTyped.Tensor.Indexing.IndexType a)
instance Data.Singletons.SingI 'Torch.GraduallyTyped.Tensor.Indexing.NewAxis
instance Data.Singletons.SingI 'Torch.GraduallyTyped.Tensor.Indexing.Ellipsis
instance Data.Singletons.SingI 'Torch.GraduallyTyped.Tensor.Indexing.SliceAll
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceAt n)
instance Data.Singletons.SingI1 'Torch.GraduallyTyped.Tensor.Indexing.SliceAt
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceAtSym0
instance Data.Singletons.SingI n => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceBool n)
instance Data.Singletons.SingI1 'Torch.GraduallyTyped.Tensor.Indexing.SliceBool
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceBoolSym0
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceFrom n)
instance Data.Singletons.SingI1 'Torch.GraduallyTyped.Tensor.Indexing.SliceFrom
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceFromSym0
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceUpTo n)
instance Data.Singletons.SingI1 'Torch.GraduallyTyped.Tensor.Indexing.SliceUpTo
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceUpToSym0
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceWithStep n)
instance Data.Singletons.SingI1 'Torch.GraduallyTyped.Tensor.Indexing.SliceWithStep
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceWithStepSym0
instance forall a (n1 :: a) (n2 :: a). (Data.Singletons.SingI n1, Data.Singletons.SingI n2) => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpTo n1 n2)
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI1 ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpTo n)
instance Data.Singletons.SingI2 'Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpTo
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToSym0
instance forall a (d :: a). Data.Singletons.SingI d => Data.Singletons.SingI (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToSym1 d)
instance Data.Singletons.SingI1 Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToSym1
instance forall a (n1 :: a) (n2 :: a). (Data.Singletons.SingI n1, Data.Singletons.SingI n2) => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStep n1 n2)
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI1 ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStep n)
instance Data.Singletons.SingI2 'Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStep
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStepSym0
instance forall a (d :: a). Data.Singletons.SingI d => Data.Singletons.SingI (Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStepSym1 d)
instance Data.Singletons.SingI1 Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStepSym1
instance forall a (n1 :: a) (n2 :: a). (Data.Singletons.SingI n1, Data.Singletons.SingI n2) => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStep n1 n2)
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI1 ('Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStep n)
instance Data.Singletons.SingI2 'Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStep
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStepSym0
instance forall a (d :: a). Data.Singletons.SingI d => Data.Singletons.SingI (Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStepSym1 d)
instance Data.Singletons.SingI1 Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStepSym1
instance forall a (n1 :: a) (n2 :: a) (n3 :: a). (Data.Singletons.SingI n1, Data.Singletons.SingI n2, Data.Singletons.SingI n3) => Data.Singletons.SingI ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStep n1 n2 n3)
instance forall a (n1 :: a) (n2 :: a). (Data.Singletons.SingI n1, Data.Singletons.SingI n2) => Data.Singletons.SingI1 ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStep n1 n2)
instance forall a (n :: a). Data.Singletons.SingI n => Data.Singletons.SingI2 ('Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStep n)
instance Data.Singletons.SingI Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym0
instance forall a (d :: a). Data.Singletons.SingI d => Data.Singletons.SingI (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym1 d)
instance Data.Singletons.SingI1 Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym1
instance forall a (d1 :: a) (d2 :: a). (Data.Singletons.SingI d1, Data.Singletons.SingI d2) => Data.Singletons.SingI (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym2 d1 d2)
instance forall a (d :: a). Data.Singletons.SingI d => Data.Singletons.SingI1 (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym2 d)
instance Data.Singletons.SingI2 Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym2
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym0
instance forall a (a6989586621679354221 :: a). Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym1 a6989586621679354221)
instance forall a (a6989586621679354221 :: a) (a6989586621679354222 :: a). Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToWithStepSym2 a6989586621679354221 a6989586621679354222)
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStepSym0
instance forall a (a6989586621679354218 :: a). Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings (Torch.GraduallyTyped.Tensor.Indexing.SliceUpToWithStepSym1 a6989586621679354218)
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStepSym0
instance forall a (a6989586621679354215 :: a). Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings (Torch.GraduallyTyped.Tensor.Indexing.SliceFromWithStepSym1 a6989586621679354215)
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToSym0
instance forall a (a6989586621679354212 :: a). Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings (Torch.GraduallyTyped.Tensor.Indexing.SliceFromUpToSym1 a6989586621679354212)
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceWithStepSym0
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceUpToSym0
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceFromSym0
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceBoolSym0
instance Data.Singletons.TH.SuppressUnusedWarnings.SuppressUnusedWarnings Torch.GraduallyTyped.Tensor.Indexing.SliceAtSym0
instance GHC.Base.Functor Torch.GraduallyTyped.Tensor.Indexing.IndexType
instance GHC.Classes.Eq a => GHC.Classes.Eq (Torch.GraduallyTyped.Tensor.Indexing.IndexType a)
instance GHC.Show.Show a => GHC.Show.Show (Torch.GraduallyTyped.Tensor.Indexing.IndexType a)

module Torch.GraduallyTyped.Shape

module Torch.GraduallyTyped.Tensor.MathOperations.Pointwise

-- | Computes the element-wise absolute value of the given <tt>input</tt>
--   tensor: &lt;math&gt; The result is returned as a new tensor.
abs :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Alias for <a>abs</a>.
absolute :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arccosine of the elements of
--   <tt>input</tt>: &lt;math&gt;
acos :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arccosine of the elements of
--   <tt>input</tt>: &lt;math&gt;
--   
--   Note that the domain of the inverse hyperbolic cosine is &lt;math&gt;,
--   and values outside this range will be mapped to &lt;math&gt;, except
--   for &lt;math&gt; for which the output is mapped to &lt;math&gt;.
acosh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise addition of one tensor and another: &lt;math&gt; The
--   result is returned as a new tensor.
--   
--   The shape of <tt>other</tt> must be broadcastable with the shape of
--   <tt>input</tt>. See <a>addScalar</a> for a version of this function
--   where the <tt>other</tt> input is a scalar.
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; sRandn' = sRandn . TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; (a, g') &lt;- sRandn' (SShape $ SName @"feature" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (b, _) &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @1 :|: SNil) g'
--   
--   &gt;&gt;&gt; result &lt;- a `add` b
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "*") ('Size 4), 'Dim ('Name "feature") ('Size 4)])
--   </pre>
add :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Adds a scalar <tt>other</tt> to a tensor <tt>input</tt>: &lt;math&gt;
--   The result is returned as a new tensor. See <a>add</a> for a version
--   of this function where the second argument is a tensor.
--   
--   TODO: add data type unification of <tt>other</tt> and
--   <tt>dataType</tt>.
addScalar :: forall other gradient layout device dataType shape m. (Scalar other, MonadThrow m) => Tensor gradient layout device dataType shape -> other -> m (Tensor gradient layout device dataType shape)

-- | Performs the element-wise division of <tt>tensor1</tt> by
--   <tt>tensor2</tt>, multiply the result by a scalar <tt>value</tt> and
--   add it to <tt>input</tt>: &lt;math&gt;
--   
--   See <a>addcmul</a> for a version of this function where
--   <tt>tensor1</tt> and <tt>tensor2</tt> are multiplied rather than
--   divided.
--   
--   Note further that for inputs of type <a>DType</a> or <a>DType</a>,
--   <tt>value</tt> must be a real number, otherwise it must be an integer.
addcdiv :: forall value gradient layout device dataType shape gradient' layout' device' dataType' shape' gradient'' layout'' device'' dataType'' shape'' m. (Scalar value, MonadThrow m) => value -> Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor gradient'' layout'' device'' dataType'' shape'' -> m (Tensor (gradient <|> (gradient' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) (shape <+> (shape' <+> shape'')))

-- | Performs the element-wise multiplication of <tt>tensor1</tt> by
--   <tt>tensor2</tt>, multiply the result by the scalar <tt>value</tt> and
--   add it to <tt>input</tt>: &lt;math&gt;
--   
--   See <a>addcdiv</a> for a version of this function where
--   <tt>tensor1</tt> and <tt>tensor2</tt> are divided rather than
--   multiplied.
--   
--   Note further that for inputs of type <a>DType</a> or <a>DType</a>,
--   <tt>value</tt> must be a real number, otherwise it must be an integer.
addcmul :: forall scalar gradient layout device dataType shape gradient' layout' device' dataType' shape' gradient'' layout'' device'' dataType'' shape'' m. (Scalar scalar, MonadThrow m) => scalar -> Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor gradient'' layout'' device'' dataType'' shape'' -> m (Tensor (gradient <|> (gradient' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) (shape <+> (shape' <+> shape'')))

-- | Returns a new tensor with the arcsine of the elements of
--   <tt>input</tt>: &lt;math&gt;
asin :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the inverse hyperbolic sine of the elements
--   of <tt>input</tt>: &lt;math&gt;
asinh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the arctangent of the elements of
--   <tt>input</tt>: &lt;math&gt;
atan :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the inverse hyperbolic tangent of the
--   elements of <tt>input</tt>: &lt;math&gt;
--   
--   Note that the domain of the inverse hyperbolic tangent is
--   &lt;math&gt;, and values outside this range will be mapped to
--   &lt;math&gt;, except for the values &lt;math&gt; and &lt;math&gt; for
--   which the output is mapped to &lt;math&gt; respectively.
atanh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise arctangent of <tt>input</tt> and <tt>other</tt> with
--   consideration of the quadrant. Returns a new tensor where each element
--   is the signed angle in radians between the vectors &lt;math&gt; and
--   &lt;math&gt;. Here $mathrm{other}_i$, the &lt;math&gt;-th element of
--   the second argument of this function, is the x coordinate while
--   $mathrm{input}_i$, the &lt;math&gt;-th element of the first argument,
--   is the y coordinate.
--   
--   Note that the shapes of <tt>input</tt> and <tt>other</tt> must be
--   broadcastable.
atan2 :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Computes the bitwise NOT of the given <tt>input</tt> tensor. The data
--   type of the <tt>input</tt> tensor must be <a>DType</a> or an integral
--   data type. For <a>DType</a> tensors, the function computes the logical
--   NOT.
bitwiseNot :: forall gradient layout device dataType shape m. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device ('DataType 'Bool) shape)

-- | Computes the bitwise AND of the <tt>input</tt> and the <tt>other</tt>
--   tensor. The data type of the tensors must be <a>DType</a> or an
--   integral data type. For <a>DType</a> tensors, the function computes
--   the logical AND.
--   
--   See <a>bitwiseAndScalar</a> for a version of this function where
--   <tt>other</tt> is a scalar.
bitwiseAnd :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' m. MonadThrow m => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (shape <+> shape'))

-- | Computes the bitwise AND of the tensor <tt>input</tt> and the scalar
--   <tt>other</tt>. The data type of the inputs must be <a>DType</a> or an
--   integral data type. If the data type is <a>DType</a>, then the
--   function computes the logical AND.
--   
--   See <a>bitwiseAnd</a> for a version of this function where
--   <tt>other</tt> is a tensor.
bitwiseAndScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Computes the bitwise OR of the <tt>input</tt> and the <tt>other</tt>
--   tensor. The data type of the tensors must be <a>DType</a> or an
--   integral data type. For <a>DType</a> tensors, the function computes
--   the logical OR.
--   
--   See <a>bitwiseOrScalar</a> for a version of this function where
--   <tt>other</tt> is a scalar.
bitwiseOr :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' m. MonadThrow m => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (shape <+> shape'))

-- | Computes the bitwise OR of the tensor <tt>input</tt> and the scalar
--   <tt>other</tt>. The data type of the inputs must be <a>DType</a> or an
--   integral data type. If the data type is <a>DType</a>, then the
--   function computes the logical OR.
--   
--   See <a>bitwiseOr</a> for a version of this function where
--   <tt>other</tt> is a tensor.
bitwiseOrScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Computes the bitwise XOR of the <tt>input</tt> and the <tt>other</tt>
--   tensor. The data type of the tensors must be <a>DType</a> or an
--   integral data type. For <a>DType</a> tensors, the function computes
--   the logical XOR.
--   
--   See <a>bitwiseXorScalar</a> for a version of this function where
--   <tt>other</tt> is a scalar.
bitwiseXor :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' m. MonadThrow m => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (shape <+> shape'))

-- | Computes the bitwise XOR of the tensor <tt>input</tt> and the scalar
--   <tt>other</tt>. The data type of the inputs must be <a>DType</a> or an
--   integral data type. If the data type is <a>DType</a>, then the
--   function computes the logical XOR.
--   
--   See <a>bitwiseXor</a> for a version of this function where
--   <tt>other</tt> is a tensor.
bitwiseXorScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the ceil of the elements of <tt>input</tt>,
--   that is, the smallest integer greater than or equal to each element:
--   &lt;math&gt; where &lt;math&gt; is the floor of the &lt;math&gt;-th
--   element of <tt>input</tt> which can be computed with <a>floor</a>.
ceil :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Clamp all elements in input into the range [ min, max ] and return the
--   result as a new tensor.
clamp :: forall min max gradient layout device dataType shape. (Scalar min, Scalar max) => min -> max -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
cos :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
cosh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape
deg2rad :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise division of the first input tensor, the
--   <tt>dividend</tt>, by the second input tensor, the <tt>divisor</tt>.
--   &lt;math&gt; The result is returned as a new tensor.
--   
--   See <a>divScalar</a> for a version of this function where the
--   <tt>divisor</tt> is a scalar.
--   
--   Note further that "true divisions" can be computed with
--   <a>trueDivide</a> or <a>trueDivideScalar</a> which can come in handy
--   when both the <tt>dividend</tt> and the <tt>divisor</tt> have
--   <a>DType</a> or integer data types.
div :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Element-wise division of the first input, the <tt>dividend</tt>
--   tensor, by the second input, the <tt>divisor</tt> scalar. &lt;math&gt;
--   The result is returned as a new tensor.
--   
--   See <a>div</a> for a version of this function where the divisor is a
--   tensor.
--   
--   Note further that "true divisions" can be computed with
--   <a>trueDivide</a> or <a>trueDivideScalar</a> which can come in handy
--   when both the dividend and the divisor have <a>DType</a> or integer
--   data types.
divScalar :: forall divisor gradient layout device dataType shape. Scalar divisor => Tensor gradient layout device dataType shape -> divisor -> Tensor gradient layout device dataType shape

-- | Computes the logarithmic derivative of the gamma function on
--   <tt>input</tt>: &lt;math&gt;
digamma :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes and returns the error function of each element of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>erfc</a> that computes the complementary error function to
--   high numerical accuracy and <a>erfinv</a> that computes the inverse of
--   the error function.
erf :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the complementary error function of each element of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>erf</a> that computes the error function and <a>erfinv</a>
--   that computes the inverse of the error function.
erfc :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the inverse error function of each element of <tt>input</tt>:
--   &lt;math&gt; where &lt;math&gt; for &lt;math&gt;. <a>erfinv</a> is not
--   defined outside this interval.
--   
--   See also <a>erf</a> that computes the error function and <a>erfc</a>
--   that computes the complementary error function.
erfinv :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the exponential of the elements of the input
--   tensor <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>expm1</a> for a high-accuracy calculation of the
--   exponential of the elements of <tt>input</tt> minus 1.
exp :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the exponential of the elements minus 1 of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See also <a>exp</a> for the exponential function.
expm1 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the floor of the elements of <tt>input</tt>,
--   that is, the largest integer less than or equal to each element.:
--   &lt;math&gt; where &lt;math&gt; is the ceil of the &lt;math&gt;-th
--   element of <tt>input</tt> which can be computed with <a>ceil</a>.
floor :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Return the element-wise division of the tensor <tt>dividend</tt> by
--   the tensor <tt>divisor</tt> rounded down to the nearest integer:
--   &lt;math&gt;
--   
--   See <a>floorDivideScalar</a> for a version of this function where
--   <tt>divisor</tt> is a scalar.
floorDivide :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Return the division of the tensor <tt>dividend</tt> by the scalar
--   <tt>divisor</tt> rounded down to the nearest integer: &lt;math&gt;
--   
--   See <a>floorDivide</a> for a version of this function where
--   <tt>divisor</tt> is a tensor.
floorDivideScalar :: forall divisor gradient layout device dataType shape. Scalar divisor => Tensor gradient layout device dataType shape -> divisor -> Tensor gradient layout device dataType shape

-- | Computes the element-wise remainder of the division of the tensor
--   <tt>dividend</tt> by the tensor <tt>divisor</tt>. The dividend and
--   divisor may contain both for integer and floating point numbers. The
--   remainder has the same sign as the <tt>dividend</tt> input.
--   
--   See <a>fmodScalar</a> for a version of this function where
--   <tt>divisor</tt> is a scalar.
fmod :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Computes the element-wise remainder of the division of the tensor
--   <tt>dividend</tt> by the scalar <tt>divisor</tt>. The dividend and
--   divisor may contain both for integer and floating point numbers. The
--   remainder has the same sign as the <tt>dividend</tt> input.
--   
--   See <a>fmodScalar</a> for a version of this function where
--   <tt>divisor</tt> is a scalar.
fmodScalar :: forall divisor gradient layout device dataType shape. Scalar divisor => divisor -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the fractional portion of each element in <tt>input</tt>:
--   &lt;math&gt;
frac :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Linear interpolation of two tensors, <tt>start</tt> and <tt>end</tt>,
--   based on a tensor <tt>weight</tt>. For linear interpolations based on
--   a scalar see <a>lerpScalar</a>.
--   
--   Returned is the result of the following computation as a tensor:
--   &lt;math&gt;
--   
--   Note that the shapes of <tt>start</tt>, <tt>end</tt>, and also
--   <tt>weight</tt> must be broadcastable.
lerp :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' gradient'' layout'' device'' dataType'' shape'' shape''' m. (MonadThrow m, shape''' ~ BroadcastShapesF shape (BroadcastShapesF shape' shape''), Catch shape''') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor gradient'' layout'' device'' dataType'' shape'' -> m (Tensor (gradient <|> (gradient' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) shape''')

-- | Linear interpolation of two tensors, <tt>start</tt> and <tt>end</tt>,
--   based on a scalar <tt>weight</tt>. For linear interpolations based on
--   a tensor see <a>lerp</a>.
--   
--   Returned is the result of the following computation as a tensor:
--   &lt;math&gt;
--   
--   Note that the shapes of <tt>start</tt> and <tt>end</tt> must be
--   broadcastable.
lerpScalar :: forall weight gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (Scalar weight, MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => weight -> Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Computes the logarithm of the gamma function on <tt>input</tt>:
--   &lt;math&gt;
lgamma :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the natural logarithm of the elements of
--   <tt>input</tt>: &lt;math&gt;
log :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the decimal logarithm of the elements of
--   <tt>input</tt>: &lt;math&gt;
log10 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the natural logarithm of &lt;math&gt;:
--   &lt;math&gt;
--   
--   Consider using this function over a literal implementation using
--   <a>log</a>. <a>log1p</a> is much more accurate for small values of
--   <tt>input</tt>.
log1p :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the logarithm to the base 2 of the elements
--   of <tt>input</tt>: &lt;math&gt;
log2 :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Logarithm of the sum of exponentiations of the inputs. Calculates
--   pointwise the function &lt;math&gt;.
--   
--   This function is useful in statistics where the calculated
--   probabilities of events may be so small as to exceed the range of
--   normal floating point numbers. In such cases the logarithm of the
--   calculated probability is stored. This function allows adding
--   probabilities stored in such a fashion.
--   
--   <a>logaddexp</a> must not be confused with <tt>logsumexp</tt> which
--   performs a reduction on a single tensor.
logaddexp :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Logarithm of the sum of exponentiations of the inputs in base-2.
--   Calculates pointwise the function &lt;math&gt;.
--   
--   See <a>logaddexp</a> for further details.
logaddexp2 :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Computes the element-wise logical AND of the given input tensors. The
--   output tensor will have the <a>DType</a> data type. If the input
--   tensors are not a bool tensors, then zeros are treated as <a>False</a>
--   and nonzeros are treated as <a>True</a>.
logicalAnd :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')

-- | Computes the element-wise logical NOT of the given input tensor. The
--   output tensor will have the <a>DType</a> data type. If the input
--   tensor is not a bool tensor, zeros are treated as <a>False</a> and
--   non-zeros are treated as <a>True</a>.
logicalNot :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor ('Gradient 'WithoutGradient) layout device ('DataType 'Bool) shape

-- | Computes the element-wise logical OR of the given input tensors. The
--   output tensor will have the <a>DType</a> data type. If the input
--   tensors are not a bool tensors, then zeros are treated as <a>False</a>
--   and nonzeros are treated as <a>True</a>.
logicalOr :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')

-- | Computes the element-wise logical XOR of the given input tensors. The
--   output tensor will have the <a>DType</a> data type. If the input
--   tensors are not a bool tensors, then zeros are treated as <a>False</a>
--   and nonzeros are treated as <a>True</a>.
logicalXor :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor ('Gradient 'WithoutGradient) (layout <+> layout') (device <+> device') ('DataType 'Bool) shape'')

-- | Element-wise multiplication of two tensors: &lt;math&gt; The result is
--   returned as a new tensor.
--   
--   The shape of <tt>other</tt> must be broadcastable with the shape of
--   <tt>input</tt>. See <a>mulScalar</a> for a version of this function
--   where the <tt>other</tt> input is a scalar.
mul :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')
mulScalar :: forall other gradient layout device dataType shape m. (Scalar other, MonadThrow m) => Tensor gradient layout device dataType shape -> other -> m (Tensor gradient layout device dataType shape)

-- | Computes the multivariate log-gamma function with dimension <tt>p</tt>
--   element-wise, given by &lt;math&gt; where &lt;math&gt; and
--   &lt;math&gt; is the gamma function.
--   
--   All elements of the input tensor must be greater than &lt;math&gt;.
--   Otherwise, the computation is halted and an exception is thrown.
mvlgamma :: forall gradient layout device dataType shape m. MonadThrow m => Int -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Returns a new tensor with the negative of the elements of
--   <tt>input</tt>: &lt;math&gt;
neg :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the &lt;math&gt;-th derivative of the digamma function
--   &lt;math&gt; on the <tt>input</tt>, where &lt;math&gt;. &lt;math&gt;
--   is called the order of the polygamma function &lt;math&gt; that is
--   defined as: &lt;math&gt; where &lt;math&gt;.
polygamma :: forall gradient layout device dataType shape. Int -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Takes the power of each element in the tensor <tt>input</tt> with the
--   corresponding element in the tensor <a>exponent</a> and returns a
--   tensor with the result.
--   
--   Note that the <a>exponent</a> and the <tt>input</tt> must be tensors
--   with broadcastable shapes. See <a>powScalar</a> for a version that
--   takes a scalar <a>exponent</a> as argument and <a>powTensor</a> for a
--   version where the <tt>input</tt> is a scalar and the <a>exponent</a> a
--   tensor.
--   
--   The following operation is applied: &lt;math&gt;
pow :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient' layout' device' dataType' shape' -> Tensor gradient layout device dataType shape -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Takes the power of each element in the tensor <tt>input</tt> with the
--   scalar <a>exponent</a> and returns a tensor with the result.
--   
--   Note that the <a>exponent</a> is a scalar. See <a>pow</a> for a
--   version that takes a tensor <a>exponent</a> as argument and
--   <a>powTensor</a> for a version where the <tt>input</tt> is a scalar
--   and the <a>exponent</a> a tensor.
--   
--   The following operation is applied: &lt;math&gt;
powScalar :: forall exponent gradient layout device dataType shape m. (Scalar exponent, MonadThrow m) => Tensor gradient layout device dataType shape -> exponent -> m (Tensor gradient layout device dataType shape)

-- | Takes the power of the scalar <tt>input</tt> with each element in the
--   tensor <a>exponent</a> and returns a tensor with the result.
--   
--   Note that the <a>exponent</a> is a tensor while the <tt>input</tt> is
--   a scalar. See <a>pow</a> for a version that takes a tensor
--   <tt>input</tt> as argument and <a>powScalar</a> for a version where
--   the <tt>input</tt> is a tensor and the <a>exponent</a> a scalar.
--   
--   The following operation is applied: &lt;math&gt;
powTensor :: forall input gradient layout device dataType shape. Scalar input => input -> Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with each of the elements of <tt>input</tt>
--   converted from angles in radians to degrees.
rad2deg :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the reciprocal of the elements of
--   <tt>input</tt>: &lt;math&gt;
reciprocal :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Computes the element-wise remainder of division.
--   
--   The dividend and divisor may contain integer and floating point
--   numbers. The remainder has the same sign as the divisor other.
--   
--   When other is a tensor, the shapes of input and other must be
--   broadcastable.
remainder :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Returns a new tensor with each of the elements of <tt>input</tt>
--   rounded to the closest integer. Note that the data type is unchanged.
round :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the reciprocal of the square-root of each of
--   the elements of <tt>input</tt>: &lt;math&gt;
rsqrt :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the sigmoid of the elements of
--   <tt>input</tt>: &lt;math&gt;
sigmoid :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the signs of the elements of <tt>input</tt>:
--   &lt;math&gt;
sign :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the sine of the elements of <tt>input</tt>:
--   &lt;math&gt;
sin :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the hyperbolic sine of the elements of
--   <tt>input</tt>: &lt;math&gt;
sinh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Element-wise subtraction of one tensor from another: &lt;math&gt; The
--   result is returned as a new tensor.
--   
--   The shape of <tt>other</tt> must be broadcastable with the shape of
--   <tt>input</tt>. See <a>subScalar</a> for a version of this function
--   where the <tt>other</tt> input is a scalar.
sub :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient) (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Subtracts a scalar <tt>other</tt> from a tensor <tt>input</tt>:
--   &lt;math&gt; The result is returned as a new tensor. See <a>sub</a>
--   for a version of this function where the second argument is a tensor.
subScalar :: forall other gradient layout device dataType shape m. (Scalar other, MonadThrow m) => Tensor gradient layout device dataType shape -> other -> m (Tensor gradient layout device dataType shape)

-- | Returns a new tensor with the square-root of the elements of
--   <tt>input</tt>: &lt;math&gt;
sqrt :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the square of the elements of
--   <tt>input</tt>: &lt;math&gt;
--   
--   See <a>pow</a>, <a>powScalar</a>, or <a>powTensor</a> for
--   exponentiation with respect to arbitrary exponents.
square :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the tangent of the elements of
--   <tt>input</tt>: &lt;math&gt;
tan :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Returns a new tensor with the hyperbolic tangent of the elements of
--   <tt>input</tt>: &lt;math&gt;
tanh :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Performs true division that always computes the division in floating
--   point: &lt;math&gt;
--   
--   <a>trueDivide</a> is completely equivalent to division using
--   <a>div</a> except when both inputs have <a>DType</a> or integer data
--   types, in which case the inputs are converted to floating data types
--   before performing the division.
--   
--   See <a>trueDivideScalar</a> for a version of this function where the
--   divisor is a scalar.
trueDivide :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' shape'' m. (MonadThrow m, shape'' ~ BroadcastShapesF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

-- | Performs true division that always computes the division in floating
--   point: &lt;math&gt;
--   
--   <a>trueDivideScalar</a> is completely equivalent to division using
--   <a>divScalar</a> except when both inputs have <a>DType</a> or integer
--   data types, in which case the inputs are converted to floating data
--   types before performing the division.
--   
--   See <a>trueDivide</a> for a version of this function where the divisor
--   is a tensor.
trueDivideScalar :: forall other gradient layout device dataType shape. Scalar other => Tensor gradient layout device dataType shape -> other -> Tensor gradient layout device dataType shape
trunc :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

module Torch.GraduallyTyped.NN.Functional.Activation

-- | Thresholds each element of the input Tensor.
threshold :: forall threshold value gradient layout device dataType shape m. (Scalar threshold, Scalar value, MonadThrow m) => threshold -> value -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the rectified linear unit function element-wise, that is,
--   &lt;math&gt;
relu :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the gaussian error linear unit function element-wise.
gelu :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the gaussian error linear unit function element-wise.
--   
--   This is the implementation of the GELU activation function from
--   Google's BERT repo (and coincidentally also from OpenAI's GPT). See
--   also <a>https://arxiv.org/abs/1606.08415</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- sFull (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SNil)) 0.5
--   
--   &gt;&gt;&gt; t' &lt;- geluNew t
--   
--   &gt;&gt;&gt; fromTensor @Float t'
--   0.345714
--   </pre>
geluNew :: forall gradient layout device dataType shape m. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the HardTanh function element-wise.
hardtanh :: forall minValue maxValue gradient layout device dataType shape m. (Scalar minValue, Scalar maxValue, MonadThrow m) => minValue -> maxValue -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the hardswish function element-wise.
hardswish :: forall gradient layout device dataType shape. Tensor gradient layout device dataType shape -> Tensor gradient layout device dataType shape

-- | Applies the exponential linear unit function element-wise, with alpha
--   input, &lt;math&gt;
elu :: forall alpha gradient layout device dataType shape m. (Scalar alpha, MonadThrow m) => alpha -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the scaled exponential linear unit function element-wise, that
--   is, &lt;math&gt; with &lt;math&gt; and &lt;math&gt;.
selu :: forall gradient layout device dataType shape m. MonadThrow m => Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the continuously differentiable exponential linear unit
--   function element-wise, that is, &lt;math&gt;
celu :: forall alpha gradient layout device dataType shape m. (Scalar alpha, MonadThrow m) => alpha -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the element-wise function: &lt;math&gt; the the angle of the
--   negative slope can be controlled. A typical value for it is 0.01.
leakyRelu :: forall negativeSlope gradient layout device dataType shape m. (Scalar negativeSlope, MonadThrow m) => negativeSlope -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Applies the parameterized rectified linear unit function element-wise,
--   that is, &lt;math&gt; The weight parameter is typically learnable.
prelu :: forall gradient' gradient layout device dataType shape m. MonadThrow m => Tensor gradient' layout device dataType shape -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

module Torch.GraduallyTyped.Tensor.MathOperations.BlasLapack
type family MatmulDimsImplF reversedDims reversedDims'
type family MatmulDimsCheckF dims dims' result
type MatmulDimsF dims dims' = MatmulDimsCheckF dims dims' (MatmulDimsImplF (Reverse dims) (Reverse dims'))
type family MatmulF shape shape'

-- | Matrix product of two tensors.
--   
--   The following code serves the examples of <tt>matmul</tt> below:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; sRandn' = sRandn . TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   </pre>
--   
--   In order to understand the behavior of <tt>matmul</tt>, consider the
--   following cases:
--   
--   <ol>
--   <li>If both tensors are 1-dimensional, the dot product (scalar) is
--   returned:<pre>&gt;&gt;&gt; (tensor1, g') &lt;- sRandn' (SShape $ SName
--   @"*" :&amp;: SSize @3 :|: SNil) g &gt;&gt;&gt; (tensor2, g'') &lt;-
--   sRandn' (SShape $ SName @"*" :&amp;: SSize @3 :|: SNil) g'
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2 &gt;&gt;&gt; :type
--   result result :: MonadThrow m =&gt; m (Tensor ('Gradient
--   'WithGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float)
--   ('Shape '[])) </pre></li>
--   <li>If both arguments are 2-dimensional, the matrix-matrix product is
--   returned:<pre>&gt;&gt;&gt; (tensor1, g') &lt;- sRandn' (SShape $ SName
--   @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   &gt;&gt;&gt; (tensor2, g'') &lt;- sRandn' (SShape $ SName @"*" :&amp;:
--   SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g' &gt;&gt;&gt;
--   result = tensor1 `matmul` tensor2 &gt;&gt;&gt; :type result result ::
--   MonadThrow m =&gt; m (Tensor ('Gradient 'WithGradient) ('Layout
--   'Dense) ('Device 'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*")
--   ('Size 3), 'Dim ('Name "*") ('Size 7)])) </pre></li>
--   <li>If the first argument is 1-dimensional and the second argument is
--   2-dimensional, a 1 is prepended to its dimension for the purpose of
--   the matrix multiply. After the matrix multiply, the prepended
--   dimension is removed:<pre>&gt;&gt;&gt; (tensor1, g') &lt;- sRandn'
--   (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g &gt;&gt;&gt;
--   (tensor2, g'') &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|:
--   SName @"*" :&amp;: SSize @7 :|: SNil) g' &gt;&gt;&gt; result = tensor1
--   `matmul` tensor2 &gt;&gt;&gt; :type result result :: MonadThrow m
--   =&gt; m (Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device
--   'CPU) ('DataType 'Float) ('Shape '[ 'Dim ('Name "*") ('Size 7)]))
--   </pre></li>
--   <li>If the first argument is 2-dimensional and the second argument is
--   1-dimensional, the matrix-vector product is returned:<pre>&gt;&gt;&gt;
--   (tensor1, g') &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @3 :|:
--   SName @"*" :&amp;: SSize @4 :|: SNil) g &gt;&gt;&gt; (tensor2, g'')
--   &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g'
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2 &gt;&gt;&gt; :type
--   result result :: MonadThrow m =&gt; m (Tensor ('Gradient
--   'WithGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Float)
--   ('Shape '[ 'Dim ('Name "*") ('Size 3)])) </pre></li>
--   <li>If both arguments are at least 1-dimensional and at least one
--   argument is &lt;math&gt;-dimensional (where &lt;math&gt;), then a
--   batched matrix multiply is returned.</li>
--   </ol>
--   
--   The following is an example of a batched matrix multiplication:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') &lt;- sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') &lt;- sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 3),
--                   'Dim ('Name "*") ('Size 7)]))
--   </pre>
--   
--   If the first argument is 1-dimensional, a 1 is prepended to its
--   dimension for the purpose of the batched matrix multiply and removed
--   after:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') &lt;- sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 7)]))
--   </pre>
--   
--   If the second argument is 1-dimensional, a 1 is appended to its
--   dimension for the purpose of the batched matrix multiply and removed
--   after:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') &lt;- sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @4 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 3)]))
--   </pre>
--   
--   The non-matrix (i.e. batch) dimensions are broadcasted (and thus must
--   be broadcastable). For example, if <tt>input</tt> is a &lt;math&gt;
--   tensor and <tt>other</tt> is a &lt;math&gt; tensor, <tt>output</tt>
--   will be a &lt;math&gt; tensor:
--   
--   <pre>
--   &gt;&gt;&gt; (tensor1, g') &lt;- sRandn' (SShape $ SName @"batch" :&amp;: SSize @10 :|: SName @"*" :&amp;: SSize @1 :|: SName @"*" :&amp;: SSize @3 :|: SName @"*" :&amp;: SSize @4 :|: SNil) g
--   
--   &gt;&gt;&gt; (tensor2, g'') &lt;- sRandn' (SShape $ SName @"*" :&amp;: SSize @5 :|: SName @"*" :&amp;: SSize @4 :|: SName @"*" :&amp;: SSize @7 :|: SNil) g'
--   
--   &gt;&gt;&gt; result = tensor1 `matmul` tensor2
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Float)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 10), 'Dim ('Name "*") ('Size 5),
--                   'Dim ('Name "*") ('Size 3), 'Dim ('Name "*") ('Size 7)]))
--   </pre>
matmul :: forall m gradient gradient' layout layout' device device' dataType dataType' shape shape' shape''. (MonadThrow m, shape'' ~ MatmulF shape shape', Catch shape'') => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') shape'')

module Torch.GraduallyTyped.Tensor.MathOperations

module Torch.GraduallyTyped.NN.Functional.NonLinearActivation
type SoftMaxErrorMessage (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply softmax on the dimension matching" % "" % "    '" <> by <> "'" % "" % "in the shape" % "" % "    '" <> dims <> "'." % ""
type family SoftmaxCheckF (by :: By Symbol Nat) (dims :: [Dim (Name Symbol) (Size Nat)]) (result :: Maybe (Dim (Name Symbol) (Size Nat))) :: [Dim (Name Symbol) (Size Nat)]
type family SoftmaxF (selectDim :: SelectDim (By Symbol Nat)) (shape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]

-- | Applies the softmax function that is defined as:
--   
--   &lt;math&gt;
--   
--   Softmax is applied to all slices along <tt>selectDim</tt>, and will
--   re-scale them so that the elements lie in the range &lt;math&gt; and
--   sum to &lt;math&gt;:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) &lt;- sRandn (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)) g
--   
--   &gt;&gt;&gt; result &lt;- softmax (SSelectDim (SByName @"feature")) input
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
softmax :: forall selectDim gradient layout device dataType shape shape' m. (MonadThrow m, shape' ~ SoftmaxF selectDim shape, Catch shape') => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')

-- | Applies the softmax function that is defined as:
--   
--   &lt;math&gt;
--   
--   Softmax is applied to all slices along <tt>selectDim</tt>, and will
--   re-scale them so that the elements lie in the range &lt;math&gt; and
--   sum to &lt;math&gt;:
--   
--   <pre>
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; (input, _) &lt;- sRandn (TensorSpec (SGradient SWithGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat) (SShape $ SName @"batch" :&amp;: SSize @32 :|: SName @"feature" :&amp;: SSize @8 :|: SNil)) g
--   
--   &gt;&gt;&gt; result &lt;- softmax (SSelectDim (SByName @"feature")) input
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 32),
--                'Dim ('Name "feature") ('Size 8)])
--   </pre>
logSoftmax :: forall selectDim gradient layout device dataType shape shape' m. (MonadThrow m, shape' ~ SoftmaxF selectDim shape, Catch shape') => SSelectDim selectDim -> Tensor gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape')

module Torch.GraduallyTyped.Random
newtype Generator (device :: Device (DeviceType Nat))
[UnsafeGenerator] :: forall device. TVar (Either (SDevice device, Word64) (ForeignPtr Generator)) -> Generator device
sMkGenerator :: forall m device. MonadThrow m => SDevice device -> Word64 -> m (Generator device)
mkGenerator :: forall m device. (SingI device, MonadThrow m) => Word64 -> m (Generator device)
sSetGeneratorDevice :: forall m generatorDevice' generatorDevice. MonadThrow m => SDevice generatorDevice' -> Generator generatorDevice -> m (Generator generatorDevice')
setGeneratorDevice :: forall m generatorDevice' generatorDevice. (SingI generatorDevice', MonadThrow m) => Generator generatorDevice -> m (Generator generatorDevice')
class SGetGeneratorDevice (device :: Device (DeviceType Nat))
sGetGenPtrDevice :: SGetGeneratorDevice device => ForeignPtr Generator -> SDevice device
sGetGeneratorDevice :: forall device. SGetGeneratorDevice device => Generator device -> SDevice device
getGeneratorDeviceType :: forall device. SGetGeneratorDevice device => Generator device -> DeviceType Int16
getGenPtr :: SGetGeneratorDevice device => TVar (Either (SDevice device, Word64) (ForeignPtr Generator)) -> IO (ForeignPtr Generator)
sCreateWithGenerator :: forall m gradient layout device dataType shape generatorDevice. (SGetGeneratorDevice generatorDevice, MonadThrow m) => TensorSpec gradient layout device dataType shape -> Generator generatorDevice -> (ForeignPtr TensorOptions -> [Dim String Integer] -> ForeignPtr Generator -> IO (ForeignPtr Tensor)) -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))
sForwardWithGenerator :: forall m gradient layout device dataType shape generatorDevice. (SGetGeneratorDevice generatorDevice, MonadThrow m) => Tensor gradient layout device dataType shape -> Generator generatorDevice -> (ForeignPtr Tensor -> ForeignPtr Generator -> IO (ForeignPtr Tensor)) -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))
instance Torch.GraduallyTyped.Random.SGetGeneratorDevice 'Torch.GraduallyTyped.Device.UncheckedDevice
instance Torch.GraduallyTyped.Random.SGetGeneratorDevice ('Torch.GraduallyTyped.Device.Device 'Torch.GraduallyTyped.Device.CPU)
instance GHC.TypeNats.KnownNat deviceIndex => Torch.GraduallyTyped.Random.SGetGeneratorDevice ('Torch.GraduallyTyped.Device.Device ('Torch.GraduallyTyped.Device.CUDA deviceIndex))

module Torch.GraduallyTyped.Tensor.Creation

-- | Create a gradually typed tensor of ones.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @32 :|: SUncheckedName "feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; :type sOnes $ TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--   sOnes $ TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithoutGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Int64)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 32),
--                   'Dim 'UncheckedName 'UncheckedSize]))
--   </pre>
sOnes :: forall gradient layout device dataType shape m. MonadThrow m => TensorSpec gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Create a typed tensor of ones.
--   
--   <pre>
--   &gt;&gt;&gt; ones :: IO (CPUParameter ('DataType 'Float) ('Shape '[]))
--   Tensor Float []  1.0000
--   
--   &gt;&gt;&gt; ones :: IO (CPUTensor ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1)]))
--   Tensor Int64 [1] [ 1]
--   </pre>
ones :: forall gradient layout device dataType shape m. MonadThrow m => (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape) => m (Tensor gradient layout device dataType shape)

-- | Create a gradually typed tensor of zeros.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @32 :|: SUncheckedName "feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; :type sZeros $ TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--   sZeros $ TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithoutGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Int64)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 32),
--                   'Dim 'UncheckedName 'UncheckedSize]))
--   </pre>
sZeros :: forall gradient layout device dataType shape m. MonadThrow m => TensorSpec gradient layout device dataType shape -> m (Tensor gradient layout device dataType shape)

-- | Create a typed tensor of zeros.
--   
--   <pre>
--   &gt;&gt;&gt; zeros :: IO (CPUParameter ('DataType 'Float) ('Shape '[]))
--   Tensor Float []  0.0000
--   
--   &gt;&gt;&gt; zeros :: IO (CPUTensor ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1)]))
--   Tensor Int64 [1] [ 0]
--   </pre>
zeros :: forall gradient layout device dataType shape m. MonadThrow m => (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape) => m (Tensor gradient layout device dataType shape)

-- | Create a gradually typed tensor filled with a given scalar value.
--   
--   <pre>
--   &gt;&gt;&gt; shape = SShape $ SName @"batch" :&amp;: SSize @32 :|: SUncheckedName "feature" :&amp;: SUncheckedSize 8 :|: SNil
--   
--   &gt;&gt;&gt; input = -1
--   
--   &gt;&gt;&gt; :type sFull (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape) input
--   sFull (TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SInt64) shape) input
--     :: MonadThrow m =&gt;
--        m (Tensor
--             ('Gradient 'WithoutGradient)
--             ('Layout 'Dense)
--             ('Device 'CPU)
--             ('DataType 'Int64)
--             ('Shape
--                '[ 'Dim ('Name "batch") ('Size 32),
--                   'Dim 'UncheckedName 'UncheckedSize]))
--   </pre>
sFull :: forall gradient layout device dataType shape input m. (MonadThrow m, Scalar input) => TensorSpec gradient layout device dataType shape -> input -> m (Tensor gradient layout device dataType shape)

-- | Create a typed tensor filled with a given scalar value.
--   
--   <pre>
--   &gt;&gt;&gt; full (-1) :: IO (CPUParameter ('DataType 'Float) ('Shape '[]))
--   Tensor Float [] -1.0000
--   
--   &gt;&gt;&gt; full (-1) :: IO (CPUTensor ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1)]))
--   Tensor Int64 [1] [-1]
--   </pre>
full :: forall gradient layout device dataType shape input m. (MonadThrow m, SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape, Scalar input) => input -> m (Tensor gradient layout device dataType shape)

-- | Create a gradually typed random tensor.
sRandn :: forall gradient layout device dataType shape generatorDevice m. (SGetGeneratorDevice generatorDevice, MonadThrow m) => TensorSpec gradient layout device dataType shape -> Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))

-- | Create typed random tensor.
randn :: forall gradient layout device dataType shape generatorDevice m. (SGetGeneratorDevice generatorDevice, MonadThrow m) => (SingI gradient, SingI layout, SingI device, SingI dataType, SingI shape) => Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))

-- | Create a gradually typed one-dimensional tensor of the numbers
--   <tt>0</tt> to <tt>size -1</tt>.
sArangeNaturals :: forall m gradient layout device dataType size shape. (MonadThrow m, shape ~ 'Shape '[ 'Dim ('Name "*") size]) => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SSize size -> m (Tensor gradient layout device dataType shape)

-- | Create a typed one-dimensional tensor of the numbers <tt>0</tt> to
--   <tt>size -1</tt>.
arangeNaturals :: forall size gradient layout device dataType shape m. (MonadThrow m, shape ~ 'Shape '[ 'Dim ('Name "*") size], SingI gradient, SingI layout, SingI device, SingI dataType, SingI size) => m (Tensor gradient layout device dataType shape)

-- | Create a gradually typed rectangular tensor with ones on the diagonal
--   and zeros elsewhere.
sEye :: forall gradient layout device dataType rows cols shape m. (MonadThrow m, shape ~ 'Shape '[ 'Dim ('Name "*") rows, 'Dim ('Name "*") cols]) => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SSize rows -> SSize cols -> m (Tensor gradient layout device dataType shape)

-- | Create a typed rectangular tensor with ones on the diagonal and zeros
--   elsewhere.
eye :: forall rows cols gradient layout device dataType shape m. (MonadThrow m, shape ~ 'Shape '[ 'Dim ('Name "*") rows, 'Dim ('Name "*") cols], SingI gradient, SingI layout, SingI device, SingI dataType, SingI rows, SingI cols) => m (Tensor gradient layout device dataType shape)

-- | Create a gradually typed square tensor with ones on the diagonal and
--   zeros elsewhere.
sEyeSquare :: forall gradient layout device dataType size shape m. (MonadThrow m, shape ~ 'Shape '[ 'Dim ('Name "*") size, 'Dim ('Name "*") size]) => SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SSize size -> m (Tensor gradient layout device dataType shape)

-- | Create a typed square tensor with ones on the diagonal and zeros
--   elsewhere.
eyeSquare :: forall size gradient layout device dataType shape m. (MonadThrow m, shape ~ 'Shape '[ 'Dim ('Name "*") size, 'Dim ('Name "*") size], SingI gradient, SingI layout, SingI device, SingI dataType, SingI size) => m (Tensor gradient layout device dataType shape)

module Torch.GraduallyTyped.Tensor

module Torch.GraduallyTyped.Autograd
class HasGrad parameters where {
    type Gradients parameters :: Type;
    type Loss parameters :: Type;
}

-- | calculate gradients of a zero-dimensional tensor with respect to a
--   list of independent tensor parameters
grad :: HasGrad parameters => Loss parameters -> parameters -> Gradients parameters
instance Torch.GraduallyTyped.Autograd.HasGrad (Torch.GraduallyTyped.Tensor.Type.Tensor ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithGradient) layout device dataType shape)

module Torch.GraduallyTyped.NN.Class
data NamedModel model
NamedModel :: Text -> model -> NamedModel model
pattern (::>) :: Text -> model -> NamedModel model
class HasForward model input generatorDevice output generatorOutputDevice | model input generatorDevice -> output, model input generatorDevice -> generatorOutputDevice

-- | <tt>forward m i g</tt> for a model <tt>m</tt>, an input <tt>i</tt>,
--   and a generator <tt>g</tt> returns the tuple <tt>(o, g')</tt> where
--   <tt>o</tt> is the output of the model applied to the input and
--   <tt>g'</tt> is the updated generator. <tt>forward m i g</tt> may throw
--   an exception if the input <tt>i</tt> or the generator <tt>g</tt> are
--   not compatible with the model <tt>m</tt>.
forward :: forall m. (HasForward model input generatorDevice output generatorOutputDevice, MonadThrow m) => model -> input -> Generator generatorDevice -> m (output, Generator generatorOutputDevice)

-- | <tt>forward m i g</tt> for a model <tt>m</tt>, an input <tt>i</tt>,
--   and a generator <tt>g</tt> returns the tuple <tt>(o, g')</tt> where
--   <tt>o</tt> is the output of the model applied to the input and
--   <tt>g'</tt> is the updated generator. <tt>forward m i g</tt> may throw
--   an exception if the input <tt>i</tt> or the generator <tt>g</tt> are
--   not compatible with the model <tt>m</tt>.
forward :: forall m. (HasForward model input generatorDevice output generatorOutputDevice, MonadThrow m, Generic model, GHasForward (Rep model) input generatorDevice output generatorOutputDevice) => model -> input -> Generator generatorDevice -> m (output, Generator generatorOutputDevice)
class GHasForward gModel input generatorDevice output generatorOutputDevice | gModel input generatorDevice -> output, gModel input generatorDevice -> generatorOutputDevice
gForward :: forall m c. (GHasForward gModel input generatorDevice output generatorOutputDevice, MonadThrow m) => gModel c -> input -> Generator generatorDevice -> m (output, Generator generatorOutputDevice)
newtype Wrap a
Wrap :: a -> Wrap a
type family ListToTuple xs = tuple | tuple -> xs
newtype ModelStack models
ModelStack :: ListToTuple models -> ModelStack models
type family ModelSpec model = spec | spec -> model
class HasInitialize model generatorDevice output generatorOutputDevice | model generatorDevice -> output, model generatorDevice -> generatorOutputDevice
initialize :: forall m. (HasInitialize model generatorDevice output generatorOutputDevice, MonadThrow m) => ModelSpec model -> Generator generatorDevice -> m (output, Generator generatorOutputDevice)
initialize :: forall m. (HasInitialize model generatorDevice output generatorOutputDevice, MonadThrow m, Generic (ModelSpec model), Generic output, GHasInitialize (Rep (ModelSpec model)) generatorDevice (Rep output) generatorOutputDevice) => ModelSpec model -> Generator generatorDevice -> m (output, Generator generatorOutputDevice)
class GHasInitialize gModelSpec generatorDevice gOutput generatorOutputDevice | gModelSpec generatorDevice -> gOutput, gModelSpec generatorDevice -> generatorOutputDevice
gInitialize :: forall m c. (GHasInitialize gModelSpec generatorDevice gOutput generatorOutputDevice, MonadThrow m) => gModelSpec c -> Generator generatorDevice -> m (gOutput c, Generator generatorOutputDevice)
data VectorSpec (n :: Nat) (a :: Type)
[VectorSpec] :: forall n a. SNat n -> Vector n (ModelSpec a) -> VectorSpec n a
type StateDictKey = Text
type StateDict = Map StateDictKey (ForeignPtr Tensor)
newtype FromStateDictError
FromStateDictKeyNotFoundError :: StateDictKey -> FromStateDictError
[fsdeExpectedKey] :: FromStateDictError -> StateDictKey
newtype ToStateDictError
ToStateDictKeyAlreadyInUseError :: StateDictKey -> ToStateDictError
[fsdeTakenKey] :: ToStateDictError -> StateDictKey
class HasStateDict model
fromStateDict :: forall m. (HasStateDict model, MonadIO m, MonadThrow m, MonadState StateDict m) => ModelSpec model -> StateDictKey -> m model
fromStateDict :: forall m. (HasStateDict model, MonadIO m, MonadThrow m, MonadState StateDict m, Generic model, Generic (ModelSpec model), GHasStateDict (Rep model) (Rep (ModelSpec model))) => ModelSpec model -> StateDictKey -> m model
toStateDict :: forall m. (HasStateDict model, MonadThrow m, MonadState StateDict m) => StateDictKey -> model -> m ()
toStateDict :: forall m. (HasStateDict model, MonadThrow m, MonadState StateDict m, Generic model, GHasStateDict (Rep model) (Rep (ModelSpec model))) => StateDictKey -> model -> m ()
class GHasStateDict gModel gModelSpec | gModelSpec -> gModel, gModel -> gModelSpec
gFromStateDict :: forall m c. (GHasStateDict gModel gModelSpec, MonadIO m, MonadThrow m, MonadState StateDict m) => gModelSpec c -> StateDictKey -> m (gModel c)
gToStateDict :: forall m c. (GHasStateDict gModel gModelSpec, MonadThrow m, MonadState StateDict m) => StateDictKey -> gModel c -> m ()

-- | Load a state dictionary from a TorchScript file.
stateDictFromFile :: FilePath -> IO StateDict

-- | Save a state dictionary to a TorchScript file.
stateDictToFile :: StateDict -> FilePath -> IO ()
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Class.NamedModel model)
instance GHC.Show.Show model => GHC.Show.Show (Torch.GraduallyTyped.NN.Class.NamedModel model)
instance GHC.Classes.Ord model => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Class.NamedModel model)
instance GHC.Classes.Eq model => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Class.NamedModel model)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Class.Wrap a)
instance GHC.Show.Show a => GHC.Show.Show (Torch.GraduallyTyped.NN.Class.Wrap a)
instance GHC.Classes.Ord a => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Class.Wrap a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Class.Wrap a)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Class.ModelStack models)
instance GHC.Show.Show Torch.GraduallyTyped.NN.Class.FromStateDictError
instance GHC.Show.Show Torch.GraduallyTyped.NN.Class.ToStateDictError
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Class.ModelSpec a) => GHC.Show.Show (Torch.GraduallyTyped.NN.Class.VectorSpec n a)
instance Torch.GraduallyTyped.NN.Class.HasStateDict a => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.Wrap a)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict model, modelSpec GHC.Types.~ Torch.GraduallyTyped.NN.Class.ModelSpec model) => Torch.GraduallyTyped.NN.Class.GHasStateDict (GHC.Generics.K1 i model) (GHC.Generics.K1 i modelSpec)
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.Shape.Type.SDim dim)
instance Torch.GraduallyTyped.NN.Class.HasStateDict ()
instance Torch.GraduallyTyped.NN.Class.HasStateDict model => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.NamedModel model)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict a, Torch.GraduallyTyped.NN.Class.HasStateDict b) => Torch.GraduallyTyped.NN.Class.HasStateDict (a, b)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict a, Torch.GraduallyTyped.NN.Class.HasStateDict b, Torch.GraduallyTyped.NN.Class.HasStateDict c) => Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict a, Torch.GraduallyTyped.NN.Class.HasStateDict b, Torch.GraduallyTyped.NN.Class.HasStateDict c, Torch.GraduallyTyped.NN.Class.HasStateDict d) => Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict a, Torch.GraduallyTyped.NN.Class.HasStateDict b, Torch.GraduallyTyped.NN.Class.HasStateDict c, Torch.GraduallyTyped.NN.Class.HasStateDict d, Torch.GraduallyTyped.NN.Class.HasStateDict e) => Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d, e)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict a, Torch.GraduallyTyped.NN.Class.HasStateDict b, Torch.GraduallyTyped.NN.Class.HasStateDict c, Torch.GraduallyTyped.NN.Class.HasStateDict d, Torch.GraduallyTyped.NN.Class.HasStateDict e, Torch.GraduallyTyped.NN.Class.HasStateDict f) => Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d, e, f)
instance (Torch.GraduallyTyped.NN.Class.HasStateDict a, Torch.GraduallyTyped.NN.Class.HasStateDict b, Torch.GraduallyTyped.NN.Class.HasStateDict c, Torch.GraduallyTyped.NN.Class.HasStateDict d, Torch.GraduallyTyped.NN.Class.HasStateDict e, Torch.GraduallyTyped.NN.Class.HasStateDict f, Torch.GraduallyTyped.NN.Class.HasStateDict g) => Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d, e, f, g)
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[])
instance Torch.GraduallyTyped.NN.Class.HasStateDict a => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (a, b) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d, e) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d, e])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d, e, f) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d, e, f])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (a, b, c, d, e, f, g) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d, e, f, g])
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape)
instance Torch.GraduallyTyped.NN.Class.HasStateDict a => Torch.GraduallyTyped.NN.Class.HasStateDict (Data.Vector.Sized.Vector n a)
instance (Torch.GraduallyTyped.NN.Class.GHasStateDict gModelA gModelSpecA, Torch.GraduallyTyped.NN.Class.GHasStateDict gModelB gModelSpecB) => Torch.GraduallyTyped.NN.Class.GHasStateDict (gModelA GHC.Generics.:*: gModelB) (gModelSpecA GHC.Generics.:*: gModelSpecB)
instance Torch.GraduallyTyped.NN.Class.GHasStateDict gModel gModelSpec => Torch.GraduallyTyped.NN.Class.GHasStateDict (GHC.Generics.M1 i t gModel) (GHC.Generics.M1 i t gModelSpec)
instance Torch.GraduallyTyped.NN.Class.GHasStateDict GHC.Generics.U1 GHC.Generics.U1
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.NN.Class.ToStateDictError
instance GHC.Exception.Type.Exception Torch.GraduallyTyped.NN.Class.FromStateDictError
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice output generatorOutputDevice, Torch.GraduallyTyped.NN.Class.HasInitialize a generatorOutputDevice output generatorOutputDevice, n' GHC.Types.~ (n GHC.TypeNats.+ 1)) => Torch.GraduallyTyped.NN.Class.HasInitialize (Data.Vector.Sized.Vector n' a) generatorDevice (Data.Vector.Sized.Vector n' output) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice a' generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.Wrap a) generatorDevice (Torch.GraduallyTyped.NN.Class.Wrap a') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize model generatorDevice output generatorOutputDevice, Torch.GraduallyTyped.NN.Class.ModelSpec model GHC.Types.~ modelSpec) => Torch.GraduallyTyped.NN.Class.GHasInitialize (GHC.Generics.K1 i modelSpec) generatorDevice (GHC.Generics.K1 i output) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.Shape.Type.SDim dim) generatorDevice (Torch.GraduallyTyped.Shape.Type.SDim dim) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize () generatorDevice () generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize model generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.NamedModel model) generatorDevice (Torch.GraduallyTyped.NN.Class.NamedModel output) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasInitialize b generatorOutputADevice outputB generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b) generatorDevice (outputA, outputB) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasInitialize b generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasInitialize c generatorOutputBDevice outputC generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c) generatorDevice (outputA, outputB, outputC) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasInitialize b generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasInitialize c generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasInitialize d generatorOutputCDevice outputD generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d) generatorDevice (outputA, outputB, outputC, outputD) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasInitialize b generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasInitialize c generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasInitialize d generatorOutputCDevice outputD generatorOutputDDevice, Torch.GraduallyTyped.NN.Class.HasInitialize e generatorOutputDDevice outputE generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d, e) generatorDevice (outputA, outputB, outputC, outputD, outputE) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasInitialize b generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasInitialize c generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasInitialize d generatorOutputCDevice outputD generatorOutputDDevice, Torch.GraduallyTyped.NN.Class.HasInitialize e generatorOutputDDevice outputE generatorOutputEDevice, Torch.GraduallyTyped.NN.Class.HasInitialize f generatorOutputEDevice outputF generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d, e, f) generatorDevice (outputA, outputB, outputC, outputD, outputE, outputF) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasInitialize b generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasInitialize c generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasInitialize d generatorOutputCDevice outputD generatorOutputDDevice, Torch.GraduallyTyped.NN.Class.HasInitialize e generatorOutputDDevice outputE generatorOutputEDevice, Torch.GraduallyTyped.NN.Class.HasInitialize f generatorOutputEDevice outputF generatorOutputFDevice, Torch.GraduallyTyped.NN.Class.HasInitialize g generatorOutputFDevice outputG generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d, e, f, g) generatorDevice (outputA, outputB, outputC, outputD, outputE, outputF, outputG) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[]) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize a generatorDevice a' generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a']) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (a, b) generatorDevice (a', b') generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a', b']) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c) generatorDevice (a', b', c') generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a', b', c']) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d) generatorDevice (a', b', c', d') generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a', b', c', d']) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d, e) generatorDevice (a', b', c', d', e') generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d, e]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a', b', c', d', e']) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d, e, f) generatorDevice (a', b', c', d', e', f') generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d, e, f]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a', b', c', d', e', f']) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (a, b, c, d, e, f, g) generatorDevice (a', b', c', d', e', f', g') generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Class.ModelStack '[a, b, c, d, e, f, g]) generatorDevice (Torch.GraduallyTyped.NN.Class.ModelStack '[a', b', c', d', e', f', g']) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.GHasInitialize gModelSpecA generatorDevice gOutputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.GHasInitialize gModelSpecB generatorOutputADevice gOutputB generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.GHasInitialize (gModelSpecA GHC.Generics.:*: gModelSpecB) generatorDevice (gOutputA GHC.Generics.:*: gOutputB) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.GHasInitialize gModelSpec generatorDevice gOutput generatorOutputDevice => Torch.GraduallyTyped.NN.Class.GHasInitialize (GHC.Generics.M1 i t gModelSpec) generatorDevice (GHC.Generics.M1 i t gOutput) generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.GHasInitialize GHC.Generics.U1 generatorDevice GHC.Generics.U1 generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Class.ListToTuple models) input generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Class.ModelStack models) input generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Class.Wrap a) input generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasForward () input generatorDevice input generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasForward model input generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Class.NamedModel model) input generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasForward model input generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.GHasForward (GHC.Generics.K1 i model) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasForward b outputA generatorOutputADevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (a, b) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasForward b outputA generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasForward c outputB generatorOutputBDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (a, b, c) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasForward b outputA generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasForward c outputB generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasForward d outputC generatorOutputCDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (a, b, c, d) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasForward b outputA generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasForward c outputB generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasForward d outputC generatorOutputCDevice outputD generatorOutputDDevice, Torch.GraduallyTyped.NN.Class.HasForward e outputD generatorOutputDDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (a, b, c, d, e) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasForward b outputA generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasForward c outputB generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasForward d outputC generatorOutputCDevice outputD generatorOutputDDevice, Torch.GraduallyTyped.NN.Class.HasForward e outputD generatorOutputDDevice outputE generatorOutputEDevice, Torch.GraduallyTyped.NN.Class.HasForward f outputE generatorOutputEDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (a, b, c, d, e, f) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.HasForward b outputA generatorOutputADevice outputB generatorOutputBDevice, Torch.GraduallyTyped.NN.Class.HasForward c outputB generatorOutputBDevice outputC generatorOutputCDevice, Torch.GraduallyTyped.NN.Class.HasForward d outputC generatorOutputCDevice outputD generatorOutputDDevice, Torch.GraduallyTyped.NN.Class.HasForward e outputD generatorOutputDDevice outputE generatorOutputEDevice, Torch.GraduallyTyped.NN.Class.HasForward f outputE generatorOutputEDevice outputF generatorOutputFDevice, Torch.GraduallyTyped.NN.Class.HasForward g outputF generatorOutputFDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (a, b, c, d, e, f, g) input generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasForward (Data.Vector.Sized.Vector 0 a) input generatorDevice input generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasForward (Data.Vector.Sized.Vector 1 a) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward a input generatorDevice output generatorOutputDevice, Torch.GraduallyTyped.NN.Class.HasForward a output generatorOutputDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Data.Vector.Sized.Vector n a) input generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.GHasForward gModelA inputA generatorDevice outputA generatorOutputADevice, Torch.GraduallyTyped.NN.Class.GHasForward gModelB outputA generatorOutputADevice outputB generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.GHasForward (gModelA GHC.Generics.:*: gModelB) inputA generatorDevice outputB generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.GHasForward gModel input generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.GHasForward (GHC.Generics.M1 i t gModel) input generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.GHasForward GHC.Generics.U1 input generatorDevice input generatorDevice

module Torch.GraduallyTyped.NN.Activation

-- | <a>Softmax</a> is a non-linear activation function.
data Softmax (selectDim :: SelectDim (By Symbol Nat))
[Softmax] :: forall selectDim. SSelectDim selectDim -> Softmax selectDim

-- | <a>LogSoftmax</a> is a non-linear activation function.
data LogSoftmax (selectDim :: SelectDim (By Symbol Nat))
[LogSoftmax] :: forall selectDim. SSelectDim selectDim -> LogSoftmax selectDim

-- | <a>Relu</a> is a step-wise linear activation function.
data Relu
[Relu] :: Relu

-- | <a>Gelu</a> is a non-linear activation function.
data Gelu
[Gelu] :: Gelu

-- | <a>GeluNew</a> is a non-linear activation function. It is a modified
--   version of the <a>Gelu</a> function.
data GeluNew
[GeluNew] :: GeluNew

-- | <a>Tanh</a> is a non-linear activation function.
data Tanh
[Tanh] :: Tanh
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Activation.Softmax selectDim)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Activation.LogSoftmax selectDim)
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Activation.Relu
instance GHC.Show.Show Torch.GraduallyTyped.NN.Activation.Relu
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Activation.Relu
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Activation.Relu
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Activation.Gelu
instance GHC.Show.Show Torch.GraduallyTyped.NN.Activation.Gelu
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Activation.Gelu
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Activation.Gelu
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Activation.GeluNew
instance GHC.Show.Show Torch.GraduallyTyped.NN.Activation.GeluNew
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Activation.GeluNew
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Activation.GeluNew
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Activation.Tanh
instance GHC.Show.Show Torch.GraduallyTyped.NN.Activation.Tanh
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Activation.Tanh
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Activation.Tanh
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.Tanh generator Torch.GraduallyTyped.NN.Activation.Tanh generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.Tanh
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.Tanh (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.GeluNew generator Torch.GraduallyTyped.NN.Activation.GeluNew generator
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.GeluNew
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.GeluNew (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.Gelu generatorDevice Torch.GraduallyTyped.NN.Activation.Gelu generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.Gelu
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.Gelu (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Activation.Relu generatorDevice Torch.GraduallyTyped.NN.Activation.Relu generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Activation.Relu
instance Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Activation.Relu (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Activation.LogSoftmax selectDim) generatorDevice (Torch.GraduallyTyped.NN.Activation.LogSoftmax selectDim) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Activation.LogSoftmax selectDim)
instance (shape' GHC.Types.~ Torch.GraduallyTyped.NN.Functional.NonLinearActivation.SoftmaxF selectDim shape, Torch.GraduallyTyped.Prelude.Catch shape', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape') => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Activation.LogSoftmax selectDim) (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator output generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Activation.Softmax selectDim) generatorDevice (Torch.GraduallyTyped.NN.Activation.Softmax selectDim) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Activation.Softmax selectDim)
instance (shape' GHC.Types.~ Torch.GraduallyTyped.NN.Functional.NonLinearActivation.SoftmaxF selectDim shape, Torch.GraduallyTyped.Prelude.Catch shape', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape') => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Activation.Softmax selectDim) (Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient layout device dataType shape) generator output generator

module Torch.GraduallyTyped.Optim

-- | Options for the Adam optimizer.
data AdamOptions
AdamOptions :: Double -> Double -> Double -> Double -> Double -> Bool -> AdamOptions

-- | learning rate
[learningRate] :: AdamOptions -> Double

-- | beta1
[beta1] :: AdamOptions -> Double

-- | beta2
[beta2] :: AdamOptions -> Double

-- | epsilon
[epsilon] :: AdamOptions -> Double

-- | weight decay
[weightDecay] :: AdamOptions -> Double

-- | use amsgrad
[amsgrad] :: AdamOptions -> Bool

-- | Default Adam options.
defaultAdamOptions :: AdamOptions

-- | Optimizer data type.
data Optimizer model
[UnsafeOptimizer] :: forall model. [StateDictKey] -> ForeignPtr Optimizer -> Optimizer model

-- | Get the model state dictionary from an optimizer.
getStateDict :: forall model. Optimizer model -> IO StateDict

-- | Extract a model from an optimizer.
getModel :: forall model. HasStateDict model => ModelSpec model -> Optimizer model -> IO model

-- | Create a new Adam optimizer from a model.
mkAdam :: forall model. HasStateDict model => AdamOptions -> model -> IO (Optimizer model)

-- | Perform one step of optimization.
stepWithGenerator :: forall model generatorDevice lossGradient lossLayout lossDataType lossDevice lossShape generatorOutputDevice. (HasStateDict model, SGetGeneratorDevice generatorDevice, SGetGeneratorDevice generatorOutputDevice, Catch (lossShape <+> 'Shape '[]), Catch (lossGradient <+> 'Gradient 'WithGradient)) => Optimizer model -> ModelSpec model -> (model -> Generator generatorDevice -> IO (Tensor lossGradient lossLayout lossDataType lossDevice lossShape, Generator generatorOutputDevice)) -> Generator generatorDevice -> IO (Tensor lossGradient lossLayout lossDataType lossDevice lossShape, Generator generatorOutputDevice)

module Torch.GraduallyTyped.NN.Transformer.Type

-- | A data type representing the style of a transformer. Every supported
--   transformer has a constructor of this type.
data TransformerStyle

-- | <tt>T5</tt> transformer style, see
--   <a>https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html</a>
T5 :: TransformerStyle

-- | <tt>ByT5</tt> transformer style, see
--   <a>https://arxiv.org/abs/2105.13626</a>
ByT5 :: TransformerStyle

-- | <tt>BART</tt> transformer style, see
--   <a>https://arxiv.org/abs/1910.13461</a>
BART :: TransformerStyle

-- | <tt>MBART</tt> transformer style, see
--   <a>https://arxiv.org/abs/2001.08210</a>
MBART :: TransformerStyle

-- | <tt>Pegasus</tt> transformer style, see
--   <a>https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html</a>
Pegasus :: TransformerStyle

-- | <tt>BERT</tt> transformer style, see
--   <a>https://arxiv.org/abs/1810.04805</a>
BERT :: TransformerStyle

-- | <tt>RoBERTa</tt> transformer style, see
--   <a>https://arxiv.org/abs/1907.11692</a>
RoBERTa :: TransformerStyle

-- | <tt>GPT2</tt> transformer style, see
--   <a>https://openai.com/blog/better-language-models/</a>
GPT2 :: TransformerStyle
type family T5Sym0 :: TransformerStyle
type family ByT5Sym0 :: TransformerStyle
type family BARTSym0 :: TransformerStyle
type family MBARTSym0 :: TransformerStyle
type family PegasusSym0 :: TransformerStyle
type family BERTSym0 :: TransformerStyle
type family RoBERTaSym0 :: TransformerStyle
type family GPT2Sym0 :: TransformerStyle
data STransformerStyle :: TransformerStyle -> Type
[ST5] :: STransformerStyle ('T5 :: TransformerStyle)
[SByT5] :: STransformerStyle ('ByT5 :: TransformerStyle)
[SBART] :: STransformerStyle ('BART :: TransformerStyle)
[SMBART] :: STransformerStyle ('MBART :: TransformerStyle)
[SPegasus] :: STransformerStyle ('Pegasus :: TransformerStyle)
[SBERT] :: STransformerStyle ('BERT :: TransformerStyle)
[SRoBERTa] :: STransformerStyle ('RoBERTa :: TransformerStyle)
[SGPT2] :: STransformerStyle ('GPT2 :: TransformerStyle)

-- | A data type representing the type of head used in a transformer.
data TransformerHead
WithoutHead :: TransformerHead
WithLMHead :: TransformerHead
type family WithoutHeadSym0 :: TransformerHead
type family WithLMHeadSym0 :: TransformerHead
data STransformerHead :: TransformerHead -> Type
[SWithoutHead] :: STransformerHead ('WithoutHead :: TransformerHead)
[SWithLMHead] :: STransformerHead ('WithLMHead :: TransformerHead)
padded :: Integral n => n -> a -> [a] -> [a]

-- | Converts a doubly-nested list of input ids to a batched input tensor.
--   The outer list is over batches, the inner list over sequences. The
--   batch size is inferred from the length of the outer list. The sequence
--   length is inferred from the length of the inner list. The input ids
--   are padded to the maximum sequence length. The output tensor is
--   truncated to the maximum sequence length.
mkTransformerInput :: forall batchDim seqDim device m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, Catch ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => Int -> SDim batchDim -> SDim seqDim -> SDevice device -> [[Int]] -> m output
type MkPosC device shape seqDim seqName seqSize output = (SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), seqDim ~ 'Dim seqName seqSize, output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") seqSize]))

-- | Computes absolute positions of the input tokens. Given an input tensor
--   of shape <tt>[batchDim, Dim seqName seqSize]</tt>, returns a tensor of
--   shape <tt>[Dim "*" seqSize]</tt>.
mkPos :: forall m gradient layout device dataType shape seqDim seqName seqSize output. (MonadThrow m, MkPosC device shape seqDim seqName seqSize output) => Tensor gradient layout device dataType shape -> m output
data MkAbsPos
MkAbsPos :: MkAbsPos
MkAbsPosWithOffset :: Int -> MkAbsPos
[absPosOffset] :: MkAbsPos -> Int

-- | Computes relative positions of the input tokens to the encoder.
--   
--   <pre>
--   &gt;&gt;&gt; mkRelPos' 32 128 21 17
--   [[0,17,18,19,20,21,22,23,24,24,24,24,25,25,25,25,26],[1,0,17,18,19,20,21,22,23,24,24,24,24,25,25,25,25],[2,1,0,17,18,19,20,21,22,23,24,24,24,24,25,25,25],[3,2,1,0,17,18,19,20,21,22,23,24,24,24,24,25,25],[4,3,2,1,0,17,18,19,20,21,22,23,24,24,24,24,25],[5,4,3,2,1,0,17,18,19,20,21,22,23,24,24,24,24],[6,5,4,3,2,1,0,17,18,19,20,21,22,23,24,24,24],[7,6,5,4,3,2,1,0,17,18,19,20,21,22,23,24,24],[8,7,6,5,4,3,2,1,0,17,18,19,20,21,22,23,24],[8,8,7,6,5,4,3,2,1,0,17,18,19,20,21,22,23],[8,8,8,7,6,5,4,3,2,1,0,17,18,19,20,21,22],[8,8,8,8,7,6,5,4,3,2,1,0,17,18,19,20,21],[9,8,8,8,8,7,6,5,4,3,2,1,0,17,18,19,20],[9,9,8,8,8,8,7,6,5,4,3,2,1,0,17,18,19],[9,9,9,8,8,8,8,7,6,5,4,3,2,1,0,17,18],[9,9,9,9,8,8,8,8,7,6,5,4,3,2,1,0,17],[10,9,9,9,9,8,8,8,8,7,6,5,4,3,2,1,0],[10,10,9,9,9,9,8,8,8,8,7,6,5,4,3,2,1],[10,10,10,9,9,9,9,8,8,8,8,7,6,5,4,3,2],[10,10,10,10,9,9,9,9,8,8,8,8,7,6,5,4,3],[10,10,10,10,10,9,9,9,9,8,8,8,8,7,6,5,4]]
--   </pre>
mkRelPos' :: Int -> Int -> Int -> Int -> [[Int]]
type MkRelPosC device shape seqDim seqName seqSize output = (SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), seqDim ~ 'Dim seqName seqSize, Catch ('[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> '[ 'Dim ('Name "*") seqSize, 'Dim ('Name "*") seqSize]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") seqSize, 'Dim ('Name "*") seqSize]))

-- | Computes relative positions of the input tokens to the encoder. Given
--   an input tensor of shape <tt>[batchDim, Dim seqName seqSize]</tt>,
--   returns a tensor of shape <tt>[1, Dim "*" seqSize, Dim "*"
--   seqSize]</tt>.
mkRelPos :: forall m gradient layout device dataType shape relPosEncBucketDim seqDim seqName seqSize output. (MonadThrow m, MkRelPosC device shape seqDim seqName seqSize output) => SDim relPosEncBucketDim -> Int -> Tensor gradient layout device dataType shape -> m output

-- | Computes relative positions of the input tokens to the decoder.
--   
--   <pre>
--   &gt;&gt;&gt; mkDecoderRelPos' 32 128 21 17
--   [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[3,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[4,3,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0],[5,4,3,2,1,0,0,0,0,0,0,0,0,0,0,0,0],[6,5,4,3,2,1,0,0,0,0,0,0,0,0,0,0,0],[7,6,5,4,3,2,1,0,0,0,0,0,0,0,0,0,0],[8,7,6,5,4,3,2,1,0,0,0,0,0,0,0,0,0],[9,8,7,6,5,4,3,2,1,0,0,0,0,0,0,0,0],[10,9,8,7,6,5,4,3,2,1,0,0,0,0,0,0,0],[11,10,9,8,7,6,5,4,3,2,1,0,0,0,0,0,0],[12,11,10,9,8,7,6,5,4,3,2,1,0,0,0,0,0],[13,12,11,10,9,8,7,6,5,4,3,2,1,0,0,0,0],[14,13,12,11,10,9,8,7,6,5,4,3,2,1,0,0,0],[15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0,0],[16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0],[16,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],[16,16,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2],[17,16,16,16,15,14,13,12,11,10,9,8,7,6,5,4,3],[17,17,16,16,16,15,14,13,12,11,10,9,8,7,6,5,4]]
--   </pre>
mkDecoderRelPos' :: Int -> Int -> Int -> Int -> [[Int]]

-- | Computes relative positions of the input tokens to the decoder. Given
--   an input tensor of shape <tt>[batchDim, Dim seqName seqSize]</tt>,
--   returns a tensor of shape <tt>[1, Dim "*" seqSize, Dim "*"
--   seqSize]</tt>.
mkDecoderRelPos :: forall m gradient layout device dataType shape relPosEncBucketDim seqDim seqName seqSize output. (MonadThrow m, MkRelPosC device shape seqDim seqName seqSize output) => SDim relPosEncBucketDim -> Int -> Tensor gradient layout device dataType shape -> m output
data MkRelPos (relPosEncBucketDim :: Dim (Name Symbol) (Size Nat))
[MkRelPos] :: forall relPosEncBucketDim. SDim relPosEncBucketDim -> Int -> MkRelPos relPosEncBucketDim
[MkDecoderRelPos] :: forall relPosEncBucketDim. SDim relPosEncBucketDim -> Int -> MkRelPos relPosEncBucketDim
type MkTransformerPaddingMaskC layout device dataType shape output = (SGetDevice device, Catch (dataType <+> 'DataType 'Int64), Catch (BroadcastShapesF shape ('Shape '[])), output ~ Tensor ('Gradient 'WithoutGradient) (layout <+> 'Layout 'Dense) device ('DataType 'Bool) (BroadcastShapesF shape ('Shape '[])))

-- | Computes the padding mask for a transformer. Given an input tensor of
--   shape <tt>[batchDim, Dim seqName seqSize]</tt>, returns a tensor of
--   shape <tt>[batchDim, Dim "*" seqSize]</tt>.
mkTransformerPaddingMask :: forall m gradient layout device dataType shape output. (MonadThrow m, MkTransformerPaddingMaskC layout device dataType shape output) => Int -> Tensor gradient layout device dataType shape -> m output
newtype MkTransformerPaddingMask
MkTransformerPaddingMask :: Int -> MkTransformerPaddingMask
[padTokenId] :: MkTransformerPaddingMask -> Int
type MkTransformerAttentionMaskC transformerDataType gradient layout device dataType shape seqDim output = (SGetLayout layout, SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), Catch (gradient <+> 'Gradient 'WithoutGradient), Catch (dataType <+> 'DataType 'Bool), Catch (UnsqueezeF ('SelectDim ('ByIndex 1)) shape), Catch (BroadcastShapesF (UnsqueezeF ('SelectDim ('ByIndex 1)) shape) ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim])), output ~ Tensor ('Gradient 'WithoutGradient) (layout <+> 'Layout 'Dense) device transformerDataType (BroadcastShapesF (UnsqueezeF ('SelectDim ('ByIndex 1)) shape) ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim])))

-- | Creates a bidirectional attention mask for a transformer. Given a
--   padding mask of shape <tt>[batchDim, seqDim]</tt>, returns a tensor of
--   shape <tt>[batchDim, seqDim, seqDim]</tt>.
mkTransformerAttentionMask :: forall m transformerDataType gradient layout device dataType shape seqDim output. (MonadThrow m, MkTransformerAttentionMaskC transformerDataType gradient layout device dataType shape seqDim output) => SDataType transformerDataType -> Double -> Tensor gradient layout device dataType shape -> m output
data MkTransformerAttentionMask (dataType :: DataType DType)
[MkTransformerAttentionMask] :: forall dataType. SDataType dataType -> Double -> MkTransformerAttentionMask dataType
type MkTransformerDecoderAttentionMaskC transformerDataType layout device shape seqDim output = (SGetLayout layout, SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), Catch seqDim, Catch (UnsqueezeF ('SelectDim ('ByIndex 1)) shape), Catch (BroadcastShapesF ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim]) (UnsqueezeF ('SelectDim ('ByIndex 1)) shape)), Catch (BroadcastShapesF (BroadcastShapesF ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim]) (UnsqueezeF ('SelectDim ('ByIndex 1)) shape)) ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim])), output ~ Tensor ('Gradient 'WithoutGradient) (layout <+> 'Layout 'Dense) device transformerDataType (BroadcastShapesF (BroadcastShapesF ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim]) (UnsqueezeF ('SelectDim ('ByIndex 1)) shape)) ('Shape '[ 'Dim ('Name "*") ('Size 1), seqDim, seqDim])))

-- | Creates a causal attention mask for a transformer decoder. Given a
--   padding mask of shape <tt>[batchDim, seqDim]</tt>, returns a tensor of
--   shape <tt>[batchDim, seqDim, seqDim]</tt>.
mkTransformerDecoderAttentionMask :: forall m transformerDataType gradient layout device dataType shape seqDim output. (MonadThrow m, MkTransformerDecoderAttentionMaskC transformerDataType layout device shape seqDim output) => SDataType transformerDataType -> Double -> Tensor gradient layout device dataType shape -> m output
data MkTransformerDecoderAttentionMask (dataType :: DataType DType)
[MkTransformerDecoderAttentionMask] :: forall dataType. SDataType dataType -> Double -> MkTransformerDecoderAttentionMask dataType
type MkTransformerCrossAttentionMaskC transformerDataType decoderInputShape decoderInputSeqDim gradient layout device dataType shape seqDim output = (SGetLayout layout, SGetDevice device, SGetShape shape, seqDim ~ (shape ! 1), SGetShape decoderInputShape, decoderInputSeqDim ~ (decoderInputShape ! 1), Catch (gradient <+> 'Gradient 'WithoutGradient), Catch (dataType <+> 'DataType 'Bool), Catch (UnsqueezeF ('SelectDim ('ByIndex 1)) shape), Catch (BroadcastShapesF (UnsqueezeF ('SelectDim ('ByIndex 1)) shape) ('Shape '[ 'Dim ('Name "*") ('Size 1), decoderInputSeqDim, seqDim])), output ~ Tensor ('Gradient 'WithoutGradient) (layout <+> 'Layout 'Dense) device transformerDataType (BroadcastShapesF (UnsqueezeF ('SelectDim ('ByIndex 1)) shape) ('Shape '[ 'Dim ('Name "*") ('Size 1), decoderInputSeqDim, seqDim])))

-- | Creates a cross-attention mask for an encoder-decoder transformer.
--   Given an encoder padding mask of shape <tt>[batchDim, seqDim]</tt>,
--   and the shape <tt>[batchDim, decoderSeqDim]</tt> of the decoder's
--   input, returns a tensor of shape <tt>[batchDim, decoderSeqDim,
--   seqDim]</tt>.
mkTransformerCrossAttentionMask :: forall m transformerDataType decoderInputShape decoderInputSeqDim gradient layout device dataType shape seqDim output. (MonadThrow m, MkTransformerCrossAttentionMaskC transformerDataType decoderInputShape decoderInputSeqDim gradient layout device dataType shape seqDim output) => SDataType transformerDataType -> SShape decoderInputShape -> Double -> Tensor gradient layout device dataType shape -> m output
data MkTransformerCrossAttentionMask (dataType :: DataType DType)
[MkTransformerCrossAttentionMask] :: forall dataType. SDataType dataType -> Double -> MkTransformerCrossAttentionMask dataType
data ShiftRight fillValue
[ShiftRight] :: forall fillValue. fillValue -> ShiftRight fillValue
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.Type.MkRelPos relPosEncBucketDim)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Type.MkRelPos relPosEncBucketDim)
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMask
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMask
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMask dataType)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMask dataType)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMask dataType)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMask dataType)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMask dataType)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMask dataType)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue)
instance GHC.Show.Show fillValue => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue)
instance GHC.Classes.Ord fillValue => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue)
instance GHC.Classes.Eq fillValue => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue)
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue) generatorDevice (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue)
instance (input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.SGetLayout inputLayout, Torch.GraduallyTyped.Tensor.Type.SGetDevice inputDevice, Torch.GraduallyTyped.Tensor.Type.SGetDataType inputDataType, Torch.GraduallyTyped.Tensor.Type.SGetShape inputShape, inputBatchDim GHC.Types.~ (inputShape Torch.GraduallyTyped.Shape.Class.! 0), inputSeqDim GHC.Types.~ (inputShape Torch.GraduallyTyped.Shape.Class.! 1), Torch.GraduallyTyped.Scalar.Scalar fillValue, rightShiftedInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> 'Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient) inputLayout inputDevice inputDataType (Torch.GraduallyTyped.Shape.Class.ReplaceDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) (inputShape Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.Shape.Type.Shape '[inputBatchDim, inputSeqDim]) (Torch.GraduallyTyped.Shape.Class.AddDimF inputSeqDim ('Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size 1))))) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight fillValue) input generator rightShiftedInput generator
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMask dataType) generatorDevice (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMask dataType) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMask dataType)
instance Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMaskC dataType decoderInputShape decoderInputSeqDim inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaksDataType inputPaddingMaskShape seqDim output => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerCrossAttentionMask dataType) (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, Torch.GraduallyTyped.Tensor.Type.Tensor inputPaddingMaskGradient inputPaddingMaskLayout inputPaddingMaskDevice inputPaddingMaksDataType inputPaddingMaskShape) generatorDevice output generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMask dataType) generatorDevice (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMask dataType) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMask dataType)
instance Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMaskC dataType decoderInputLayout decoderInputDevice decoderInputShape seqDim output => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerDecoderAttentionMask dataType) (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape) generatorDevice output generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMask dataType) generatorDevice (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMask dataType) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMask dataType)
instance Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMaskC dataType inputGradient inputLayout inputDevice inputDataType inputShape seqDim output => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerAttentionMask dataType) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape) generatorDevice output generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMask generatorDevice Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMask generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMask
instance Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMaskC layout device dataType shape output => Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Transformer.Type.MkTransformerPaddingMask (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice output generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.Type.MkRelPos relPosEncBucketDim) generatorDevice (Torch.GraduallyTyped.NN.Transformer.Type.MkRelPos relPosEncBucketDim) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.Type.MkRelPos relPosEncBucketDim)
instance Torch.GraduallyTyped.NN.Transformer.Type.MkRelPosC device shape seqDim seqName seqSize output => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.MkRelPos relPosEncBucketDim) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device ('Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) ('Torch.GraduallyTyped.Shape.Type.Shape '[ 'Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") ('Torch.GraduallyTyped.Shape.Type.Size 1), 'Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") seqSize, 'Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") seqSize])) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos generatorDevice Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos
instance Torch.GraduallyTyped.NN.Transformer.Type.MkPosC device shape seqDim seqName seqSize output => Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Transformer.Type.MkAbsPos (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor ('Torch.GraduallyTyped.RequiresGradient.Gradient 'Torch.GraduallyTyped.RequiresGradient.WithoutGradient) ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device ('Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64) ('Torch.GraduallyTyped.Shape.Type.Shape '[ 'Torch.GraduallyTyped.Shape.Type.Dim ('Torch.GraduallyTyped.Shape.Type.Name "*") seqSize])) generatorDevice
instance Data.Singletons.SingKind Torch.GraduallyTyped.NN.Transformer.Type.TransformerHead
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.WithoutHead
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.WithLMHead
instance Data.Singletons.SingKind Torch.GraduallyTyped.NN.Transformer.Type.TransformerStyle
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.T5
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.ByT5
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.BART
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.MBART
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.Pegasus
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.BERT
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.RoBERTa
instance Data.Singletons.SingI 'Torch.GraduallyTyped.NN.Transformer.Type.GPT2
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.Type.TransformerStyle
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.Type.TransformerStyle

module Torch.GraduallyTyped.NN.Training

-- | Train the model for one epoch.
train :: forall m model input generatorDevice lossGradient lossLayout lossDataType lossDevice lossShape generatorOutputDevice. (MonadIO m, HasStateDict model, HasForward model input generatorDevice (Tensor lossGradient lossLayout lossDataType lossDevice lossShape) generatorOutputDevice, HasForward model input generatorOutputDevice (Tensor lossGradient lossLayout lossDataType lossDevice lossShape) generatorOutputDevice, SGetGeneratorDevice generatorDevice, SGetGeneratorDevice generatorOutputDevice, SGetGradient lossGradient, SGetShape lossShape, Catch (lossShape <+> 'Shape '[]), Catch (lossGradient <+> 'Gradient 'WithGradient)) => Optimizer model -> ModelSpec model -> ListT m input -> Generator generatorDevice -> m (Either (Generator generatorDevice) (Tensor ('Gradient 'WithoutGradient) lossLayout lossDataType lossDevice ('Shape '[]), Generator generatorOutputDevice))

-- | Evaluate the model on the given examples.
eval :: (MonadIO m, HasStateDict model, HasForward model input generatorDevice (Tensor lossGradient lossLayout lossDataType lossDevice lossShape) generatorOutputDevice, HasForward model input generatorOutputDevice (Tensor lossGradient lossLayout lossDataType lossDevice lossShape) generatorOutputDevice, SGetGradient lossGradient, SGetShape lossShape, Catch (lossShape <+> 'Shape '[]), Catch (lossGradient <+> 'Gradient 'WithoutGradient)) => model -> ListT m input -> Generator generatorDevice -> m (Either (Generator generatorDevice) (Tensor ('Gradient 'WithoutGradient) lossLayout lossDataType lossDevice ('Shape '[]), Generator generatorOutputDevice))

module Torch.GraduallyTyped.NN.Initialization

-- | Note: Identity = linear w/o activation
data ForNonLinearity
ForIdentity :: ForNonLinearity
ForSigmoid :: ForNonLinearity
ForTanh :: ForNonLinearity
ForRelu :: ForNonLinearity
ForLeakyRelu :: Float -> ForNonLinearity
data FanMode
FanIn :: FanMode
FanOut :: FanMode
errorPrefix :: String

-- | Gain scaling value for He initialization
calculateGain :: ForNonLinearity -> Float

-- | Fan-in / Fan-out scaling calculation
calculateFan :: [Dim String Integer] -> (Integer, Integer)

-- | Xavier uniform initialization
sXavierUniform :: forall gradient layout device dataType shape gain generatorDevice m. (Num gain, Floating gain, Scalar gain, MonadThrow m, SGetGeneratorDevice generatorDevice) => TensorSpec gradient layout device dataType shape -> gain -> Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))

-- | Xavier normal initialization
sXavierNormal :: forall gradient layout device dataType shape gain generatorDevice m. (Num gain, Floating gain, Scalar gain, MonadThrow m, SGetGeneratorDevice generatorDevice) => TensorSpec gradient layout device dataType shape -> gain -> Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))

-- | Get fan in or fan out value depending on selected fan mode, used by
--   Kaiming
getter :: forall a. FanMode -> (a, a) -> a

-- | Kaiming uniform initialization
sKaimingUniform :: forall gradient layout device dataType shape generatorDevice m. (MonadThrow m, SGetGeneratorDevice generatorDevice) => TensorSpec gradient layout device dataType shape -> FanMode -> ForNonLinearity -> Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))

-- | Kaiming normal initialization
sKaimingNormal :: forall gradient layout device dataType shape generatorDevice m. (MonadThrow m, SGetGeneratorDevice generatorDevice) => TensorSpec gradient layout device dataType shape -> FanMode -> ForNonLinearity -> Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Show.Show Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Initialization.ForNonLinearity
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Initialization.FanMode
instance GHC.Show.Show Torch.GraduallyTyped.NN.Initialization.FanMode
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Initialization.FanMode
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Initialization.FanMode

module Torch.GraduallyTyped.NN.Functional.Sparse
type EmbedDimsErrorMessage (embedDims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply the embedding." % "The embedding weight tensor must have exactly two dimensions," % "but the following dimensions were found:" % "" % "    " <> embedDims <> "." % ""
type family EmbeddingF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
embedding :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. (SGetLayout layout, Catch (dataType' <+> 'DataType 'Int64)) => Maybe Natural -> Bool -> Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') dataType (EmbeddingF shape shape')

module Torch.GraduallyTyped.NN.Sparse
data Embedding (gradient :: Gradient RequiresGradient) (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (embedNumDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (paddingIdx :: Maybe Nat)
[Embedding] :: forall gradient layout device dataType embedNumDim embedDim paddingIdx. Tensor gradient layout device dataType ('Shape '[embedNumDim, embedDim]) -> Embedding gradient layout device dataType embedNumDim embedDim paddingIdx
data EmbeddingSpec (gradient :: Gradient RequiresGradient) (layout :: Layout LayoutType) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (embedNumDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (paddingIdx :: Maybe Nat)
[EmbeddingSpec] :: forall gradient layout device dataType embedNumDim embedDim paddingIdx. SGradient gradient -> SLayout layout -> SDevice device -> SDataType dataType -> SDim embedNumDim -> SDim embedDim -> SMaybe paddingIdx -> EmbeddingSpec gradient layout device dataType embedNumDim embedDim paddingIdx
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim paddingIdx)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim paddingIdx)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Sparse.EmbeddingSpec gradient layout device dataType embedNumDim embedDim paddingIdx)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Sparse.EmbeddingSpec gradient layout device dataType embedNumDim embedDim paddingIdx)
instance (output GHC.Types.~ Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout (device Torch.GraduallyTyped.Unify.<+> generatorDevice) dataType embedNumDim embedDim paddingIdx, generatorOutputDevice GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> generatorDevice), Torch.GraduallyTyped.Random.SGetGeneratorDevice generatorDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim paddingIdx) generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim paddingIdx)
instance (Torch.GraduallyTyped.Tensor.Type.SGetLayout layout, Torch.GraduallyTyped.Prelude.Catch (dataType' Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') (layout Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') dataType (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[embedNumDim, embedDim]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim 'GHC.Maybe.Nothing) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice output generatorDevice
instance (Torch.GraduallyTyped.Tensor.Type.SGetLayout layout, GHC.TypeNats.KnownNat paddingIdx, Torch.GraduallyTyped.Prelude.Catch (dataType' Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') (layout Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') dataType (Torch.GraduallyTyped.NN.Functional.Sparse.EmbeddingF ('Torch.GraduallyTyped.Shape.Type.Shape '[embedNumDim, embedDim]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Sparse.Embedding gradient layout device dataType embedNumDim embedDim ('GHC.Maybe.Just paddingIdx)) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice output generatorDevice

module Torch.GraduallyTyped.NN.Functional.Normalization
type family LayerNormImplF (reverseNormalizedDims :: [Dim (Name Symbol) (Size Nat)]) (reverseInputDims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type LayerNormShapeErrorMessage = "Cannot apply the layer norm. " % "The normalized shape exceeds the input shape."
type family LayerNormWithBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (biasShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
layerNormWithBias :: forall gradient gradient' gradient'' layout layout' layout'' device device' device'' dataType dataType' dataType'' shape shape' shape''. SGetShape shape => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Double -> Tensor gradient'' layout'' device'' dataType'' shape'' -> Tensor (gradient' <|> (gradient' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) (LayerNormWithBiasF shape shape' shape'')
type family LayerNormWithoutBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family LayerNormWithoutBiasSelectDimsF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: SelectDims [By Symbol Nat]
type family LayerNormWithoutBiasBysF (weightDims :: [Dim (Name Symbol) (Size Nat)]) (inputDims :: [Dim (Name Symbol) (Size Nat)]) (inputDimsLength :: Nat) (counter :: Nat) :: [By Symbol Nat]

-- | T5-style layer norm
layerNormWithoutBias :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. (SGetShape shape, SGetShape shape') => Tensor gradient layout device dataType shape -> Double -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (LayerNormWithoutBiasF shape shape')

module Torch.GraduallyTyped.NN.Normalization
data LayerNorm (hasBias :: HasBias) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (normalizedShape :: Shape [Dim (Name Symbol) (Size Nat)])
[LayerNormWithBias] :: forall gradient device dataType normalizedShape. Tensor gradient ('Layout 'Dense) device dataType normalizedShape -> Tensor gradient ('Layout 'Dense) device dataType normalizedShape -> Double -> LayerNorm 'WithBias gradient device dataType normalizedShape
[LayerNormWithoutBias] :: forall gradient device dataType normalizedShape. Tensor gradient ('Layout 'Dense) device dataType normalizedShape -> Double -> LayerNorm 'WithoutBias gradient device dataType normalizedShape
data LayerNormSpec (hasBias :: HasBias) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (normalizedShape :: Shape [Dim (Name Symbol) (Size Nat)])
[LayerNormSpec] :: forall hasBias gradient device dataType normalizedShape. SHasBias hasBias -> SGradient gradient -> SDevice device -> SDataType dataType -> SShape normalizedShape -> Double -> LayerNormSpec hasBias gradient device dataType normalizedShape
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Normalization.LayerNormSpec hasBias gradient device dataType normalizedShape)
instance GHC.Show.Show (Torch.GraduallyTyped.NN.Normalization.LayerNormSpec hasBias gradient device dataType normalizedShape)
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Normalization.LayerNorm hasBias gradient device dataType normalizedShape) generatorDevice (Torch.GraduallyTyped.NN.Normalization.LayerNorm hasBias gradient device dataType normalizedShape) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Normalization.LayerNorm hasBias gradient device dataType normalizedShape)
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape normalizedShape, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithBiasF normalizedShape normalizedShape shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice output generatorDevice
instance (Torch.GraduallyTyped.Tensor.Type.SGetShape normalizedShape, Torch.GraduallyTyped.Tensor.Type.SGetShape shape', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Normalization.LayerNormWithoutBiasF normalizedShape shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Normalization.LayerNorm 'Torch.GraduallyTyped.NN.Type.WithoutBias gradient device dataType normalizedShape) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice output generatorDevice

module Torch.GraduallyTyped.NN.Functional.Loss

-- | Compute the mean squared error between two tensors.
mseLoss :: forall m gradient layout device dataType shape gradient' layout' device' dataType' shape'. (MonadThrow m, Catch (shape <+> shape')) => Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> m (Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') ('Shape '[]))

module Torch.GraduallyTyped.NN.Loss
data MSELoss
MSELoss :: MSELoss
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Loss.MSELoss
instance GHC.Show.Show Torch.GraduallyTyped.NN.Loss.MSELoss
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Loss.MSELoss
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Loss.MSELoss
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Loss.MSELoss generatorDevice Torch.GraduallyTyped.NN.Loss.MSELoss generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Loss.MSELoss
instance (Torch.GraduallyTyped.Prelude.Catch (predShape Torch.GraduallyTyped.Unify.<+> targetShape), output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (predGradient Torch.GraduallyTyped.Unify.<|> targetGradient) (predLayout Torch.GraduallyTyped.Unify.<+> targetLayout) (predDevice Torch.GraduallyTyped.Unify.<+> targetDevice) (predDataType Torch.GraduallyTyped.Unify.<+> targetDataType) ('Torch.GraduallyTyped.Shape.Type.Shape '[])) => Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Loss.MSELoss (Torch.GraduallyTyped.Tensor.Type.Tensor predGradient predLayout predDevice predDataType predShape, Torch.GraduallyTyped.Tensor.Type.Tensor targetGradient targetLayout targetDevice targetDataType targetShape) generatorDevice output generatorDevice

module Torch.GraduallyTyped.NN.Functional.Linear

-- | Compute the output shape of a linear transformation.
--   
--   <pre>
--   &gt;&gt;&gt; type InputDim = 'Dim ('Name "input") ('Size 5)
--   
--   &gt;&gt;&gt; type OutputDim = 'Dim ('Name "output") ('Size 10)
--   
--   &gt;&gt;&gt; type BatchDim = 'Dim ('Name "batch") ('Size 20)
--   
--   &gt;&gt;&gt; type WeightShape = 'Shape '[OutputDim, InputDim]
--   
--   &gt;&gt;&gt; type BiasShape = 'Shape '[OutputDim]
--   
--   &gt;&gt;&gt; type InputShape = 'Shape '[BatchDim, InputDim]
--   
--   &gt;&gt;&gt; :kind! LinearWithBiasF WeightShape BiasShape InputShape
--   LinearWithBiasF WeightShape BiasShape InputShape :: Shape
--                                                         [Dim (Name Symbol) (Size Natural)]
--   = 'Shape
--       '[ 'Dim ('Name "batch") ('Size 20),
--          'Dim ('Name "output") ('Size 10)]
--   </pre>
type family LinearWithBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (biasShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family LinearWithBiasDimsF (weightDims :: [Dim (Name Symbol) (Size Nat)]) (biasDims :: [Dim (Name Symbol) (Size Nat)]) (reversedInputDims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
type LinearInputDimsErrorMessage = "Cannot apply the linear transformation." % "The input tensor does not have the minimum required number of dimensions." % "At least one dimension is needed, but none were found."
type LinearBiasDimsErrorMessage (biasDims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply the linear transformation." % "The bias tensor must have exactly one dimension," % "but the following dimensions were found:" % "" % "    " <> biasDims <> "." % ""
type LinearWeightDimsErrorMessage (weightDims :: [Dim (Name Symbol) (Size Nat)]) = "Cannot apply the linear transformation." % "The weight tensor must have exactly two dimensions," % "but the following dimensions were found:" % "" % "    " <> weightDims <> "." % ""

-- | Applies a linear transformation to the incoming data: &lt;math&gt;
--   
--   Supported shapes:
--   
--   <ul>
--   <li><tt>input</tt>: &lt;math&gt;, where &lt;math&gt; is the batch
--   size, &lt;math&gt; means any number of additional dimensions and
--   &lt;math&gt; are the input features.</li>
--   <li><tt>weight</tt>: &lt;math&gt;</li>
--   <li><tt>bias</tt>: &lt;math&gt;</li>
--   <li><tt>output</tt>: &lt;math&gt;</li>
--   </ul>
--   
--   Examples:
--   
--   <pre>
--   &gt;&gt;&gt; inputDim = SName @"input" :&amp;: SSize @5
--   
--   &gt;&gt;&gt; outputDim = SName @"output" :&amp;: SSize @10
--   
--   &gt;&gt;&gt; batchDim = SName @"batch" :&amp;: SSize @20
--   
--   &gt;&gt;&gt; weightShape = SShape $ outputDim :|: inputDim :|: SNil
--   
--   &gt;&gt;&gt; biasShape = SShape $ outputDim :|: SNil
--   
--   &gt;&gt;&gt; inputShape = SShape $ batchDim :|: inputDim :|: SNil
--   
--   &gt;&gt;&gt; g &lt;- sMkGenerator (SDevice SCPU) 0
--   
--   &gt;&gt;&gt; sRandn' = sRandn . TensorSpec (SGradient SWithoutGradient) (SLayout SDense) (SDevice SCPU) (SDataType SFloat)
--   
--   &gt;&gt;&gt; (weight, g') &lt;- sRandn' weightShape g
--   [W TensorImpl.h:1463] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())
--   
--   &gt;&gt;&gt; (bias, g'') &lt;- sRandn' biasShape g'
--   
--   &gt;&gt;&gt; (input, _) &lt;- sRandn' inputShape g''
--   
--   &gt;&gt;&gt; result = linearWithBias weight bias input
--   
--   &gt;&gt;&gt; :type result
--   result
--     :: Tensor
--          ('Gradient 'WithoutGradient)
--          ('Layout 'Dense)
--          ('Device 'CPU)
--          ('DataType 'Float)
--          ('Shape
--             '[ 'Dim ('Name "batch") ('Size 20),
--                'Dim ('Name "output") ('Size 10)])
--   </pre>
linearWithBias :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape' gradient'' layout'' device'' dataType'' shape''. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor gradient'' layout'' device'' dataType'' shape'' -> Tensor (gradient' <|> (gradient'' <|> gradient'')) (layout <+> (layout' <+> layout'')) (device <+> (device' <+> device'')) (dataType <+> (dataType' <+> dataType'')) (LinearWithBiasF shape shape' shape'')
type family LinearWithoutBiasF (weightShape :: Shape [Dim (Name Symbol) (Size Nat)]) (inputShape :: Shape [Dim (Name Symbol) (Size Nat)]) :: Shape [Dim (Name Symbol) (Size Nat)]
type family LinearWithoutBiasDimsF (weightDims :: [Dim (Name Symbol) (Size Nat)]) (reversedInputDims :: [Dim (Name Symbol) (Size Nat)]) :: [Dim (Name Symbol) (Size Nat)]
linearWithoutBias :: forall gradient layout device dataType shape gradient' layout' device' dataType' shape'. Tensor gradient layout device dataType shape -> Tensor gradient' layout' device' dataType' shape' -> Tensor (gradient <|> gradient') (layout <+> layout') (device <+> device') (dataType <+> dataType') (LinearWithoutBiasF shape shape')
testLinearWithoutBias :: Tensor ('Gradient 'WithGradient) ('Layout 'Dense) 'UncheckedDevice ('DataType 'Float) ('Shape '[ 'Dim ('Name "output") ('Size 2)])

module Torch.GraduallyTyped.NN.Linear

-- | Generic linear model with weight and optional bias.
data GLinear (weight :: Type) (bias :: Type)
[GLinear] :: forall weight bias. weight -> bias -> GLinear weight bias
type family GLinearF (hasBias :: HasBias) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputDim :: Dim (Name Symbol) (Size Nat)) (outputDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family LinearWeightF (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputDim :: Dim (Name Symbol) (Size Nat)) (outputDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family LinearBiasF (hasBias :: HasBias) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (outputDim :: Dim (Name Symbol) (Size Nat)) :: Type
linearSpec :: forall hasBias gradient device dataType inputDim outputDim. SHasBias hasBias -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim inputDim -> SDim outputDim -> ModelSpec (GLinearF hasBias gradient device dataType inputDim outputDim)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Linear.GLinear weight bias)
instance (GHC.Show.Show weight, GHC.Show.Show bias) => GHC.Show.Show (Torch.GraduallyTyped.NN.Linear.GLinear weight bias)
instance (GHC.Classes.Ord weight, GHC.Classes.Ord bias) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Linear.GLinear weight bias)
instance (GHC.Classes.Eq weight, GHC.Classes.Eq bias) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Linear.GLinear weight bias)
instance (output GHC.Types.~ Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) (device Torch.GraduallyTyped.Unify.<+> generatorDevice) dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim])) (), generatorOutputDevice GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> generatorDevice), Torch.GraduallyTyped.Random.SGetGeneratorDevice generatorDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim])) ()) generatorDevice output generatorOutputDevice
instance (output GHC.Types.~ Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) (device Torch.GraduallyTyped.Unify.<+> generatorDevice) dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim])) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) (device Torch.GraduallyTyped.Unify.<+> generatorDevice) dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim])), generatorOutputDevice GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> generatorDevice), Torch.GraduallyTyped.Random.SGetGeneratorDevice generatorDevice, Torch.GraduallyTyped.Random.SGetGeneratorDevice generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim])) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim]))) generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Linear.GLinear weight bias) generatorDevice (Torch.GraduallyTyped.NN.Linear.GLinear weight bias) generatorDevice => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.NN.Class.NamedModel weight) (Torch.GraduallyTyped.NN.Class.NamedModel bias)) generatorDevice (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.NN.Class.NamedModel weight) (Torch.GraduallyTyped.NN.Class.NamedModel bias)) generatorDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict weight, Torch.GraduallyTyped.NN.Class.HasStateDict bias) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Linear.GLinear weight bias)
instance (output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Linear.LinearWithoutBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim])) ()) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice output generatorDevice
instance (output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> gradient') ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense Torch.GraduallyTyped.Unify.<+> layout') (device Torch.GraduallyTyped.Unify.<+> device') (dataType Torch.GraduallyTyped.Unify.<+> dataType') (Torch.GraduallyTyped.NN.Functional.Linear.LinearWithBiasF ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim]) ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim]) shape')) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim, inputDim])) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient ('Torch.GraduallyTyped.Layout.Layout 'Torch.GraduallyTyped.Layout.Dense) device dataType ('Torch.GraduallyTyped.Shape.Type.Shape '[outputDim]))) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice output generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Linear.GLinear weight bias) input generatorDevice output generatorDevice => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Linear.GLinear (Torch.GraduallyTyped.NN.Class.NamedModel weight) (Torch.GraduallyTyped.NN.Class.NamedModel bias)) input generatorDevice output generatorDevice

module Torch.GraduallyTyped.NN.Transformer.GPooler
data GPooler (dense :: Type) (activation :: Type)
[GPooler] :: forall dense activation. dense -> activation -> GPooler dense activation
poolerSpec :: forall style gradient device dataType inputEmbedDim. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim inputEmbedDim -> GPooler (PoolerDenseF style gradient device dataType inputEmbedDim) (PoolerActivationF style)
type family PoolerDenseF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type
type family PoolerActivationF (style :: TransformerStyle) :: Type
instance (Torch.GraduallyTyped.NN.Class.HasForward dense (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward activation tensor0 generatorDevice0 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GPooler.GPooler dense activation) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GLMHead

-- | A data type that represents whether or not the language modelling head
--   has a scaled decoder output.
data LMHeadHasScaling
LMHeadWithScaling :: LMHeadHasScaling
LMHeadWithoutScaling :: LMHeadHasScaling

-- | Generic language modelling head for transformer encoders and decoders.
--   
--   <ul>
--   <li><tt>inputEmbedDim</tt> is the dimension of the input
--   embedding.</li>
--   <li><tt>dense</tt> is a dense layer.</li>
--   <li><tt>activation</tt> is an activation function.</li>
--   <li><tt>layerNorm</tt> is a layer normalization layer.</li>
--   <li><tt>decoder</tt> is a decoder layer.</li>
--   <li><tt>bias</tt> is a bias layer.</li>
--   </ul>
data GLMHead (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (dense :: Type) (activation :: Type) (layerNorm :: Type) (decoder :: Type) (bias :: Type)
[GLMHead] :: forall inputEmbedDim dense activation layerNorm decoder bias. SDim inputEmbedDim -> dense -> activation -> layerNorm -> decoder -> bias -> LMHeadHasScaling -> GLMHead inputEmbedDim dense activation layerNorm decoder bias

-- | Generic data type for biasing the language model head.
data GBias (bias :: Type)
[GBias] :: forall bias. bias -> GBias bias
type family GLMHeadF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the dense layer of the language model head.
type family LMHeadDenseF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the activation function of the language model head.
type family LMHeadActivationF (style :: TransformerStyle) :: Type

-- | Specifies the layer normalization layer of the language model head.
type family LMHeadLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the decoder layer of the language model head.
type family LMHeadDecoderF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the bias layer of the language model head.
type family LMHeadBiasF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the parameters of the language model head.
lmHeadSpec :: forall style gradient device dataType inputEmbedDim vocabDim. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim inputEmbedDim -> SDim vocabDim -> Double -> ModelSpec (GLMHeadF style gradient device dataType inputEmbedDim vocabDim)
type family LMHeadOutputF (style :: TransformerStyle) (decoderOutput :: Type) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GLMHead.GLMHead inputEmbedDim dense activation layerNorm decoder bias)
instance (GHC.Show.Show dense, GHC.Show.Show activation, GHC.Show.Show layerNorm, GHC.Show.Show decoder, GHC.Show.Show bias) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GLMHead.GLMHead inputEmbedDim dense activation layerNorm decoder bias)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias bias)
instance GHC.Show.Show bias => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias bias)
instance GHC.Classes.Ord bias => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias bias)
instance GHC.Classes.Eq bias => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias bias)
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias ()) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias ()) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias (Torch.GraduallyTyped.Tensor.Type.Tensor biasGradient biasLayout biasDevice biasDataType biasShape)) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias (Torch.GraduallyTyped.Tensor.Type.Tensor biasGradient biasLayout biasDevice biasDataType biasShape)) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias (Torch.GraduallyTyped.NN.Class.NamedModel (Torch.GraduallyTyped.Tensor.Type.Tensor biasGradient biasLayout biasDevice biasDataType biasShape))) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias (Torch.GraduallyTyped.NN.Class.NamedModel (Torch.GraduallyTyped.Tensor.Type.Tensor biasGradient biasLayout biasDevice biasDataType biasShape))) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict bias => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias bias)
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias ()) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice
instance (shape' GHC.Types.~ Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape biasShape, Torch.GraduallyTyped.Prelude.Catch shape', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> biasGradient) (layout Torch.GraduallyTyped.Unify.<+> biasLayout) (device Torch.GraduallyTyped.Unify.<+> biasDevice) (dataType Torch.GraduallyTyped.Unify.<+> biasDataType) shape') => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias (Torch.GraduallyTyped.Tensor.Type.Tensor biasGradient biasLayout biasDevice biasDataType biasShape)) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice output generatorDevice
instance (shape' GHC.Types.~ Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape biasShape, Torch.GraduallyTyped.Prelude.Catch shape', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (gradient Torch.GraduallyTyped.Unify.<|> biasGradient) (layout Torch.GraduallyTyped.Unify.<+> biasLayout) (device Torch.GraduallyTyped.Unify.<+> biasDevice) (dataType Torch.GraduallyTyped.Unify.<+> biasDataType) shape') => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GLMHead.GBias (Torch.GraduallyTyped.NN.Class.NamedModel (Torch.GraduallyTyped.Tensor.Type.Tensor biasGradient biasLayout biasDevice biasDataType biasShape))) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice output generatorDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize dense generatorDevice dense' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize activation generatorDevice0 activation' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize layerNorm generatorDevice1 layerNorm' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize decoder generatorDevice2 decoder' generatorDevice3, Torch.GraduallyTyped.NN.Class.HasInitialize bias generatorDevice3 bias' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GLMHead.GLMHead inputEmbedDim dense activation layerNorm decoder bias) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GLMHead.GLMHead inputEmbedDim dense' activation' layerNorm' decoder' bias') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict dense, Torch.GraduallyTyped.NN.Class.HasStateDict activation, Torch.GraduallyTyped.NN.Class.HasStateDict layerNorm, Torch.GraduallyTyped.NN.Class.HasStateDict decoder, Torch.GraduallyTyped.NN.Class.HasStateDict bias) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GLMHead.GLMHead inputEmbedDim dense activation layerNorm decoder bias)
instance (Torch.GraduallyTyped.NN.Class.HasForward dense (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward activation tensor0 generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward layerNorm tensor1 generatorDevice1 tensor2 generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward decoder tensor2 generatorDevice2 (Torch.GraduallyTyped.Tensor.Type.Tensor gradient3 layout3 device3 dataType3 shape3) generatorDevice3, Torch.GraduallyTyped.NN.Class.HasForward bias (Torch.GraduallyTyped.Tensor.Type.Tensor gradient3 layout3 device3 dataType3 shape3) generatorDevice3 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GLMHead.GLMHead inputEmbedDim dense activation layerNorm decoder bias) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling generatorDevice Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Transformer.GLMHead.LMHeadHasScaling

module Torch.GraduallyTyped.NN.Functional.Dropout

-- | Dropout randomly zeroes some of the elements of the input tensor with
--   probability <tt>p</tt> using samples from a Bernoulli distribution.
dropout :: forall gradient layout device dataType shape generatorDevice m. (SGetDevice device, SGetGeneratorDevice generatorDevice, MonadThrow m) => Double -> Tensor gradient layout device dataType shape -> Generator generatorDevice -> m (Tensor gradient layout (device <+> generatorDevice) dataType shape, Generator (device <+> generatorDevice))

module Torch.GraduallyTyped.NN.Functional

module Torch.GraduallyTyped.NN.Dropout

-- | Given a random generator, randomly zeroes some of the elements of the
--   input tensor with probability <tt>p</tt> using samples from a
--   Bernoulli distribution. Each channel will be zeroed out independently
--   on every <a>forward</a> call.
newtype Dropout
[Dropout] :: Double -> Dropout
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Dropout.Dropout
instance GHC.Show.Show Torch.GraduallyTyped.NN.Dropout.Dropout
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Dropout.Dropout
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Dropout.Dropout
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Dropout.Dropout generatorDevice Torch.GraduallyTyped.NN.Dropout.Dropout generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Dropout.Dropout
instance (input GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape, output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout (device Torch.GraduallyTyped.Unify.<+> generatorDevice) dataType shape, generatorOutputDevice GHC.Types.~ (device Torch.GraduallyTyped.Unify.<+> generatorDevice), Torch.GraduallyTyped.Tensor.Type.SGetDevice device, Torch.GraduallyTyped.Random.SGetGeneratorDevice generatorDevice) => Torch.GraduallyTyped.NN.Class.HasForward Torch.GraduallyTyped.NN.Dropout.Dropout input generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention

-- | Data type for representing whether or not (and, if so, where) scaling
--   is applied in the multi-headed attention layer.
data MultiHeadAttentionHasScaling

-- | Scaling is not done.
MultiHeadAttentionWithoutScaling :: MultiHeadAttentionHasScaling

-- | Scaling is applied to the query after in the in-projection.
MultiHeadAttentionWithQueryScaling :: MultiHeadAttentionHasScaling

-- | Scaling is applied to the attention weights.
MultiHeadAttentionWithWeightScaling :: MultiHeadAttentionHasScaling

-- | Generic multi-headed attention layer.
--   
--   <ul>
--   <li><tt>headDim</tt> is the dimension of the attention heads.</li>
--   <li><tt>headEmbedDim</tt> is the dimension of the attention head
--   embedding.</li>
--   <li><tt>embedDim</tt> is the dimension of the embedding.</li>
--   <li><tt>qInProj</tt> is the type of the query projection.</li>
--   <li><tt>kInProj</tt> is the type of the key projection.</li>
--   <li><tt>vInProj</tt> is the type of the value projection.</li>
--   <li><tt>outProj</tt> is the type of the output projection.</li>
--   <li><tt>dropout</tt> is the type of the dropout layer.</li>
--   </ul>
data GMultiHeadAttention (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (qInProj :: Type) (kInProj :: Type) (vInProj :: Type) (outProj :: Type) (dropout :: Type)
[GMultiHeadAttention] :: forall headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout. SDim headDim -> SDim headEmbedDim -> SDim embedDim -> qInProj -> kInProj -> vInProj -> outProj -> dropout -> MultiHeadAttentionHasScaling -> GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout
type family GMultiHeadAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (valueEmbedDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the linear transformation of the query.
type family QInProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the linear transformation of the key.
type family KInProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the linear transformation of the value.
type family VInProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (valueEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the type of the out-projection layer.
type family OutProjF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the type of the dropout layer.
type family DropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a multi-headed attention layer.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the attention layer, e.g. <a>ST5</a>,
--   <a>ByT5</a>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the
--   attention layer.</li>
--   <li><tt>device</tt>: the computational device on which to allocate the
--   attention layer.</li>
--   <li><tt>dataType</tt>: the data type of the attention layer.</li>
--   <li><tt>headDim</tt>: the dimension of the attention heads.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the attention head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the input embeddings.</li>
--   <li><tt>queryEmbedDim</tt>: the dimension of the query
--   embeddings.</li>
--   <li><tt>keyEmbedDim</tt>: the dimension of the key embeddings.</li>
--   <li><tt>valueEmbedDim</tt>: the dimension of the value
--   embeddings.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   </ul>
multiHeadAttentionSpec :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim hasDropout. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SDim keyEmbedDim -> SDim valueEmbedDim -> SHasDropout hasDropout -> Double -> ModelSpec (GMultiHeadAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim valueEmbedDim hasDropout)
type BatchDim queryShape keyShape valueShape = (queryShape ! 0) <+> (keyShape ! 0) <+> (valueShape ! 0)
getBatchDim :: forall m queryShape keyShape valueShape batchDim. (MonadThrow m, batchDim ~ BatchDim queryShape keyShape valueShape) => SShape queryShape -> SShape keyShape -> SShape valueShape -> m (SDim batchDim)
type QuerySeqDim queryShape = queryShape ! 1
getQuerySeqDim :: forall m queryShape querySeqDim. (MonadThrow m, querySeqDim ~ QuerySeqDim queryShape) => SShape queryShape -> m (SDim querySeqDim)
type KeySeqDim keyShape valueShape = (keyShape ! 1) <+> (valueShape ! 1)
getKeySeqDim :: forall m keyShape valueShape keySeqDim. (MonadThrow m, keySeqDim ~ KeySeqDim keyShape valueShape) => SShape keyShape -> SShape valueShape -> m (SDim keySeqDim)
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout)
instance (GHC.Show.Show qInProj, GHC.Show.Show kInProj, GHC.Show.Show vInProj, GHC.Show.Show outProj, GHC.Show.Show dropout) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout)
instance (Torch.GraduallyTyped.NN.Class.HasForward qInProj (Torch.GraduallyTyped.Tensor.Type.Tensor queryRequiresGradient queryLayout queryDevice queryDataType queryShape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor qRequiresGradient qLayout qDevice qDataType qShape0) qGeneratorOutputDevice, reshapedQShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF qShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, querySeqDim, headDim, headEmbedDim]), Torch.GraduallyTyped.Prelude.Catch reshapedQShape0, qShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) reshapedQShape0, Torch.GraduallyTyped.Prelude.Catch qShape, Torch.GraduallyTyped.NN.Class.HasForward kInProj (Torch.GraduallyTyped.Tensor.Type.Tensor keyRequiresGradient keyLayout keyDevice keyDataType keyShape) qGeneratorOutputDevice (Torch.GraduallyTyped.Tensor.Type.Tensor qRequiresGradient kLayout kDevice kDataType kShape0) kGeneratorOutputDevice, reshapedKShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF kShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, keySeqDim, headDim, headEmbedDim]), Torch.GraduallyTyped.Prelude.Catch reshapedKShape0, transposedReshapedKShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) reshapedKShape0, Torch.GraduallyTyped.Prelude.Catch transposedReshapedKShape0, doubleTransposedReshapedKShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) transposedReshapedKShape0, Torch.GraduallyTyped.Prelude.Catch doubleTransposedReshapedKShape0, multipliedQDoubleTransposedReshapedKShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.MathOperations.BlasLapack.MatmulF qShape doubleTransposedReshapedKShape0, Torch.GraduallyTyped.Prelude.Catch multipliedQDoubleTransposedReshapedKShape0, weightsShape0 GHC.Types.~ Torch.GraduallyTyped.NN.Functional.NonLinearActivation.SoftmaxF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF multipliedQDoubleTransposedReshapedKShape0 attentionBiasShape), Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF multipliedQDoubleTransposedReshapedKShape0 attentionBiasShape), Torch.GraduallyTyped.Prelude.Catch weightsShape0, Torch.GraduallyTyped.NN.Class.HasForward dropout (Torch.GraduallyTyped.Tensor.Type.Tensor (qRequiresGradient Torch.GraduallyTyped.Unify.<|> attentionBiasRequiresGradient) (qLayout Torch.GraduallyTyped.Unify.<+> (kLayout Torch.GraduallyTyped.Unify.<+> attentionBiasLayout)) (qDevice Torch.GraduallyTyped.Unify.<+> (kDevice Torch.GraduallyTyped.Unify.<+> attentionBiasDevice)) (qDataType Torch.GraduallyTyped.Unify.<+> (kDataType Torch.GraduallyTyped.Unify.<+> attentionBiasDataType)) weightsShape0) kGeneratorOutputDevice (Torch.GraduallyTyped.Tensor.Type.Tensor weightsRequiresGradient weightsLayout weightsDevice weightsDataType weightsShape) weightsGeneratorOutputDevice, Torch.GraduallyTyped.NN.Class.HasForward vInProj (Torch.GraduallyTyped.Tensor.Type.Tensor valueRequiresGradient valueLayout valueDevice valueDataType valueShape) weightsGeneratorOutputDevice (Torch.GraduallyTyped.Tensor.Type.Tensor weightsRequiresGradient vLayout vDevice vDataType vShape0) vGeneratorOutputDevice, reshapedVShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF vShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, keySeqDim, headDim, headEmbedDim]), Torch.GraduallyTyped.Prelude.Catch reshapedVShape0, transposedReshapedVShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) reshapedVShape0, Torch.GraduallyTyped.Prelude.Catch transposedReshapedVShape, multipliedWeightsTransposedReshapedVShape GHC.Types.~ Torch.GraduallyTyped.Tensor.MathOperations.BlasLapack.MatmulF weightsShape transposedReshapedVShape, Torch.GraduallyTyped.Prelude.Catch multipliedWeightsTransposedReshapedVShape, outputQueryShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) multipliedWeightsTransposedReshapedVShape, Torch.GraduallyTyped.Prelude.Catch outputQueryShape0, Torch.GraduallyTyped.NN.Class.HasForward outProj (Torch.GraduallyTyped.Tensor.Type.Tensor weightsRequiresGradient (weightsLayout Torch.GraduallyTyped.Unify.<+> vLayout) (weightsDevice Torch.GraduallyTyped.Unify.<+> vDevice) (weightsDataType Torch.GraduallyTyped.Unify.<+> vDataType) reshapedOutputQueryShape0) vGeneratorOutputDevice output generatorOutputDevice, reshapedOutputQueryShape0 GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.ReshapeF outputQueryShape0 ('Torch.GraduallyTyped.Shape.Type.Shape '[batchDim, querySeqDim, embedDim]), Torch.GraduallyTyped.Prelude.Catch reshapedOutputQueryShape0, Torch.GraduallyTyped.Tensor.Type.SGetShape queryShape, Torch.GraduallyTyped.Tensor.Type.SGetShape keyShape, Torch.GraduallyTyped.Tensor.Type.SGetShape valueShape, batchDim GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.BatchDim queryShape keyShape valueShape, querySeqDim GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.QuerySeqDim queryShape, keySeqDim GHC.Types.~ Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.KeySeqDim keyShape valueShape) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout) (Torch.GraduallyTyped.Tensor.Type.Tensor queryRequiresGradient queryLayout queryDevice queryDataType queryShape, Torch.GraduallyTyped.Tensor.Type.Tensor keyRequiresGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor valueRequiresGradient valueLayout valueDevice valueDataType valueShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasRequiresGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize qInProj generatorDevice qInProj' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize kInProj generatorDevice0 kInProj' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize vInProj generatorDevice1 vInProj' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize outProj generatorDevice2 outProj' generatorDevice3, Torch.GraduallyTyped.NN.Class.HasInitialize dropout generatorDevice3 dropout' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj' kInProj' vInProj' outProj' dropout') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict qInProj, Torch.GraduallyTyped.NN.Class.HasStateDict vInProj, Torch.GraduallyTyped.NN.Class.HasStateDict kInProj, Torch.GraduallyTyped.NN.Class.HasStateDict outProj, Torch.GraduallyTyped.NN.Class.HasStateDict dropout) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.GMultiHeadAttention headDim headEmbedDim embedDim qInProj kInProj vInProj outProj dropout)
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling generatorDevice Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Transformer.GMultiHeadAttention.MultiHeadAttentionHasScaling

module Torch.GraduallyTyped.NN.Transformer.GSelfAttention

-- | Generic self-attention layer data type.
--   
--   <ul>
--   <li><tt>initialLayerNorm</tt>: the initial layer normalization</li>
--   <li><tt>mha</tt>: the multi-headed attention layer</li>
--   <li><tt>dropout</tt>: the dropout layer</li>
--   <li><tt>finalLayerNorm</tt>: the final layer normalization</li>
--   </ul>
data GSelfAttention (initialLayerNorm :: Type) (mha :: Type) (dropout :: Type) (finalLayerNorm :: Type)
[GSelfAttention] :: forall initialLayerNorm mha dropout finalLayerNorm. initialLayerNorm -> mha -> dropout -> finalLayerNorm -> GSelfAttention initialLayerNorm mha dropout finalLayerNorm
type family GSelfAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the initial layer normalization of the self-attention layer.
type family SAInitialLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the multi-headed attention layer of the self-attention
--   layer.
type family SAMultiheadAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the dropout layer of the self-attention layer.
type family SADropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the final layer normalization of the self-attention layer.
type family SAFinalLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the parameters of a self-attention layer.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer stack, e.g.
--   <a>ST5</a>, <a>SByT5</a>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the stack's
--   parameters.</li>
--   <li><tt>device</tt>: the computational device on which the stack is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the stack's parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   stack.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>queryEmbedDim</tt>: the dimension of the transformer query
--   embeddings.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
selfAttentionSpec :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim hasDropout. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (GSelfAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim hasDropout)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm mha dropout finalLayerNorm)
instance (GHC.Show.Show initialLayerNorm, GHC.Show.Show mha, GHC.Show.Show dropout, GHC.Show.Show finalLayerNorm) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm mha dropout finalLayerNorm)
instance (GHC.Classes.Ord initialLayerNorm, GHC.Classes.Ord mha, GHC.Classes.Ord dropout, GHC.Classes.Ord finalLayerNorm) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm mha dropout finalLayerNorm)
instance (GHC.Classes.Eq initialLayerNorm, GHC.Classes.Eq mha, GHC.Classes.Eq dropout, GHC.Classes.Eq finalLayerNorm) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm mha dropout finalLayerNorm)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize initialLayerNorm generatorDevice initialLayerNorm' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize multiHeadAttention generatorDevice0 multiHeadAttention' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize dropout generatorDevice1 dropout' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize finalLayerNorm generatorDevice2 finalLayerNorm' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm multiHeadAttention dropout finalLayerNorm) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm' multiHeadAttention' dropout' finalLayerNorm') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict initialLayerNorm, Torch.GraduallyTyped.NN.Class.HasStateDict multiHeadAttention, Torch.GraduallyTyped.NN.Class.HasStateDict dropout, Torch.GraduallyTyped.NN.Class.HasStateDict finalLayerNorm) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm multiHeadAttention dropout finalLayerNorm)
instance (Torch.GraduallyTyped.NN.Class.HasForward initialLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward multiHeadAttention (tensor0, tensor0, tensor0, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward dropout tensor1 generatorDevice1 (Torch.GraduallyTyped.Tensor.Type.Tensor gradient2 layout2 device2 dataType2 shape2) generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward finalLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> gradient2) (queryLayout Torch.GraduallyTyped.Unify.<+> layout2) (queryDevice Torch.GraduallyTyped.Unify.<+> device2) (queryDataType Torch.GraduallyTyped.Unify.<+> dataType2) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape shape2)) generatorDevice2 output generatorOutputDevice, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape shape2)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GSelfAttention.GSelfAttention initialLayerNorm multiHeadAttention dropout finalLayerNorm) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork

-- | Generic two-layer gate with activation function.
--   
--   <ul>
--   <li><tt>layer0</tt> is the first layer.</li>
--   <li><tt>activation</tt> is the activation function.</li>
--   <li><tt>layer1</tt> is the second layer.</li>
--   </ul>
data GGate (layer0 :: Type) (activation :: Type) (layer1 :: Type)
[GGate] :: forall layer0 activation layer1. layer0 -> activation -> layer1 -> GGate layer0 activation layer1

-- | Generic transformer feed-forward network.
--   
--   <ul>
--   <li><tt>inputLayerNorm</tt> is the layer normalization for the
--   input.</li>
--   <li><tt>inputTransformation</tt> is the input transformation.</li>
--   <li><tt>activation</tt> is the activation function.</li>
--   <li><tt>activationDropout</tt> is the activation dropout layer.</li>
--   <li><tt>outputProjection</tt> is the output projection.</li>
--   <li><tt>outputDropout</tt> is the dropout layer for the output.</li>
--   <li><tt>outputLayerNorm</tt> is the layer normalization for the
--   output.</li>
--   </ul>
data GTransformerFeedForwardNetwork (inputLayerNorm :: Type) (inputTransformation :: Type) (activation :: Type) (activationDropout :: Type) (outputProjection :: Type) (outputDropout :: Type) (outputLayerNorm :: Type)
[GTransformerFeedForwardNetwork] :: forall inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm. inputLayerNorm -> inputTransformation -> activation -> activationDropout -> outputProjection -> outputDropout -> outputLayerNorm -> GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm
type family GTransformerFeedForwardNetworkF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the layer normalization for the input.
type family FFNInputLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the first input projection.
type family FFNInputTransformationF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the activation.
type family FFNActivationF (style :: TransformerStyle) :: Type

-- | Specifies the activation dropout.
type family FFNActivationDropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the output projection.
type family FFNOutputProjectionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the dropout for the output.
type family FFNOutputDropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the layer normalization for the output.
type family FFNOutputLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the parameters of the transformer feed forward network.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer feed forward network,
--   e.g. <a>ST5</a>, <a>SByT5</a>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the
--   network's parameters.</li>
--   <li><tt>device</tt>: the computational device on which the parameters
--   are allocated.</li>
--   <li><tt>dataType</tt>: the data type of the parameters.</li>
--   <li><tt>queryEmbedDim</tt>: the dimension of the query embedding.</li>
--   <li><tt>ffnDim</tt>: the dimension of the feed forward network's
--   hidden state.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
transformerFeedForwardNetworkSpec :: forall style gradient device dataType queryEmbedDim ffnDim hasDropout. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim queryEmbedDim -> SDim ffnDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (GTransformerFeedForwardNetworkF style gradient device dataType queryEmbedDim ffnDim hasDropout)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1)
instance (GHC.Show.Show layer0, GHC.Show.Show activation, GHC.Show.Show layer1) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1)
instance (GHC.Classes.Ord layer0, GHC.Classes.Ord activation, GHC.Classes.Ord layer1) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1)
instance (GHC.Classes.Eq layer0, GHC.Classes.Eq activation, GHC.Classes.Eq layer1) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm)
instance (GHC.Show.Show inputLayerNorm, GHC.Show.Show inputTransformation, GHC.Show.Show activation, GHC.Show.Show activationDropout, GHC.Show.Show outputProjection, GHC.Show.Show outputDropout, GHC.Show.Show outputLayerNorm) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm)
instance (GHC.Classes.Ord inputLayerNorm, GHC.Classes.Ord inputTransformation, GHC.Classes.Ord activation, GHC.Classes.Ord activationDropout, GHC.Classes.Ord outputProjection, GHC.Classes.Ord outputDropout, GHC.Classes.Ord outputLayerNorm) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm)
instance (GHC.Classes.Eq inputLayerNorm, GHC.Classes.Eq inputTransformation, GHC.Classes.Eq activation, GHC.Classes.Eq activationDropout, GHC.Classes.Eq outputProjection, GHC.Classes.Eq outputDropout, GHC.Classes.Eq outputLayerNorm) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize inputLayerNorm generatorDevice inputLayerNorm' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize inputTransformation generatorDevice0 inputTransformation' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize activation generatorDevice1 activation' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize activationDropout generatorDevice2 activationDropout' generatorDevice3, Torch.GraduallyTyped.NN.Class.HasInitialize outputProjection generatorDevice3 outputProjection' generatorDevice4, Torch.GraduallyTyped.NN.Class.HasInitialize outputDropout generatorDevice4 outputDropout' generatorDevice5, Torch.GraduallyTyped.NN.Class.HasInitialize outputLayerNorm generatorDevice5 outputLayerNorm' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm' inputTransformation' activation' activationDropout' outputProjection' outputDropout' outputLayerNorm') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict inputLayerNorm, Torch.GraduallyTyped.NN.Class.HasStateDict inputTransformation, Torch.GraduallyTyped.NN.Class.HasStateDict activation, Torch.GraduallyTyped.NN.Class.HasStateDict activationDropout, Torch.GraduallyTyped.NN.Class.HasStateDict outputProjection, Torch.GraduallyTyped.NN.Class.HasStateDict outputDropout, Torch.GraduallyTyped.NN.Class.HasStateDict outputLayerNorm) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm)
instance (Torch.GraduallyTyped.NN.Class.HasForward inputLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward inputTransformation tensor0 generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward activation tensor1 generatorDevice1 tensor2 generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward activationDropout tensor2 generatorDevice2 tensor3 generatorDevice3, Torch.GraduallyTyped.NN.Class.HasForward outputProjection tensor3 generatorDevice3 tensor4 generatorDevice4, Torch.GraduallyTyped.NN.Class.HasForward outputDropout tensor4 generatorDevice4 (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient5 queryLayout5 queryDevice5 queryDataType5 queryShape5) generatorDevice5, Torch.GraduallyTyped.NN.Class.HasForward outputLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> queryGradient5) (queryLayout Torch.GraduallyTyped.Unify.<+> queryLayout5) (queryDevice Torch.GraduallyTyped.Unify.<+> queryDevice5) (queryDataType Torch.GraduallyTyped.Unify.<+> queryDataType5) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape queryShape5)) generatorDevice5 output generatorOutputDevice, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape queryShape5)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GTransformerFeedForwardNetwork inputLayerNorm inputTransformation activation activationDropout outputProjection outputDropout outputLayerNorm) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize layer0 generatorDevice layer0' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize activation generatorDevice0 activation' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize layer1 generatorDevice1 layer1' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0' activation' layer1') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict layer0, Torch.GraduallyTyped.NN.Class.HasStateDict activation, Torch.GraduallyTyped.NN.Class.HasStateDict layer1) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1)
instance (Torch.GraduallyTyped.NN.Class.HasForward layer0 (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice', Torch.GraduallyTyped.NN.Class.HasForward activation (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice' (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice', Torch.GraduallyTyped.NN.Class.HasForward layer1 (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice' (Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape') generatorDevice'', output GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape', generatorOutputDevice GHC.Types.~ generatorDevice'') => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GFeedForwardNetwork.GGate layer0 activation layer1) (Torch.GraduallyTyped.Tensor.Type.Tensor gradient layout device dataType shape) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GCrossAttention

-- | Generic cross-attention layer data type.
--   
--   <ul>
--   <li><tt>initialLayerNorm</tt>: the initial layer normalization</li>
--   <li><tt>mha</tt>: the multi-headed attention layer</li>
--   <li><tt>dropout</tt>: the dropout layer</li>
--   <li><tt>finalLayerNorm</tt>: the final layer normalization</li>
--   </ul>
data GCrossAttention (initialLayerNorm :: Type) (mha :: Type) (dropout :: Type) (finalLayerNorm :: Type)
[GCrossAttention] :: forall initialLayerNorm mha dropout finalLayerNorm. initialLayerNorm -> mha -> dropout -> finalLayerNorm -> GCrossAttention initialLayerNorm mha dropout finalLayerNorm
type family GCrossAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the initial layer normalization of the cross-attention
--   layer.
type family CAInitialLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the multi-headed attention layer specialized for
--   cross-attention.
type family CAMultiheadAttentionF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the dropout layer of the cross-attention layer.
type family CADropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the final layer normalization of the cross-attention layer.
type family CAFinalLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the parameters of a cross-attention layer.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer stack, e.g.
--   <a>ST5</a>, <a>SByT5</a>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the stack's
--   parameters.</li>
--   <li><tt>device</tt>: the computational device on which the stack is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the stack's parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   stack.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>queryEmbedDim</tt>: the dimension of the transformer query
--   embeddings.</li>
--   <li><tt>keyEmbedDim</tt>: the dimension of the transformer key
--   embeddings.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
crossAttentionSpec :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim hasDropout. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SDim keyEmbedDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (GCrossAttentionF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim hasDropout)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm mha dropout finalLayerNorm)
instance (GHC.Show.Show initialLayerNorm, GHC.Show.Show mha, GHC.Show.Show dropout, GHC.Show.Show finalLayerNorm) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm mha dropout finalLayerNorm)
instance (GHC.Classes.Ord initialLayerNorm, GHC.Classes.Ord mha, GHC.Classes.Ord dropout, GHC.Classes.Ord finalLayerNorm) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm mha dropout finalLayerNorm)
instance (GHC.Classes.Eq initialLayerNorm, GHC.Classes.Eq mha, GHC.Classes.Eq dropout, GHC.Classes.Eq finalLayerNorm) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm mha dropout finalLayerNorm)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize initialLayerNorm generatorDevice initialLayerNorm' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize multiHeadAttention generatorDevice0 multiHeadAttention' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize dropout generatorDevice1 dropout' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize finalLayerNorm generatorDevice2 finalLayerNorm' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm multiHeadAttention dropout finalLayerNorm) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm' multiHeadAttention' dropout' finalLayerNorm') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict initialLayerNorm, Torch.GraduallyTyped.NN.Class.HasStateDict multiHeadAttention, Torch.GraduallyTyped.NN.Class.HasStateDict dropout, Torch.GraduallyTyped.NN.Class.HasStateDict finalLayerNorm) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm multiHeadAttention dropout finalLayerNorm)
instance (Torch.GraduallyTyped.NN.Class.HasForward initialLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward multiHeadAttention (tensor0, Torch.GraduallyTyped.Tensor.Type.Tensor keyGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor keyGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward dropout tensor1 generatorDevice1 (Torch.GraduallyTyped.Tensor.Type.Tensor gradient2 layout2 device2 dataType2 shape2) generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward finalLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor (queryGradient Torch.GraduallyTyped.Unify.<|> gradient2) (queryLayout Torch.GraduallyTyped.Unify.<+> layout2) (queryDevice Torch.GraduallyTyped.Unify.<+> device2) (queryDataType Torch.GraduallyTyped.Unify.<+> dataType2) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape shape2)) generatorDevice2 output generatorOutputDevice, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF queryShape shape2)) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GCrossAttention.GCrossAttention initialLayerNorm multiHeadAttention dropout finalLayerNorm) (Torch.GraduallyTyped.Tensor.Type.Tensor queryGradient queryLayout queryDevice queryDataType queryShape, Torch.GraduallyTyped.Tensor.Type.Tensor keyGradient keyLayout keyDevice keyDataType keyShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionBiasGradient attentionBiasLayout attentionBiasDevice attentionBiasDataType attentionBiasShape) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GBlock

-- | Generic transformer encoder block consisting of self-attention,
--   cross-attention, and a feed-forward network.
--   
--   <ul>
--   <li><tt>selfAttention</tt> is a self-attention layer.</li>
--   <li><tt>crossAttention</tt> is a cross-attention layer.</li>
--   <li><tt>feedForwardNetwork</tt> is a feed-forward layer.</li>
--   </ul>
--   
--   TODO: Some transformers use LayerDrop, see
--   <a>https://arxiv.org/abs/1909.11556</a>, during training. To support
--   this, we will need a layer wrapper that is either the identity
--   function or the wrapped layer based on a uniformly random draw from a
--   supplied generator.
data GTransformerBlock (selfAttention :: Type) (crossAttention :: Type) (feedForwardNetwork :: Type)
[GTransformerBlock] :: forall selfAttention crossAttention feedForwardNetwork. selfAttention -> crossAttention -> feedForwardNetwork -> GTransformerBlock selfAttention crossAttention feedForwardNetwork
type family EncoderBlockF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout)
encoderBlockSpec :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim hasDropout. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SDim ffnDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (EncoderBlockF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim hasDropout)
type family DecoderBlockF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout)
decoderBlockSpec :: forall style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim hasDropout. STransformerStyle style -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SDim keyEmbedDim -> SDim ffnDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (DecoderBlockF style gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim hasDropout)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork)
instance (GHC.Show.Show selfAttention, GHC.Show.Show crossAttention, GHC.Show.Show feedForwardNetwork) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork)
instance (GHC.Classes.Ord selfAttention, GHC.Classes.Ord crossAttention, GHC.Classes.Ord feedForwardNetwork) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork)
instance (GHC.Classes.Eq selfAttention, GHC.Classes.Eq crossAttention, GHC.Classes.Eq feedForwardNetwork) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize selfAttention generatorDevice selfAttention' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize crossAttention generatorDevice0 crossAttention' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize feedForwardNetwork generatorDevice1 feedForwardNetwork' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention' crossAttention' feedForwardNetwork') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict selfAttention, Torch.GraduallyTyped.NN.Class.HasStateDict crossAttention, Torch.GraduallyTyped.NN.Class.HasStateDict feedForwardNetwork) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork)
instance (Torch.GraduallyTyped.NN.Class.HasForward selfAttention (query, attentionBias) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward feedForwardNetwork tensor0 generatorDevice0 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention () feedForwardNetwork) (query, attentionBias) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward selfAttention (query, attentionBias) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward crossAttention (tensor0, key, crossAttentionBias) generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward feedForwardNetwork tensor1 generatorDevice1 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GBlock.GTransformerBlock selfAttention crossAttention feedForwardNetwork) (query, key, attentionBias, crossAttentionBias) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GStack

-- | Generic transformer stack.
--   
--   <ul>
--   <li><tt>stack</tt> is a stack of tranformer blocks.</li>
--   </ul>
newtype GTransformerStack (stack :: Type)
[GTransformerStack] :: forall stack. stack -> GTransformerStack stack
type family EncoderStackF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout)

-- | Specifies the parameters of a transformer stack in an encoder
--   configuration.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer stack, e.g.
--   <tt>ST5</tt>, <tt>SByT5</tt>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the stack's
--   parameters.</li>
--   <li><tt>device</tt>: the computational device on which the stack is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the stack's parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   stack.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>queryEmbedDim</tt>: the dimension of the transformer query
--   embeddings.</li>
--   <li><tt>ffnDim</tt>: the dimension of the feed-forward network.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
encoderStackSpec :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim hasDropout. STransformerStyle style -> SNat numLayers -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SDim ffnDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (EncoderStackF style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim ffnDim hasDropout)
type family DecoderStackF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (queryEmbedDim :: Dim (Name Symbol) (Size Nat)) (keyEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout)

-- | Specifies the parameters of a transformer stack in a decoder
--   configuration.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer stack, e.g.
--   <tt>ST5</tt>, <tt>SByT5</tt>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the stack's
--   parameters.</li>
--   <li><tt>device</tt>: the computational device on which the stack is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the stack's parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   stack.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>queryEmbedDim</tt>: the dimension of the transformer query
--   embeddings.</li>
--   <li><tt>keyEmbedDim</tt>: the dimension of the transformer key
--   embeddings.</li>
--   <li><tt>ffnDim</tt>: the dimension of the feed-forward network.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
decoderStackSpec :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim hasDropout. STransformerStyle style -> SNat numLayers -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim queryEmbedDim -> SDim keyEmbedDim -> SDim ffnDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (DecoderStackF style numLayers gradient device dataType headDim headEmbedDim embedDim queryEmbedDim keyEmbedDim ffnDim hasDropout)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack stack)
instance GHC.Show.Show stack => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack stack)
instance GHC.Classes.Ord stack => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack stack)
instance GHC.Classes.Eq stack => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack stack)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize block generatorDevice block' generatorDevice, numLayers' GHC.Types.~ (numLayers GHC.TypeNats.+ 1)) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector numLayers' block)) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector numLayers' block')) generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict block => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector numLayers block))
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector 0 block)) (query, attentionBias) generatorDevice query generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasForward block (query, attentionBias) generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector 1 block)) (query, attentionBias) generatorDevice output generatorOutputDevice
instance Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector 0 block)) (query, key, attentionBias, crossAttentionBias) generator query generator
instance Torch.GraduallyTyped.NN.Class.HasForward block (query, key, attentionBias, crossAttentionBias) generatorDevice output generatorOutputDevice => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector 1 block)) (query, key, attentionBias, crossAttentionBias) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward block (query, attentionBias) generatorDevice output generatorOutputDevice, Torch.GraduallyTyped.NN.Class.HasForward block (output, attentionBias) generatorOutputDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector n block)) (query, attentionBias) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward block (query, key, attentionBias, crossAttentionBias) generatorDevice output generatorOutputDevice, Torch.GraduallyTyped.NN.Class.HasForward block (output, key, attentionBias, crossAttentionBias) generatorOutputDevice output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GStack.GTransformerStack (Data.Vector.Sized.Vector n block)) (query, key, attentionBias, crossAttentionBias) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GTransformer

-- | Generic transformer. Can specialize to either encoder or decoder.
--   
--   <ul>
--   <li><tt>posEnc</tt>: an absolute positional encoding layer as used by,
--   e.g., BERT.</li>
--   <li><tt>relPosEnc</tt>: a relative positional encoding layer as used
--   by, e.g., T5.</li>
--   <li><tt>initialLayerNorm</tt>: a layer normalization layer for the
--   embeddings.</li>
--   <li><tt>initialDropout</tt>: a dropout layer for the embeddings.</li>
--   <li><tt>stack</tt>: a stack of transformer blocks.</li>
--   <li><tt>finalLayerNorm</tt>: the final layer normalization layer.</li>
--   <li><tt>finalDropout</tt>: the final dropout layer.</li>
--   </ul>
data GTransformer (posEnc :: Type) (relPosEnc :: Type) (initialLayerNorm :: Type) (initialDropout :: Type) (stack :: Type) (finalLayerNorm :: Type) (finalDropout :: Type)
[GTransformer] :: forall posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout. posEnc -> relPosEnc -> initialLayerNorm -> initialDropout -> stack -> finalLayerNorm -> finalDropout -> GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout
type family TransformerEncoderF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the absolute positional encoding layer of a transformer
--   encoder.
type family TEPosEncF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the relative positional encoding layer of a transformer
--   encoder.
type family TERelPosEncF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the initial layer normalization layer of a transformer
--   encoder.
type family TEInitialLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the initial dropout layer of a transformer encoder.
type family TEInitialDropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the transformer block stack of a transformer encoder.
type family TEStackF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the final layer normalization layer of a transformer
--   encoder.
type family TEFinalLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the final dropout layer of a transformer encoder.
type family TEFinalDropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a transformer in an encoder configuration.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer stack, e.g.
--   <a>ST5</a>, <a>SByT5</a>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the stack's
--   parameters.</li>
--   <li><tt>device</tt>: the computational device on which the stack is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the stack's parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   stack.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>inputEmbedDim</tt>: the dimension of the transformer query
--   embeddings.</li>
--   <li><tt>ffnDim</tt>: the dimension of the feed-forward network.</li>
--   <li><tt>posEncDim</tt>: the dimension of the positional encoding.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
transformerEncoderSpec :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim hasDropout. STransformerStyle style -> SNat numLayers -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim inputEmbedDim -> SDim ffnDim -> SDim posEncDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (TransformerEncoderF style numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim hasDropout)
type family TransformerDecoderF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoderOutputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the absolute positional encoding layer of a transformer
--   decoder.
type family TDPosEncF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the relative positional encoding layer of a transformer
--   decoder.
type family TDRelPosEncF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the initial layer normalization layer of a transformer
--   decoder.
type family TDInitialLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the initial dropout layer of a transformer decoder.
type family TDInitialDropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the transformer block stack of a transformer decoder.
type family TDStackF (style :: TransformerStyle) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (decoderInputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoderOutputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the final layer normalization layer of a transformer
--   decoder.
type family TDFinalLayerNormF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the final dropout layer of a transformer decoder.
type family TDFinalDropoutF (style :: TransformerStyle) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a transformer in a decoder configuration.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the transformer stack, e.g.
--   <a>ST5</a>, <a>SByT5</a>, etc.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the stack's
--   parameters.</li>
--   <li><tt>device</tt>: the computational device on which the stack is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the stack's parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   stack.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>decoderInputEmbedDim</tt>: the dimension of the decoder input
--   embeddings.</li>
--   <li><tt>encoderOutputEmbedDim</tt>: the dimension of the encoder
--   output embeddings.</li>
--   <li><tt>ffnDim</tt>: the dimension of the feed-forward network.</li>
--   <li><tt>posEncDim</tt>: the dimension of the positional encoding.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
transformerDecoderSpec :: forall style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim hasDropout. STransformerStyle style -> SNat numLayers -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim decoderInputEmbedDim -> SDim encoderOutputEmbedDim -> SDim ffnDim -> SDim posEncDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (TransformerDecoderF style numLayers gradient device dataType headDim headEmbedDim embedDim decoderInputEmbedDim encoderOutputEmbedDim ffnDim posEncDim hasDropout)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout)
instance (GHC.Show.Show posEnc, GHC.Show.Show relPosEnc, GHC.Show.Show initialLayerNorm, GHC.Show.Show initialDropout, GHC.Show.Show stack, GHC.Show.Show finalLayerNorm, GHC.Show.Show finalDropout) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout)
instance (GHC.Classes.Ord posEnc, GHC.Classes.Ord relPosEnc, GHC.Classes.Ord initialLayerNorm, GHC.Classes.Ord initialDropout, GHC.Classes.Ord stack, GHC.Classes.Ord finalLayerNorm, GHC.Classes.Ord finalDropout) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout)
instance (GHC.Classes.Eq posEnc, GHC.Classes.Eq relPosEnc, GHC.Classes.Eq initialLayerNorm, GHC.Classes.Eq initialDropout, GHC.Classes.Eq stack, GHC.Classes.Eq finalLayerNorm, GHC.Classes.Eq finalDropout) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize posEnc generatorDevice posEnc' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize relPosEnc generatorDevice0 relPosEnc' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize initialLayerNorm generatorDevice1 initialLayerNorm' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize initialDropout generatorDevice2 initialDropout' generatorDevice3, Torch.GraduallyTyped.NN.Class.HasInitialize stack generatorDevice3 stack' generatorDevice4, Torch.GraduallyTyped.NN.Class.HasInitialize finalLayerNorm generatorDevice4 finalLayerNorm' generatorDevice5, Torch.GraduallyTyped.NN.Class.HasInitialize finalDropout generatorDevice5 finalDropout' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc' relPosEnc' initialLayerNorm' initialDropout' stack' finalLayerNorm' finalDropout') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict posEnc, Torch.GraduallyTyped.NN.Class.HasStateDict relPosEnc, Torch.GraduallyTyped.NN.Class.HasStateDict initialLayerNorm, Torch.GraduallyTyped.NN.Class.HasStateDict initialDropout, Torch.GraduallyTyped.NN.Class.HasStateDict stack, Torch.GraduallyTyped.NN.Class.HasStateDict finalLayerNorm, Torch.GraduallyTyped.NN.Class.HasStateDict finalDropout) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout)
instance (Torch.GraduallyTyped.NN.Class.HasForward posEnc (Torch.GraduallyTyped.Tensor.Type.Tensor posGradient posLayout posDevice posDataType posShape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor posEncGradient posEncLayout posEncDevice posEncDataType posEncShape) generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward initialLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor (inputGradient Torch.GraduallyTyped.Unify.<|> posEncGradient) (inputLayout Torch.GraduallyTyped.Unify.<+> posEncLayout) (inputDevice Torch.GraduallyTyped.Unify.<+> posEncDevice) (inputDataType Torch.GraduallyTyped.Unify.<+> posEncDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF inputShape posEncShape)) generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF inputShape posEncShape), Torch.GraduallyTyped.NN.Class.HasForward initialDropout tensor1 generatorDevice1 tensor2 generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward stack (tensor2, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape)) generatorDevice2 tensor3 generatorDevice3, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape), Torch.GraduallyTyped.NN.Class.HasForward finalLayerNorm tensor3 generatorDevice3 tensor4 generatorDevice4, Torch.GraduallyTyped.NN.Class.HasForward finalDropout tensor4 generatorDevice4 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc () initialLayerNorm initialDropout stack finalLayerNorm finalDropout) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.Tensor posGradient posLayout posDevice posDataType posShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward initialLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward initialDropout tensor0 generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward relPosEnc (Torch.GraduallyTyped.Tensor.Type.Tensor relPosGradient relPosLayout relPosDevice relPosDataType relPosShape) generatorDevice1 (Torch.GraduallyTyped.Tensor.Type.Tensor relPosEncGradient relPosEncLayout relPosEncDevice relPosEncDataType relPosEncShape) generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward stack (tensor1, Torch.GraduallyTyped.Tensor.Type.Tensor (relPosEncGradient Torch.GraduallyTyped.Unify.<|> attentionMaskGradient) (relPosEncLayout Torch.GraduallyTyped.Unify.<+> attentionMaskLayout) (relPosEncDevice Torch.GraduallyTyped.Unify.<+> attentionMaskDevice) (relPosEncDataType Torch.GraduallyTyped.Unify.<+> attentionMaskDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF doubleTransposedRelPosEncShape unsqueezedAttentionMaskShape)) generatorDevice2 tensor3 generatorDevice3, transposedRelPosEncShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) relPosEncShape, Torch.GraduallyTyped.Prelude.Catch transposedRelPosEncShape, doubleTransposedRelPosEncShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) transposedRelPosEncShape, Torch.GraduallyTyped.Prelude.Catch doubleTransposedRelPosEncShape, unsqueezedAttentionMaskShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) attentionMaskShape, Torch.GraduallyTyped.Prelude.Catch unsqueezedAttentionMaskShape, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF doubleTransposedRelPosEncShape unsqueezedAttentionMaskShape), Torch.GraduallyTyped.NN.Class.HasForward finalLayerNorm tensor3 generatorDevice3 tensor4 generatorDevice4, Torch.GraduallyTyped.NN.Class.HasForward finalDropout tensor4 generatorDevice4 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer () relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout) (Torch.GraduallyTyped.Tensor.Type.Tensor inputGradient inputLayout inputDevice inputDataType inputShape, Torch.GraduallyTyped.Tensor.Type.Tensor relPosGradient relPosLayout relPosDevice relPosDataType relPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor attentionMaskGradient attentionMaskLayout attentionMaskDevice attentionMaskDataType attentionMaskShape) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward posEnc (Torch.GraduallyTyped.Tensor.Type.Tensor decoderPosGradient decoderPosLayout decoderPosDevice decoderPosDataType decoderPosShape) generatorDevice (Torch.GraduallyTyped.Tensor.Type.Tensor decoderPosEncGradient decoderPosEncLayout decoderPosEncDevice decoderPosEncDataType decoderPosEncShape) generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward initialLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor (decoderInputGradient Torch.GraduallyTyped.Unify.<|> decoderPosEncGradient) (decoderInputLayout Torch.GraduallyTyped.Unify.<+> decoderPosEncLayout) (decoderInputDevice Torch.GraduallyTyped.Unify.<+> decoderPosEncDevice) (decoderInputDataType Torch.GraduallyTyped.Unify.<+> decoderPosEncDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF decoderInputShape decoderPosEncShape)) generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward initialDropout tensor1 generatorDevice1 tensor2 generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward stack (tensor2, Torch.GraduallyTyped.Tensor.Type.Tensor encoderOutputGradient encoderOutputLayout encoderOutputDevice encoderOutputDataType encoderOutputShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape), Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape)) generatorDevice2 tensor3 generatorDevice3, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape), Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape), Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF decoderInputShape decoderPosEncShape), Torch.GraduallyTyped.NN.Class.HasForward finalLayerNorm tensor3 generatorDevice3 tensor4 generatorDevice4, Torch.GraduallyTyped.NN.Class.HasForward finalDropout tensor4 generatorDevice4 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer posEnc () initialLayerNorm initialDropout stack finalLayerNorm finalDropout) (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, Torch.GraduallyTyped.Tensor.Type.Tensor encoderOutputGradient encoderOutputLayout encoderOutputDevice encoderOutputDataType encoderOutputShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderPosGradient decoderPosLayout decoderPosDevice decoderPosDataType decoderPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType decoderAttentionMaskShape, Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType crossAttentionMaskShape) generatorDevice output generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward initialLayerNorm (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape) generatorDevice tensor0 generatorDevice0, Torch.GraduallyTyped.NN.Class.HasForward initialDropout tensor0 generatorDevice0 tensor1 generatorDevice1, Torch.GraduallyTyped.NN.Class.HasForward relPosEnc (Torch.GraduallyTyped.Tensor.Type.Tensor decoderRelPosGradient decoderRelPosLayout decoderRelPosDevice decoderRelPosDataType decoderRelPosShape) generatorDevice1 (Torch.GraduallyTyped.Tensor.Type.Tensor decoderRelPosEncGradient decoderRelPosEncLayout decoderRelPosEncDevice decoderRelPosEncDataType decoderRelPosEncShape) generatorDevice2, Torch.GraduallyTyped.NN.Class.HasForward stack (tensor1, Torch.GraduallyTyped.Tensor.Type.Tensor encoderOutputGradient encoderOutputLayout encoderOutputDevice encoderOutputDataType encoderOutputShape, Torch.GraduallyTyped.Tensor.Type.Tensor (decoderRelPosEncGradient Torch.GraduallyTyped.Unify.<|> decoderAttentionMaskGradient) (decoderRelPosEncLayout Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskLayout) (decoderRelPosEncDevice Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskDevice) (decoderRelPosEncDataType Torch.GraduallyTyped.Unify.<+> decoderAttentionMaskDataType) (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF doubleTransposedDecoderRelPosEncShape unsqueezedDecoderAttentionMaskShape), Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType unsqueezedCrossAttentionMaskShape) generatorDevice2 tensor3 generatorDevice3, transposedDecoderRelPosEncShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 3)) decoderRelPosEncShape, Torch.GraduallyTyped.Prelude.Catch transposedDecoderRelPosEncShape, doubleTransposedDecoderRelPosEncShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.TransposeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) transposedDecoderRelPosEncShape, Torch.GraduallyTyped.Prelude.Catch doubleTransposedDecoderRelPosEncShape, unsqueezedDecoderAttentionMaskShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) decoderAttentionMaskShape, Torch.GraduallyTyped.Prelude.Catch unsqueezedDecoderAttentionMaskShape, unsqueezedCrossAttentionMaskShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 1)) crossAttentionMaskShape, Torch.GraduallyTyped.Prelude.Catch unsqueezedCrossAttentionMaskShape, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF doubleTransposedDecoderRelPosEncShape unsqueezedDecoderAttentionMaskShape), Torch.GraduallyTyped.NN.Class.HasForward finalLayerNorm tensor3 generatorDevice3 tensor4 generatorDevice4, Torch.GraduallyTyped.NN.Class.HasForward finalDropout tensor4 generatorDevice4 output generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GTransformer.GTransformer () relPosEnc initialLayerNorm initialDropout stack finalLayerNorm finalDropout) (Torch.GraduallyTyped.Tensor.Type.Tensor decoderInputGradient decoderInputLayout decoderInputDevice decoderInputDataType decoderInputShape, Torch.GraduallyTyped.Tensor.Type.Tensor encoderOutputGradient encoderOutputLayout encoderOutputDevice encoderOutputDataType encoderOutputShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderRelPosGradient decoderRelPosLayout decoderRelPosDevice decoderRelPosDataType decoderRelPosShape, Torch.GraduallyTyped.Tensor.Type.Tensor decoderAttentionMaskGradient decoderAttentionMaskLayout decoderAttentionMaskDevice decoderAttentionMaskDataType decoderAttentionMaskShape, Torch.GraduallyTyped.Tensor.Type.Tensor crossAttentionMaskGradient crossAttentionMaskLayout crossAttentionMaskDevice crossAttentionMaskDataType crossAttentionMaskShape) generatorDevice output generatorOutputDevice

module Torch.GraduallyTyped.NN.Transformer.GEncoderOnly

-- | Data type that is used to represent whether the encoder-only
--   transformer model has a scaled embedding.
data EncoderOnlyTransformerHasEmbedScaling
EncoderOnlyTransformerWithEmbedScaling :: EncoderOnlyTransformerHasEmbedScaling
EncoderOnlyTransformerWithoutEmbedScaling :: EncoderOnlyTransformerHasEmbedScaling

-- | Generic encoder-only transformer model. This is a transformer model
--   that only encodes the input, e.g. BERT.
--   
--   <ul>
--   <li><tt>inputEmbedDim</tt>: the dimension of the input embedding.</li>
--   <li><tt>encoder</tt>: a transformer encoder.</li>
--   <li><tt>encoderEmbedding</tt>: an embedding layer for the input.</li>
--   <li><tt>encoderTypeEmbedding</tt>: an embedding layer for the type of
--   the input.</li>
--   <li><tt>head</tt>: a head layer for the output.</li>
--   </ul>
data GEncoderOnlyTransformer (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoder :: Type) (encoderEmbedding :: Type) (encoderTypeEmbedding :: Type) (head :: Type)
[GEncoderOnlyTransformer] :: forall inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head. SDim inputEmbedDim -> encoder -> encoderEmbedding -> encoderTypeEmbedding -> head -> EncoderOnlyTransformerHasEmbedScaling -> GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head
type family GEncoderOnlyTransformerF (style :: TransformerStyle) (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the embedding layer of the encoder-only transformer model.
type family EOTEmbeddingF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the type embedding layer of the encoder-only transformer
--   model.
type family EOTTypeEmbeddingF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the head layer of the encoder-only transformer model.
type family EOTHeadF (style :: TransformerStyle) (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the parameters of an encoder-only transformer model.
--   
--   <ul>
--   <li><tt>style</tt>: the style of the encoder-only transformer model,
--   e.g. <a>SBERT</a>, <a>SRoBERTa</a>, etc.</li>
--   <li><tt>transformerHead</tt>: the head of the encoder-only transformer
--   model.</li>
--   <li><tt>numLayers</tt>: the number of layers of the encoder-only
--   transformer model.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the model
--   parameters</li>
--   <li><tt>device</tt>: the computational device on which the model is
--   allocated.</li>
--   <li><tt>dataType</tt>: the data type of the model parameters.</li>
--   <li><tt>headDim</tt>: the dimension of all transformer heads in the
--   encoder-only transformer model.</li>
--   <li><tt>headEmbedDim</tt>: the dimension of the transformer head
--   embeddings.</li>
--   <li><tt>embedDim</tt>: the dimension of the transformer
--   embeddings.</li>
--   <li><tt>inputEmbedDim</tt>: the dimension of the input
--   embeddings.</li>
--   <li><tt>ffnDim</tt>: the dimension of the feed-forward network.</li>
--   <li><tt>posEncDim</tt>: the dimension of the positional
--   embeddings.</li>
--   <li><tt>vocabDim</tt>: the dimension of the vocabulary.</li>
--   <li><tt>typeVocabDim</tt>: the dimension of the type vocabulary.</li>
--   <li><tt>dropoutP</tt>: the dropout rate.</li>
--   <li><tt>eps</tt>: the epsilon value for numerical stability of the
--   layer normalization.</li>
--   </ul>
encoderOnlyTransformerSpec :: forall style transformerHead numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim hasDropout. STransformerStyle style -> STransformerHead transformerHead -> SNat numLayers -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim inputEmbedDim -> SDim ffnDim -> SDim posEncDim -> SDim vocabDim -> SDim typeVocabDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (GEncoderOnlyTransformerF style transformerHead numLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim typeVocabDim hasDropout)
data GSimplifiedEncoderOnlyTransformer (model :: Type) (mkPos :: Type) (mkPaddingMask :: Type) (mkAttentionMask :: Type)
[GSimplifiedEncoderOnlyTransformer] :: forall model mkPos mkPaddingMask mkAttentionMask. model -> mkPos -> mkPaddingMask -> mkAttentionMask -> GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask

-- | Input data type for use with an encoder-only transformer.
data EncoderOnlyTransformerInput input inputType pos attentionMask
[EncoderOnlyTransformerInput] :: forall input inputType pos attentionMask. input -> inputType -> pos -> attentionMask -> EncoderOnlyTransformerInput input inputType pos attentionMask
data SimplifiedEncoderOnlyTransformerInput input inputType
[SimplifiedEncoderOnlyTransformerInput] :: forall input inputType. input -> inputType -> SimplifiedEncoderOnlyTransformerInput input inputType

-- | Output data type for use with an encoder-only transformer.
data EncoderOnlyTransformerOutput output
[EncoderOnlyTransformerOutput] :: forall output. output -> EncoderOnlyTransformerOutput output
data SimplifiedEncoderOnlyTransformerOutput output paddingMask
[SimplifiedEncoderOnlyTransformerOutput] :: forall output paddingMask. output -> paddingMask -> SimplifiedEncoderOnlyTransformerOutput output paddingMask
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head)
instance (GHC.Show.Show encoder, GHC.Show.Show encoderEmbedding, GHC.Show.Show encoderTypeEmbedding, GHC.Show.Show head) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask)
instance (GHC.Show.Show model, GHC.Show.Show mkPos, GHC.Show.Show mkPaddingMask, GHC.Show.Show mkAttentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask)
instance (GHC.Classes.Ord model, GHC.Classes.Ord mkPos, GHC.Classes.Ord mkPaddingMask, GHC.Classes.Ord mkAttentionMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask)
instance (GHC.Classes.Eq model, GHC.Classes.Eq mkPos, GHC.Classes.Eq mkPaddingMask, GHC.Classes.Eq mkAttentionMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask)
instance (GHC.Show.Show input, GHC.Show.Show inputType, GHC.Show.Show pos, GHC.Show.Show attentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask)
instance (GHC.Classes.Ord input, GHC.Classes.Ord inputType, GHC.Classes.Ord pos, GHC.Classes.Ord attentionMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask)
instance (GHC.Classes.Eq input, GHC.Classes.Eq inputType, GHC.Classes.Eq pos, GHC.Classes.Eq attentionMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerInput input inputType)
instance (GHC.Show.Show input, GHC.Show.Show inputType) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerInput input inputType)
instance (GHC.Classes.Ord input, GHC.Classes.Ord inputType) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerInput input inputType)
instance (GHC.Classes.Eq input, GHC.Classes.Eq inputType) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerInput input inputType)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerOutput output)
instance GHC.Show.Show output => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerOutput output)
instance GHC.Classes.Ord output => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerOutput output)
instance GHC.Classes.Eq output => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerOutput output)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerOutput output paddingMask)
instance (GHC.Show.Show output, GHC.Show.Show paddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerOutput output paddingMask)
instance (GHC.Classes.Ord output, GHC.Classes.Ord paddingMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerOutput output paddingMask)
instance (GHC.Classes.Eq output, GHC.Classes.Eq paddingMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerOutput output paddingMask)
instance (Torch.GraduallyTyped.NN.Class.HasForward mkPaddingMask input generatorDevice paddingMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkAttentionMask paddingMask generatorDevice attentionMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkPos input generatorDevice pos generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward model (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerOutput output) generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerInput input inputType) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.SimplifiedEncoderOnlyTransformerOutput output paddingMask) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward encoderEmbedding input generatorDevice embeddingOutput embeddingGeneratorOutputDevice, embeddingOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient' layout' device' dataType' shape', Torch.GraduallyTyped.NN.Class.HasForward encoderTypeEmbedding inputType embeddingGeneratorOutputDevice typeEmbeddingOutput typeEmbeddingGeneratorOutputDevice, typeEmbeddingOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor gradient'' layout'' device'' dataType'' shape'', Torch.GraduallyTyped.NN.Class.HasForward encoder (Torch.GraduallyTyped.Tensor.Type.Tensor (gradient' Torch.GraduallyTyped.Unify.<|> gradient'') (layout' Torch.GraduallyTyped.Unify.<+> layout'') (device' Torch.GraduallyTyped.Unify.<+> device'') (dataType' Torch.GraduallyTyped.Unify.<+> dataType'') (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape' shape''), pos, attentionMask) typeEmbeddingGeneratorOutputDevice encoderOutput encoderGeneratorOutputDevice, Torch.GraduallyTyped.Prelude.Catch (Torch.GraduallyTyped.Shape.Class.BroadcastShapesF shape' shape''), Torch.GraduallyTyped.NN.Class.HasForward head encoderOutput encoderGeneratorOutputDevice headOutput generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head) (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerInput input inputType pos attentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerOutput headOutput) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize model generatorDevice model' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize mkPos generatorDevice0 mkPos' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize mkPaddingMask generatorDevice1 mkPaddingMask' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize mkAttentionMask generatorDevice2 mkAttentionMask' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model' mkPos' mkPaddingMask' mkAttentionMask') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict model, Torch.GraduallyTyped.NN.Class.HasStateDict mkPos, Torch.GraduallyTyped.NN.Class.HasStateDict mkPaddingMask, Torch.GraduallyTyped.NN.Class.HasStateDict mkAttentionMask) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GSimplifiedEncoderOnlyTransformer model mkPos mkPaddingMask mkAttentionMask)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize encoder generatorDevice encoder' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize encoderEmbedding generatorDevice0 encoderEmbedding' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize encoderTypeEmbedding generatorDevice1 encoderTypeEmbedding' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize head generatorDevice2 head' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GEncoderOnlyTransformer inputEmbedDim encoder' encoderEmbedding' encoderTypeEmbedding' head') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict encoder, Torch.GraduallyTyped.NN.Class.HasStateDict encoderEmbedding, Torch.GraduallyTyped.NN.Class.HasStateDict encoderTypeEmbedding, Torch.GraduallyTyped.NN.Class.HasStateDict head) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.GEncoderOnlyTransformer inputEmbedDim encoder encoderEmbedding encoderTypeEmbedding head)
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling generatorDevice Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Transformer.GEncoderOnly.EncoderOnlyTransformerHasEmbedScaling

module Torch.GraduallyTyped.NN.Transformer.RoBERTa.Common

-- | RoBERTa dType.
type RoBERTaDType = 'Float

-- | RoBERTa dType singleton.
robertaDType :: SDType RoBERTaDType

-- | RoBERTa data type.
type RoBERTaDataType = 'DataType RoBERTaDType

-- | RoBERTa data type singleton.
robertaDataType :: SDataType RoBERTaDataType

-- | RoBERTa dropout rate. 'dropout_rate = 0.1'
robertaDropoutP :: Double

-- | RoBERTa positional encoding dimension.
--   
--   Note the two extra dimensions.
type RoBERTaPosEncDim = 'Dim ('Name "*") ('Size 514)

-- | RoBERTa positional encoding dimension singleton.
robertaPosEncDim :: SDim RoBERTaPosEncDim

-- | RoBERTa layer-norm epsilon. 'layer_norm_epsilon = 1e-5'
robertaEps :: Double

-- | RoBERTa maximum number of position embeddings.
--   'max_position_embeddings = 514'
robertaMaxPositionEmbeddings :: Int

-- | RoBERTa padding token id. 'pad_token_id = 1'
robertaPadTokenId :: Int

-- | RoBERTa begin-of-sentence token id. 'bos_token_id = 0'
robertaBOSTokenId :: Int

-- | RoBERTa end-of-sentence token id. 'eos_token_id = 0'
robertaEOSTokenId :: Int

-- | RoBERTa attention mask bias
robertaAttentionMaskBias :: Double

-- | Specifies the RoBERTa model.
type family RoBERTaModelF (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a RoBERTa model.
--   
--   <ul>
--   <li><tt>transformerHead</tt>: the head of the RoBERTa model.</li>
--   <li><tt>numLayers</tt>: the number of layers in the RoBERTa
--   model.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the RoBERTa
--   model.</li>
--   <li><tt>device</tt>: the computational device on which the RoBERTa
--   model parameters are to be allocated.</li>
--   </ul>
robertaModelSpec :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim hasDropout. (SingI headDim, SingI headEmbedDim, SingI embedDim, SingI inputEmbedDim, SingI ffnDim, SingI vocabDim, SingI typeVocabDim) => STransformerHead transformerHead -> SNat numLayers -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (RoBERTaModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim hasDropout)
mkRoBERTaInput :: forall batchDim seqDim device m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, Catch ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> SDevice device -> [[Int]] -> m output

module Torch.GraduallyTyped.NN.Transformer.RoBERTa.Base

-- | RoBERTa-Base number of layers. 'num_hidden_layers = 12'
type RoBERTaBaseNumLayers = 12

-- | RoBERTa-Base number of layers singleton.
robertaBaseNumLayers :: SNat RoBERTaBaseNumLayers

-- | RoBERTa-Base number of attention heads. 'num_attention_heads = 12'
type RoBERTaBaseHeadDim = 'Dim ('Name "*") ('Size 12)

-- | RoBERTa-Base head embedding dimension. 'd_kv = 64'
type RoBERTaBaseHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | RoBERTa-Base embedding dimension. 'hidden_size = n_heads * d_kv = 768'
type RoBERTaBaseEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | RoBERTa-Base model dimension. 'hidden_size = 768'
type RoBERTaBaseInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | RoBERTa-Base feed-forward network dimension. 'intermediate_size =
--   3072'
type RoBERTaBaseFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | RoBERTa-Base vocabulary dimension. 'vocab_size = 50265'
type RoBERTaBaseVocabDim = 'Dim ('Name "*") ('Size 50265)

-- | RoBERTa-Base vocabulary dimension singleton.
robertaBaseVocabDim :: SDim RoBERTaBaseVocabDim

-- | RoBERTa-Base type vocabulary dimension. 'type_vocab_size = 1'
type RoBERTaBaseTypeVocabDim = 'Dim ('Name "*") ('Size 1)

-- | RoBERTa-Base model.
type RoBERTaBase (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = RoBERTaModelF transformerHead RoBERTaBaseNumLayers gradient device RoBERTaBaseHeadDim RoBERTaBaseHeadEmbedDim RoBERTaBaseEmbedDim RoBERTaBaseInputEmbedDim RoBERTaBaseFFNDim RoBERTaBaseVocabDim RoBERTaBaseTypeVocabDim hasDropout

-- | RoBERTa-Base model specification.
robertaBaseSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (RoBERTaBase transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.RoBERTa

module Torch.GraduallyTyped.NN.Transformer.BERT.Common

-- | BERT dType.
type BERTDType = 'Float

-- | BERT dType singleton.
bertDType :: SDType BERTDType

-- | BERT data type.
type BERTDataType = 'DataType BERTDType

-- | BERT data type singleton.
bertDataType :: SDataType BERTDataType

-- | BERT dropout rate. 'dropout_rate = 0.1'
bertDropoutP :: Double

-- | BERT positional encoding dimension.
type BERTPosEncDim = 'Dim ('Name "*") ('Size 512)

-- | BERT positional encoding dimension singleton.
bertPosEncDim :: SDim BERTPosEncDim

-- | BERT layer-norm epsilon. 'layer_norm_epsilon = 1e-12'
bertEps :: Double

-- | BERT maximum number of position embeddings. 'max_position_embeddings =
--   512'
bertMaxPositionEmbeddings :: Int

-- | BERT padding token id. 'pad_token_id = 0'
bertPadTokenId :: Int

-- | BERT attention mask bias
bertAttentionMaskBias :: Double

-- | Specifies the BERT model.
type family BERTModelF (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (typeVocabDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a BERT model.
--   
--   <ul>
--   <li><tt>transformerHead</tt>: the head of the BERT model.</li>
--   <li><tt>numLayers</tt>: the number of layers in the BERT model.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the BERT
--   model.</li>
--   <li><tt>device</tt>: the computational device on which the BERT model
--   parameters are to be allocated.</li>
--   </ul>
bertModelSpec :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim hasDropout. (SingI headDim, SingI headEmbedDim, SingI embedDim, SingI inputEmbedDim, SingI ffnDim, SingI vocabDim, SingI typeVocabDim) => STransformerHead transformerHead -> SNat numLayers -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (BERTModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim typeVocabDim hasDropout)
mkBERTInput :: forall batchDim seqDim device m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, Catch ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> SDevice device -> [[Int]] -> m output

module Torch.GraduallyTyped.NN.Transformer.BERT.BaseUncased

-- | BERT-Base-Uncased number of layers. 'num_hidden_layers = 12'
type BERTBaseUncasedNumLayers = 12

-- | BERT-Base-Uncased number of layers singleton.
bertBaseUncasedNumLayers :: SNat BERTBaseUncasedNumLayers

-- | BERT-Base-Uncased number of attention heads. 'num_attention_heads =
--   12'
type BERTBaseUncasedHeadDim = 'Dim ('Name "*") ('Size 12)

-- | BERT-Base-Uncased number of attention heads singleton.
bertBaseUncasedHeadDim :: SDim BERTBaseUncasedHeadDim

-- | BERT-Base-Uncased head embedding dimension. 'd_kv = 64'
type BERTBaseUncasedHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | BERT-Base-Uncased head embedding dimension singleton.
bertBaseUncasedHeadEmbedDim :: SDim BERTBaseUncasedHeadEmbedDim

-- | BERT-Base-Uncased embedding dimension. 'hidden_size = n_heads * d_kv =
--   768'
type BERTBaseUncasedEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BERT-Base-Uncased embedding dimension singleton.
bertBaseUncasedEmbedDim :: SDim BERTBaseUncasedEmbedDim

-- | BERT-Base-Uncased model dimension. 'hidden_size = 768'
type BERTBaseUncasedInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BERT-Base-Uncased model dimension singleton.
bertBaseUncasedInputEmbedDim :: SDim BERTBaseUncasedInputEmbedDim

-- | BERT-Base-Uncased feed-forward network dimension. 'intermediate_size =
--   3072'
type BERTBaseUncasedFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | BERT-Base-Uncased feed-forward network dimension singleton.
bertBaseUncasedFFNDim :: SDim BERTBaseUncasedFFNDim

-- | BERT-Base-Uncased vocabulary dimension. 'vocab_size = 30522'
type BERTBaseUncasedVocabDim = 'Dim ('Name "*") ('Size 30522)

-- | BERT-Base-Uncased vocabulary dimension singleton.
bertBaseUncasedVocabDim :: SDim BERTBaseUncasedVocabDim

-- | BERT-Base-Uncased type vocabulary dimension. 'type_vocab_size = 2'
type BERTBaseUncasedTypeVocabDim = 'Dim ('Name "*") ('Size 2)

-- | BERT-Base-Uncased type vocabulary dimension singleton.
bertBaseUncasedTypeVocabDim :: SDim BERTBaseUncasedTypeVocabDim

-- | BERT-Base-Uncased model.
type BERTBaseUncased (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = BERTModelF transformerHead BERTBaseUncasedNumLayers gradient device BERTBaseUncasedHeadDim BERTBaseUncasedHeadEmbedDim BERTBaseUncasedEmbedDim BERTBaseUncasedInputEmbedDim BERTBaseUncasedFFNDim BERTBaseUncasedVocabDim BERTBaseUncasedTypeVocabDim hasDropout

-- | BERT-Base-Uncased model specification.
bertBaseUnchasedSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (BERTBaseUncased transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.BERT

module Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder

-- | Data type that is used to represent whether the encoder-decoder
--   transformer model has a scaled embedding.
data EncoderDecoderTransformerHasEmbedScaling
EncoderDecoderTransformerWithEmbedScaling :: EncoderDecoderTransformerHasEmbedScaling
EncoderDecoderTransformerWithoutEmbedScaling :: EncoderDecoderTransformerHasEmbedScaling

-- | Generic encoder-decoder transformer model. This is a model that can be
--   used to encode and decode sequences of variable length.
--   
--   <ul>
--   <li><tt>inputEmbedDim</tt>: the dimension of the input embedding.</li>
--   <li><tt>encoder</tt>: a transformer encoder.</li>
--   <li><tt>decoder</tt>: a transformer decoder.</li>
--   <li><tt>sharedEmbedding</tt>: a shared embedding layer.</li>
--   <li><tt>head</tt>: a head layer for the output.</li>
--   </ul>
data GEncoderDecoderTransformer (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (encoder :: Type) (decoder :: Type) (sharedEmbedding :: Type) (head :: Type)
[GEncoderDecoderTransformer] :: forall inputEmbedDim encoder decoder sharedEmbedding head. SDim inputEmbedDim -> encoder -> decoder -> sharedEmbedding -> head -> EncoderDecoderTransformerHasEmbedScaling -> GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head
type family GEncoderDecoderTransformerF (style :: TransformerStyle) (transformerHead :: TransformerHead) (numEncoderLayers :: Nat) (numDecoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the encoder of the encoder-decoder transformer model.
type family EDTEncoderF (style :: TransformerStyle) (numEncoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the decoder of the encoder-decoder transformer model.
type family EDTDecoderF (style :: TransformerStyle) (numDecoderLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (posEncDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the shared embedding layer of the encoder-decoder
--   transformer model.
type family EDTSharedEmbeddingF (style :: TransformerStyle) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat))

-- | Specifies the head of the encoder-decoder transformer model.
type family EDTHeadF (style :: TransformerStyle) (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (dataType :: DataType DType) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) :: Type

-- | Specifies the parameters of an encoder-decoder transformer model.
encoderDecoderTransformerSpec :: forall style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim hasDropout. STransformerStyle style -> STransformerHead transformerHead -> SNat numEncoderLayers -> SNat numDecoderLayers -> SGradient gradient -> SDevice device -> SDataType dataType -> SDim headDim -> SDim headEmbedDim -> SDim embedDim -> SDim inputEmbedDim -> SDim ffnDim -> SDim posEncDim -> SDim vocabDim -> SHasDropout hasDropout -> Double -> Double -> ModelSpec (GEncoderDecoderTransformerF style transformerHead numEncoderLayers numDecoderLayers gradient device dataType headDim headEmbedDim embedDim inputEmbedDim ffnDim posEncDim vocabDim hasDropout)
data GSimplifiedEncoderDecoderTransformer (model :: Type) (mkPos :: Type) (mkDecoderPos :: Type) (mkPaddingMask :: Type) (mkAttentionMask :: Type) (mkCrossAttentionMask :: Type) (mkDecoderAttentionMask :: Type)
[GSimplifiedEncoderDecoderTransformer] :: forall model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask. model -> ShiftRight Int -> ShiftRight Int -> mkPos -> mkDecoderPos -> mkPaddingMask -> mkAttentionMask -> mkCrossAttentionMask -> mkDecoderAttentionMask -> GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask

-- | Input data type for use with an encoder-decoder transformer. Use this
--   for training.
data EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask
[EncoderDecoderTransformerInput] :: forall input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask. input -> decoderInput -> pos -> decoderPos -> attentionMask -> decoderAttentionMask -> crossAttentionMask -> EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask
data EncoderDecoderTransformerInput' input pos attentionMask
[EncoderDecoderTransformerInput'] :: forall input pos attentionMask. input -> pos -> attentionMask -> EncoderDecoderTransformerInput' input pos attentionMask
data SimplifiedEncoderDecoderTransformerInput input decoderInput
[SimplifiedEncoderDecoderTransformerInput] :: forall input decoderInput. input -> decoderInput -> SimplifiedEncoderDecoderTransformerInput input decoderInput
data SimplifiedEncoderDecoderTransformerInput' input
[SimplifiedEncoderDecoderTransformerInput'] :: forall input. input -> SimplifiedEncoderDecoderTransformerInput' input
data SimplifiedEncoderDecoderTransformerTrainingInput input target
[SimplifiedEncoderDecoderTransformerTrainingInput] :: forall input target. input -> target -> SimplifiedEncoderDecoderTransformerTrainingInput input target

-- | Output data type for use with an encoder-decoder transformer.
data EncoderDecoderTransformerOutput decoderOutput encoderOutput
[EncoderDecoderTransformerOutput] :: forall decoderOutput encoderOutput. decoderOutput -> encoderOutput -> EncoderDecoderTransformerOutput decoderOutput encoderOutput
data EncoderDecoderTransformerOutput' encoderOutput
[EncoderDecoderTransformerOutput'] :: forall encoderOutput. encoderOutput -> EncoderDecoderTransformerOutput' encoderOutput
data SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask
[SimplifiedEncoderDecoderTransformerOutput] :: forall decoderOutput encoderOutput decoderInput inputPaddingMask. decoderOutput -> encoderOutput -> decoderInput -> inputPaddingMask -> SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask
data SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask
[SimplifiedEncoderDecoderTransformerOutput'] :: forall encoderOutput inputPaddingMask. encoderOutput -> inputPaddingMask -> SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask
data SimplifiedEncoderDecoderTransformerTrainingOutput loss
[SimplifiedEncoderDecoderTransformerTrainingOutput] :: forall loss. loss -> SimplifiedEncoderDecoderTransformerTrainingOutput loss

-- | Input data type for use with an encoder-decoder transformer. Use this
--   for inference.
data EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask
[EncoderDecoderTransformerGenerationInput] :: forall decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask. decoderInput -> encoderOutput -> decoderPos -> decoderAttentionMask -> crossAttentionMask -> EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask
data SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask
[SimplifiedEncoderDecoderTransformerGenerationInput] :: forall decoderInput encoderOutput inputPaddingMask. decoderInput -> encoderOutput -> inputPaddingMask -> SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask
instance GHC.Generics.Generic Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling
instance GHC.Show.Show Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling
instance GHC.Classes.Ord Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling
instance GHC.Classes.Eq Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head)
instance (GHC.Show.Show encoder, GHC.Show.Show decoder, GHC.Show.Show sharedEmbedding, GHC.Show.Show head) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask)
instance (GHC.Show.Show model, GHC.Show.Show mkPos, GHC.Show.Show mkDecoderPos, GHC.Show.Show mkPaddingMask, GHC.Show.Show mkAttentionMask, GHC.Show.Show mkCrossAttentionMask, GHC.Show.Show mkDecoderAttentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask)
instance (GHC.Classes.Ord model, GHC.Classes.Ord mkPos, GHC.Classes.Ord mkDecoderPos, GHC.Classes.Ord mkPaddingMask, GHC.Classes.Ord mkAttentionMask, GHC.Classes.Ord mkCrossAttentionMask, GHC.Classes.Ord mkDecoderAttentionMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask)
instance (GHC.Classes.Eq model, GHC.Classes.Eq mkPos, GHC.Classes.Eq mkDecoderPos, GHC.Classes.Eq mkPaddingMask, GHC.Classes.Eq mkAttentionMask, GHC.Classes.Eq mkCrossAttentionMask, GHC.Classes.Eq mkDecoderAttentionMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask)
instance (GHC.Show.Show input, GHC.Show.Show decoderInput, GHC.Show.Show pos, GHC.Show.Show decoderPos, GHC.Show.Show attentionMask, GHC.Show.Show decoderAttentionMask, GHC.Show.Show crossAttentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask)
instance (GHC.Classes.Ord input, GHC.Classes.Ord decoderInput, GHC.Classes.Ord pos, GHC.Classes.Ord decoderPos, GHC.Classes.Ord attentionMask, GHC.Classes.Ord decoderAttentionMask, GHC.Classes.Ord crossAttentionMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask)
instance (GHC.Classes.Eq input, GHC.Classes.Eq decoderInput, GHC.Classes.Eq pos, GHC.Classes.Eq decoderPos, GHC.Classes.Eq attentionMask, GHC.Classes.Eq decoderAttentionMask, GHC.Classes.Eq crossAttentionMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask)
instance (GHC.Show.Show input, GHC.Show.Show pos, GHC.Show.Show attentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask)
instance (GHC.Classes.Ord input, GHC.Classes.Ord pos, GHC.Classes.Ord attentionMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask)
instance (GHC.Classes.Eq input, GHC.Classes.Eq pos, GHC.Classes.Eq attentionMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput input decoderInput)
instance (GHC.Show.Show input, GHC.Show.Show decoderInput) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput input decoderInput)
instance (GHC.Classes.Ord input, GHC.Classes.Ord decoderInput) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput input decoderInput)
instance (GHC.Classes.Eq input, GHC.Classes.Eq decoderInput) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput input decoderInput)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput' input)
instance GHC.Show.Show input => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput' input)
instance GHC.Classes.Ord input => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput' input)
instance GHC.Classes.Eq input => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput' input)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingInput input target)
instance (GHC.Show.Show input, GHC.Show.Show target) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingInput input target)
instance (GHC.Classes.Ord input, GHC.Classes.Ord target) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingInput input target)
instance (GHC.Classes.Eq input, GHC.Classes.Eq target) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingInput input target)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput decoderOutput encoderOutput)
instance (GHC.Show.Show decoderOutput, GHC.Show.Show encoderOutput) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput decoderOutput encoderOutput)
instance (GHC.Classes.Ord decoderOutput, GHC.Classes.Ord encoderOutput) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput decoderOutput encoderOutput)
instance (GHC.Classes.Eq decoderOutput, GHC.Classes.Eq encoderOutput) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput decoderOutput encoderOutput)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput)
instance GHC.Show.Show encoderOutput => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput)
instance GHC.Classes.Ord encoderOutput => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput)
instance GHC.Classes.Eq encoderOutput => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask)
instance (GHC.Show.Show decoderOutput, GHC.Show.Show encoderOutput, GHC.Show.Show decoderInput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask)
instance (GHC.Classes.Ord decoderOutput, GHC.Classes.Ord encoderOutput, GHC.Classes.Ord decoderInput, GHC.Classes.Ord inputPaddingMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask)
instance (GHC.Classes.Eq decoderOutput, GHC.Classes.Eq encoderOutput, GHC.Classes.Eq decoderInput, GHC.Classes.Eq inputPaddingMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask)
instance (GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask)
instance (GHC.Classes.Ord encoderOutput, GHC.Classes.Ord inputPaddingMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask)
instance (GHC.Classes.Eq encoderOutput, GHC.Classes.Eq inputPaddingMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingOutput loss)
instance GHC.Show.Show loss => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingOutput loss)
instance GHC.Classes.Ord loss => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingOutput loss)
instance GHC.Classes.Eq loss => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingOutput loss)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask)
instance (GHC.Show.Show decoderInput, GHC.Show.Show encoderOutput, GHC.Show.Show decoderPos, GHC.Show.Show decoderAttentionMask, GHC.Show.Show crossAttentionMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask)
instance (GHC.Classes.Ord decoderInput, GHC.Classes.Ord encoderOutput, GHC.Classes.Ord decoderPos, GHC.Classes.Ord decoderAttentionMask, GHC.Classes.Ord crossAttentionMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask)
instance (GHC.Classes.Eq decoderInput, GHC.Classes.Eq encoderOutput, GHC.Classes.Eq decoderPos, GHC.Classes.Eq decoderAttentionMask, GHC.Classes.Eq crossAttentionMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask)
instance GHC.Generics.Generic (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask)
instance (GHC.Show.Show decoderInput, GHC.Show.Show encoderOutput, GHC.Show.Show inputPaddingMask) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask)
instance (GHC.Classes.Ord decoderInput, GHC.Classes.Ord encoderOutput, GHC.Classes.Ord inputPaddingMask) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask)
instance (GHC.Classes.Eq decoderInput, GHC.Classes.Eq encoderOutput, GHC.Classes.Eq inputPaddingMask) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask)
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput' input) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask) generatorDevice', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask) generatorDevice' (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput input decoderInput) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward mkPaddingMask decoderInput generatorDevice decoderInputPaddingMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkCrossAttentionMask (rightShiftedDecoderInput, inputPaddingMask) generatorDevice crossAttentionMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkDecoderAttentionMask rightShiftedDecoderInputPaddingMask generatorDevice decoderAttentionMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInput generatorDevice rightShiftedDecoderInput generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.Type.ShiftRight GHC.Types.Int) decoderInputPaddingMask generatorDevice rightShiftedDecoderInputPaddingMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkDecoderPos rightShiftedDecoderInput generatorDevice decoderPos generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward model (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput rightShiftedDecoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput decoderOutput encoderOutput) generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput) generatorDevice', Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generatorDevice' (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput headOutput encoderOutput) generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput input decoderInput pos decoderPos attentionMask decoderAttentionMask crossAttentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput headOutput encoderOutput) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward sharedEmbedding decoderInput generatorDevice embeddingOutput' embeddingGeneratorOutputDevice', embeddingOutput' GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient'' layout'' device'' dataType'' shape'', Torch.GraduallyTyped.NN.Class.HasForward decoder (embeddingOutput', encoderOutput, decoderPos, decoderAttentionMask, crossAttentionMask) embeddingGeneratorOutputDevice' decoderOutput decoderGeneratorOutputDevice, Torch.GraduallyTyped.NN.Class.HasForward head decoderOutput decoderGeneratorOutputDevice headOutput generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerGenerationInput decoderInput encoderOutput decoderPos decoderAttentionMask crossAttentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput headOutput encoderOutput) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput input decoderInput) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorOutputDevice, decoderInput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor targetGradient targetLayout targetDevice targetDataType (Torch.GraduallyTyped.Tensor.Indexing.IndexDims ('Torch.GraduallyTyped.Tensor.Indexing.Indices '[ 'Torch.GraduallyTyped.Tensor.Indexing.SliceAll, 'Torch.GraduallyTyped.Tensor.Indexing.SliceUpTo ('Torch.GraduallyTyped.Index.Type.NegativeIndex 1)]) targetShape), decoderOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor doGradient doLayout doDevice doDataType doShape, logProbsShape GHC.Types.~ Torch.GraduallyTyped.NN.Functional.NonLinearActivation.SoftmaxF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) doShape, Torch.GraduallyTyped.Prelude.Catch logProbsShape, unsqueezedTargetShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.UnsqueezeF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) targetShape, Torch.GraduallyTyped.Prelude.Catch unsqueezedTargetShape, gatheredLogProbsShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.GatherDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) unsqueezedTargetShape logProbsShape, Torch.GraduallyTyped.Prelude.Catch gatheredLogProbsShape, Torch.GraduallyTyped.Prelude.Catch (targetDataType Torch.GraduallyTyped.Unify.<+> 'Torch.GraduallyTyped.DType.DataType 'Torch.GraduallyTyped.DType.Int64), logLikelihoodShape GHC.Types.~ Torch.GraduallyTyped.Tensor.IndexingSlicingJoining.SqueezeDimF ('Torch.GraduallyTyped.Shape.Type.SelectDim ('Torch.GraduallyTyped.Shape.Type.ByIndex 2)) gatheredLogProbsShape, Torch.GraduallyTyped.Prelude.Catch logLikelihoodShape, Torch.GraduallyTyped.Tensor.MathOperations.Reduction.MeanAllCheckF logLikelihoodShape, loss GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor (targetGradient Torch.GraduallyTyped.Unify.<|> doGradient) (targetLayout Torch.GraduallyTyped.Unify.<+> doLayout) (targetDevice Torch.GraduallyTyped.Unify.<+> doDevice) doDataType ('Torch.GraduallyTyped.Shape.Type.Shape '[]), generatorOutputDevice GHC.Types.~ generatorDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingInput input (Torch.GraduallyTyped.Tensor.Type.Tensor targetGradient targetLayout targetDevice targetDataType targetShape)) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerTrainingOutput loss) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward mkPaddingMask input generatorDevice inputPaddingMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkAttentionMask inputPaddingMask generatorDevice attentionMask generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward mkPos input generatorDevice pos generatorDevice, Torch.GraduallyTyped.NN.Class.HasForward model (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput) generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerInput' input) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.SimplifiedEncoderDecoderTransformerOutput' encoderOutput inputPaddingMask) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasForward sharedEmbedding input generatorDevice embeddingOutput embeddingGeneratorOutputDevice, embeddingOutput GHC.Types.~ Torch.GraduallyTyped.Tensor.Type.Tensor requiresGradient' layout' device' dataType' shape', Torch.GraduallyTyped.NN.Class.HasForward encoder (embeddingOutput, pos, attentionMask) embeddingGeneratorOutputDevice encoderOutput generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasForward (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head) (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerInput' input pos attentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerOutput' encoderOutput) generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasInitialize model generatorDevice model' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize mkPos generatorDevice0 mkPos' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize mkDecoderPos generatorDevice1 mkDecoderPos' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize mkPaddingMask generatorDevice2 mkPaddingMask' generatorDevice3, Torch.GraduallyTyped.NN.Class.HasInitialize mkAttentionMask generatorDevice3 mkAttentionMask' generatorDevice4, Torch.GraduallyTyped.NN.Class.HasInitialize mkCrossAttentionMask generatorDevice4 mkCrossAttentionMask' generatorDevice5, Torch.GraduallyTyped.NN.Class.HasInitialize mkDecoderAttentionMask generatorDevice5 mkDecoderAttentionMask' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model' mkPos' mkDecoderPos' mkPaddingMask' mkAttentionMask' mkCrossAttentionMask' mkDecoderAttentionMask') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict model, Torch.GraduallyTyped.NN.Class.HasStateDict mkPos, Torch.GraduallyTyped.NN.Class.HasStateDict mkDecoderPos, Torch.GraduallyTyped.NN.Class.HasStateDict mkPaddingMask, Torch.GraduallyTyped.NN.Class.HasStateDict mkAttentionMask, Torch.GraduallyTyped.NN.Class.HasStateDict mkCrossAttentionMask, Torch.GraduallyTyped.NN.Class.HasStateDict mkDecoderAttentionMask) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GSimplifiedEncoderDecoderTransformer model mkPos mkDecoderPos mkPaddingMask mkAttentionMask mkCrossAttentionMask mkDecoderAttentionMask)
instance (Torch.GraduallyTyped.NN.Class.HasInitialize encoder generatorDevice encoder' generatorDevice0, Torch.GraduallyTyped.NN.Class.HasInitialize decoder generatorDevice0 decoder' generatorDevice1, Torch.GraduallyTyped.NN.Class.HasInitialize sharedEmbedding generatorDevice1 sharedEmbedding' generatorDevice2, Torch.GraduallyTyped.NN.Class.HasInitialize head generatorDevice2 head' generatorOutputDevice) => Torch.GraduallyTyped.NN.Class.HasInitialize (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head) generatorDevice (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder' decoder' sharedEmbedding' head') generatorOutputDevice
instance (Torch.GraduallyTyped.NN.Class.HasStateDict encoder, Torch.GraduallyTyped.NN.Class.HasStateDict decoder, Torch.GraduallyTyped.NN.Class.HasStateDict sharedEmbedding, Torch.GraduallyTyped.NN.Class.HasStateDict head) => Torch.GraduallyTyped.NN.Class.HasStateDict (Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.GEncoderDecoderTransformer inputEmbedDim encoder decoder sharedEmbedding head)
instance Torch.GraduallyTyped.NN.Class.HasInitialize Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling generatorDevice Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling generatorDevice
instance Torch.GraduallyTyped.NN.Class.HasStateDict Torch.GraduallyTyped.NN.Transformer.GEncoderDecoder.EncoderDecoderTransformerHasEmbedScaling

module Torch.GraduallyTyped.NN.Transformer.T5.Common

-- | T5 dType.
type T5DType = 'Float

-- | T5 dType singleton.
t5DType :: SDType T5DType

-- | T5 data type.
type T5DataType = 'DataType T5DType

-- | T5 data type singleton.
t5DataType :: SDataType T5DataType

-- | T5 dropout rate. 'dropout_rate = 0.1'
t5DropoutP :: Double

-- | T5 relative positional encoding bucket dimension.
--   'relative_attention_num_buckets = 32'
type T5RelPosEncBucketDim = 'Dim ('Name "*") ('Size 32)

-- | T5 relative positional encoding bucket dimension singleton.
t5RelPosEncBucketDim :: SDim T5RelPosEncBucketDim

-- | T5 layer-norm epsilon. 'layer_norm_epsilon = 1e-06'
t5Eps :: Double

-- | T5 maximum distance for relative positional encoding.
t5MaxDistance :: Int

-- | T5 padding token id. 'pad_token_id = 0'
t5PadTokenId :: Int

-- | T5 begin-of-sentence token id.
t5BOSTokenId :: Int

-- | T5 end-of-sentence token id. 'eos_token_id = 1'
t5EOSTokenId :: Int

-- | T5 attention mask bias
t5AttentionMaskBias :: Double

-- | Specifies a T5 or ByT5 model.
type family T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout

-- | Specifies the parameters of a T5 or ByT5 model.
--   
--   <ul>
--   <li><tt>transformerHead</tt>: the head of the T5 or ByT5 model.</li>
--   <li><tt>numLayers</tt>: the number of layers in the T5 or ByT5
--   model.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the T5 or
--   ByT5 model.</li>
--   <li><tt>device</tt>: the computational device on which the T5 or ByT5
--   model parameters are to be allocated.</li>
--   </ul>
t5ModelSpec :: forall style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout. (SingI headDim, SingI headEmbedDim, SingI embedDim, SingI inputEmbedDim, SingI ffnDim, SingI vocabDim) => STransformerStyle style -> STransformerHead transformerHead -> SNat numEncoderLayers -> SNat numDecoderLayers -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (T5ModelF style transformerHead numEncoderLayers numDecoderLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout)
mkT5Input :: forall batchDim seqDim device m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, Catch ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> SDevice device -> [[Int]] -> m output

module Torch.GraduallyTyped.NN.Transformer.T5.ThreeB

-- | T5-3B number of layers. 'num_layers = 24'
type T5ThreeBNumLayers = 24

-- | T5-3B number of layers singleton.
t5ThreeBNumLayers :: SNat T5ThreeBNumLayers

-- | T5-3B number of attention heads. 'n_heads = 32'
type T5ThreeBHeadDim = 'Dim ('Name "*") ('Size 32)

-- | T5-3B head embedding dimension. 'd_kv = 128'
type T5ThreeBHeadEmbedDim = 'Dim ('Name "*") ('Size 128)

-- | T5-3B embedding dimension. 'inner_dim = n_heads * d_kv = 4096'
type T5ThreeBEmbedDim = 'Dim ('Name "*") ('Size 4096)

-- | T5-3B model dimension. 'd_model = 1024'
type T5ThreeBInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-3B feed-forward network dimension. 'd_ff = 16384'
type T5ThreeBFFNDim = 'Dim ('Name "*") ('Size 16384)

-- | T5-3B vocabulary dimension. 'vocab_size = 32128'
type T5ThreeBVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-3B model.
type T5ThreeB (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = T5ModelF 'T5 transformerHead T5ThreeBNumLayers T5ThreeBNumLayers gradient device T5ThreeBHeadDim T5ThreeBHeadEmbedDim T5ThreeBEmbedDim T5ThreeBInputEmbedDim T5ThreeBFFNDim T5ThreeBVocabDim hasDropout

-- | T5-3B model specification.
t5ThreeBSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (T5ThreeB transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.T5.Small

-- | T5-Small number of layers. 'num_layers = 6'
type T5SmallNumLayers = 6

-- | T5-Small number of layers singleton.
t5SmallNumLayers :: SNat T5SmallNumLayers

-- | T5-Small number of attention heads. 'n_heads = 8'
type T5SmallHeadDim = 'Dim ('Name "*") ('Size 8)

-- | T5-Small head embedding dimension. 'd_kv = 64'
type T5SmallHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | T5-Small embedding dimension. 'inner_dim = n_heads * d_kv = 512'
type T5SmallEmbedDim = 'Dim ('Name "*") ('Size 512)

-- | T5-Small model dimension. 'd_model = 512'
type T5SmallInputEmbedDim = 'Dim ('Name "*") ('Size 512)

-- | T5-Small feed-forward network dimension. 'd_ff = 2048'
type T5SmallFFNDim = 'Dim ('Name "*") ('Size 2048)

-- | T5-Small vocabulary dimension. 'vocab_size = 32128'
type T5SmallVocabDim = 'Dim ('Name "*") ('Size 32128)
t5SmallVocabDim :: SDim T5SmallVocabDim

-- | T5-Small model.
type T5Small (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = T5ModelF 'T5 transformerHead T5SmallNumLayers T5SmallNumLayers gradient device T5SmallHeadDim T5SmallHeadEmbedDim T5SmallEmbedDim T5SmallInputEmbedDim T5SmallFFNDim T5SmallVocabDim hasDropout

-- | T5-Small model specification.
t5SmallSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (T5Small transformerHead gradient device hasDropout)

-- | ByT5-Small number of encoder layers. 'num_layers = 12'
type ByT5SmallNumEncoderLayers = 12

-- | ByT5-Small number of encoder layers singleton.
byT5SmallNumEncoderLayers :: SNat ByT5SmallNumEncoderLayers

-- | ByT5-Small number of decoder layers. 'num_decoder_layers = 4'
type ByT5SmallNumDecoderLayers = 4

-- | ByT5-Small number of encoder layers singleton.
byT5SmallNumDecoderLayers :: SNat ByT5SmallNumDecoderLayers

-- | ByT5-Small number of attention heads. 'n_heads = 6'
type ByT5SmallHeadDim = 'Dim ('Name "*") ('Size 6)

-- | ByT5-Small head embedding dimension. 'd_kv = 64'
type ByT5SmallHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | ByT5-Small embedding dimension. 'inner_dim = n_heads * d_kv = 384'
type ByT5SmallEmbedDim = 'Dim ('Name "*") ('Size 384)

-- | ByT5-Small model dimension. 'd_model = 1472'
type ByT5SmallInputEmbedDim = 'Dim ('Name "*") ('Size 1472)

-- | T5-Small feed-forward network dimension. 'd_ff = 3584'
type ByT5SmallFFNDim = 'Dim ('Name "*") ('Size 3584)

-- | T5-Small vocabulary dimension. 'vocab_size = 384'
type ByT5SmallVocabDim = 'Dim ('Name "*") ('Size 384)

-- | ByT5-Small model.
type ByT5Small (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = T5ModelF 'ByT5 transformerHead ByT5SmallNumEncoderLayers ByT5SmallNumDecoderLayers gradient device ByT5SmallHeadDim ByT5SmallHeadEmbedDim ByT5SmallEmbedDim ByT5SmallInputEmbedDim ByT5SmallFFNDim ByT5SmallVocabDim hasDropout

-- | ByT5-Small model specification.
byT5SmallSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (ByT5Small transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.T5.Large

-- | T5-Large number of layers. 'num_layers = 24'
type T5LargeNumLayers = 24

-- | T5-Large number of layers singleton.
t5LargeNumLayers :: SNat T5LargeNumLayers

-- | T5-Large number of attention heads. 'n_heads = 16'
type T5LargeHeadDim = 'Dim ('Name "*") ('Size 16)

-- | T5-Large head embedding dimension. 'd_kv = 64'
type T5LargeHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | T5-Large embedding dimension. 'inner_dim = n_heads * d_kv = 1024'
type T5LargeEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-Large model dimension. 'd_model = 1024'
type T5LargeInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-Large feed-forward network dimension. 'd_ff = 4096'
type T5LargeFFNDim = 'Dim ('Name "*") ('Size 4096)

-- | T5-Large vocabulary dimension. 'vocab_size = 32128'
type T5LargeVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-Large model.
type T5Large (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = T5ModelF 'T5 transformerHead T5LargeNumLayers T5LargeNumLayers gradient device T5LargeHeadDim T5LargeHeadEmbedDim T5LargeEmbedDim T5LargeInputEmbedDim T5LargeFFNDim T5LargeVocabDim hasDropout

-- | T5-Large model specification.
t5LargeSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (T5Large transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.T5.ElevenB

-- | T5-11B number of layers. 'num_layers = 24'
type T5ElevenBNumLayers = 24

-- | T5-11B number of layers singleton.
t5ElevenBNumLayers :: SNat T5ElevenBNumLayers

-- | T5-11B number of attention heads. 'n_heads = 128'
type T5ElevenBHeadDim = 'Dim ('Name "*") ('Size 128)

-- | T5-11B head embedding dimension. 'd_kv = 128'
type T5ElevenBHeadEmbedDim = 'Dim ('Name "*") ('Size 128)

-- | T5-11B embedding dimension. 'inner_dim = n_heads * d_kv = 16384'
type T5ElevenBEmbedDim = 'Dim ('Name "*") ('Size 16384)

-- | T5-11B model dimension. 'd_model = 1024'
type T5ElevenBInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | T5-11B feed-forward network dimension. 'd_ff = 65536'
type T5ElevenBFFNDim = 'Dim ('Name "*") ('Size 65536)

-- | T5-11B vocabulary dimension. 'vocab_size = 32128'
type T5ElevenBVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-11B model.
type T5ElevenB (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = T5ModelF 'T5 transformerHead T5ElevenBNumLayers T5ElevenBNumLayers gradient device T5ElevenBHeadDim T5ElevenBHeadEmbedDim T5ElevenBEmbedDim T5ElevenBInputEmbedDim T5ElevenBFFNDim T5ElevenBVocabDim hasDropout

-- | T5-11B model specification.
t5ElevenBSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (T5ElevenB transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.T5.Base

-- | T5-Base number of layers. 'num_layers = 12'
type T5BaseNumLayers = 12

-- | T5-Base number of layers singleton.
t5BaseNumLayers :: SNat T5BaseNumLayers

-- | T5-Base number of attention heads. 'n_heads = 12'
type T5BaseHeadDim = 'Dim ('Name "*") ('Size 12)

-- | T5-Base head embedding dimension. 'd_kv = 64'
type T5BaseHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | T5-Base embedding dimension. 'inner_dim = n_heads * d_kv = 768'
type T5BaseEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | T5-Base model dimension. 'd_model = 768'
type T5BaseInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | T5-Base feed-forward network dimension. 'd_ff = 3072'
type T5BaseFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | T5-Base vocabulary dimension. 'vocab_size = 32128'
type T5BaseVocabDim = 'Dim ('Name "*") ('Size 32128)

-- | T5-Base model.
type T5Base (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = T5ModelF 'T5 transformerHead T5BaseNumLayers T5BaseNumLayers gradient device T5BaseHeadDim T5BaseHeadEmbedDim T5BaseEmbedDim T5BaseInputEmbedDim T5BaseFFNDim T5BaseVocabDim hasDropout

-- | T5-Base model specification.
t5BaseSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (T5Base transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.Pegasus.Common

-- | Pegasus dType.
type PegasusDType = 'Float

-- | Pegasus dType singleton.
pegasusDType :: SDType PegasusDType

-- | Pegasus data type.
type PegasusDataType = 'DataType PegasusDType

-- | Pegasus data type singleton.
pegasusDataType :: SDataType PegasusDataType

-- | Pegasus dropout rate. 'dropout_rate = 0.1'
pegasusDropoutP :: Double

-- | Pegasus positional encoding dimension.
type PegasusPosEncDim = 'Dim ('Name "*") ('Size 512)

-- | Pegasus positional encoding dimension singleton.
pegasusPosEncDim :: SDim PegasusPosEncDim

-- | Pegasus layer-norm epsilon.
pegasusEps :: Double

-- | Pegasus maximum number of position embeddings.
--   'max_position_embeddings = 512'
pegasusMaxPositionEmbeddings :: Int

-- | Pegasus padding token id. 'pad_token_id = 0'
pegasusPadTokenId :: Int

-- | Pegasus begin-of-sentence token id. 'bos_token_id = 0'
pegasusBOSTokenId :: Int

-- | Pegasus end-of-sentence token id. 'eos_token_id = 0'
pegasusEOSTokenId :: Int

-- | Pegasus attention mask bias
pegasusAttentionMaskBias :: Double

-- | Specifies the Pegasus model.
type family PegasusModelF (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a Pegasus model.
--   
--   <ul>
--   <li><tt>transformerHead</tt>: the head of the Pegasus model.</li>
--   <li><tt>numLayers</tt>: the number of layers in the Pegasus
--   model.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the Pegasus
--   model.</li>
--   <li><tt>device</tt>: the computational device on which the Pegasus
--   model parameters are to be allocated.</li>
--   </ul>
pegasusModelSpec :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout. (SingI headDim, SingI headEmbedDim, SingI embedDim, SingI inputEmbedDim, SingI ffnDim, SingI vocabDim) => STransformerHead transformerHead -> SNat numLayers -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (PegasusModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout)
mkPegasusInput :: forall batchDim seqDim device m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, Catch ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> SDevice device -> [[Int]] -> m output

module Torch.GraduallyTyped.NN.Transformer.Pegasus.XSum

-- | Pegasus-XSum number of layers. 'encoder_layers = 16' 'decoder_layers =
--   16'
type PegasusXSumNumLayers = 16
pegasusXSumNumLayers :: SNat PegasusXSumNumLayers

-- | Pegasus-XSum number of attention heads. 'encoder_attention_heads = 16'
--   'decoder_attention_heads = 16'
type PegasusXSumHeadDim = 'Dim ('Name "*") ('Size 16)

-- | Pegasus-XSum head embedding dimension. 'd_kv = 64'
type PegasusXSumHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | Pegasus-XSum embedding dimension. 'hidden_size = n_heads * d_kv =
--   1024'
type PegasusXSumEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | Pegasus-XSum model dimension. 'd_model = 1024'
type PegasusXSumInputEmbedDim = 'Dim ('Name "*") ('Size 1024)
pegasusXSumInputEmbedDim :: SDim PegasusXSumInputEmbedDim

-- | Pegasus-XSum feed-forward network dimension. 'encoder_ffn_dim = 4096'
--   'decoder_ffn_dim = 4096'
type PegasusXSumFFNDim = 'Dim ('Name "*") ('Size 4096)

-- | Pegasus-XSum vocabulary dimension. 'vocab_size = 96103'
type PegasusXSumVocabDim = 'Dim ('Name "*") ('Size 96103)

-- | Pegasus-XSum vocabulary dimension singleton.
pegasusXSumVocabDim :: SDim PegasusXSumVocabDim

-- | Pegasus-XSum model.
type PegasusXSum (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = PegasusModelF transformerHead PegasusXSumNumLayers gradient device PegasusXSumHeadDim PegasusXSumHeadEmbedDim PegasusXSumEmbedDim PegasusXSumInputEmbedDim PegasusXSumFFNDim PegasusXSumVocabDim hasDropout

-- | Pegasus-XSum model specification.
pegasusXSumSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (PegasusXSum transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.Pegasus

module Torch.GraduallyTyped.NN.Transformer.Generation
decode :: Monad m => (x -> s -> m (Maybe (x, s))) -> x -> s -> m (x, s)
sedtOutputToInput :: Monad m => Lens (SimplifiedEncoderDecoderTransformerOutput logits encoderOutput decoderInput inputPaddingMask) (m (SimplifiedEncoderDecoderTransformerGenerationInput decoderInput' encoderOutput inputPaddingMask)) (logits, decoderInput) (m decoderInput')
prepNext :: (logits ~ Tensor logitsGradient logitsLayout logitsDevice logitsDataType logitsShape, ntShape' ~ UnsqueezeF ('SelectDim ('ByIndex 1)) ntShape, Catch ntShape', tensors ~ '[decoderInput, Tensor ntGradient ntLayout ntDevice ntDataType ntShape'], decoderInput' ~ CatHListF ('SelectDim ('ByIndex 1)) tensors, Castable decoderInput' (ForeignPtr Tensor), Castable (HList tensors) (ForeignPtr TensorList), MonadThrow m) => Lens (logits, decoderInput) (m decoderInput') logits (m (Tensor ntGradient ntLayout ntDevice ntDataType ntShape))
greedyNextTokens :: (nextTokenLogitsShape ~ IndexDims ('Indices '[ 'SliceAll, 'SliceAt ('NegativeIndex 1), 'SliceAll]) logitsShape, nextTokensShape ~ ArgmaxF ('SelectDim ('ByIndex 1)) nextTokenLogitsShape, Catch nextTokensShape, nextTokensShape' ~ SqueezeDimF ('SelectDim ('ByIndex 1)) nextTokensShape, ntShape ~ 'Shape '[ntDim], Catch (nextTokensShape' <+> ntShape), SGetShape nextTokensShape', SGetDim ntDim, Catch ntDim, Catch nextTokensShape', MonadThrow m, MonadState (Tensor ('Gradient 'WithoutGradient) logitsLayout logitsDevice ('DataType 'Int64) ntShape) m, SGetDevice logitsDevice, SGetLayout logitsLayout) => Int -> Int -> Tensor logitsGradient logitsLayout logitsDevice logitsDataType logitsShape -> m (Tensor ('Gradient 'WithoutGradient) logitsLayout logitsDevice ('DataType 'Int64) ntShape)
allSequencesFinished :: (SGetLayout usLayout, SGetDevice usDevice, MonadThrow m, Catch (usDataType <+> 'DataType 'Int64), Catch (BroadcastShapesF usShape ('Shape '[])), MaxAllCheckF usShape) => Tensor usGradient usLayout usDevice usDataType usShape -> m Bool
applyUnfinishedSequences :: (MonadThrow m, kntDataType ~ (usDataType <+> ntDataType), kntShape ~ BroadcastShapesF usShape ntShape, Catch kntShape, ntGradient' ~ (usGradient <|> ntGradient), ntLayout' ~ ((usLayout <+> ntLayout) <+> usLayout), ntDevice' ~ ((usDevice <+> ntDevice) <+> usDevice), ntDataType' ~ ((usDataType <+> ntDataType) <+> usDataType), ntShape' ~ BroadcastShapesF kntShape usShape, Catch ntShape') => Int -> Tensor usGradient usLayout usDevice usDataType usShape -> Tensor ntGradient ntLayout ntDevice ntDataType ntShape -> m (Tensor ntGradient' ntLayout' ntDevice' ntDataType' ntShape')
updateUnfinishedSequences :: (Catch (ntDataType <+> 'DataType 'Int64), Catch (BroadcastShapesF ntShape ('Shape '[])), Catch usShape', SGetDataType usDataType, SGetDevice ntDevice, SGetLayout ntLayout, MonadThrow m, BroadcastShapesF usShape (BroadcastShapesF ntShape ('Shape '[])) ~ usShape', (usGradient <|> 'Gradient 'WithoutGradient) ~ usGradient', (usLayout <+> ntLayout) ~ usLayout', (usDevice <+> ntDevice) ~ usDevice') => Int -> Tensor ntGradient ntLayout ntDevice ntDataType ntShape -> Tensor usGradient usLayout usDevice usDataType usShape -> m (Tensor usGradient' usLayout' usDevice' usDataType usShape')

module Torch.GraduallyTyped.NN.Transformer.BART.Common

-- | BART dType.
type BARTDType = 'Float

-- | BART dType singleton.
bartDType :: SDType BARTDType

-- | BART data type.
type BARTDataType = 'DataType BARTDType

-- | BART data type singleton.
bartDataType :: SDataType BARTDataType

-- | BART dropout rate. 'dropout_rate = 0.1'
bartDropoutP :: Double

-- | BART positional encoding dimension.
type BARTPosEncDim = 'Dim ('Name "*") ('Size 1026)

-- | BART positional encoding dimension singleton.
bartPosEncDim :: SDim BARTPosEncDim

-- | BART layer-norm epsilon.
bartEps :: Double

-- | BART maximum number of position embeddings. 'max_position_embeddings =
--   1024'
bartMaxPositionEmbeddings :: Int

-- | BART padding token id. 'pad_token_id = 1'
bartPadTokenId :: Int

-- | BART begin-of-sentence token id. 'bos_token_id = 0'
bartBOSTokenId :: Int

-- | BART end-of-sentence token id. 'eos_token_id = 2'
bartEOSTokenId :: Int

-- | BART attention mask bias
bartAttentionMaskBias :: Double

-- | Specifies the BART model.
type family BARTModelF (transformerHead :: TransformerHead) (numLayers :: Nat) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (headDim :: Dim (Name Symbol) (Size Nat)) (headEmbedDim :: Dim (Name Symbol) (Size Nat)) (embedDim :: Dim (Name Symbol) (Size Nat)) (inputEmbedDim :: Dim (Name Symbol) (Size Nat)) (ffnDim :: Dim (Name Symbol) (Size Nat)) (vocabDim :: Dim (Name Symbol) (Size Nat)) (hasDropout :: HasDropout) :: Type

-- | Specifies the parameters of a BART model.
--   
--   <ul>
--   <li><tt>transformerHead</tt>: the head of the BART model.</li>
--   <li><tt>numLayers</tt>: the number of layers in the BART model.</li>
--   <li><tt>gradient</tt>: whether to compute the gradient of the BART
--   model.</li>
--   <li><tt>device</tt>: the computational device on which the BART model
--   parameters are to be allocated.</li>
--   </ul>
bartModelSpec :: forall transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout. (SingI headDim, SingI headEmbedDim, SingI embedDim, SingI inputEmbedDim, SingI ffnDim, SingI vocabDim) => STransformerHead transformerHead -> SNat numLayers -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (BARTModelF transformerHead numLayers gradient device headDim headEmbedDim embedDim inputEmbedDim ffnDim vocabDim hasDropout)
mkBARTInput :: forall batchDim seqDim device m output. (MonadThrow m, SGetDim batchDim, SGetDim seqDim, Catch ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize] <+> 'Shape '[batchDim, seqDim]), output ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) device ('DataType 'Int64) ('Shape '[batchDim, seqDim])) => SDim batchDim -> SDim seqDim -> SDevice device -> [[Int]] -> m output

module Torch.GraduallyTyped.NN.Transformer.BART.Large

-- | BART-Large number of layers. 'encoder_layers = 12' 'decoder_layers =
--   12'
type BARTLargeNumLayers = 12

-- | BART-Large number of layers singleton.
bartLargeNumLayers :: SNat BARTLargeNumLayers

-- | BART-Large number of attention heads. 'encoder_attention_heads = 16'
--   'decoder_attention_heads = 16'
type BARTLargeHeadDim = 'Dim ('Name "*") ('Size 16)

-- | BART-Large number of attention heads singleton.
bartLargeHeadDim :: SDim BARTLargeHeadDim

-- | BART-Large head embedding dimension. 'd_kv = 64'
type BARTLargeHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | BART-Large head embedding dimension singleton.
bartLargeHeadEmbedDim :: SDim BARTLargeHeadEmbedDim

-- | BART-Large embedding dimension. 'hidden_size = n_heads * d_kv = 1024'
type BARTLargeEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | BART-Large embedding dimension singleton.
bartLargeEmbedDim :: SDim BARTLargeEmbedDim

-- | BART-Large model dimension. 'd_model = 1024'
type BARTLargeInputEmbedDim = 'Dim ('Name "*") ('Size 1024)

-- | BART-Large model dimension singleton.
bartLargeInputEmbedDim :: SDim BARTLargeInputEmbedDim

-- | BART-Large feed-forward network dimension. 'encoder_ffn_dim = 4096'
--   'decoder_ffn_dim = 4096'
type BARTLargeFFNDim = 'Dim ('Name "*") ('Size 4096)

-- | BART-Large feed-forward network dimension singleton.
bartLargeFFNDim :: SDim BARTLargeFFNDim

-- | BART-Large vocabulary dimension. 'vocab_size = 50265'
type BARTLargeVocabDim = 'Dim ('Name "*") ('Size 50265)

-- | BART-Large vocabulary dimension singleton.
bartLargeVocabDim :: SDim BARTLargeVocabDim

-- | BART-Large model.
type BARTLarge (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = BARTModelF transformerHead BARTLargeNumLayers gradient device BARTLargeHeadDim BARTLargeHeadEmbedDim BARTLargeEmbedDim BARTLargeInputEmbedDim BARTLargeFFNDim BARTLargeVocabDim hasDropout

-- | BART-Large model specification.
bartLargeSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (BARTLarge transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.BART.Base

-- | BART-Base number of layers. 'encoder_layers = 6' 'decoder_layers = 6'
type BARTBaseNumLayers = 6

-- | BART-Base number of layers singleton.
bartBaseNumLayers :: SNat BARTBaseNumLayers

-- | BART-Base number of attention heads. 'encoder_attention_heads = 12'
--   'decoder_attention_heads = 12'
type BARTBaseHeadDim = 'Dim ('Name "*") ('Size 12)

-- | BART-Base number of attention heads singleton.
bartBaseHeadDim :: SDim BARTBaseHeadDim

-- | BART-Base head embedding dimension. 'd_kv = 64'
type BARTBaseHeadEmbedDim = 'Dim ('Name "*") ('Size 64)

-- | BART-Base head embedding dimension singleton.
bartBaseHeadEmbedDim :: SDim BARTBaseHeadEmbedDim

-- | BART-Base embedding dimension. 'hidden_size = n_heads * d_kv = 768'
type BARTBaseEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BART-Base embedding dimension singleton.
bartBaseEmbedDim :: SDim BARTBaseEmbedDim

-- | BART-Base model dimension. 'd_model = 768'
type BARTBaseInputEmbedDim = 'Dim ('Name "*") ('Size 768)

-- | BART-Base model dimension singleton.
bartBaseInputEmbedDim :: SDim BARTBaseInputEmbedDim

-- | BART-Base feed-forward network dimension. 'encoder_ffn_dim = 3072'
--   'decoder_ffn_dim = 3072'
type BARTBaseFFNDim = 'Dim ('Name "*") ('Size 3072)

-- | BART-Base feed-forward network dimension singleton.
bartBaseFFNDim :: SDim BARTBaseFFNDim

-- | BART-Base vocabulary dimension. 'vocab_size = 50265'
type BARTBaseVocabDim = 'Dim ('Name "*") ('Size 50265)

-- | BART-Base vocabulary dimension singleton.
bartBaseVocabDim :: SDim BARTBaseVocabDim

-- | BART-Base model.
type BARTBase (transformerHead :: TransformerHead) (gradient :: Gradient RequiresGradient) (device :: Device (DeviceType Nat)) (hasDropout :: HasDropout) = BARTModelF transformerHead BARTBaseNumLayers gradient device BARTBaseHeadDim BARTBaseHeadEmbedDim BARTBaseEmbedDim BARTBaseInputEmbedDim BARTBaseFFNDim BARTBaseVocabDim hasDropout

-- | BART-Base model specification.
bartBaseSpec :: STransformerHead transformerHead -> SGradient gradient -> SDevice device -> SHasDropout hasDropout -> ModelSpec (BARTBase transformerHead gradient device hasDropout)

module Torch.GraduallyTyped.NN.Transformer.BART


-- | This module contains bits and pieces to work with the Spider SQL
--   language.
--   
--   TODO: * don't accept reserved keywords in names without quoting *
--   aliases have to be defined in scope, otherwise fall back to table id *
--   don't define an alias twice in the same scope * optionally (?) use the
--   schema to constrain column and table names * test the parser(s) on
--   more examples * pretty printing * random generation of
--   <a>SpiderSQL</a> values
module Torch.Language.SpiderSQL
data SpiderSQL
SpiderSQL :: Select -> From -> Maybe Cond -> [ColUnit] -> Maybe OrderBy -> Maybe Cond -> Maybe Int -> Maybe SpiderSQL -> Maybe SpiderSQL -> Maybe SpiderSQL -> SpiderSQL
[spiderSQLSelect] :: SpiderSQL -> Select
[spiderSQLFrom] :: SpiderSQL -> From
[spiderSQLWhere] :: SpiderSQL -> Maybe Cond
[spiderSQLGroupBy] :: SpiderSQL -> [ColUnit]
[spiderSQLOrderBy] :: SpiderSQL -> Maybe OrderBy
[spiderSQLHaving] :: SpiderSQL -> Maybe Cond
[spiderSQLLimit] :: SpiderSQL -> Maybe Int
[spiderSQLIntersect] :: SpiderSQL -> Maybe SpiderSQL
[spiderSQLExcept] :: SpiderSQL -> Maybe SpiderSQL
[spiderSQLUnion] :: SpiderSQL -> Maybe SpiderSQL
data Select
Select :: [Agg] -> Select
SelectDistinct :: [Agg] -> Select
data From
From :: [TableUnit] -> Maybe Cond -> From
[fromTableUnits] :: From -> [TableUnit]
[fromCond] :: From -> Maybe Cond
data Cond
And :: Cond -> Cond -> Cond
Or :: Cond -> Cond -> Cond
Not :: Cond -> Cond
Between :: ValUnit -> Val -> Val -> Cond
Eq :: ValUnit -> Val -> Cond
Gt :: ValUnit -> Val -> Cond
Lt :: ValUnit -> Val -> Cond
Ge :: ValUnit -> Val -> Cond
Le :: ValUnit -> Val -> Cond
Ne :: ValUnit -> Val -> Cond
In :: ValUnit -> Val -> Cond
Like :: ValUnit -> Val -> Cond
data ColUnit
ColUnit :: AggType -> Maybe (Either Alias TableId) -> ColumnId -> ColUnit
[colUnitAggId] :: ColUnit -> AggType
[colUnitTable] :: ColUnit -> Maybe (Either Alias TableId)
[colUnitColId] :: ColUnit -> ColumnId
DistinctColUnit :: AggType -> Maybe (Either Alias TableId) -> ColumnId -> ColUnit
[distinctColUnitAggId] :: ColUnit -> AggType
[distinctColUnitTable] :: ColUnit -> Maybe (Either Alias TableId)
[distinctColUnitColdId] :: ColUnit -> ColumnId
data OrderBy
OrderBy :: OrderByOrder -> [ValUnit] -> OrderBy
data OrderByOrder
Asc :: OrderByOrder
Desc :: OrderByOrder
data Agg
Agg :: AggType -> ValUnit -> Agg
data TableUnit
TableUnitSQL :: SpiderSQL -> Maybe Alias -> TableUnit
Table :: TableId -> Maybe Alias -> TableUnit
data ValUnit
Column :: ColUnit -> ValUnit
Minus :: ColUnit -> ColUnit -> ValUnit
Plus :: ColUnit -> ColUnit -> ValUnit
Times :: ColUnit -> ColUnit -> ValUnit
Divide :: ColUnit -> ColUnit -> ValUnit
data Val
ValColUnit :: ColUnit -> Val
Number :: Double -> Val
ValString :: String -> Val
ValSQL :: SpiderSQL -> Val
Terminal :: Val
data AggType
NoneAggOp :: AggType
Max :: AggType
Min :: AggType
Count :: AggType
Sum :: AggType
Avg :: AggType
data ColumnId
Star :: ColumnId
ColumnId :: String -> ColumnId
newtype TableId
TableId :: String -> TableId
newtype Alias
Alias :: String -> Alias

-- | <tt>keyword k</tt> is a parser that consumes <a>Char</a> tokens and
--   yields them if and only if they assemble the <a>String</a> <tt>s</tt>.
--   The parser is not sensitive to letter casing.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (isKeyword "mykeyword") "MYKEYWORD"
--   ("MYKEYWORD","")
--   </pre>
isKeyword :: CharParsing m => String -> m String
isSelect :: CharParsing m => m String
isDistinct :: CharParsing m => m String
isStar :: CharParsing m => m String
isComma :: CharParsing m => m String
isDot :: CharParsing m => m String
isSemicolon :: CharParsing m => m String
isEq :: CharParsing m => m String
isGt :: CharParsing m => m String
isLt :: CharParsing m => m String
isGe :: CharParsing m => m String
isLe :: CharParsing m => m String
isNe :: CharParsing m => m String
isIn :: CharParsing m => m String
isLike :: CharParsing m => m String
isBetween :: CharParsing m => m String
isAnd :: CharParsing m => m String
isOr :: CharParsing m => m String
isNot :: CharParsing m => m String
isMinus :: CharParsing m => m String
isPlus :: CharParsing m => m String
isTimes :: CharParsing m => m String
isDivide :: CharParsing m => m String
isMax :: CharParsing m => m String
isMin :: CharParsing m => m String
isCount :: CharParsing m => m String
isSum :: CharParsing m => m String
isAvg :: CharParsing m => m String
isFrom :: CharParsing m => m String
isJoin :: CharParsing m => m String
isAs :: CharParsing m => m String
isOn :: CharParsing m => m String
isWhere :: CharParsing m => m String
isGroupBy :: CharParsing m => m String
isOrderBy :: CharParsing m => m String
isAsc :: CharParsing m => m String
isDesc :: CharParsing m => m String
isHaving :: CharParsing m => m String
isLimit :: CharParsing m => m String
isIntersect :: CharParsing m => m String
isExcept :: CharParsing m => m String
isUnion :: CharParsing m => m String
betweenParentheses :: CharParsing m => m a -> m a
betweenOptionalParentheses :: CharParsing m => m a -> m a

-- | <a>Select</a> parser
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] select "select count table.*"
--   (Select [Agg Count (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "table")), colUnitColId = Star}))],"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] select "SELECT COUNT (DISTINCT t5.title)"
--   (Select [Agg Count (Column (DistinctColUnit {distinctColUnitAggId = NoneAggOp, distinctColUnitTable = Just (Left (Alias "t5")), distinctColUnitColdId = ColumnId "title"}))],"")
--   </pre>
select :: (TokenParsing m, Monad m) => m Select

-- | <a>Agg</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] agg "count table.id"
--   (Agg Count (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "table")), colUnitColId = ColumnId "id"})),"")
--   </pre>
agg :: (TokenParsing m, Monad m) => m Agg

-- | <a>AggType</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] aggType ""
--   (NoneAggOp,"")
--   </pre>
aggType :: CharParsing m => m AggType

-- | <a>ValUnit</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] valUnit "t1.stadium_id"
--   (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"}),"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head . filter (null . snd) $ parseString @[] valUnit "t1.stadium_length * t1.stadium_width"
--   (Times (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_length"}) (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_width"}),"")
--   </pre>
valUnit :: (TokenParsing m, Monad m) => m ValUnit

-- | <a>ColUnit</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] colUnit "count ( distinct my_table.* )"
--   (DistinctColUnit {distinctColUnitAggId = Count, distinctColUnitTable = Just (Left (Alias "my_table")), distinctColUnitColdId = Star},"")
--   </pre>
colUnit :: (TokenParsing m, Monad m) => m ColUnit

-- | <a>TableId</a> parser.
tableId :: CharParsing m => m TableId

-- | <a>Alias</a> parser.
alias :: CharParsing m => m Alias

-- | <a>ColumnId</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; parseString @[] columnId "*"
--   [(Star,"")]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; parseString @[] columnId "c"
--   [(ColumnId "c","")]
--   </pre>
columnId :: CharParsing m => m ColumnId
tableUnitAlias :: TableUnit -> Maybe Alias
tableUnitTableId :: TableUnit -> Maybe TableId
condAliases :: Cond -> [Alias]
condTableIds :: Cond -> [TableId]
valUnitAliases :: ValUnit -> [Alias]
valUnitTableIds :: ValUnit -> [TableId]
colUnitAlias :: ColUnit -> Maybe Alias
colUnitTableId :: ColUnit -> Maybe TableId
valAlias :: Val -> Maybe Alias
valTableId :: Val -> Maybe TableId

-- | <a>From</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] from "FROM people AS t1 JOIN pets AS t2 ON t1.pet_id = t2.pet_id"
--   (From {fromTableUnits = [Table (TableId "people") (Just (Alias "t1")),Table (TableId "pets") (Just (Alias "t2"))], fromCond = Just (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "pet_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "pet_id"})))},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] from "FROM organization AS t3 JOIN author AS t1 ON t3.oid = t1.oid JOIN writes AS t4 ON t4.aid = t1.aid JOIN publication AS t5 ON t4.pid = t5.pid JOIN conference AS t2 ON t5.cid = t2.cid"
--   (From {fromTableUnits = [Table (TableId "organization") (Just (Alias "t3")),Table (TableId "author") (Just (Alias "t1")),Table (TableId "writes") (Just (Alias "t4")),Table (TableId "publication") (Just (Alias "t5")),Table (TableId "conference") (Just (Alias "t2"))], fromCond = Just (And (And (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "oid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "oid"}))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "aid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "aid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "pid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "pid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "cid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "cid"}))))},"")
--   </pre>
from :: forall m. (TokenParsing m, Monad m) => m From

-- | <a>TableUnit</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] tableUnit "people as t1"
--   (Table (TableId "people") (Just (Alias "t1")),"")
--   </pre>
tableUnit :: (TokenParsing m, Monad m) => m TableUnit

-- | <a>Cond</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] cond "t1.stadium_id = t2.stadium_id"
--   (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "stadium_id"})),"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (cond &lt;* isToken ';') "t2.name = \"VLDB\" AND t3.name = \"University of Michigan\";"
--   (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "name"})) (ValString "VLDB")) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "name"})) (ValString "University of Michigan")),"")
--   </pre>
cond :: (TokenParsing m, Monad m) => m Cond

-- | <a>Val</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] val "count t1.stadium_id"
--   (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = ColumnId "count"})," t1.stadium_id")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] val "(select *)"
--   (ValSQL (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [], fromCond = Nothing}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing}),"")
--   </pre>
val :: (TokenParsing m, Monad m) => m Val

-- | Parser for quoted strings.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] quotedString "\"hello world\""
--   ("hello world","")
--   </pre>
quotedString :: CharParsing m => m String

-- | Parser for where clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] whereCond "where t1.id = t2.id"
--   (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "id"})),"")
--   </pre>
whereCond :: (TokenParsing m, Monad m) => m Cond

-- | Parser for group-by clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] groupBy "group by count t1.id, t2.id"
--   ([ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = ColumnId "count"}]," t1.id, t2.id")
--   </pre>
groupBy :: (TokenParsing m, Monad m) => m [ColUnit]

-- | <a>OrderBy</a> Parser.
--   
--   <pre>
--   &gt;&gt;&gt; head . filter (null . snd) $ parseString @[] orderBy "order by t1.stadium_id, t2.pet_id desc"
--   (OrderBy Desc [Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"}),Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "pet_id"})],"")
--   </pre>
orderBy :: (TokenParsing m, Monad m) => m OrderBy

-- | Parser for having clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] havingCond "having count(t1.customer_id) = 10"
--   (Eq (Column (ColUnit {colUnitAggId = Count, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "customer_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = ColumnId "10"})),"")
--   </pre>
havingCond :: (TokenParsing m, Monad m) => m Cond

-- | Parser for limit clauses.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] limit "limit 10"
--   (10,"")
--   </pre>
limit :: (TokenParsing m, Monad m) => m Int

-- | <a>SpiderSQL</a> parser.
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "select * ;"
--   (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [], fromCond = Nothing}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "select * from concert;"
--   (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [Table (TableId "concert") Nothing], fromCond = Nothing}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "select T2.name, count(*) from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id group by t1.stadium_id;"
--   (SpiderSQL {spiderSQLSelect = Select [Agg NoneAggOp (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "T2")), colUnitColId = ColumnId "name"})),Agg NoneAggOp (Column (ColUnit {colUnitAggId = Count, colUnitTable = Nothing, colUnitColId = Star}))], spiderSQLFrom = From {fromTableUnits = [Table (TableId "concert") (Just (Alias "t1")),Table (TableId "stadium") (Just (Alias "t2"))], fromCond = Just (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "stadium_id"})))}, spiderSQLWhere = Nothing, spiderSQLGroupBy = [ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "stadium_id"}], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; head $ parseString @[] (spiderSQL &lt;* spaces &lt;* isSemicolon) "SELECT COUNT ( DISTINCT t5.title ) FROM organization AS t3 JOIN author AS t1 ON t3.oid = t1.oid JOIN writes AS t4 ON t4.aid = t1.aid JOIN publication AS t5 ON t4.pid = t5.pid JOIN conference AS t2 ON t5.cid = t2.cid WHERE t2.name = \"VLDB\" AND t3.name = \"University of Michigan\";"
--   (SpiderSQL {spiderSQLSelect = Select [Agg Count (Column (DistinctColUnit {distinctColUnitAggId = NoneAggOp, distinctColUnitTable = Just (Left (Alias "t5")), distinctColUnitColdId = ColumnId "title"}))], spiderSQLFrom = From {fromTableUnits = [Table (TableId "organization") (Just (Alias "t3")),Table (TableId "author") (Just (Alias "t1")),Table (TableId "writes") (Just (Alias "t4")),Table (TableId "publication") (Just (Alias "t5")),Table (TableId "conference") (Just (Alias "t2"))], fromCond = Just (And (And (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "oid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "oid"}))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "aid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t1")), colUnitColId = ColumnId "aid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t4")), colUnitColId = ColumnId "pid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "pid"})))) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t5")), colUnitColId = ColumnId "cid"})) (ValColUnit (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "cid"}))))}, spiderSQLWhere = Just (And (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t2")), colUnitColId = ColumnId "name"})) (ValString "VLDB")) (Eq (Column (ColUnit {colUnitAggId = NoneAggOp, colUnitTable = Just (Left (Alias "t3")), colUnitColId = ColumnId "name"})) (ValString "University of Michigan"))), spiderSQLGroupBy = [], spiderSQLOrderBy = Nothing, spiderSQLHaving = Nothing, spiderSQLLimit = Nothing, spiderSQLIntersect = Nothing, spiderSQLExcept = Nothing, spiderSQLUnion = Nothing},"")
--   </pre>
spiderSQL :: (TokenParsing m, Monad m) => m SpiderSQL

-- | Auxiliary parser for table names, column names, and aliases.
name :: CharParsing m => m String
instance GHC.Show.Show Torch.Language.SpiderSQL.OrderByOrder
instance GHC.Classes.Eq Torch.Language.SpiderSQL.OrderByOrder
instance GHC.Show.Show Torch.Language.SpiderSQL.AggType
instance GHC.Classes.Eq Torch.Language.SpiderSQL.AggType
instance GHC.Show.Show Torch.Language.SpiderSQL.ColumnId
instance GHC.Classes.Eq Torch.Language.SpiderSQL.ColumnId
instance GHC.Show.Show Torch.Language.SpiderSQL.TableId
instance GHC.Classes.Eq Torch.Language.SpiderSQL.TableId
instance GHC.Show.Show Torch.Language.SpiderSQL.Alias
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Alias
instance GHC.Show.Show Torch.Language.SpiderSQL.ColUnit
instance GHC.Classes.Eq Torch.Language.SpiderSQL.ColUnit
instance GHC.Show.Show Torch.Language.SpiderSQL.ValUnit
instance GHC.Classes.Eq Torch.Language.SpiderSQL.ValUnit
instance GHC.Show.Show Torch.Language.SpiderSQL.Agg
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Agg
instance GHC.Show.Show Torch.Language.SpiderSQL.Select
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Select
instance GHC.Show.Show Torch.Language.SpiderSQL.OrderBy
instance GHC.Classes.Eq Torch.Language.SpiderSQL.OrderBy
instance GHC.Show.Show Torch.Language.SpiderSQL.TableUnit
instance GHC.Classes.Eq Torch.Language.SpiderSQL.TableUnit
instance GHC.Show.Show Torch.Language.SpiderSQL.From
instance GHC.Classes.Eq Torch.Language.SpiderSQL.From
instance GHC.Show.Show Torch.Language.SpiderSQL.Val
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Val
instance GHC.Show.Show Torch.Language.SpiderSQL.Cond
instance GHC.Classes.Eq Torch.Language.SpiderSQL.Cond
instance GHC.Show.Show Torch.Language.SpiderSQL.SpiderSQL
instance GHC.Classes.Eq Torch.Language.SpiderSQL.SpiderSQL

module Torch.Language

module Torch.GraduallyTyped.NN.Transformer.T5.Generation
data IsFinished
Finished :: IsFinished
Unfinished :: IsFinished
data Beams a b
[Beams] :: forall a b. [Hypothesis 'Finished a b] -> [Hypothesis 'Unfinished a b] -> Beams a b
data Hypothesis (isFinished :: IsFinished) a b
[InitialHypothesis] :: forall a b. Hypothesis 'Unfinished a b
[UnfinishedHypothesis] :: forall a b. a -> Float -> Hypothesis 'Unfinished a b -> Hypothesis 'Unfinished a b
[FinishedHypothesis] :: forall a b. a -> Float -> Hypothesis 'Unfinished a b -> b -> Hypothesis 'Finished a b
getTokens :: forall a b. Hypothesis 'Unfinished a b -> [a]
getScore :: forall a b. Hypothesis 'Unfinished a b -> Float
data SomeHypothesis a b
SomeHypothesis :: Hypothesis isFinished a b -> SomeHypothesis a b
[unSomeHypothesis] :: SomeHypothesis a b -> Hypothesis isFinished a b
beamSearch :: forall a b m. Monad m => Int -> Int -> ([Hypothesis 'Unfinished a b] -> m [SomeHypothesis a b]) -> m [Beams a b]
beamSearchStep :: forall a b m. Functor m => ([Hypothesis 'Unfinished a b] -> m [SomeHypothesis a b]) -> Beams a b -> m (Beams a b)
runBeamSearch :: forall model input decoderInput encoderOutput encoderOutputShape encoderOutput' inputPaddingMask decoderOutput generatorDevice. (HasForward model (SimplifiedEncoderDecoderTransformerInput input decoderInput) generatorDevice (SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorDevice, encoderOutput ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType encoderOutputShape, 'UncheckedShape ~ BroadcastShapesF encoderOutputShape 'UncheckedShape, SGetShape encoderOutputShape, HasForward model (SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput' inputPaddingMask) generatorDevice (SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput' decoderInput inputPaddingMask) generatorDevice, encoderOutput' ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType 'UncheckedShape, decoderInput ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") 'UncheckedSize, 'Dim ('Name "*") 'UncheckedSize]), decoderOutput ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType 'UncheckedShape, generatorDevice ~ 'Device 'CPU) => Int -> Int -> model -> input -> Generator generatorDevice -> IO [Beams Int [Int]]
testBeamSearch :: IO ()
next :: forall t b i a. (i ~ Int, Show i, MonadTrans t, Monad (t (StateT [i] b)), Alternative (t (StateT [i] b)), Monad b, Foldable b) => t (StateT [i] b) i -> Parser (StateT [i] b) i a -> (Parser (StateT [i] b) i a -> t (StateT [i] b) a) -> t (StateT [i] b) a
notNull :: (Monad b, Foldable b) => Parser (StateT [i] b) i a -> StateT [i] b Bool
hasFree :: (Monad b, Foldable b) => Parser (StateT [i] b) i a -> StateT [i] b Bool

-- | <tt>transParser vocab p</tt> transforms a parser <tt>p</tt> over
--   characters <a>Char</a> into a parser over token indices <a>Int</a>
--   using the vocabulary <tt>vocab</tt>.
transParser :: MonadPlus b => Map Int String -> Parser b Char a -> Parser b Int a

-- | Get continuations from model
getIs :: forall model input generatorDevice b decoderInput encoderOutput decoderOutput inputPaddingMask s. (Alternative b, MonadThrow b, s ~ (Maybe (encoderOutput, inputPaddingMask), Generator generatorDevice), decoderInput ~ Tensor ('Gradient 'WithoutGradient) ('Layout 'Dense) ('Device 'CPU) ('DataType 'Int64) ('Shape '[ 'Dim ('Name "*") ('Size 1), 'Dim ('Name "*") 'UncheckedSize]), decoderOutput ~ Tensor ('Gradient 'WithGradient) ('Layout 'Dense) ('Device 'CPU) T5DataType 'UncheckedShape, HasForward model (SimplifiedEncoderDecoderTransformerInput input decoderInput) generatorDevice (SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorDevice, HasForward model (SimplifiedEncoderDecoderTransformerGenerationInput decoderInput encoderOutput inputPaddingMask) generatorDevice (SimplifiedEncoderDecoderTransformerOutput decoderOutput encoderOutput decoderInput inputPaddingMask) generatorDevice) => Int -> model -> input -> StateT s (StateT [Int] b) Int
runParser :: forall model input generatorDevice b a. _ => Int -> model -> input -> Generator generatorDevice -> Parser (StateT [Int] b) Int a -> b (a, [Int])

-- | <tt>t5Text</tt> parses a <a>Char</a> sequence delimited by
--   <tt><a>/s</a></tt> as a <a>String</a>.
t5Text :: MonadPlus b => Parser b Char String
t5Test :: MonadPlus b => Parser b Char String

-- | <tt>t5Sql</tt> parses a <a>Char</a> sequence starting with <tt>"</tt>
--   and ending with <tt>" <a>/s</a></tt> as <a>SpiderSQL</a>.
t5Sql :: (TokenParsing (FreeT ((->) Char) b), MonadPlus b) => Parser b Char SpiderSQL
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Beams a b)
instance (GHC.Classes.Eq a, GHC.Classes.Eq b) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Unfinished a b)
instance (GHC.Classes.Eq a, GHC.Classes.Eq b) => GHC.Classes.Eq (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Finished a b)
instance (GHC.Classes.Ord a, GHC.Classes.Ord b) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Unfinished a b)
instance (GHC.Classes.Ord a, GHC.Classes.Ord b) => GHC.Classes.Ord (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Finished a b)
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Unfinished a b)
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Torch.GraduallyTyped.NN.Transformer.T5.Generation.Hypothesis 'Torch.GraduallyTyped.NN.Transformer.T5.Generation.Finished a b)
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Combinators.Parsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Int] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Char.CharParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Int] b))
instance (GHC.Base.Alternative b, Data.Foldable.Foldable b, GHC.Base.MonadPlus b) => Text.Parser.Token.TokenParsing (Control.Monad.Trans.Free.FreeT ((->) GHC.Types.Char) (Control.Monad.Trans.State.Lazy.StateT [GHC.Types.Int] b))

module Torch.GraduallyTyped.NN.Transformer.T5

module Torch.GraduallyTyped.NN.Transformer

module Torch.GraduallyTyped.NN

module Torch.GraduallyTyped
class HasGrad parameters where {
    type Gradients parameters :: Type;
    type Loss parameters :: Type;
}

-- | calculate gradients of a zero-dimensional tensor with respect to a
--   list of independent tensor parameters
grad :: HasGrad parameters => Loss parameters -> parameters -> Gradients parameters
