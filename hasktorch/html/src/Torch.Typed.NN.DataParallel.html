<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE AllowAmbiguousTypes #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE FunctionalDependencies #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE PolyKinds #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-12"></span><span>
</span><span id="line-13"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Typed.NN.DataParallel</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-14"></span><span>
</span><span id="line-15"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier">Control.Concurrent.Async</span></a></span><span>
</span><span id="line-16"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Kind</span></span><span>
</span><span id="line-17"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span>
</span><span id="line-18"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Device.html"><span class="hs-identifier">Torch.Device</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-19"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.HList.html"><span class="hs-identifier">Torch.HList</span></a></span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Class</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Tensor.html"><span class="hs-identifier">Torch.Tensor</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Autograd.html"><span class="hs-identifier">Torch.Typed.Autograd</span></a></span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Device.html"><span class="hs-identifier">Torch.Typed.Device</span></a></span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Optim.html"><span class="hs-identifier">Torch.Typed.Optim</span></a></span><span>
</span><span id="line-27"></span><span>
</span><span id="line-28"></span><span class="hs-keyword">data</span><span> </span><span id="ForwardConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="ForwardConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span id="ForwardConcurrentlyStochF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyStochF"><span class="hs-identifier hs-var">ForwardConcurrentlyStochF</span></a></span></span><span>
</span><span id="line-29"></span><span>
</span><span id="line-30"></span><span id="local-6989586621679667943"><span id="local-6989586621679667944"><span id="local-6989586621679667945"><span class="hs-keyword">instance</span><span>
</span><span id="line-31"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667945"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667944"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667943"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-32"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-33"></span><span>  </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-type">ForwardConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679667945"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679667944"><span class="hs-identifier hs-type">input</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667943"><span class="hs-identifier hs-type">output</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-34"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-35"></span><span>  </span><span id="local-6989586621679667940"><span class="annot"><span class="annottext">apply' :: ForwardConcurrentlyF -&gt; (model, input) -&gt; Concurrently output
</span><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667938"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667938"><span class="hs-identifier hs-var">model</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679667937"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667937"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO output -&gt; Concurrently output
forall a. IO a -&gt; Concurrently a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var">Concurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(IO output -&gt; Concurrently output)
-&gt; (input -&gt; IO output) -&gt; input -&gt; Concurrently output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">output -&gt; IO output
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(output -&gt; IO output) -&gt; (input -&gt; output) -&gt; input -&gt; IO output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">model -&gt; input -&gt; output
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667938"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">(input -&gt; Concurrently output) -&gt; input -&gt; Concurrently output
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667937"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-36"></span><span>  </span><span class="annot"><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var">apply'</span></a></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyStochF"><span class="hs-identifier hs-var">ForwardConcurrentlyStochF</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667933"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667933"><span class="hs-identifier hs-var">model</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679667932"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667932"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO output -&gt; Concurrently output
forall a. IO a -&gt; Concurrently a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var">Concurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(IO output -&gt; Concurrently output)
-&gt; (input -&gt; IO output) -&gt; input -&gt; Concurrently output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">model -&gt; input -&gt; IO output
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; IO b
</span><a href="Torch.NN.html#forwardStoch"><span class="hs-identifier hs-var">forwardStoch</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667933"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">(input -&gt; Concurrently output) -&gt; input -&gt; Concurrently output
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667932"><span class="hs-identifier hs-var">input</span></a></span></span></span></span><span>
</span><span id="line-37"></span><span>
</span><span id="line-38"></span><span class="hs-comment">-- Run a `model` concurrently on an `input`.</span><span>
</span><span id="line-39"></span><span class="hs-comment">--</span><span>
</span><span id="line-40"></span><span class="hs-comment">-- The `model` is replicated over the supplied `devices'`, and the `input` is scattered</span><span>
</span><span id="line-41"></span><span class="hs-comment">-- over them as well. Then the `forward` function of the replicated `models` is run</span><span>
</span><span id="line-42"></span><span class="hs-comment">-- concurrently on the scattered `inputs`. Finally, the `outputs` are gathered on the</span><span>
</span><span id="line-43"></span><span class="hs-comment">-- target `device'`</span><span>
</span><span id="line-44"></span><span class="hs-comment">--</span><span>
</span><span id="line-45"></span><span class="hs-comment">-- &gt;&gt;&gt; model &lt;- A.sample (LinearSpec @1 @1 @'D.Float @'( 'D.CPU, 0))</span><span>
</span><span id="line-46"></span><span class="hs-comment">-- &gt;&gt;&gt; t = ones @'[2, 1] @'D.Float @'( 'D.CPU, 0)</span><span>
</span><span id="line-47"></span><span class="hs-comment">--</span><span>
</span><span id="line-48"></span><span class="hs-comment">-- &gt;&gt;&gt; :t forward model t</span><span>
</span><span id="line-49"></span><span class="hs-comment">-- forward model t :: IO (Tensor '( 'D.CPU, 0) 'D.Float '[2, 1])</span><span>
</span><span id="line-50"></span><span class="hs-comment">-- &gt;&gt;&gt; forward model t</span><span>
</span><span id="line-51"></span><span class="hs-comment">-- Tensor Float [2,1] [[ 0.2478   ],</span><span>
</span><span id="line-52"></span><span class="hs-comment">--                     [ 0.2478   ]]</span><span>
</span><span id="line-53"></span><span class="hs-comment">--</span><span>
</span><span id="line-54"></span><span class="hs-comment">-- &gt;&gt;&gt; :t forwardConcurrently' @'[ '( 'D.CPU, 0), '( 'D.CUDA, 0)] @'( 'D.CPU, 0) model t</span><span>
</span><span id="line-55"></span><span class="hs-comment">-- forwardConcurrently' @'[ '( 'D.CPU, 0), '( 'D.CUDA, 0)] @'( 'D.CPU, 0) model t</span><span>
</span><span id="line-56"></span><span class="hs-comment">--   :: IO (Tensor '( 'D.CPU, 0) 'D.Float '[2, 1])</span><span>
</span><span id="line-57"></span><span class="hs-comment">-- &gt;&gt;&gt; forwardConcurrently' @'[ '( 'D.CPU, 0), '( 'D.CUDA, 0)] @'( 'D.CPU, 0) model t</span><span>
</span><span id="line-58"></span><span class="hs-comment">-- Tensor Float [2,1] [[ 0.2478   ],</span><span>
</span><span id="line-59"></span><span class="hs-comment">--                     [ 0.2478   ]]</span><span>
</span><span id="line-60"></span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently%27"><span class="hs-identifier hs-type">forwardConcurrently'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-61"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch%27"><span class="hs-identifier hs-type">forwardConcurrentlyStoch'</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-62"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679667928"><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span></span><span> </span><span id="local-6989586621679667927"><span class="annot"><a href="#local-6989586621679667927"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679667926"><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679667925"><span class="annot"><a href="#local-6989586621679667925"><span class="hs-identifier hs-type">model</span></a></span></span><span> </span><span id="local-6989586621679667924"><span class="annot"><a href="#local-6989586621679667924"><span class="hs-identifier hs-type">input</span></a></span></span><span> </span><span id="local-6989586621679667923"><span class="annot"><a href="#local-6989586621679667923"><span class="hs-identifier hs-type">output</span></a></span></span><span> </span><span id="local-6989586621679667922"><span class="annot"><a href="#local-6989586621679667922"><span class="hs-identifier hs-type">models</span></a></span></span><span> </span><span id="local-6989586621679667921"><span class="annot"><a href="#local-6989586621679667921"><span class="hs-identifier hs-type">inputs</span></a></span></span><span> </span><span id="local-6989586621679667920"><span class="annot"><a href="#local-6989586621679667920"><span class="hs-identifier hs-type">outputs</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-63"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#GetDevice"><span class="hs-identifier hs-type">GetDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667925"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-64"></span><span>      </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#GetDevice"><span class="hs-identifier hs-type">GetDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667924"><span class="hs-identifier hs-type">input</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-65"></span><span>      </span><span class="annot"><a href="Torch.Typed.Device.html#HasScatter"><span class="hs-identifier hs-type">HasScatter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667924"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667921"><span class="hs-identifier hs-type">inputs</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-66"></span><span>      </span><span class="annot"><a href="Torch.Typed.Device.html#HasReplicate"><span class="hs-identifier hs-type">HasReplicate</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667925"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667922"><span class="hs-identifier hs-type">models</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-67"></span><span>      </span><span class="annot"><a href="Torch.HList.html#HZipWithM"><span class="hs-identifier hs-type">HZipWithM</span></a></span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-type">ForwardConcurrentlyF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667922"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667921"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667920"><span class="hs-identifier hs-type">outputs</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-68"></span><span>      </span><span class="annot"><a href="Torch.Typed.Device.html#HasGather"><span class="hs-identifier hs-type">HasGather</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667927"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667920"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667923"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-69"></span><span>    </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-70"></span><span>    </span><span class="annot"><a href="#local-6989586621679667925"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-71"></span><span>    </span><span class="annot"><a href="#local-6989586621679667924"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-72"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="#local-6989586621679667923"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-73"></span><span id="forwardConcurrently%27"><span class="annot"><span class="annottext">forwardConcurrently' :: model -&gt; input -&gt; IO output
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently%27"><span class="hs-identifier hs-var hs-var">forwardConcurrently'</span></a></span></span><span> </span><span id="local-6989586621679667918"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667918"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679667917"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667917"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-74"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679667916"><span class="annot"><span class="annottext">models :: HList models
</span><a href="#local-6989586621679667916"><span class="hs-identifier hs-var hs-var">models</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">model -&gt; HList models
forall (devices' :: [(DeviceType, Nat)])
       (device :: (DeviceType, Nat)) f (gs :: [*]).
HasReplicate devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#replicate"><span class="hs-identifier hs-var">Torch.Typed.Device.replicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667925"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667922"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667918"><span class="hs-identifier hs-var">model</span></a></span><span>
</span><span id="line-75"></span><span>      </span><span id="local-6989586621679667914"><span class="annot"><span class="annottext">inputs :: HList inputs
</span><a href="#local-6989586621679667914"><span class="hs-identifier hs-var hs-var">inputs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">input -&gt; HList inputs
forall k k k (devices' :: k) (device :: k) f (gs :: [k]).
HasScatter devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#scatter"><span class="hs-identifier hs-var">scatter</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667924"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667921"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667917"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-76"></span><span>  </span><span id="local-6989586621679667912"><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679667912"><span class="hs-identifier hs-var">outputs</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a. Concurrently a -&gt; IO a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var hs-var">runConcurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(Concurrently (HList outputs) -&gt; IO (HList outputs))
-&gt; Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (models :: [k]) (inputs :: [k]) (outputs :: [k]).
HZipWithM
  Concurrently ForwardConcurrentlyF models inputs outputs =&gt;
HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently"><span class="hs-identifier hs-var">forwardConcurrently</span></a></span><span> </span><span class="annot"><span class="annottext">HList models
</span><a href="#local-6989586621679667916"><span class="hs-identifier hs-var">models</span></a></span><span> </span><span class="annot"><span class="annottext">HList inputs
</span><a href="#local-6989586621679667914"><span class="hs-identifier hs-var">inputs</span></a></span><span>
</span><span id="line-77"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679667909"><span class="annot"><span class="annottext">output :: output
</span><a href="#local-6989586621679667909"><span class="hs-identifier hs-var hs-var">output</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList outputs -&gt; output
forall k k k (device' :: k) (devices :: k) (fs :: [k]) g.
HasGather device' devices fs g =&gt;
HList fs -&gt; g
</span><a href="Torch.Typed.Device.html#gather"><span class="hs-identifier hs-var">gather</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667927"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667920"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667923"><span class="hs-identifier hs-type">output</span></a></span><span> </span><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679667912"><span class="hs-identifier hs-var">outputs</span></a></span><span>
</span><span id="line-78"></span><span>  </span><span class="annot"><span class="annottext">output -&gt; IO output
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">output
</span><a href="#local-6989586621679667909"><span class="hs-identifier hs-var">output</span></a></span><span>
</span><span id="line-79"></span><span id="forwardConcurrentlyStoch%27"><span class="annot"><span class="annottext">forwardConcurrentlyStoch' :: model -&gt; input -&gt; IO output
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch%27"><span class="hs-identifier hs-var hs-var">forwardConcurrentlyStoch'</span></a></span></span><span> </span><span id="local-6989586621679667907"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667907"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679667906"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667906"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-80"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679667905"><span class="annot"><span class="annottext">models :: HList models
</span><a href="#local-6989586621679667905"><span class="hs-identifier hs-var hs-var">models</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">model -&gt; HList models
forall (devices' :: [(DeviceType, Nat)])
       (device :: (DeviceType, Nat)) f (gs :: [*]).
HasReplicate devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#replicate"><span class="hs-identifier hs-var">Torch.Typed.Device.replicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667925"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667922"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679667907"><span class="hs-identifier hs-var">model</span></a></span><span>
</span><span id="line-81"></span><span>      </span><span id="local-6989586621679667904"><span class="annot"><span class="annottext">inputs :: HList inputs
</span><a href="#local-6989586621679667904"><span class="hs-identifier hs-var hs-var">inputs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">input -&gt; HList inputs
forall k k k (devices' :: k) (device :: k) f (gs :: [k]).
HasScatter devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#scatter"><span class="hs-identifier hs-var">scatter</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667926"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667924"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667921"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679667906"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-82"></span><span>  </span><span id="local-6989586621679667903"><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679667903"><span class="hs-identifier hs-var">outputs</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a. Concurrently a -&gt; IO a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var hs-var">runConcurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(Concurrently (HList outputs) -&gt; IO (HList outputs))
-&gt; Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (models :: [k]) (inputs :: [k]) (outputs :: [k]).
HZipWithM
  Concurrently ForwardConcurrentlyF models inputs outputs =&gt;
HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch"><span class="hs-identifier hs-var">forwardConcurrentlyStoch</span></a></span><span> </span><span class="annot"><span class="annottext">HList models
</span><a href="#local-6989586621679667905"><span class="hs-identifier hs-var">models</span></a></span><span> </span><span class="annot"><span class="annottext">HList inputs
</span><a href="#local-6989586621679667904"><span class="hs-identifier hs-var">inputs</span></a></span><span>
</span><span id="line-83"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679667901"><span class="annot"><span class="annottext">output :: output
</span><a href="#local-6989586621679667901"><span class="hs-identifier hs-var hs-var">output</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList outputs -&gt; output
forall k k k (device' :: k) (devices :: k) (fs :: [k]) g.
HasGather device' devices fs g =&gt;
HList fs -&gt; g
</span><a href="Torch.Typed.Device.html#gather"><span class="hs-identifier hs-var">gather</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667927"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667928"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667920"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667923"><span class="hs-identifier hs-type">output</span></a></span><span> </span><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679667903"><span class="hs-identifier hs-var">outputs</span></a></span><span>
</span><span id="line-84"></span><span>  </span><span class="annot"><span class="annottext">output -&gt; IO output
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">output
</span><a href="#local-6989586621679667901"><span class="hs-identifier hs-var">output</span></a></span><span>
</span><span id="line-85"></span><span>
</span><span id="line-86"></span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently"><span class="hs-identifier hs-type">forwardConcurrently</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-87"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch"><span class="hs-identifier hs-type">forwardConcurrentlyStoch</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-88"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679668056"><span class="annot"><a href="#local-6989586621679668056"><span class="hs-identifier hs-type">models</span></a></span></span><span> </span><span id="local-6989586621679668055"><span class="annot"><a href="#local-6989586621679668055"><span class="hs-identifier hs-type">inputs</span></a></span></span><span> </span><span id="local-6989586621679668054"><span class="annot"><a href="#local-6989586621679668054"><span class="hs-identifier hs-type">outputs</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-89"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HZipWithM"><span class="hs-identifier hs-type">HZipWithM</span></a></span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-type">ForwardConcurrentlyF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668056"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668055"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668054"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-90"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668056"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-91"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668055"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-92"></span><span>    </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668054"><span class="hs-identifier hs-type">outputs</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-93"></span><span id="forwardConcurrently"><span class="annot"><span class="annottext">forwardConcurrently :: HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently"><span class="hs-identifier hs-var hs-var">forwardConcurrently</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
-&gt; HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (m :: * -&gt; *) f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWithM m f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; m (HList zs)
</span><a href="Torch.HList.html#hzipWithM"><span class="hs-identifier hs-var">hzipWithM</span></a></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span><span>
</span><span id="line-94"></span><span id="forwardConcurrentlyStoch"><span class="annot"><span class="annottext">forwardConcurrentlyStoch :: HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch"><span class="hs-identifier hs-var hs-var">forwardConcurrentlyStoch</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
-&gt; HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (m :: * -&gt; *) f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWithM m f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; m (HList zs)
</span><a href="Torch.HList.html#hzipWithM"><span class="hs-identifier hs-var">hzipWithM</span></a></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyStochF"><span class="hs-identifier hs-var">ForwardConcurrentlyStochF</span></a></span><span>
</span><span id="line-95"></span><span>
</span><span id="line-96"></span><span class="hs-keyword">class</span><span> </span><span id="HasGradConcurrently"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#HasGradConcurrently"><span class="hs-identifier hs-var">HasGradConcurrently</span></a></span></span><span> </span><span id="local-6989586621679667987"><span class="annot"><a href="#local-6989586621679667987"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679667986"><span class="annot"><a href="#local-6989586621679667986"><span class="hs-identifier hs-type">devices</span></a></span></span><span> </span><span id="local-6989586621679667990"><span class="annot"><a href="#local-6989586621679667990"><span class="hs-identifier hs-type">parameters</span></a></span></span><span> </span><span id="local-6989586621679667989"><span class="annot"><a href="#local-6989586621679667989"><span class="hs-identifier hs-type">losses</span></a></span></span><span> </span><span id="local-6989586621679667988"><span class="annot"><a href="#local-6989586621679667988"><span class="hs-identifier hs-type">gradients</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span class="annot"><a href="#local-6989586621679667987"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667986"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667990"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667989"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679667988"><span class="hs-identifier hs-type">gradients</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-97"></span><span>  </span><span id="gradConcurrently"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#gradConcurrently"><span class="hs-identifier hs-type">gradConcurrently</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667990"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667989"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667988"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-98"></span><span>
</span><span id="line-99"></span><span class="hs-keyword">data</span><span> </span><span id="GradConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="GradConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span></span><span>
</span><span id="line-100"></span><span>
</span><span id="line-101"></span><span id="local-6989586621679667894"><span id="local-6989586621679667895"><span id="local-6989586621679667896"><span id="local-6989586621679667897"><span class="hs-keyword">instance</span><span>
</span><span id="line-102"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Autograd.html#HasGrad"><span class="hs-identifier hs-type">HasGrad</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667897"><span class="hs-identifier hs-type">parameters</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667896"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-103"></span><span>    </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667896"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.Tensor.html#ATenTensor"><span class="hs-identifier hs-type">D.ATenTensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-104"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-105"></span><span>  </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-type">GradConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667897"><span class="hs-identifier hs-type">parameters</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Optim.html#Loss"><span class="hs-identifier hs-type">Loss</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667895"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667894"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667896"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-106"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-107"></span><span>  </span><span id="local-6989586621679667892"><span class="annot"><span class="annottext">apply' :: GradConcurrentlyF
-&gt; (HList parameters, Loss device dtype)
-&gt; Concurrently (HList gradients)
</span><a href="#local-6989586621679667892"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><span class="annottext">GradConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667891"><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679667891"><span class="hs-identifier hs-var">parameters</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679667890"><span class="annot"><span class="annottext">Loss device dtype
</span><a href="#local-6989586621679667890"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (HList gradients) -&gt; Concurrently (HList gradients)
forall a. IO a -&gt; Concurrently a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var">Concurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (HList gradients) -&gt; Concurrently (HList gradients))
-&gt; (HList parameters -&gt; IO (HList gradients))
-&gt; HList parameters
-&gt; Concurrently (HList gradients)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">HList gradients -&gt; IO (HList gradients)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(HList gradients -&gt; IO (HList gradients))
-&gt; (HList parameters -&gt; HList gradients)
-&gt; HList parameters
-&gt; IO (HList gradients)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Loss device dtype -&gt; HList parameters -&gt; HList gradients
forall a b (dtype :: DType) (device :: (DeviceType, Nat)).
HasGrad a b =&gt;
Tensor device dtype '[] -&gt; a -&gt; b
</span><a href="Torch.Typed.Autograd.html#grad"><span class="hs-identifier hs-var">grad</span></a></span><span> </span><span class="annot"><span class="annottext">Loss device dtype
</span><a href="#local-6989586621679667890"><span class="hs-identifier hs-var">loss</span></a></span><span> </span><span class="annot"><span class="annottext">(HList parameters -&gt; Concurrently (HList gradients))
-&gt; HList parameters -&gt; Concurrently (HList gradients)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679667891"><span class="hs-identifier hs-var">parameters</span></a></span></span></span></span></span><span>
</span><span id="line-108"></span><span>
</span><span id="line-109"></span><span id="local-6989586621679667883"><span id="local-6989586621679667884"><span id="local-6989586621679667885"><span id="local-6989586621679667886"><span id="local-6989586621679667887"><span id="local-6989586621679667888"><span class="hs-keyword">instance</span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HZipWithM"><span class="hs-identifier hs-type">HZipWithM</span></a></span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-type">GradConcurrentlyF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667888"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667887"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667886"><span class="hs-identifier hs-type">gradients'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-111"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667885"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667884"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667886"><span class="hs-identifier hs-type">gradients'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667883"><span class="hs-identifier hs-type">gradients</span></a></span><span>
</span><span id="line-112"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-113"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#HasGradConcurrently"><span class="hs-identifier hs-type">HasGradConcurrently</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667885"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667884"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667888"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667887"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667883"><span class="hs-identifier hs-type">gradients</span></a></span><span>
</span><span id="line-114"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-115"></span><span>  </span><span id="local-6989586621679667880"><span class="annot"><span class="annottext">gradConcurrently :: HList parameters -&gt; HList losses -&gt; Concurrently (HList gradients)
</span><a href="#local-6989586621679667880"><span class="hs-identifier hs-var hs-var hs-var hs-var">gradConcurrently</span></a></span></span><span> </span><span id="local-6989586621679667879"><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679667879"><span class="hs-identifier hs-var">parameters</span></a></span></span><span> </span><span id="local-6989586621679667878"><span class="annot"><span class="annottext">HList losses
</span><a href="#local-6989586621679667878"><span class="hs-identifier hs-var">losses</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-116"></span><span>    </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679667877"><span class="annot"><span class="annottext">gradients :: Concurrently (HList gradients')
</span><a href="#local-6989586621679667877"><span class="hs-identifier hs-var hs-var">gradients</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GradConcurrentlyF
-&gt; HList parameters
-&gt; HList losses
-&gt; Concurrently (HList gradients')
forall k (m :: * -&gt; *) f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWithM m f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; m (HList zs)
</span><a href="Torch.HList.html#hzipWithM"><span class="hs-identifier hs-var">hzipWithM</span></a></span><span> </span><span class="annot"><span class="annottext">GradConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span><span> </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679667879"><span class="hs-identifier hs-var">parameters</span></a></span><span> </span><span class="annot"><span class="annottext">HList losses
</span><a href="#local-6989586621679667878"><span class="hs-identifier hs-var">losses</span></a></span><span>
</span><span id="line-117"></span><span>     </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">forall (xxs :: [k]) (ys :: [k]).
ReduceGradients device' devices xxs ys =&gt;
HList xxs -&gt; HList ys
forall k k (device' :: (DeviceType, Nat))
       (devices :: [(DeviceType, Nat)]) (xxs :: [k]) (ys :: [k]).
ReduceGradients device' devices xxs ys =&gt;
HList xxs -&gt; HList ys
</span><a href="Torch.Typed.NN.DataParallel.html#reduceGradients"><span class="hs-identifier hs-var">reduceGradients</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667885"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667884"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><span class="annottext">(HList gradients' -&gt; HList gradients)
-&gt; Concurrently (HList gradients')
-&gt; Concurrently (HList gradients)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Concurrently (HList gradients')
</span><a href="#local-6989586621679667877"><span class="hs-identifier hs-var">gradients</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-118"></span><span>
</span><span id="line-119"></span><span class="hs-keyword">class</span><span> </span><span id="ReduceGradients"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-var">ReduceGradients</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667981"><span class="annot"><a href="#local-6989586621679667981"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667980"><span class="annot"><a href="#local-6989586621679667980"><span class="hs-identifier hs-type">devices</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679667985"><span class="annot"><a href="#local-6989586621679667985"><span class="hs-identifier hs-type">xxs</span></a></span></span><span> </span><span id="local-6989586621679667984"><span class="annot"><a href="#local-6989586621679667984"><span class="hs-identifier hs-type">ys</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span class="annot"><a href="#local-6989586621679667981"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667980"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667985"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679667984"><span class="hs-identifier hs-type">ys</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-120"></span><span>  </span><span id="reduceGradients"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#reduceGradients"><span class="hs-identifier hs-type">reduceGradients</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667985"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667984"><span class="hs-identifier hs-type">ys</span></a></span><span>
</span><span id="line-121"></span><span>
</span><span id="line-122"></span><span id="local-6989586621679667871"><span id="local-6989586621679667872"><span id="local-6989586621679667873"><span id="local-6989586621679667874"><span class="hs-keyword">instance</span><span>
</span><span id="line-123"></span><span>  </span><span class="hs-pragma">{-# OVERLAPS</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-124"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#HasToDevice"><span class="hs-identifier hs-type">HasToDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667874"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667873"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667872"><span class="hs-identifier hs-type">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667871"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-125"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-126"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667874"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679667873"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679667872"><span class="hs-identifier hs-type">xs</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Type</span></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679667871"><span class="hs-identifier hs-type">ys</span></a></span><span>
</span><span id="line-127"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-128"></span><span>  </span><span id="local-6989586621679667868"><span class="annot"><span class="annottext">reduceGradients :: HList '[HList xs] -&gt; HList ys
</span><a href="#local-6989586621679667868"><span class="hs-identifier hs-var hs-var hs-var hs-var">reduceGradients</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667867"><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679667867"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList xs -&gt; HList ys
forall (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat))
       f g.
HasToDevice device' device f g =&gt;
f -&gt; g
</span><a href="Torch.Typed.Device.html#toDevice"><span class="hs-identifier hs-var">Torch.Typed.Device.toDevice</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667874"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667873"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679667867"><span class="hs-identifier hs-var">xs</span></a></span></span></span></span></span><span>
</span><span id="line-129"></span><span>
</span><span id="line-130"></span><span class="hs-keyword">data</span><span> </span><span id="SumF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-var">SumF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="SumF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-var">SumF</span></a></span></span><span>
</span><span id="line-131"></span><span>
</span><span id="line-132"></span><span id="local-6989586621679667862"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Num</span></span><span> </span><span class="annot"><a href="#local-6989586621679667862"><span class="hs-identifier hs-type">y</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-type">SumF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679667862"><span class="hs-identifier hs-type">y</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679667862"><span class="hs-identifier hs-type">y</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679667862"><span class="hs-identifier hs-type">y</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-133"></span><span>  </span><span id="local-6989586621679667860"><span class="annot"><span class="annottext">apply' :: SumF -&gt; (y, y) -&gt; y
</span><a href="#local-6989586621679667860"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><span class="annottext">SumF
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(y, y) -&gt; y
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">sum</span></span></span><span>
</span><span id="line-134"></span><span>
</span><span id="line-135"></span><span id="local-6989586621679667853"><span id="local-6989586621679667854"><span id="local-6989586621679667855"><span id="local-6989586621679667856"><span id="local-6989586621679667857"><span id="local-6989586621679667858"><span class="hs-keyword">instance</span><span>
</span><span id="line-136"></span><span>  </span><span class="hs-pragma">{-# OVERLAPPABLE</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-137"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#HasToDevice"><span class="hs-identifier hs-type">HasToDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667858"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667857"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667856"><span class="hs-identifier hs-type">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667855"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-138"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667858"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667854"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667853"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667855"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-139"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HZipWith"><span class="hs-identifier hs-type">HZipWith</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-type">SumF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667855"><span class="hs-identifier hs-type">ys</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667855"><span class="hs-identifier hs-type">ys</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667855"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-140"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="Torch.HList.html#ListLength"><span class="hs-identifier hs-type">ListLength</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667853"><span class="hs-identifier hs-type">xxs</span></a></span><span>
</span><span id="line-141"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-142"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679667858"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679667857"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="annot"><a href="#local-6989586621679667854"><span class="hs-identifier hs-type">devices</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679667856"><span class="hs-identifier hs-type">xs</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Type</span></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="annot"><a href="#local-6989586621679667853"><span class="hs-identifier hs-type">xxs</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679667855"><span class="hs-identifier hs-type">ys</span></a></span><span>
</span><span id="line-143"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-144"></span><span>  </span><span id="local-6989586621679667851"><span class="annot"><span class="annottext">reduceGradients :: HList (HList xs : xxs) -&gt; HList ys
</span><a href="#local-6989586621679667851"><span class="hs-identifier hs-var hs-var hs-var hs-var">reduceGradients</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679667850"><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679667850"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679667849"><span class="annot"><span class="annottext">HList xxs
</span><a href="#local-6989586621679667849"><span class="hs-identifier hs-var">xxs</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SumF -&gt; HList ys -&gt; HList ys -&gt; HList ys
forall k f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWith f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; HList zs
</span><a href="Torch.HList.html#hzipWith"><span class="hs-identifier hs-var">hzipWith</span></a></span><span> </span><span class="annot"><span class="annottext">SumF
</span><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-var">SumF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">HList xs -&gt; HList ys
forall (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat))
       f g.
HasToDevice device' device f g =&gt;
f -&gt; g
</span><a href="Torch.Typed.Device.html#toDevice"><span class="hs-identifier hs-var">Torch.Typed.Device.toDevice</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667858"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667857"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679667850"><span class="hs-identifier hs-var">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">HList xxs -&gt; HList ys
forall k k (device' :: (DeviceType, Nat))
       (devices :: [(DeviceType, Nat)]) (xxs :: [k]) (ys :: [k]).
ReduceGradients device' devices xxs ys =&gt;
HList xxs -&gt; HList ys
</span><a href="Torch.Typed.NN.DataParallel.html#reduceGradients"><span class="hs-identifier hs-var">reduceGradients</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667858"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667854"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679667853"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="annot"><span class="annottext">HList xxs
</span><a href="#local-6989586621679667849"><span class="hs-identifier hs-var">xxs</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-145"></span></pre></body></html>