<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE AllowAmbiguousTypes #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE FunctionalDependencies #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE PolyKinds #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-12"></span><span>
</span><span id="line-13"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Typed.NN.DataParallel</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-14"></span><span>
</span><span id="line-15"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier">Control.Concurrent.Async</span></a></span><span>
</span><span id="line-16"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Kind</span></span><span>
</span><span id="line-17"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span>
</span><span id="line-18"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Device.html"><span class="hs-identifier">Torch.Device</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-19"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.HList.html"><span class="hs-identifier">Torch.HList</span></a></span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Class</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Tensor.html"><span class="hs-identifier">Torch.Tensor</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Autograd.html"><span class="hs-identifier">Torch.Typed.Autograd</span></a></span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Device.html"><span class="hs-identifier">Torch.Typed.Device</span></a></span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Optim.html"><span class="hs-identifier">Torch.Typed.Optim</span></a></span><span>
</span><span id="line-27"></span><span>
</span><span id="line-28"></span><span class="hs-keyword">data</span><span> </span><span id="ForwardConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="ForwardConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span id="ForwardConcurrentlyStochF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyStochF"><span class="hs-identifier hs-var">ForwardConcurrentlyStochF</span></a></span></span><span>
</span><span id="line-29"></span><span>
</span><span id="line-30"></span><span id="local-6989586621679668261"><span id="local-6989586621679668262"><span id="local-6989586621679668263"><span class="hs-keyword">instance</span><span>
</span><span id="line-31"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668263"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668262"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668261"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-32"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-33"></span><span>  </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-type">ForwardConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679668263"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679668262"><span class="hs-identifier hs-type">input</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668261"><span class="hs-identifier hs-type">output</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-34"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-35"></span><span>  </span><span id="local-6989586621679668258"><span class="annot"><span class="annottext">apply' :: ForwardConcurrentlyF -&gt; (model, input) -&gt; Concurrently output
</span><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668256"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668256"><span class="hs-identifier hs-var">model</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679668255"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668255"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO output -&gt; Concurrently output
forall a. IO a -&gt; Concurrently a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var">Concurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(IO output -&gt; Concurrently output)
-&gt; (input -&gt; IO output) -&gt; input -&gt; Concurrently output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">output -&gt; IO output
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(output -&gt; IO output) -&gt; (input -&gt; output) -&gt; input -&gt; IO output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">model -&gt; input -&gt; output
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668256"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">(input -&gt; Concurrently output) -&gt; input -&gt; Concurrently output
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668255"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-36"></span><span>  </span><span class="annot"><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var">apply'</span></a></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyStochF"><span class="hs-identifier hs-var">ForwardConcurrentlyStochF</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668251"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668251"><span class="hs-identifier hs-var">model</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679668250"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668250"><span class="hs-identifier hs-var">input</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO output -&gt; Concurrently output
forall a. IO a -&gt; Concurrently a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var">Concurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(IO output -&gt; Concurrently output)
-&gt; (input -&gt; IO output) -&gt; input -&gt; Concurrently output
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">model -&gt; input -&gt; IO output
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; IO b
</span><a href="Torch.NN.html#forwardStoch"><span class="hs-identifier hs-var">forwardStoch</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668251"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">(input -&gt; Concurrently output) -&gt; input -&gt; Concurrently output
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668250"><span class="hs-identifier hs-var">input</span></a></span></span></span></span><span>
</span><span id="line-37"></span><span>
</span><span id="line-38"></span><span class="hs-comment">-- Run a `model` concurrently on an `input`.</span><span>
</span><span id="line-39"></span><span class="hs-comment">--</span><span>
</span><span id="line-40"></span><span class="hs-comment">-- The `model` is replicated over the supplied `devices'`, and the `input` is scattered</span><span>
</span><span id="line-41"></span><span class="hs-comment">-- over them as well. Then the `forward` function of the replicated `models` is run</span><span>
</span><span id="line-42"></span><span class="hs-comment">-- concurrently on the scattered `inputs`. Finally, the `outputs` are gathered on the</span><span>
</span><span id="line-43"></span><span class="hs-comment">-- target `device'`</span><span>
</span><span id="line-44"></span><span class="hs-comment">--</span><span>
</span><span id="line-45"></span><span class="hs-comment">-- &gt;&gt;&gt; model &lt;- A.sample (LinearSpec @1 @1 @'D.Float @'( 'D.CPU, 0))</span><span>
</span><span id="line-46"></span><span class="hs-comment">-- &gt;&gt;&gt; t = ones @'[2, 1] @'D.Float @'( 'D.CPU, 0)</span><span>
</span><span id="line-47"></span><span class="hs-comment">--</span><span>
</span><span id="line-48"></span><span class="hs-comment">-- &gt;&gt;&gt; :t forward model t</span><span>
</span><span id="line-49"></span><span class="hs-comment">-- forward model t :: IO (Tensor '( 'D.CPU, 0) 'D.Float '[2, 1])</span><span>
</span><span id="line-50"></span><span class="hs-comment">-- &gt;&gt;&gt; forward model t</span><span>
</span><span id="line-51"></span><span class="hs-comment">-- Tensor Float [2,1] [[ 0.2478   ],</span><span>
</span><span id="line-52"></span><span class="hs-comment">--                     [ 0.2478   ]]</span><span>
</span><span id="line-53"></span><span class="hs-comment">--</span><span>
</span><span id="line-54"></span><span class="hs-comment">-- &gt;&gt;&gt; :t forwardConcurrently' @'[ '( 'D.CPU, 0), '( 'D.CUDA, 0)] @'( 'D.CPU, 0) model t</span><span>
</span><span id="line-55"></span><span class="hs-comment">-- forwardConcurrently' @'[ '( 'D.CPU, 0), '( 'D.CUDA, 0)] @'( 'D.CPU, 0) model t</span><span>
</span><span id="line-56"></span><span class="hs-comment">--   :: IO (Tensor '( 'D.CPU, 0) 'D.Float '[2, 1])</span><span>
</span><span id="line-57"></span><span class="hs-comment">-- &gt;&gt;&gt; forwardConcurrently' @'[ '( 'D.CPU, 0), '( 'D.CUDA, 0)] @'( 'D.CPU, 0) model t</span><span>
</span><span id="line-58"></span><span class="hs-comment">-- Tensor Float [2,1] [[ 0.2478   ],</span><span>
</span><span id="line-59"></span><span class="hs-comment">--                     [ 0.2478   ]]</span><span>
</span><span id="line-60"></span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently%27"><span class="hs-identifier hs-type">forwardConcurrently'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-61"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch%27"><span class="hs-identifier hs-type">forwardConcurrentlyStoch'</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-62"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679668246"><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span></span><span> </span><span id="local-6989586621679668245"><span class="annot"><a href="#local-6989586621679668245"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679668244"><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679668243"><span class="annot"><a href="#local-6989586621679668243"><span class="hs-identifier hs-type">model</span></a></span></span><span> </span><span id="local-6989586621679668242"><span class="annot"><a href="#local-6989586621679668242"><span class="hs-identifier hs-type">input</span></a></span></span><span> </span><span id="local-6989586621679668241"><span class="annot"><a href="#local-6989586621679668241"><span class="hs-identifier hs-type">output</span></a></span></span><span> </span><span id="local-6989586621679668240"><span class="annot"><a href="#local-6989586621679668240"><span class="hs-identifier hs-type">models</span></a></span></span><span> </span><span id="local-6989586621679668239"><span class="annot"><a href="#local-6989586621679668239"><span class="hs-identifier hs-type">inputs</span></a></span></span><span> </span><span id="local-6989586621679668238"><span class="annot"><a href="#local-6989586621679668238"><span class="hs-identifier hs-type">outputs</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-63"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#GetDevice"><span class="hs-identifier hs-type">GetDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668243"><span class="hs-identifier hs-type">model</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-64"></span><span>      </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#GetDevice"><span class="hs-identifier hs-type">GetDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668242"><span class="hs-identifier hs-type">input</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-65"></span><span>      </span><span class="annot"><a href="Torch.Typed.Device.html#HasScatter"><span class="hs-identifier hs-type">HasScatter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668242"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668239"><span class="hs-identifier hs-type">inputs</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-66"></span><span>      </span><span class="annot"><a href="Torch.Typed.Device.html#HasReplicate"><span class="hs-identifier hs-type">HasReplicate</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668243"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668240"><span class="hs-identifier hs-type">models</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-67"></span><span>      </span><span class="annot"><a href="Torch.HList.html#HZipWithM"><span class="hs-identifier hs-type">HZipWithM</span></a></span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-type">ForwardConcurrentlyF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668240"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668239"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668238"><span class="hs-identifier hs-type">outputs</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-68"></span><span>      </span><span class="annot"><a href="Torch.Typed.Device.html#HasGather"><span class="hs-identifier hs-type">HasGather</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668245"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668238"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668241"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-69"></span><span>    </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-70"></span><span>    </span><span class="annot"><a href="#local-6989586621679668243"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-71"></span><span>    </span><span class="annot"><a href="#local-6989586621679668242"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-72"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="#local-6989586621679668241"><span class="hs-identifier hs-type">output</span></a></span><span>
</span><span id="line-73"></span><span id="forwardConcurrently%27"><span class="annot"><span class="annottext">forwardConcurrently' :: model -&gt; input -&gt; IO output
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently%27"><span class="hs-identifier hs-var hs-var">forwardConcurrently'</span></a></span></span><span> </span><span id="local-6989586621679668236"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668236"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679668235"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668235"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-74"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679668234"><span class="annot"><span class="annottext">models :: HList models
</span><a href="#local-6989586621679668234"><span class="hs-identifier hs-var hs-var">models</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">model -&gt; HList models
forall (devices' :: [(DeviceType, Nat)])
       (device :: (DeviceType, Nat)) f (gs :: [*]).
HasReplicate devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#replicate"><span class="hs-identifier hs-var">Torch.Typed.Device.replicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668243"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668240"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668236"><span class="hs-identifier hs-var">model</span></a></span><span>
</span><span id="line-75"></span><span>      </span><span id="local-6989586621679668232"><span class="annot"><span class="annottext">inputs :: HList inputs
</span><a href="#local-6989586621679668232"><span class="hs-identifier hs-var hs-var">inputs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">input -&gt; HList inputs
forall k k k (devices' :: k) (device :: k) f (gs :: [k]).
HasScatter devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#scatter"><span class="hs-identifier hs-var">scatter</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668242"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668239"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668235"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-76"></span><span>  </span><span id="local-6989586621679668230"><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679668230"><span class="hs-identifier hs-var">outputs</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a. Concurrently a -&gt; IO a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var hs-var">runConcurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(Concurrently (HList outputs) -&gt; IO (HList outputs))
-&gt; Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (models :: [k]) (inputs :: [k]) (outputs :: [k]).
HZipWithM
  Concurrently ForwardConcurrentlyF models inputs outputs =&gt;
HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently"><span class="hs-identifier hs-var">forwardConcurrently</span></a></span><span> </span><span class="annot"><span class="annottext">HList models
</span><a href="#local-6989586621679668234"><span class="hs-identifier hs-var">models</span></a></span><span> </span><span class="annot"><span class="annottext">HList inputs
</span><a href="#local-6989586621679668232"><span class="hs-identifier hs-var">inputs</span></a></span><span>
</span><span id="line-77"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679668227"><span class="annot"><span class="annottext">output :: output
</span><a href="#local-6989586621679668227"><span class="hs-identifier hs-var hs-var">output</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList outputs -&gt; output
forall k k k (device' :: k) (devices :: k) (fs :: [k]) g.
HasGather device' devices fs g =&gt;
HList fs -&gt; g
</span><a href="Torch.Typed.Device.html#gather"><span class="hs-identifier hs-var">gather</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668245"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668238"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668241"><span class="hs-identifier hs-type">output</span></a></span><span> </span><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679668230"><span class="hs-identifier hs-var">outputs</span></a></span><span>
</span><span id="line-78"></span><span>  </span><span class="annot"><span class="annottext">output -&gt; IO output
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">output
</span><a href="#local-6989586621679668227"><span class="hs-identifier hs-var">output</span></a></span><span>
</span><span id="line-79"></span><span id="forwardConcurrentlyStoch%27"><span class="annot"><span class="annottext">forwardConcurrentlyStoch' :: model -&gt; input -&gt; IO output
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch%27"><span class="hs-identifier hs-var hs-var">forwardConcurrentlyStoch'</span></a></span></span><span> </span><span id="local-6989586621679668225"><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668225"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679668224"><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668224"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-80"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679668223"><span class="annot"><span class="annottext">models :: HList models
</span><a href="#local-6989586621679668223"><span class="hs-identifier hs-var hs-var">models</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">model -&gt; HList models
forall (devices' :: [(DeviceType, Nat)])
       (device :: (DeviceType, Nat)) f (gs :: [*]).
HasReplicate devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#replicate"><span class="hs-identifier hs-var">Torch.Typed.Device.replicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668243"><span class="hs-identifier hs-type">model</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668240"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><span class="annottext">model
</span><a href="#local-6989586621679668225"><span class="hs-identifier hs-var">model</span></a></span><span>
</span><span id="line-81"></span><span>      </span><span id="local-6989586621679668222"><span class="annot"><span class="annottext">inputs :: HList inputs
</span><a href="#local-6989586621679668222"><span class="hs-identifier hs-var hs-var">inputs</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">input -&gt; HList inputs
forall k k k (devices' :: k) (device :: k) f (gs :: [k]).
HasScatter devices' device f gs =&gt;
f -&gt; HList gs
</span><a href="Torch.Typed.Device.html#scatter"><span class="hs-identifier hs-var">scatter</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668244"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668242"><span class="hs-identifier hs-type">input</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668239"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><span class="annottext">input
</span><a href="#local-6989586621679668224"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-82"></span><span>  </span><span id="local-6989586621679668221"><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679668221"><span class="hs-identifier hs-var">outputs</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a. Concurrently a -&gt; IO a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var hs-var">runConcurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(Concurrently (HList outputs) -&gt; IO (HList outputs))
-&gt; Concurrently (HList outputs) -&gt; IO (HList outputs)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (models :: [k]) (inputs :: [k]) (outputs :: [k]).
HZipWithM
  Concurrently ForwardConcurrentlyF models inputs outputs =&gt;
HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch"><span class="hs-identifier hs-var">forwardConcurrentlyStoch</span></a></span><span> </span><span class="annot"><span class="annottext">HList models
</span><a href="#local-6989586621679668223"><span class="hs-identifier hs-var">models</span></a></span><span> </span><span class="annot"><span class="annottext">HList inputs
</span><a href="#local-6989586621679668222"><span class="hs-identifier hs-var">inputs</span></a></span><span>
</span><span id="line-83"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679668219"><span class="annot"><span class="annottext">output :: output
</span><a href="#local-6989586621679668219"><span class="hs-identifier hs-var hs-var">output</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList outputs -&gt; output
forall k k k (device' :: k) (devices :: k) (fs :: [k]) g.
HasGather device' devices fs g =&gt;
HList fs -&gt; g
</span><a href="Torch.Typed.Device.html#gather"><span class="hs-identifier hs-var">gather</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668245"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668246"><span class="hs-identifier hs-type">devices'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668238"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668241"><span class="hs-identifier hs-type">output</span></a></span><span> </span><span class="annot"><span class="annottext">HList outputs
</span><a href="#local-6989586621679668221"><span class="hs-identifier hs-var">outputs</span></a></span><span>
</span><span id="line-84"></span><span>  </span><span class="annot"><span class="annottext">output -&gt; IO output
forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">output
</span><a href="#local-6989586621679668219"><span class="hs-identifier hs-var">output</span></a></span><span>
</span><span id="line-85"></span><span>
</span><span id="line-86"></span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently"><span class="hs-identifier hs-type">forwardConcurrently</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-87"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch"><span class="hs-identifier hs-type">forwardConcurrentlyStoch</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-88"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679668374"><span class="annot"><a href="#local-6989586621679668374"><span class="hs-identifier hs-type">models</span></a></span></span><span> </span><span id="local-6989586621679668373"><span class="annot"><a href="#local-6989586621679668373"><span class="hs-identifier hs-type">inputs</span></a></span></span><span> </span><span id="local-6989586621679668372"><span class="annot"><a href="#local-6989586621679668372"><span class="hs-identifier hs-type">outputs</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-89"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HZipWithM"><span class="hs-identifier hs-type">HZipWithM</span></a></span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-type">ForwardConcurrentlyF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668374"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668373"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668372"><span class="hs-identifier hs-type">outputs</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-90"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668374"><span class="hs-identifier hs-type">models</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-91"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668373"><span class="hs-identifier hs-type">inputs</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-92"></span><span>    </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668372"><span class="hs-identifier hs-type">outputs</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-93"></span><span id="forwardConcurrently"><span class="annot"><span class="annottext">forwardConcurrently :: HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrently"><span class="hs-identifier hs-var hs-var">forwardConcurrently</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
-&gt; HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (m :: * -&gt; *) f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWithM m f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; m (HList zs)
</span><a href="Torch.HList.html#hzipWithM"><span class="hs-identifier hs-var">hzipWithM</span></a></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyF"><span class="hs-identifier hs-var">ForwardConcurrentlyF</span></a></span><span>
</span><span id="line-94"></span><span id="forwardConcurrentlyStoch"><span class="annot"><span class="annottext">forwardConcurrentlyStoch :: HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
</span><a href="Torch.Typed.NN.DataParallel.html#forwardConcurrentlyStoch"><span class="hs-identifier hs-var hs-var">forwardConcurrentlyStoch</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
-&gt; HList models -&gt; HList inputs -&gt; Concurrently (HList outputs)
forall k (m :: * -&gt; *) f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWithM m f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; m (HList zs)
</span><a href="Torch.HList.html#hzipWithM"><span class="hs-identifier hs-var">hzipWithM</span></a></span><span> </span><span class="annot"><span class="annottext">ForwardConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#ForwardConcurrentlyStochF"><span class="hs-identifier hs-var">ForwardConcurrentlyStochF</span></a></span><span>
</span><span id="line-95"></span><span>
</span><span id="line-96"></span><span class="hs-keyword">class</span><span> </span><span id="HasGradConcurrently"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#HasGradConcurrently"><span class="hs-identifier hs-var">HasGradConcurrently</span></a></span></span><span> </span><span id="local-6989586621679668305"><span class="annot"><a href="#local-6989586621679668305"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span id="local-6989586621679668304"><span class="annot"><a href="#local-6989586621679668304"><span class="hs-identifier hs-type">devices</span></a></span></span><span> </span><span id="local-6989586621679668308"><span class="annot"><a href="#local-6989586621679668308"><span class="hs-identifier hs-type">parameters</span></a></span></span><span> </span><span id="local-6989586621679668307"><span class="annot"><a href="#local-6989586621679668307"><span class="hs-identifier hs-type">losses</span></a></span></span><span> </span><span id="local-6989586621679668306"><span class="annot"><a href="#local-6989586621679668306"><span class="hs-identifier hs-type">gradients</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span class="annot"><a href="#local-6989586621679668305"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668304"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668308"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668307"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679668306"><span class="hs-identifier hs-type">gradients</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-97"></span><span>  </span><span id="gradConcurrently"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#gradConcurrently"><span class="hs-identifier hs-type">gradConcurrently</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668308"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668307"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668306"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-98"></span><span>
</span><span id="line-99"></span><span class="hs-keyword">data</span><span> </span><span id="GradConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="GradConcurrentlyF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span></span><span>
</span><span id="line-100"></span><span>
</span><span id="line-101"></span><span id="local-6989586621679668212"><span id="local-6989586621679668213"><span id="local-6989586621679668214"><span id="local-6989586621679668215"><span class="hs-keyword">instance</span><span>
</span><span id="line-102"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Autograd.html#HasGrad"><span class="hs-identifier hs-type">HasGrad</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668215"><span class="hs-identifier hs-type">parameters</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668214"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-103"></span><span>    </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668214"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.Tensor.html#ATenTensor"><span class="hs-identifier hs-type">D.ATenTensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-104"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-105"></span><span>  </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-type">GradConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668215"><span class="hs-identifier hs-type">parameters</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Optim.html#Loss"><span class="hs-identifier hs-type">Loss</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668213"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668212"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668214"><span class="hs-identifier hs-type">gradients</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-106"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-107"></span><span>  </span><span id="local-6989586621679668210"><span class="annot"><span class="annottext">apply' :: GradConcurrentlyF
-&gt; (HList parameters, Loss device dtype)
-&gt; Concurrently (HList gradients)
</span><a href="#local-6989586621679668210"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><span class="annottext">GradConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668209"><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679668209"><span class="hs-identifier hs-var">parameters</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679668208"><span class="annot"><span class="annottext">Loss device dtype
</span><a href="#local-6989586621679668208"><span class="hs-identifier hs-var">loss</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (HList gradients) -&gt; Concurrently (HList gradients)
forall a. IO a -&gt; Concurrently a
</span><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-var">Concurrently</span></a></span><span> </span><span class="annot"><span class="annottext">(IO (HList gradients) -&gt; Concurrently (HList gradients))
-&gt; (HList parameters -&gt; IO (HList gradients))
-&gt; HList parameters
-&gt; Concurrently (HList gradients)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">HList gradients -&gt; IO (HList gradients)
forall (f :: * -&gt; *) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(HList gradients -&gt; IO (HList gradients))
-&gt; (HList parameters -&gt; HList gradients)
-&gt; HList parameters
-&gt; IO (HList gradients)
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Loss device dtype -&gt; HList parameters -&gt; HList gradients
forall a b (dtype :: DType) (device :: (DeviceType, Nat)).
HasGrad a b =&gt;
Tensor device dtype '[] -&gt; a -&gt; b
</span><a href="Torch.Typed.Autograd.html#grad"><span class="hs-identifier hs-var">grad</span></a></span><span> </span><span class="annot"><span class="annottext">Loss device dtype
</span><a href="#local-6989586621679668208"><span class="hs-identifier hs-var">loss</span></a></span><span> </span><span class="annot"><span class="annottext">(HList parameters -&gt; Concurrently (HList gradients))
-&gt; HList parameters -&gt; Concurrently (HList gradients)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679668209"><span class="hs-identifier hs-var">parameters</span></a></span></span></span></span></span><span>
</span><span id="line-108"></span><span>
</span><span id="line-109"></span><span id="local-6989586621679668201"><span id="local-6989586621679668202"><span id="local-6989586621679668203"><span id="local-6989586621679668204"><span id="local-6989586621679668205"><span id="local-6989586621679668206"><span class="hs-keyword">instance</span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HZipWithM"><span class="hs-identifier hs-type">HZipWithM</span></a></span><span> </span><span class="annot"><a href="../file:///nix/store/yj6dwhg8k2s8y0ag1bjxfcyqhi21h45p-async-lib-async-2.2.3-haddock-doc/share/doc/async/html/src"><span class="hs-identifier hs-type">Concurrently</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-type">GradConcurrentlyF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668206"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668205"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668204"><span class="hs-identifier hs-type">gradients'</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-111"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668203"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668202"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668204"><span class="hs-identifier hs-type">gradients'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668201"><span class="hs-identifier hs-type">gradients</span></a></span><span>
</span><span id="line-112"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-113"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#HasGradConcurrently"><span class="hs-identifier hs-type">HasGradConcurrently</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668203"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668202"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668206"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668205"><span class="hs-identifier hs-type">losses</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668201"><span class="hs-identifier hs-type">gradients</span></a></span><span>
</span><span id="line-114"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-115"></span><span>  </span><span id="local-6989586621679668198"><span class="annot"><span class="annottext">gradConcurrently :: HList parameters -&gt; HList losses -&gt; Concurrently (HList gradients)
</span><a href="#local-6989586621679668198"><span class="hs-identifier hs-var hs-var hs-var hs-var">gradConcurrently</span></a></span></span><span> </span><span id="local-6989586621679668197"><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679668197"><span class="hs-identifier hs-var">parameters</span></a></span></span><span> </span><span id="local-6989586621679668196"><span class="annot"><span class="annottext">HList losses
</span><a href="#local-6989586621679668196"><span class="hs-identifier hs-var">losses</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-116"></span><span>    </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679668195"><span class="annot"><span class="annottext">gradients :: Concurrently (HList gradients')
</span><a href="#local-6989586621679668195"><span class="hs-identifier hs-var hs-var">gradients</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GradConcurrentlyF
-&gt; HList parameters
-&gt; HList losses
-&gt; Concurrently (HList gradients')
forall k (m :: * -&gt; *) f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWithM m f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; m (HList zs)
</span><a href="Torch.HList.html#hzipWithM"><span class="hs-identifier hs-var">hzipWithM</span></a></span><span> </span><span class="annot"><span class="annottext">GradConcurrentlyF
</span><a href="Torch.Typed.NN.DataParallel.html#GradConcurrentlyF"><span class="hs-identifier hs-var">GradConcurrentlyF</span></a></span><span> </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679668197"><span class="hs-identifier hs-var">parameters</span></a></span><span> </span><span class="annot"><span class="annottext">HList losses
</span><a href="#local-6989586621679668196"><span class="hs-identifier hs-var">losses</span></a></span><span>
</span><span id="line-117"></span><span>     </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">forall (xxs :: [k]) (ys :: [k]).
ReduceGradients device' devices xxs ys =&gt;
HList xxs -&gt; HList ys
forall k k (device' :: (DeviceType, Nat))
       (devices :: [(DeviceType, Nat)]) (xxs :: [k]) (ys :: [k]).
ReduceGradients device' devices xxs ys =&gt;
HList xxs -&gt; HList ys
</span><a href="Torch.Typed.NN.DataParallel.html#reduceGradients"><span class="hs-identifier hs-var">reduceGradients</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668203"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668202"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><span class="annottext">(HList gradients' -&gt; HList gradients)
-&gt; Concurrently (HList gradients')
-&gt; Concurrently (HList gradients)
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Concurrently (HList gradients')
</span><a href="#local-6989586621679668195"><span class="hs-identifier hs-var">gradients</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-118"></span><span>
</span><span id="line-119"></span><span class="hs-keyword">class</span><span> </span><span id="ReduceGradients"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-var">ReduceGradients</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668299"><span class="annot"><a href="#local-6989586621679668299"><span class="hs-identifier hs-type">device'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668298"><span class="annot"><a href="#local-6989586621679668298"><span class="hs-identifier hs-type">devices</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679668303"><span class="annot"><a href="#local-6989586621679668303"><span class="hs-identifier hs-type">xxs</span></a></span></span><span> </span><span id="local-6989586621679668302"><span class="annot"><a href="#local-6989586621679668302"><span class="hs-identifier hs-type">ys</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span class="annot"><a href="#local-6989586621679668299"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668298"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668303"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="#local-6989586621679668302"><span class="hs-identifier hs-type">ys</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-120"></span><span>  </span><span id="reduceGradients"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#reduceGradients"><span class="hs-identifier hs-type">reduceGradients</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668303"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668302"><span class="hs-identifier hs-type">ys</span></a></span><span>
</span><span id="line-121"></span><span>
</span><span id="line-122"></span><span id="local-6989586621679668189"><span id="local-6989586621679668190"><span id="local-6989586621679668191"><span id="local-6989586621679668192"><span class="hs-keyword">instance</span><span>
</span><span id="line-123"></span><span>  </span><span class="hs-pragma">{-# OVERLAPS</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-124"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#HasToDevice"><span class="hs-identifier hs-type">HasToDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668192"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668191"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668190"><span class="hs-identifier hs-type">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668189"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-125"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-126"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668192"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679668191"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679668190"><span class="hs-identifier hs-type">xs</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Type</span></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679668189"><span class="hs-identifier hs-type">ys</span></a></span><span>
</span><span id="line-127"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-128"></span><span>  </span><span id="local-6989586621679668186"><span class="annot"><span class="annottext">reduceGradients :: HList '[HList xs] -&gt; HList ys
</span><a href="#local-6989586621679668186"><span class="hs-identifier hs-var hs-var hs-var hs-var">reduceGradients</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668185"><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679668185"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList xs -&gt; HList ys
forall (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat))
       f g.
HasToDevice device' device f g =&gt;
f -&gt; g
</span><a href="Torch.Typed.Device.html#toDevice"><span class="hs-identifier hs-var">Torch.Typed.Device.toDevice</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668192"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668191"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679668185"><span class="hs-identifier hs-var">xs</span></a></span></span></span></span></span><span>
</span><span id="line-129"></span><span>
</span><span id="line-130"></span><span class="hs-keyword">data</span><span> </span><span id="SumF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-var">SumF</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="SumF"><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-var">SumF</span></a></span></span><span>
</span><span id="line-131"></span><span>
</span><span id="line-132"></span><span id="local-6989586621679668180"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Num</span></span><span> </span><span class="annot"><a href="#local-6989586621679668180"><span class="hs-identifier hs-type">y</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-type">SumF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679668180"><span class="hs-identifier hs-type">y</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679668180"><span class="hs-identifier hs-type">y</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679668180"><span class="hs-identifier hs-type">y</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-133"></span><span>  </span><span id="local-6989586621679668178"><span class="annot"><span class="annottext">apply' :: SumF -&gt; (y, y) -&gt; y
</span><a href="#local-6989586621679668178"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><span class="annottext">SumF
</span><span class="hs-identifier">_</span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">(y, y) -&gt; y
forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">sum</span></span></span><span>
</span><span id="line-134"></span><span>
</span><span id="line-135"></span><span id="local-6989586621679668171"><span id="local-6989586621679668172"><span id="local-6989586621679668173"><span id="local-6989586621679668174"><span id="local-6989586621679668175"><span id="local-6989586621679668176"><span class="hs-keyword">instance</span><span>
</span><span id="line-136"></span><span>  </span><span class="hs-pragma">{-# OVERLAPPABLE</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-137"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Device.html#HasToDevice"><span class="hs-identifier hs-type">HasToDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668176"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668175"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668174"><span class="hs-identifier hs-type">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668173"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-138"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668176"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668172"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668171"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668173"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-139"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HZipWith"><span class="hs-identifier hs-type">HZipWith</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-type">SumF</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668173"><span class="hs-identifier hs-type">ys</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668173"><span class="hs-identifier hs-type">ys</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668173"><span class="hs-identifier hs-type">ys</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-140"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="Torch.HList.html#ListLength"><span class="hs-identifier hs-type">ListLength</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668171"><span class="hs-identifier hs-type">xxs</span></a></span><span>
</span><span id="line-141"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-142"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.DataParallel.html#ReduceGradients"><span class="hs-identifier hs-type">ReduceGradients</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679668176"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679668175"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="annot"><a href="#local-6989586621679668172"><span class="hs-identifier hs-type">devices</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679668174"><span class="hs-identifier hs-type">xs</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Type</span></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">'</span><span class="hs-glyph">:</span><span> </span><span class="annot"><a href="#local-6989586621679668171"><span class="hs-identifier hs-type">xxs</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679668173"><span class="hs-identifier hs-type">ys</span></a></span><span>
</span><span id="line-143"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-144"></span><span>  </span><span id="local-6989586621679668169"><span class="annot"><span class="annottext">reduceGradients :: HList (HList xs : xxs) -&gt; HList ys
</span><a href="#local-6989586621679668169"><span class="hs-identifier hs-var hs-var hs-var hs-var">reduceGradients</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679668168"><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679668168"><span class="hs-identifier hs-var">xs</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679668167"><span class="annot"><span class="annottext">HList xxs
</span><a href="#local-6989586621679668167"><span class="hs-identifier hs-var">xxs</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">SumF -&gt; HList ys -&gt; HList ys -&gt; HList ys
forall k f (xs :: [k]) (ys :: [k]) (zs :: [k]).
HZipWith f xs ys zs =&gt;
f -&gt; HList xs -&gt; HList ys -&gt; HList zs
</span><a href="Torch.HList.html#hzipWith"><span class="hs-identifier hs-var">hzipWith</span></a></span><span> </span><span class="annot"><span class="annottext">SumF
</span><a href="Torch.Typed.NN.DataParallel.html#SumF"><span class="hs-identifier hs-var">SumF</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">HList xs -&gt; HList ys
forall (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat))
       f g.
HasToDevice device' device f g =&gt;
f -&gt; g
</span><a href="Torch.Typed.Device.html#toDevice"><span class="hs-identifier hs-var">Torch.Typed.Device.toDevice</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668176"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668175"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><span class="annottext">HList xs
</span><a href="#local-6989586621679668168"><span class="hs-identifier hs-var">xs</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">HList xxs -&gt; HList ys
forall k k (device' :: (DeviceType, Nat))
       (devices :: [(DeviceType, Nat)]) (xxs :: [k]) (ys :: [k]).
ReduceGradients device' devices xxs ys =&gt;
HList xxs -&gt; HList ys
</span><a href="Torch.Typed.NN.DataParallel.html#reduceGradients"><span class="hs-identifier hs-var">reduceGradients</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668176"><span class="hs-identifier hs-type">device'</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668172"><span class="hs-identifier hs-type">devices</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679668171"><span class="hs-identifier hs-type">xxs</span></a></span><span> </span><span class="annot"><span class="annottext">HList xxs
</span><a href="#local-6989586621679668167"><span class="hs-identifier hs-var">xxs</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-145"></span></pre></body></html>