<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE DeriveAnyClass #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE DeriveGeneric #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE StandaloneDeriving #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-15"></span><span class="hs-pragma">{-# LANGUAGE NoStarIsType #-}</span><span>
</span><span id="line-16"></span><span class="hs-pragma">{-# OPTIONS_GHC -fconstraint-solver-iterations=0 #-}</span><span>
</span><span id="line-17"></span><span>
</span><span id="line-18"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Typed.NN.Transformer</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-19"></span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad</span></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Proxy</span></span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.Generics</span></span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">System.IO.Unsafe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">unsafePerformIO</span></span><span class="hs-special">)</span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.DType.html"><span class="hs-identifier">Torch.DType</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Device.html"><span class="hs-identifier">Torch.Device</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-27"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.HList.html"><span class="hs-identifier">Torch.HList</span></a></span><span>
</span><span id="line-28"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-29"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">A</span></span><span>
</span><span id="line-30"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html"><span class="hs-identifier">Torch.Typed.Aux</span></a></span><span>
</span><span id="line-31"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html"><span class="hs-identifier">Torch.Typed.Factories</span></a></span><span>
</span><span id="line-32"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html"><span class="hs-identifier">Torch.Typed.Functional</span></a></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#linear"><span class="hs-identifier">linear</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#log"><span class="hs-identifier">log</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-33"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html"><span class="hs-identifier">Torch.Typed.NN.Dropout</span></a></span><span>
</span><span id="line-34"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html"><span class="hs-identifier">Torch.Typed.NN.Linear</span></a></span><span>
</span><span id="line-35"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html"><span class="hs-identifier">Torch.Typed.NN.Normalization</span></a></span><span>
</span><span id="line-36"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html"><span class="hs-identifier">Torch.Typed.NN.Sparse</span></a></span><span>
</span><span id="line-37"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html"><span class="hs-identifier">Torch.Typed.Parameter</span></a></span><span>
</span><span id="line-38"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html"><span class="hs-identifier">Torch.Typed.Tensor</span></a></span><span>
</span><span id="line-39"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">cos</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">exp</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">sin</span></span><span class="hs-special">)</span><span>
</span><span id="line-40"></span><span>
</span><span id="line-41"></span><span id="residual"><span class="annot"><span class="annottext">residual :: (Tensor device dtype shape -&gt; m (Tensor device dtype' shape'))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
      (CheckBroadcast
         shape
         shape'
         (ComputeBroadcast
            (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
    -&gt; m b)
-&gt; Tensor device dtype shape
-&gt; m b
</span><a href="Torch.Typed.NN.Transformer.html#residual"><span class="hs-identifier hs-var hs-var">residual</span></a></span></span><span> </span><span id="local-6989586621679726660"><span class="annot"><span class="annottext">Tensor device dtype shape -&gt; m (Tensor device dtype' shape')
</span><a href="#local-6989586621679726660"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span id="local-6989586621679726659"><span class="annot"><span class="annottext">Tensor
  device
  (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
  (CheckBroadcast
     shape
     shape'
     (ComputeBroadcast
        (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
-&gt; m b
</span><a href="#local-6989586621679726659"><span class="hs-identifier hs-var">g</span></a></span></span><span> </span><span id="local-6989586621679726658"><span class="annot"><span class="annottext">Tensor device dtype shape
</span><a href="#local-6989586621679726658"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype shape -&gt; m (Tensor device dtype' shape')
</span><a href="#local-6989586621679726660"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype shape
</span><a href="#local-6989586621679726658"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">m (Tensor device dtype' shape')
-&gt; (Tensor device dtype' shape' -&gt; m b) -&gt; m b
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span id="local-6989586621679726657"><span class="annot"><span class="annottext">Tensor device dtype' shape'
</span><a href="#local-6989586621679726657"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
  (CheckBroadcast
     shape
     shape'
     (ComputeBroadcast
        (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
-&gt; m b
</span><a href="#local-6989586621679726659"><span class="hs-identifier hs-var">g</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype shape
</span><a href="#local-6989586621679726658"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype shape
-&gt; Tensor device dtype' shape'
-&gt; Tensor
     device
     (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
     (CheckBroadcast
        shape
        shape'
        (ComputeBroadcast
           (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype' shape'
</span><a href="#local-6989586621679726657"><span class="hs-identifier hs-var">x'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-42"></span><span>
</span><span id="line-43"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-44"></span><span class="hs-comment">-- Relation-Aware Multi-Headed Attention Layer</span><span>
</span><span id="line-45"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-46"></span><span>
</span><span id="line-47"></span><span class="hs-keyword">data</span><span>
</span><span id="line-48"></span><span>  </span><span id="MultiheadAttentionSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-var">MultiheadAttentionSpec</span></a></span></span><span>
</span><span id="line-49"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726655"><span class="annot"><a href="#local-6989586621679726655"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-50"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726654"><span class="annot"><a href="#local-6989586621679726654"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-51"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726653"><span class="annot"><a href="#local-6989586621679726653"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-52"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726652"><span class="annot"><a href="#local-6989586621679726652"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-53"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726651"><span class="annot"><a href="#local-6989586621679726651"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-54"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726650"><span class="annot"><a href="#local-6989586621679726650"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-55"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-56"></span><span>  </span><span id="local-6989586621679726644"><span id="local-6989586621679726645"><span id="local-6989586621679726646"><span id="local-6989586621679726647"><span id="local-6989586621679726648"><span id="local-6989586621679726649"><span id="MultiheadAttentionSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-var">MultiheadAttentionSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-57"></span><span>    </span><span class="hs-comment">-- | spec for dropout</span><span>
</span><span id="line-58"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-59"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726649"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726648"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726647"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726646"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726645"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726644"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-60"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726637"><span id="local-6989586621679726639"><span id="local-6989586621679726641"><span class="annot"><span class="annottext">Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
[MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
(Int
 -&gt; MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; ShowS)
-&gt; (MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; String)
-&gt; ([MultiheadAttentionSpec
       embedDim kEmbedDim vEmbedDim numHeads dtype device]
    -&gt; ShowS)
-&gt; Show
     (MultiheadAttentionSpec
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showList :: [MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
show :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showsPrec :: Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726632"><span id="local-6989586621679726634"><span class="annot"><span class="annottext">MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
(MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; Bool)
-&gt; (MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; MultiheadAttentionSpec
         embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; Bool)
-&gt; Eq
     (MultiheadAttentionSpec
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
/= :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
$c/= :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
== :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
$c== :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-61"></span><span>
</span><span id="line-62"></span><span id="local-6989586621679726629"><span id="local-6989586621679726630"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-63"></span><span>  </span><span id="MultiheadAttention"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span></span><span>
</span><span id="line-64"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726628"><span class="annot"><a href="#local-6989586621679726628"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-65"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726627"><span class="annot"><a href="#local-6989586621679726627"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-66"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726626"><span class="annot"><a href="#local-6989586621679726626"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-67"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726625"><span class="annot"><a href="#local-6989586621679726625"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-68"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726624"><span class="annot"><a href="#local-6989586621679726624"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-69"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726623"><span class="annot"><a href="#local-6989586621679726623"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-70"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-71"></span><span>  </span><span id="local-6989586621679727291"><span id="local-6989586621679727292"><span id="local-6989586621679727293"><span id="local-6989586621679727294"><span id="local-6989586621679727295"><span id="local-6989586621679727296"><span id="MultiheadAttention"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-72"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | in-projection for query</span><span>
</span><span id="line-73"></span><span>      </span><span id="mhaQInProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaQInProj"><span class="hs-identifier hs-var hs-var">mhaQInProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727292"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727291"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-74"></span><span>      </span><span class="hs-comment">-- | in-projection for key</span><span>
</span><span id="line-75"></span><span>      </span><span id="mhaKInProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear kEmbedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaKInProj"><span class="hs-identifier hs-var hs-var">mhaKInProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727295"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727292"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727291"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-76"></span><span>      </span><span class="hs-comment">-- | in-projection for value</span><span>
</span><span id="line-77"></span><span>      </span><span id="mhaVInProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear vEmbedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaVInProj"><span class="hs-identifier hs-var hs-var">mhaVInProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727294"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727292"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727291"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-78"></span><span>      </span><span class="hs-comment">-- | out-projection</span><span>
</span><span id="line-79"></span><span>      </span><span id="mhaOutProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaOutProj"><span class="hs-identifier hs-var hs-var">mhaOutProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727292"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727291"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-80"></span><span>      </span><span class="hs-comment">-- | dropout</span><span>
</span><span id="line-81"></span><span>      </span><span id="mhaDropout"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#mhaDropout"><span class="hs-identifier hs-var hs-var">mhaDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-82"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-83"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727296"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727295"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727294"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727293"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727292"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727291"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-84"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726611"><span id="local-6989586621679726613"><span id="local-6989586621679726615"><span class="annot"><span class="annottext">Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
[MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
(Int
 -&gt; MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; ShowS)
-&gt; (MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; String)
-&gt; ([MultiheadAttention
       embedDim kEmbedDim vEmbedDim numHeads dtype device]
    -&gt; ShowS)
-&gt; Show
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showList :: [MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
show :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showsPrec :: Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; Rep
      (MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
      x)
-&gt; (forall x.
    Rep
      (MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
      x
    -&gt; MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; Generic
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall x.
Rep
  (MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device)
  x
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
forall x.
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Rep
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device)
  x
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Rep
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
     x
$cto :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device)
  x
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
$cfrom :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Rep
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726604"><span id="local-6989586621679726606"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
(MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; HList
      (Parameters
         (MultiheadAttention
            embedDim kEmbedDim vEmbedDim numHeads dtype device)))
-&gt; (MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; HList
         (Parameters
            (MultiheadAttention
               embedDim kEmbedDim vEmbedDim numHeads dtype device))
    -&gt; MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; Parameterized
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall f.
(f -&gt; HList (Parameters f))
-&gt; (f -&gt; HList (Parameters f) -&gt; f) -&gt; Parameterized f
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
replaceParameters :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
$creplaceParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
flattenParameters :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
$cflattenParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
</span><a href="Torch.Typed.Parameter.html#C%3AParameterized"><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Parameterized</span></a></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-85"></span><span>
</span><span id="line-86"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-type">multiheadAttention</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-87"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679726992"><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679726990"><span class="annot"><a href="#local-6989586621679726990"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679726989"><span class="annot"><a href="#local-6989586621679726989"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679726994"><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679726988"><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679726987"><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span></span><span> </span><span id="local-6989586621679726986"><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679726991"><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span></span><span> </span><span id="local-6989586621679726985"><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726984"><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-88"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-89"></span><span>    </span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-90"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726990"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726989"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-91"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-92"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-93"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-94"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-95"></span><span>    </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-96"></span><span>    </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-97"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-98"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-99"></span><span>  </span><span class="hs-comment">-- | multi-head attention model ADT</span><span>
</span><span id="line-100"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726990"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726989"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-101"></span><span>  </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-102"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-103"></span><span>  </span><span class="hs-comment">-- | optional attention mask</span><span>
</span><span id="line-104"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-105"></span><span>  </span><span class="hs-comment">-- | optional key padding mask</span><span>
</span><span id="line-106"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-107"></span><span>  </span><span class="hs-comment">-- | optional key relations</span><span>
</span><span id="line-108"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-109"></span><span>  </span><span class="hs-comment">-- | optional value relations</span><span>
</span><span id="line-110"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-111"></span><span>  </span><span class="hs-comment">-- | query representation</span><span>
</span><span id="line-112"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-113"></span><span>  </span><span class="hs-comment">-- | key representation</span><span>
</span><span id="line-114"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726990"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-115"></span><span>  </span><span class="hs-comment">-- | value representation</span><span>
</span><span id="line-116"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726989"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-117"></span><span>  </span><span class="hs-comment">-- | attention and attention averaged over heads</span><span>
</span><span id="line-118"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-119"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-120"></span><span>      </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726988"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-121"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-122"></span><span id="multiheadAttention"><span class="annot"><span class="annottext">multiheadAttention :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-var hs-var">multiheadAttention</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726597"><span id="local-6989586621679726598"><span id="local-6989586621679726599"><span id="local-6989586621679726600"><span id="local-6989586621679726601"><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
Linear kEmbedDim embedDim dtype device
Linear vEmbedDim embedDim dtype device
Dropout
mhaDropout :: Dropout
mhaOutProj :: Linear embedDim embedDim dtype device
mhaVInProj :: Linear vEmbedDim embedDim dtype device
mhaKInProj :: Linear kEmbedDim embedDim dtype device
mhaQInProj :: Linear embedDim embedDim dtype device
mhaDropout :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
mhaOutProj :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
mhaVInProj :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear vEmbedDim embedDim dtype device
mhaKInProj :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear kEmbedDim embedDim dtype device
mhaQInProj :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
</span><a href="#local-6989586621679726597"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679726596"><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726596"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679726595"><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679726595"><span class="hs-identifier hs-var">attentionMask</span></a></span></span><span> </span><span id="local-6989586621679726594"><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726594"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span></span><span> </span><span id="local-6989586621679726593"><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726593"><span class="hs-identifier hs-var">keyRelations</span></a></span></span><span> </span><span id="local-6989586621679726592"><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726592"><span class="hs-identifier hs-var">valueRelations</span></a></span></span><span> </span><span id="local-6989586621679726591"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726591"><span class="hs-identifier hs-var">query</span></a></span></span><span> </span><span id="local-6989586621679726590"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679726590"><span class="hs-identifier hs-var">key</span></a></span></span><span> </span><span id="local-6989586621679726589"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679726589"><span class="hs-identifier hs-var">value</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-123"></span><span>  </span><span id="local-6989586621679726588"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726588"><span class="hs-identifier hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span>
</span><span id="line-124"></span><span>    </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679726597"><span class="hs-identifier hs-var">mhaDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726596"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-125"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; IO
      (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]))
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 3, DimOutOfBoundCheck shape 3, KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
forall (dim :: Nat) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat dim, DimOutOfBoundCheck shape dim, KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#softmax"><span class="hs-identifier hs-var">softmax</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span>
</span><span id="line-126"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726584"><span class="hs-identifier hs-var">_maskKeyPaddings</span></a></span><span>
</span><span id="line-127"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726583"><span class="hs-identifier hs-var">_maskAttention</span></a></span><span>
</span><span id="line-128"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; IO
      (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]))
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726582"><span class="hs-identifier hs-var">_attentionWeights</span></a></span><span>
</span><span id="line-129"></span><span>  </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim],
 Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726581"><span class="hs-identifier hs-var">_attention</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726588"><span class="hs-identifier hs-var">weights</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679726580"><span class="hs-identifier hs-var">averageOverHeads</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726588"><span class="hs-identifier hs-var">weights</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-130"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-131"></span><span>    </span><span id="local-6989586621679726582"><span class="annot"><span class="annottext">_attentionWeights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726582"><span class="hs-identifier hs-var hs-var">_attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-132"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726579"><span class="annot"><span class="annottext">scaling :: Double
</span><a href="#local-6989586621679726579"><span class="hs-identifier hs-var hs-var">scaling</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">Prelude.sqrt</span></span><span> </span><span class="annot"><span class="annottext">(Double -&gt; Double) -&gt; (Int -&gt; Double) -&gt; Int -&gt; Double
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Double) -&gt; Int -&gt; Double
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat headDim =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-133"></span><span>          </span><span id="local-6989586621679726576"><span class="annot"><span class="annottext">q :: Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
</span><a href="#local-6989586621679726576"><span class="hs-identifier hs-var hs-var">q</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall (seqLen'' :: Nat).
KnownNat seqLen'' =&gt;
Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679726575"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Double
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#divScalar"><span class="hs-identifier hs-var">divScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679726579"><span class="hs-identifier hs-var">scaling</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
</span><a href="#local-6989586621679726601"><span class="hs-identifier hs-var">mhaQInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726591"><span class="hs-identifier hs-var">query</span></a></span><span>
</span><span id="line-134"></span><span>          </span><span id="local-6989586621679726572"><span class="annot"><span class="annottext">k :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679726572"><span class="hs-identifier hs-var hs-var">k</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall (seqLen'' :: Nat).
KnownNat seqLen'' =&gt;
Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679726575"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, kEmbedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear kEmbedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear kEmbedDim embedDim dtype device
</span><a href="#local-6989586621679726600"><span class="hs-identifier hs-var">mhaKInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, kEmbedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679726590"><span class="hs-identifier hs-var">key</span></a></span><span>
</span><span id="line-135"></span><span>          </span><span id="local-6989586621679726571"><span class="annot"><span class="annottext">weights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726571"><span class="hs-identifier hs-var hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, headDim, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
</span><a href="#local-6989586621679726576"><span class="hs-identifier hs-var">q</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, headDim, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679726572"><span class="hs-identifier hs-var">k</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-136"></span><span>          </span><span id="local-6989586621679726568"><span class="annot"><span class="annottext">weights' :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726568"><span class="hs-identifier hs-var hs-var">weights'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726593"><span class="hs-identifier hs-var">keyRelations</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-137"></span><span>            </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726571"><span class="hs-identifier hs-var">weights</span></a></span><span>
</span><span id="line-138"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679726567"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679726567"><span class="hs-identifier hs-var">kr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726571"><span class="hs-identifier hs-var">weights</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
</span><a href="#local-6989586621679726576"><span class="hs-identifier hs-var">q</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', headDim, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-operator hs-var">`matmul`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', headDim, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679726567"><span class="hs-identifier hs-var">kr</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-139"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726568"><span class="hs-identifier hs-var">weights'</span></a></span><span>
</span><span id="line-140"></span><span>    </span><span id="local-6989586621679726583"><span class="annot"><span class="annottext">_maskAttention :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726583"><span class="hs-identifier hs-var hs-var">_maskAttention</span></a></span></span><span> </span><span id="local-6989586621679726566"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726566"><span class="hs-identifier hs-var">attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-141"></span><span>      </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679726595"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-142"></span><span>        </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726566"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span>
</span><span id="line-143"></span><span>        </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679726565"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679726565"><span class="hs-identifier hs-var">am</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726566"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, 1, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, 1, seqLen', seqLen]
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679726565"><span class="hs-identifier hs-var">am</span></a></span><span>
</span><span id="line-144"></span><span>    </span><span id="local-6989586621679726584"><span class="annot"><span class="annottext">_maskKeyPaddings :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726584"><span class="hs-identifier hs-var hs-var">_maskKeyPaddings</span></a></span></span><span> </span><span id="local-6989586621679726563"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726563"><span class="hs-identifier hs-var">attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-145"></span><span>      </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726594"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-146"></span><span>        </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726563"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span>
</span><span id="line-147"></span><span>        </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679726562"><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679726562"><span class="hs-identifier hs-var">kpm</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-148"></span><span>          </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726561"><span class="annot"><span class="annottext">keyPaddingMask' :: Tensor device 'Bool '[batchSize, 1, 1, seqLen]
</span><a href="#local-6989586621679726561"><span class="hs-identifier hs-var hs-var">keyPaddingMask'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 2, shape' ~ Unsqueeze shape 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, 1, seqLen]
 -&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen])
-&gt; (Tensor device 'Bool '[batchSize, seqLen]
    -&gt; Tensor device 'Bool '[batchSize, 1, seqLen])
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ Unsqueeze shape 1) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, seqLen]
 -&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen])
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679726562"><span class="hs-identifier hs-var">kpm</span></a></span><span>
</span><span id="line-149"></span><span>           </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, 1, 1, seqLen]
-&gt; Double
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall a (shape :: [Nat]) (shape' :: [Nat]) (shape'' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(Scalar a, shape'' ~ Broadcast shape shape') =&gt;
Tensor device 'Bool shape'
-&gt; a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Functional.html#maskedFill"><span class="hs-identifier hs-var">maskedFill</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, 1, 1, seqLen]
</span><a href="#local-6989586621679726561"><span class="hs-identifier hs-var">keyPaddingMask'</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726563"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span>
</span><span id="line-150"></span><span>    </span><span id="local-6989586621679726581"><span class="annot"><span class="annottext">_attention :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726581"><span class="hs-identifier hs-var hs-var">_attention</span></a></span></span><span> </span><span id="local-6989586621679726558"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726558"><span class="hs-identifier hs-var">attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-151"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726557"><span class="annot"><span class="annottext">v :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679726557"><span class="hs-identifier hs-var hs-var">v</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall (seqLen'' :: Nat).
KnownNat seqLen'' =&gt;
Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679726575"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, vEmbedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear vEmbedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear vEmbedDim embedDim dtype device
</span><a href="#local-6989586621679726599"><span class="hs-identifier hs-var">mhaVInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, vEmbedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679726589"><span class="hs-identifier hs-var">value</span></a></span><span>
</span><span id="line-152"></span><span>          </span><span id="local-6989586621679726556"><span class="annot"><span class="annottext">attention :: Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679726556"><span class="hs-identifier hs-var hs-var">attention</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726558"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679726557"><span class="hs-identifier hs-var">v</span></a></span><span>
</span><span id="line-153"></span><span>          </span><span id="local-6989586621679726555"><span class="annot"><span class="annottext">attention' :: Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679726555"><span class="hs-identifier hs-var hs-var">attention'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726592"><span class="hs-identifier hs-var">valueRelations</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-154"></span><span>            </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679726556"><span class="hs-identifier hs-var">attention</span></a></span><span>
</span><span id="line-155"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679726554"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679726554"><span class="hs-identifier hs-var">vr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679726556"><span class="hs-identifier hs-var">attention</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679726558"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679726554"><span class="hs-identifier hs-var">vr</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-156"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
</span><a href="#local-6989586621679726598"><span class="hs-identifier hs-var">mhaOutProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape '[batchSize, seqLen', embedDim],
 Numel shape ~ Numel '[batchSize, seqLen', embedDim]) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726987"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679726555"><span class="hs-identifier hs-var">attention'</span></a></span><span>
</span><span id="line-157"></span><span>    </span><span id="local-6989586621679726580"><span class="annot"><span class="annottext">averageOverHeads :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679726580"><span class="hs-identifier hs-var hs-var">averageOverHeads</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-158"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726552"><span class="annot"><span class="annottext">numHeads' :: Int
</span><a href="#local-6989586621679726552"><span class="hs-identifier hs-var hs-var">numHeads'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">KnownNat numHeads =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-159"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Int
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#divScalar"><span class="hs-identifier hs-var">divScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679726552"><span class="hs-identifier hs-var">numHeads'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', seqLen]
 -&gt; Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (dtype' :: DType) (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ DropValue shape 1,
 SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape'
forall (d :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (device :: (DeviceType, Nat)).
(KnownNat d, shape' ~ DropValue shape d,
 SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape'
</span><a href="Torch.Typed.Functional.html#sumDim"><span class="hs-identifier hs-var">sumDim</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-160"></span><span>    </span><span class="annot"><a href="#local-6989586621679726575"><span class="hs-identifier hs-type">reshape'</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-161"></span><span>      </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679727268"><span class="annot"><a href="#local-6989586621679727268"><span class="hs-identifier hs-type">seqLen''</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-162"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679727268"><span class="hs-identifier hs-type">seqLen''</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-163"></span><span>      </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679727268"><span class="hs-identifier hs-type">seqLen''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726992"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-164"></span><span>      </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726984"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726985"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679727268"><span class="hs-identifier hs-type">seqLen''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-165"></span><span>    </span><span id="local-6989586621679726575"><span class="annot"><span class="annottext">reshape' :: Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679726575"><span class="hs-identifier hs-var hs-var">reshape'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen'', numHeads, headDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen'', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen'', numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape '[batchSize, seqLen'', numHeads, headDim],
 Numel shape ~ Numel '[batchSize, seqLen'', numHeads, headDim]) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype '[batchSize, seqLen'', numHeads, headDim]
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726986"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679727268"><span class="hs-identifier hs-type">seqLen''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726994"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726991"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-166"></span><span>
</span><span id="line-167"></span><span id="local-6989586621679726545"><span id="local-6989586621679726546"><span id="local-6989586621679726547"><span id="local-6989586621679726548"><span id="local-6989586621679726549"><span id="local-6989586621679726550"><span class="hs-keyword">instance</span><span>
</span><span id="line-168"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726550"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726549"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726548"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726547"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-169"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726546"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-170"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726545"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-171"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726545"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726546"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-172"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-173"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-174"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726550"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726549"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726548"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726547"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726546"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726545"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-175"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726550"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726549"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726548"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726547"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726546"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726545"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-176"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-177"></span><span>  </span><span id="local-6989586621679726542"><span class="annot"><span class="annottext">sample :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span id="local-6989586621679726540"><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679726540"><span class="hs-identifier hs-var">mhaDropoutSpec</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-178"></span><span>    </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Linear kEmbedDim embedDim dtype device
-&gt; Linear vEmbedDim embedDim dtype device
-&gt; Linear embedDim embedDim dtype device
-&gt; Dropout
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
forall (embedDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat).
Linear embedDim embedDim dtype device
-&gt; Linear kEmbedDim embedDim dtype device
-&gt; Linear vEmbedDim embedDim dtype device
-&gt; Linear embedDim embedDim dtype device
-&gt; Dropout
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span><span>
</span><span id="line-179"></span><span>      </span><span class="annot"><span class="annottext">(Linear embedDim embedDim dtype device
 -&gt; Linear kEmbedDim embedDim dtype device
 -&gt; Linear vEmbedDim embedDim dtype device
 -&gt; Linear embedDim embedDim dtype device
 -&gt; Dropout
 -&gt; MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear embedDim embedDim dtype device)
-&gt; IO
     (Linear kEmbedDim embedDim dtype device
      -&gt; Linear vEmbedDim embedDim dtype device
      -&gt; Linear embedDim embedDim dtype device
      -&gt; Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
-&gt; IO (Linear embedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-180"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear kEmbedDim embedDim dtype device
   -&gt; Linear vEmbedDim embedDim dtype device
   -&gt; Linear embedDim embedDim dtype device
   -&gt; Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear kEmbedDim embedDim dtype device)
-&gt; IO
     (Linear vEmbedDim embedDim dtype device
      -&gt; Linear embedDim embedDim dtype device
      -&gt; Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec kEmbedDim embedDim dtype device
-&gt; IO (Linear kEmbedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec kEmbedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-181"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear vEmbedDim embedDim dtype device
   -&gt; Linear embedDim embedDim dtype device
   -&gt; Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear vEmbedDim embedDim dtype device)
-&gt; IO
     (Linear embedDim embedDim dtype device
      -&gt; Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec vEmbedDim embedDim dtype device
-&gt; IO (Linear vEmbedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec vEmbedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-182"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear embedDim embedDim dtype device
   -&gt; Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear embedDim embedDim dtype device)
-&gt; IO
     (Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
-&gt; IO (Linear embedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-183"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO Dropout
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679726540"><span class="hs-identifier hs-var">mhaDropoutSpec</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-184"></span><span>
</span><span id="line-185"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-186"></span><span class="hs-comment">-- Transformer MLP Layer</span><span>
</span><span id="line-187"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-188"></span><span>
</span><span id="line-189"></span><span class="hs-keyword">data</span><span>
</span><span id="line-190"></span><span>  </span><span id="TransformerMLPSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-var">TransformerMLPSpec</span></a></span></span><span>
</span><span id="line-191"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726537"><span class="annot"><a href="#local-6989586621679726537"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-192"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726536"><span class="annot"><a href="#local-6989586621679726536"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-193"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726535"><span class="annot"><a href="#local-6989586621679726535"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-194"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726534"><span class="annot"><a href="#local-6989586621679726534"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-195"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-196"></span><span>  </span><span id="TransformerMLPSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-var">TransformerMLPSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-197"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679727115"><span class="annot"><a href="#local-6989586621679727115"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679727114"><span class="annot"><a href="#local-6989586621679727114"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679727113"><span class="annot"><a href="#local-6989586621679727113"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679727112"><span class="annot"><a href="#local-6989586621679727112"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-198"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | spec for relu dropout</span><span>
</span><span id="line-199"></span><span>      </span><span id="dropout0Spec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#dropout0Spec"><span class="hs-identifier hs-var hs-var">dropout0Spec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-200"></span><span>      </span><span class="hs-comment">-- | spec for other dropout</span><span>
</span><span id="line-201"></span><span>      </span><span id="dropout1Spec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#dropout1Spec"><span class="hs-identifier hs-var hs-var">dropout1Spec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-202"></span><span>      </span><span class="hs-comment">-- | epsilon for layer norm</span><span>
</span><span id="line-203"></span><span>      </span><span id="epsSpec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; Double
</span><a href="Torch.Typed.NN.Transformer.html#epsSpec"><span class="hs-identifier hs-var hs-var">epsSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-204"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-205"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727115"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727114"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727113"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727112"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-206"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726524"><span id="local-6989586621679726526"><span id="local-6989586621679726528"><span class="annot"><span class="annottext">Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
(Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerMLPSpec embedDim ffnDim dtype device -&gt; String)
-&gt; ([TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS)
-&gt; Show (TransformerMLPSpec embedDim ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
showList :: [TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
show :: TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
showsPrec :: Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726520"><span id="local-6989586621679726522"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
(TransformerMLPSpec embedDim ffnDim dtype device
 -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool)
-&gt; (TransformerMLPSpec embedDim ffnDim dtype device
    -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool)
-&gt; Eq (TransformerMLPSpec embedDim ffnDim dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
/= :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
$c/= :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
== :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
$c== :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-207"></span><span>
</span><span id="line-208"></span><span id="local-6989586621679726518"><span id="local-6989586621679726519"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-209"></span><span>  </span><span id="TransformerMLP"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span></span><span>
</span><span id="line-210"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726517"><span class="annot"><a href="#local-6989586621679726517"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-211"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726516"><span class="annot"><a href="#local-6989586621679726516"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-212"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726515"><span class="annot"><a href="#local-6989586621679726515"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-213"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726514"><span class="annot"><a href="#local-6989586621679726514"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-214"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-215"></span><span>  </span><span id="TransformerMLP"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-216"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679727129"><span class="annot"><a href="#local-6989586621679727129"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679727128"><span class="annot"><a href="#local-6989586621679727128"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679727127"><span class="annot"><a href="#local-6989586621679727127"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679727126"><span class="annot"><a href="#local-6989586621679727126"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-217"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | first fully connected layer</span><span>
</span><span id="line-218"></span><span>      </span><span id="linear0"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Linear embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#linear0"><span class="hs-identifier hs-var hs-var">linear0</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727129"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727128"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727127"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727126"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-219"></span><span>      </span><span class="hs-comment">-- | second fully connected layer</span><span>
</span><span id="line-220"></span><span>      </span><span id="linear1"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#linear1"><span class="hs-identifier hs-var hs-var">linear1</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727128"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727129"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727127"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727126"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-221"></span><span>      </span><span class="hs-comment">-- | relu dropout</span><span>
</span><span id="line-222"></span><span>      </span><span id="dropout0"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#dropout0"><span class="hs-identifier hs-var hs-var">dropout0</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-223"></span><span>      </span><span class="hs-comment">-- | other dropout</span><span>
</span><span id="line-224"></span><span>      </span><span id="dropout1"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#dropout1"><span class="hs-identifier hs-var hs-var">dropout1</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-225"></span><span>      </span><span class="hs-comment">-- | layer norm</span><span>
</span><span id="line-226"></span><span>      </span><span id="ln"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
</span><a href="Torch.Typed.NN.Transformer.html#ln"><span class="hs-identifier hs-var hs-var">ln</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html#LayerNorm"><span class="hs-identifier hs-type">LayerNorm</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679727129"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679727127"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727126"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-227"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-228"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727129"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727128"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727127"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727126"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-229"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726502"><span id="local-6989586621679726504"><span id="local-6989586621679726506"><span class="annot"><span class="annottext">Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
TransformerMLP embedDim ffnDim dtype device -&gt; String
(Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerMLP embedDim ffnDim dtype device -&gt; String)
-&gt; ([TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS)
-&gt; Show (TransformerMLP embedDim ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; String
showList :: [TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
show :: TransformerMLP embedDim ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; String
showsPrec :: Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TransformerMLP embedDim ffnDim dtype device
 -&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x)
-&gt; (forall x.
    Rep (TransformerMLP embedDim ffnDim dtype device) x
    -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; Generic (TransformerMLP embedDim ffnDim dtype device)
forall x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
forall x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
$cto :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
$cfrom :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726496"><span id="local-6989586621679726498"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
(TransformerMLP embedDim ffnDim dtype device
 -&gt; HList
      (Parameters (TransformerMLP embedDim ffnDim dtype device)))
-&gt; (TransformerMLP embedDim ffnDim dtype device
    -&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
    -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; Parameterized (TransformerMLP embedDim ffnDim dtype device)
forall f.
(f -&gt; HList (Parameters f))
-&gt; (f -&gt; HList (Parameters f) -&gt; f) -&gt; Parameterized f
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
replaceParameters :: TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
$creplaceParameters :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
flattenParameters :: TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
$cflattenParameters :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
</span><a href="#local-6989586621679726496"><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Parameterized</span></a></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-230"></span><span>
</span><span id="line-231"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-type">transformerMLP</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-232"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679726979"><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679726975"><span class="annot"><a href="#local-6989586621679726975"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679726978"><span class="annot"><a href="#local-6989586621679726978"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679726977"><span class="annot"><a href="#local-6989586621679726977"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679726980"><span class="annot"><a href="#local-6989586621679726980"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726981"><span class="annot"><a href="#local-6989586621679726981"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-233"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726981"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726980"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-234"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726981"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726980"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-235"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-236"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#IsSuffixOf"><span class="hs-identifier hs-type">IsSuffixOf</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726978"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726977"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-237"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-238"></span><span>  </span><span class="hs-comment">-- | MLP model ADT for transformer</span><span>
</span><span id="line-239"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726975"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726980"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726981"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-240"></span><span>  </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-241"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-242"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726981"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726980"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726978"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726977"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-comment">-- input</span><span>
</span><span id="line-243"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726981"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726980"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726978"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726977"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726979"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-comment">-- output</span><span>
</span><span id="line-244"></span><span id="transformerMLP"><span class="annot"><span class="annottext">transformerMLP :: TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-var hs-var">transformerMLP</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726490"><span id="local-6989586621679726491"><span id="local-6989586621679726492"><span id="local-6989586621679726493"><span id="local-6989586621679726494"><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
Linear embedDim ffnDim dtype device
Linear ffnDim embedDim dtype device
Dropout
ln :: LayerNorm '[embedDim] dtype device
dropout1 :: Dropout
dropout0 :: Dropout
linear1 :: Linear ffnDim embedDim dtype device
linear0 :: Linear embedDim ffnDim dtype device
ln :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
dropout1 :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
dropout0 :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
linear1 :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
linear0 :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; Linear embedDim ffnDim dtype device
</span><a href="#local-6989586621679726490"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679726489"><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726489"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679726488"><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679726488"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-245"></span><span>  </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype (CmpDType dtype dtype))
      (CheckBroadcast
         '[seqLen, batchSize, embedDim]
         '[seqLen, batchSize, embedDim]
         (ComputeBroadcast
            (ReverseImpl '[seqLen, batchSize, embedDim] '[])
            (ReverseImpl '[seqLen, batchSize, embedDim] '[])))
    -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (dtype' :: DType) (m :: Type -&gt; Type) (shape :: [Nat])
       (shape' :: [Nat]) b.
(BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid
   device (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')),
 Monad m) =&gt;
(Tensor device dtype shape -&gt; m (Tensor device dtype' shape'))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
      (CheckBroadcast
         shape
         shape'
         (ComputeBroadcast
            (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
    -&gt; m b)
-&gt; Tensor device dtype shape
-&gt; m b
</span><a href="Torch.Typed.NN.Transformer.html#residual"><span class="hs-identifier hs-var">residual</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="#local-6989586621679726487"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
</span><a href="#local-6989586621679726490"><span class="hs-identifier hs-var">ln</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679726488"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-246"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-247"></span><span>    </span><span id="local-6989586621679726487"><span class="annot"><span class="annottext">f :: Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="#local-6989586621679726487"><span class="hs-identifier hs-var hs-var">f</span></a></span></span><span> </span><span id="local-6989586621679726486"><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679726486"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-248"></span><span>      </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679726491"><span class="hs-identifier hs-var">dropout1</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726489"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-249"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, ffnDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear ffnDim embedDim dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear ffnDim embedDim dtype device
</span><a href="#local-6989586621679726493"><span class="hs-identifier hs-var">linear1</span></a></span><span>
</span><span id="line-250"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679726492"><span class="hs-identifier hs-var">dropout0</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726489"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-251"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#relu"><span class="hs-identifier hs-var">relu</span></a></span><span>
</span><span id="line-252"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
</span><a href="#local-6989586621679726494"><span class="hs-identifier hs-var">linear0</span></a></span><span>
</span><span id="line-253"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim]))
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679726486"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-254"></span><span>
</span><span id="line-255"></span><span id="local-6989586621679726480"><span id="local-6989586621679726481"><span id="local-6989586621679726482"><span id="local-6989586621679726483"><span class="hs-keyword">instance</span><span>
</span><span id="line-256"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726483"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726482"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-257"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726481"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-258"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726480"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-259"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726480"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726481"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-260"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-261"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-262"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726483"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726482"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726481"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726480"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-263"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726483"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726482"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726481"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726480"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-264"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-265"></span><span>  </span><span id="local-6989586621679726478"><span class="annot"><span class="annottext">sample :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
</span><a href="#local-6989586621679726478"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726475"><span id="local-6989586621679726476"><span id="local-6989586621679726477"><span class="annot"><span class="annottext">Double
DropoutSpec
epsSpec :: Double
dropout1Spec :: DropoutSpec
dropout0Spec :: DropoutSpec
epsSpec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; Double
dropout1Spec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
dropout0Spec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="#local-6989586621679726475"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-266"></span><span>    </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
-&gt; Dropout
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Linear embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
-&gt; Dropout
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span><span>
</span><span id="line-267"></span><span>      </span><span class="annot"><span class="annottext">(Linear embedDim ffnDim dtype device
 -&gt; Linear ffnDim embedDim dtype device
 -&gt; Dropout
 -&gt; Dropout
 -&gt; LayerNorm '[embedDim] dtype device
 -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (Linear embedDim ffnDim dtype device)
-&gt; IO
     (Linear ffnDim embedDim dtype device
      -&gt; Dropout
      -&gt; Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim ffnDim dtype device
-&gt; IO (Linear embedDim ffnDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim ffnDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-268"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear ffnDim embedDim dtype device
   -&gt; Dropout
   -&gt; Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (Linear ffnDim embedDim dtype device)
-&gt; IO
     (Dropout
      -&gt; Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec ffnDim embedDim dtype device
-&gt; IO (Linear ffnDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec ffnDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-269"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679726477"><span class="hs-identifier hs-var">dropout0Spec</span></a></span><span>
</span><span id="line-270"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679726476"><span class="hs-identifier hs-var">dropout1Spec</span></a></span><span>
</span><span id="line-271"></span><span>      </span><span class="annot"><span class="annottext">IO
  (LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (LayerNorm '[embedDim] dtype device)
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayerNormSpec '[embedDim] dtype device
-&gt; IO (LayerNorm '[embedDim] dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double -&gt; LayerNormSpec '[embedDim] dtype device
forall (normalizedShape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Double -&gt; LayerNormSpec normalizedShape dtype device
</span><a href="Torch.Typed.NN.Normalization.html#LayerNormSpec"><span class="hs-identifier hs-var">LayerNormSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679726475"><span class="hs-identifier hs-var">epsSpec</span></a></span><span class="hs-special">)</span></span></span></span></span><span>
</span><span id="line-272"></span><span>
</span><span id="line-273"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-274"></span><span class="hs-comment">-- Relation-Aware Transformer Layer</span><span>
</span><span id="line-275"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-276"></span><span>
</span><span id="line-277"></span><span class="hs-keyword">data</span><span>
</span><span id="line-278"></span><span>  </span><span id="TransformerLayerSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-var">TransformerLayerSpec</span></a></span></span><span>
</span><span id="line-279"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726473"><span class="annot"><a href="#local-6989586621679726473"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-280"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726472"><span class="annot"><a href="#local-6989586621679726472"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-281"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726471"><span class="annot"><a href="#local-6989586621679726471"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-282"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726470"><span class="annot"><a href="#local-6989586621679726470"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-283"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726469"><span class="annot"><a href="#local-6989586621679726469"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-284"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726468"><span class="annot"><a href="#local-6989586621679726468"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-285"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726467"><span class="annot"><a href="#local-6989586621679726467"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-286"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-287"></span><span>  </span><span id="TransformerLayerSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-var">TransformerLayerSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-288"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679726967"><span class="annot"><a href="#local-6989586621679726967"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679726966"><span class="annot"><a href="#local-6989586621679726966"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679726965"><span class="annot"><a href="#local-6989586621679726965"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679726964"><span class="annot"><a href="#local-6989586621679726964"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679726963"><span class="annot"><a href="#local-6989586621679726963"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679726962"><span class="annot"><a href="#local-6989586621679726962"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726961"><span class="annot"><a href="#local-6989586621679726961"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-289"></span><span>    </span><span class="hs-special">{</span><span> </span><span id="mhaSpec"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaSpec"><span class="hs-identifier hs-var hs-var">mhaSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726967"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726966"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726965"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726964"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726962"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726961"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-290"></span><span>      </span><span id="attnDropoutSpec"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#attnDropoutSpec"><span class="hs-identifier hs-var hs-var">attnDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-291"></span><span>      </span><span id="epsSpec%27"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Double
</span><a href="Torch.Typed.NN.Transformer.html#epsSpec%27"><span class="hs-identifier hs-var hs-var">epsSpec'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-292"></span><span>      </span><span id="mlpSpec"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mlpSpec"><span class="hs-identifier hs-var hs-var">mlpSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726967"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726963"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726962"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726961"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-293"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-294"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726967"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726966"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726965"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726964"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726963"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726962"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726961"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-295"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726456"><span id="local-6989586621679726458"><span id="local-6989586621679726460"><span class="annot"><span class="annottext">Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
[TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
(Int
 -&gt; TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; ShowS)
-&gt; (TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; String)
-&gt; ([TransformerLayerSpec
       embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLayerSpec
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showList :: [TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
show :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showsPrec :: Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726452"><span id="local-6989586621679726454"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
(TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; Bool)
-&gt; (TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; TransformerLayerSpec
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; Bool)
-&gt; Eq
     (TransformerLayerSpec
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
/= :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
$c/= :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
== :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
$c== :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-296"></span><span>
</span><span id="line-297"></span><span id="local-6989586621679726450"><span id="local-6989586621679726451"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-298"></span><span>  </span><span id="TransformerLayer"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span></span><span>
</span><span id="line-299"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726449"><span class="annot"><a href="#local-6989586621679726449"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-300"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726448"><span class="annot"><a href="#local-6989586621679726448"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-301"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726447"><span class="annot"><a href="#local-6989586621679726447"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-302"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726446"><span class="annot"><a href="#local-6989586621679726446"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-303"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726445"><span class="annot"><a href="#local-6989586621679726445"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-304"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726444"><span class="annot"><a href="#local-6989586621679726444"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-305"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726443"><span class="annot"><a href="#local-6989586621679726443"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-306"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-307"></span><span>  </span><span id="TransformerLayer"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-308"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679727001"><span class="annot"><a href="#local-6989586621679727001"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679727000"><span class="annot"><a href="#local-6989586621679727000"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679726999"><span class="annot"><a href="#local-6989586621679726999"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679726998"><span class="annot"><a href="#local-6989586621679726998"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679726997"><span class="annot"><a href="#local-6989586621679726997"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679726996"><span class="annot"><a href="#local-6989586621679726996"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726995"><span class="annot"><a href="#local-6989586621679726995"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-309"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | multi-head attention</span><span>
</span><span id="line-310"></span><span>      </span><span id="transformerLayer_mha"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_mha"><span class="hs-identifier hs-var hs-var">transformerLayer_mha</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727001"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727000"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726999"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726998"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726995"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-311"></span><span>      </span><span class="hs-comment">-- | dropout</span><span>
</span><span id="line-312"></span><span>      </span><span id="transformerLayer_attnDropout"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_attnDropout"><span class="hs-identifier hs-var hs-var">transformerLayer_attnDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-313"></span><span>      </span><span class="hs-comment">-- | layer norm</span><span>
</span><span id="line-314"></span><span>      </span><span id="transformerLayer_ln"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_ln"><span class="hs-identifier hs-var hs-var">transformerLayer_ln</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html#LayerNorm"><span class="hs-identifier hs-type">LayerNorm</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679727001"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679726996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726995"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-315"></span><span>      </span><span class="hs-comment">-- | MLP</span><span>
</span><span id="line-316"></span><span>      </span><span id="transformerLayer_mlp"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_mlp"><span class="hs-identifier hs-var hs-var">transformerLayer_mlp</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727001"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726997"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726995"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-317"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-318"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727001"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679727000"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726999"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726998"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726997"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726995"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-319"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726432"><span id="local-6989586621679726434"><span id="local-6989586621679726436"><span class="annot"><span class="annottext">Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
[TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
(Int
 -&gt; TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; ShowS)
-&gt; (TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; String)
-&gt; ([TransformerLayer
       embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showList :: [TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
show :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showsPrec :: Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; Rep
      (TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
      x)
-&gt; (forall x.
    Rep
      (TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
      x
    -&gt; TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; Generic
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall x.
Rep
  (TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
  x
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
forall x.
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Rep
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
  x
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Rep
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
     x
$cto :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
  x
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
$cfrom :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Rep
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726426"><span id="local-6989586621679726428"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
(TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; HList
      (Parameters
         (TransformerLayer
            embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)))
-&gt; (TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; HList
         (Parameters
            (TransformerLayer
               embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
    -&gt; TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; Parameterized
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall f.
(f -&gt; HList (Parameters f))
-&gt; (f -&gt; HList (Parameters f) -&gt; f) -&gt; Parameterized f
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
replaceParameters :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
$creplaceParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
flattenParameters :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
$cflattenParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
</span><a href="#local-6989586621679726426"><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Parameterized</span></a></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-320"></span><span>
</span><span id="line-321"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-type">transformerLayer</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-322"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726823"><span class="annot"><a href="#local-6989586621679726823"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726813"><span class="annot"><a href="#local-6989586621679726813"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726822"><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726820"><span class="annot"><a href="#local-6989586621679726820"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726819"><span class="annot"><a href="#local-6989586621679726819"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726821"><span class="annot"><a href="#local-6989586621679726821"><span class="hs-identifier hs-type">headDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726818"><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726817"><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726816"><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679726815"><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726814"><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-323"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726823"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-324"></span><span>    </span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679726821"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679726823"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-325"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726820"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726819"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726823"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726821"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-326"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#IsSuffixOf"><span class="hs-identifier hs-type">IsSuffixOf</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-327"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-328"></span><span>    </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-329"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-330"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-331"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-332"></span><span>    </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-333"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-334"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-335"></span><span>  </span><span class="hs-comment">-- | transformer layer model ADT</span><span>
</span><span id="line-336"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726820"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726819"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726823"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726813"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-337"></span><span>  </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-338"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-339"></span><span>  </span><span class="hs-comment">-- | optional attention mask</span><span>
</span><span id="line-340"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-341"></span><span>  </span><span class="hs-comment">-- | optional key padding mask</span><span>
</span><span id="line-342"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-343"></span><span>  </span><span class="hs-comment">-- | optional key relations</span><span>
</span><span id="line-344"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726821"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-345"></span><span>  </span><span class="hs-comment">-- | optional value relations</span><span>
</span><span id="line-346"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726821"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-347"></span><span>  </span><span class="hs-comment">-- | query representation</span><span>
</span><span id="line-348"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-349"></span><span>  </span><span class="hs-comment">-- | key representation</span><span>
</span><span id="line-350"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726820"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-351"></span><span>  </span><span class="hs-comment">-- | value representation</span><span>
</span><span id="line-352"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726818"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726819"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-353"></span><span>  </span><span class="hs-comment">-- | transformer layer output representation</span><span>
</span><span id="line-354"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726814"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726815"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726816"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726817"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726822"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-355"></span><span id="transformerLayer"><span class="annot"><span class="annottext">transformerLayer :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-var hs-var">transformerLayer</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726421"><span id="local-6989586621679726422"><span id="local-6989586621679726423"><span id="local-6989586621679726424"><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
Dropout
TransformerMLP embedDim ffnDim dtype device
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
transformerLayer_mlp :: TransformerMLP embedDim ffnDim dtype device
transformerLayer_ln :: LayerNorm '[embedDim] dtype device
transformerLayer_attnDropout :: Dropout
transformerLayer_mha :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
transformerLayer_mlp :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
transformerLayer_ln :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
transformerLayer_attnDropout :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Dropout
transformerLayer_mha :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="#local-6989586621679726421"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679726420"><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726420"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679726419"><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679726419"><span class="hs-identifier hs-var">attentionMask</span></a></span></span><span> </span><span id="local-6989586621679726418"><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726418"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span></span><span> </span><span id="local-6989586621679726417"><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726417"><span class="hs-identifier hs-var">keyRelations</span></a></span></span><span> </span><span id="local-6989586621679726416"><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726416"><span class="hs-identifier hs-var">valueRelations</span></a></span></span><span> </span><span id="local-6989586621679726415"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726415"><span class="hs-identifier hs-var">query</span></a></span></span><span> </span><span id="local-6989586621679726414"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679726414"><span class="hs-identifier hs-var">key</span></a></span></span><span> </span><span id="local-6989586621679726413"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679726413"><span class="hs-identifier hs-var">value</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-356"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726412"><span class="annot"><span class="annottext">f :: Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="#local-6989586621679726412"><span class="hs-identifier hs-var hs-var">f</span></a></span></span><span> </span><span id="local-6989586621679726411"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726411"><span class="hs-identifier hs-var">query'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-357"></span><span>        </span><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (seqLen :: Nat) (seqLen' :: Nat)
       (batchSize :: Nat) (headDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(1 &lt;= numHeads, embedDim ~ (headDim * numHeads),
 All
   KnownNat
   '[embedDim, kEmbedDim, vEmbedDim, numHeads, seqLen, seqLen',
     batchSize, headDim],
 KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype,
 MatMulDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype, dtype ~ SumDType dtype,
 SumDTypeIsValid device dtype, KnownDevice device) =&gt;
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-var">multiheadAttention</span></a></span><span> </span><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="#local-6989586621679726424"><span class="hs-identifier hs-var">transformerLayer_mha</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726420"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679726419"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726418"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726417"><span class="hs-identifier hs-var">keyRelations</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679726416"><span class="hs-identifier hs-var">valueRelations</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726411"><span class="hs-identifier hs-var">query'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679726414"><span class="hs-identifier hs-var">key</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679726413"><span class="hs-identifier hs-var">value</span></a></span><span>
</span><span id="line-358"></span><span>          </span><span class="annot"><span class="annottext">IO
  (Tensor device dtype '[batchSize, seqLen', embedDim],
   Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; ((Tensor device dtype '[batchSize, seqLen', embedDim],
     Tensor device dtype '[batchSize, seqLen', seqLen])
    -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679726423"><span class="hs-identifier hs-var">transformerLayer_attnDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726420"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; ((Tensor device dtype '[batchSize, seqLen', embedDim],
     Tensor device dtype '[batchSize, seqLen', seqLen])
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim],
    Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim],
 Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall a b. (a, b) -&gt; a
</span><span class="hs-identifier hs-var">fst</span></span><span>
</span><span id="line-359"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype (CmpDType dtype dtype))
      (CheckBroadcast
         '[batchSize, seqLen', embedDim]
         '[batchSize, seqLen', embedDim]
         (ComputeBroadcast
            (ReverseImpl '[batchSize, seqLen', embedDim] '[])
            (ReverseImpl '[batchSize, seqLen', embedDim] '[])))
    -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (dtype' :: DType) (m :: Type -&gt; Type) (shape :: [Nat])
       (shape' :: [Nat]) b.
(BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid
   device (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')),
 Monad m) =&gt;
(Tensor device dtype shape -&gt; m (Tensor device dtype' shape'))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
      (CheckBroadcast
         shape
         shape'
         (ComputeBroadcast
            (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
    -&gt; m b)
-&gt; Tensor device dtype shape
-&gt; m b
</span><a href="Torch.Typed.NN.Transformer.html#residual"><span class="hs-identifier hs-var">residual</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="#local-6989586621679726412"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
</span><a href="#local-6989586621679726422"><span class="hs-identifier hs-var">transformerLayer_ln</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679726415"><span class="hs-identifier hs-var">query</span></a></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (embedDim :: Nat) (ffnDim :: Nat) (seqLen :: Nat)
       (batchSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
(BasicArithmeticDTypeIsValid device dtype,
 StandardFloatingPointDTypeValidation device dtype,
 KnownNat embedDim,
 IsSuffixOf '[embedDim] '[seqLen, batchSize, embedDim]) =&gt;
TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-var">transformerMLP</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
</span><a href="#local-6989586621679726421"><span class="hs-identifier hs-var">transformerLayer_mlp</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726420"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-360"></span><span>
</span><span id="line-361"></span><span id="local-6989586621679726404"><span id="local-6989586621679726405"><span id="local-6989586621679726406"><span id="local-6989586621679726407"><span id="local-6989586621679726408"><span id="local-6989586621679726409"><span id="local-6989586621679726410"><span class="hs-keyword">instance</span><span>
</span><span id="line-362"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726410"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726409"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726408"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726407"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726406"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-363"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726405"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-364"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726404"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-365"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726404"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726405"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-366"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-367"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-368"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726410"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726409"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726408"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726407"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726406"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726405"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726404"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-369"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726410"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726409"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726408"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726407"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726406"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726405"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726404"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-370"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-371"></span><span>  </span><span id="local-6989586621679726402"><span class="annot"><span class="annottext">sample :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; IO
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
</span><a href="#local-6989586621679726402"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726398"><span id="local-6989586621679726399"><span id="local-6989586621679726400"><span id="local-6989586621679726401"><span class="annot"><span class="annottext">Double
DropoutSpec
TransformerMLPSpec embedDim ffnDim dtype device
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
mlpSpec :: TransformerMLPSpec embedDim ffnDim dtype device
epsSpec' :: Double
attnDropoutSpec :: DropoutSpec
mhaSpec :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
mlpSpec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device
epsSpec' :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Double
attnDropoutSpec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; DropoutSpec
mhaSpec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="#local-6989586621679726398"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-372"></span><span>    </span><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span><span>
</span><span id="line-373"></span><span>      </span><span class="annot"><span class="annottext">(MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; Dropout
 -&gt; LayerNorm '[embedDim] dtype device
 -&gt; TransformerMLP embedDim ffnDim dtype device
 -&gt; TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO
     (Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="#local-6989586621679726401"><span class="hs-identifier hs-var">mhaSpec</span></a></span><span>
</span><span id="line-374"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679726400"><span class="hs-identifier hs-var">attnDropoutSpec</span></a></span><span>
</span><span id="line-375"></span><span>      </span><span class="annot"><span class="annottext">IO
  (LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO (LayerNorm '[embedDim] dtype device)
-&gt; IO
     (TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayerNormSpec '[embedDim] dtype device
-&gt; IO (LayerNorm '[embedDim] dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double -&gt; LayerNormSpec '[embedDim] dtype device
forall (normalizedShape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Double -&gt; LayerNormSpec normalizedShape dtype device
</span><a href="Torch.Typed.NN.Normalization.html#LayerNormSpec"><span class="hs-identifier hs-var">LayerNormSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679726399"><span class="hs-identifier hs-var">epsSpec'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-376"></span><span>      </span><span class="annot"><span class="annottext">IO
  (TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
-&gt; IO
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
</span><a href="#local-6989586621679726398"><span class="hs-identifier hs-var">mlpSpec</span></a></span></span></span></span></span></span></span></span><span>
</span><span id="line-377"></span><span>
</span><span id="line-378"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-379"></span><span class="hs-comment">-- Transformer Language Model (GPT-2)</span><span>
</span><span id="line-380"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-381"></span><span>
</span><span id="line-382"></span><span class="hs-keyword">data</span><span>
</span><span id="line-383"></span><span>  </span><span id="TransformerLMSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-var">TransformerLMSpec</span></a></span></span><span>
</span><span id="line-384"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726397"><span class="annot"><a href="#local-6989586621679726397"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-385"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726396"><span class="annot"><a href="#local-6989586621679726396"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-386"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726395"><span class="annot"><a href="#local-6989586621679726395"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-387"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726394"><span class="annot"><a href="#local-6989586621679726394"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-388"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726393"><span class="annot"><a href="#local-6989586621679726393"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-389"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726392"><span class="annot"><a href="#local-6989586621679726392"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-390"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726391"><span class="annot"><a href="#local-6989586621679726391"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-391"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726390"><span class="annot"><a href="#local-6989586621679726390"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-392"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-393"></span><span>  </span><span id="TransformerLMSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-var">TransformerLMSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-394"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679726691"><span class="annot"><a href="#local-6989586621679726691"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span id="local-6989586621679726690"><span class="annot"><a href="#local-6989586621679726690"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679726689"><span class="annot"><a href="#local-6989586621679726689"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679726688"><span class="annot"><a href="#local-6989586621679726688"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span id="local-6989586621679726687"><span class="annot"><a href="#local-6989586621679726687"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679726686"><span class="annot"><a href="#local-6989586621679726686"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679726685"><span class="annot"><a href="#local-6989586621679726685"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726684"><span class="annot"><a href="#local-6989586621679726684"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-395"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | dropout spec</span><span>
</span><span id="line-396"></span><span>      </span><span id="lmDropoutSpec"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#lmDropoutSpec"><span class="hs-identifier hs-var hs-var">lmDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-397"></span><span>      </span><span class="hs-comment">-- | spec for each and every transformer layer</span><span>
</span><span id="line-398"></span><span>      </span><span id="lmLayerSpec"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLayerSpec
     embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#lmLayerSpec"><span class="hs-identifier hs-var hs-var">lmLayerSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726686"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726686"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726686"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726690"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726689"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726685"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726684"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-399"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-400"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726691"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726690"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726689"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726688"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726687"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726686"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726685"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726684"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-401"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726381"><span id="local-6989586621679726383"><span id="local-6989586621679726385"><span class="annot"><span class="annottext">Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
(Int
 -&gt; TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
 -&gt; ShowS)
-&gt; (TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
    -&gt; String)
-&gt; ([TransformerLMSpec
       numAttnLayers
       numHeads
       ffnDim
       paddingIdx
       numEmbeds
       embedDim
       dtype
       device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLMSpec
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
showList :: [TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
$cshowList :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
show :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
$cshow :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
showsPrec :: Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
$cshowsPrec :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726377"><span id="local-6989586621679726379"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
(TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device
 -&gt; TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
 -&gt; Bool)
-&gt; (TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
    -&gt; TransformerLMSpec
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device
    -&gt; Bool)
-&gt; Eq
     (TransformerLMSpec
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
/= :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
$c/= :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
== :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
$c== :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-402"></span><span>
</span><span id="line-403"></span><span id="local-6989586621679726375"><span id="local-6989586621679726376"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-404"></span><span>  </span><span id="TransformerLM"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span></span><span>
</span><span id="line-405"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726374"><span class="annot"><a href="#local-6989586621679726374"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-406"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726373"><span class="annot"><a href="#local-6989586621679726373"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-407"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726372"><span class="annot"><a href="#local-6989586621679726372"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-408"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726371"><span class="annot"><a href="#local-6989586621679726371"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-409"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726370"><span class="annot"><a href="#local-6989586621679726370"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-410"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726369"><span class="annot"><a href="#local-6989586621679726369"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-411"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726368"><span class="annot"><a href="#local-6989586621679726368"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-412"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726367"><span class="annot"><a href="#local-6989586621679726367"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-413"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-414"></span><span>  </span><span id="TransformerLM"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-415"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679726800"><span class="annot"><a href="#local-6989586621679726800"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span id="local-6989586621679726799"><span class="annot"><a href="#local-6989586621679726799"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679726798"><span class="annot"><a href="#local-6989586621679726798"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679726797"><span class="annot"><a href="#local-6989586621679726797"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span id="local-6989586621679726796"><span class="annot"><a href="#local-6989586621679726796"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679726795"><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679726794"><span class="annot"><a href="#local-6989586621679726794"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679726793"><span class="annot"><a href="#local-6989586621679726793"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-416"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | token embedding</span><span>
</span><span id="line-417"></span><span>      </span><span id="tEmbedding"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding
     ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tEmbedding"><span class="hs-identifier hs-var hs-var">tEmbedding</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Embedding"><span class="hs-identifier hs-type">Embedding</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679726797"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679726796"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Learned"><span class="hs-identifier hs-type">Learned</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726794"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726793"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-418"></span><span>      </span><span class="hs-comment">-- | positional embedding</span><span>
</span><span id="line-419"></span><span>      </span><span id="tPosEmbedding"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tPosEmbedding"><span class="hs-identifier hs-var hs-var">tPosEmbedding</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Embedding"><span class="hs-identifier hs-type">Embedding</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="annot"><span class="hs-number">2048</span></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Constant"><span class="hs-identifier hs-type">Constant</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726794"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726793"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-420"></span><span>      </span><span class="hs-comment">-- | transformer dropout</span><span>
</span><span id="line-421"></span><span>      </span><span id="tDropout"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#tDropout"><span class="hs-identifier hs-var hs-var">tDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-422"></span><span>      </span><span class="hs-comment">-- | transformer layers</span><span>
</span><span id="line-423"></span><span>      </span><span id="tLayers"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
</span><a href="Torch.Typed.NN.Transformer.html#tLayers"><span class="hs-identifier hs-var hs-var">tLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726800"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726799"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726798"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726794"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726793"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-424"></span><span>      </span><span class="hs-comment">-- | final output projection</span><span>
</span><span id="line-425"></span><span>      </span><span id="tProj"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Linear embedDim numEmbeds dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tProj"><span class="hs-identifier hs-var hs-var">tProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726796"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726794"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726793"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-426"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-427"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726800"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726799"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726798"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726797"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726796"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726795"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726794"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726793"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-428"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(forall x.
 TransformerLM
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device
 -&gt; Rep
      (TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
      x)
-&gt; (forall x.
    Rep
      (TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
      x
    -&gt; TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
-&gt; Generic
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
$cto :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
$cfrom :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-429"></span><span>
</span><span id="line-430"></span><span id="local-6989586621679726345"><span id="local-6989586621679726347"><span id="local-6989586621679726349"><span id="local-6989586621679726351"><span id="local-6989586621679726352"><span id="local-6989586621679726353"><span id="local-6989586621679726354"><span id="local-6989586621679726355"><span id="local-6989586621679726356"><span id="local-6989586621679726357"><span id="local-6989586621679726358"><span class="hs-keyword">deriving</span><span> </span><span class="hs-keyword">instance</span><span>
</span><span id="line-431"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span>
</span><span id="line-432"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span>
</span><span id="line-433"></span><span>          </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span>
</span><span id="line-434"></span><span>              </span><span class="annot"><a href="#local-6989586621679726358"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span>
</span><span id="line-435"></span><span>              </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span>
</span><span id="line-436"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726357"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-437"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726357"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-438"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726357"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-439"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726356"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-440"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726355"><span class="hs-identifier hs-type">ffnDim</span></a></span><span>
</span><span id="line-441"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726354"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-442"></span><span>                  </span><span class="annot"><a href="#local-6989586621679726353"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-443"></span><span>              </span><span class="hs-special">)</span><span>
</span><span id="line-444"></span><span>          </span><span class="hs-special">)</span><span>
</span><span id="line-445"></span><span>      </span><span class="hs-special">)</span><span>
</span><span id="line-446"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-447"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726358"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726356"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726355"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726352"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726351"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726357"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726354"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726353"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-448"></span><span>
</span><span id="line-449"></span><span id="local-6989586621679726336"><span id="local-6989586621679726337"><span id="local-6989586621679726338"><span id="local-6989586621679726339"><span id="local-6989586621679726340"><span id="local-6989586621679726341"><span id="local-6989586621679726342"><span id="local-6989586621679726343"><span id="local-6989586621679726344"><span class="hs-keyword">instance</span><span>
</span><span id="line-450"></span><span>  </span><span id="local-6989586621679726332"><span id="local-6989586621679726334"><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679726344"><span class="hs-identifier hs-type">layers</span></a></span><span>
</span><span id="line-451"></span><span>      </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span>
</span><span id="line-452"></span><span>            </span><span class="annot"><a href="#local-6989586621679726343"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span>
</span><span id="line-453"></span><span>            </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span>
</span><span id="line-454"></span><span>                </span><span class="annot"><a href="#local-6989586621679726342"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-455"></span><span>                </span><span class="annot"><a href="#local-6989586621679726342"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-456"></span><span>                </span><span class="annot"><a href="#local-6989586621679726342"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-457"></span><span>                </span><span class="annot"><a href="#local-6989586621679726341"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-458"></span><span>                </span><span class="annot"><a href="#local-6989586621679726340"><span class="hs-identifier hs-type">ffnDim</span></a></span><span>
</span><span id="line-459"></span><span>                </span><span class="annot"><a href="#local-6989586621679726339"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-460"></span><span>                </span><span class="annot"><a href="#local-6989586621679726338"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-461"></span><span>            </span><span class="hs-special">)</span><span>
</span><span id="line-462"></span><span>        </span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-463"></span><span>    </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameterized"><span class="hs-identifier hs-type">Parameterized</span></a></span><span>
</span><span id="line-464"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span>
</span><span id="line-465"></span><span>          </span><span class="annot"><a href="#local-6989586621679726344"><span class="hs-identifier hs-type">layers</span></a></span><span>
</span><span id="line-466"></span><span>      </span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-467"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HAppendFD"><span class="hs-identifier hs-type">HAppendFD</span></a></span><span>
</span><span id="line-468"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameters"><span class="hs-identifier hs-type">Parameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726344"><span class="hs-identifier hs-type">layers</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-469"></span><span>      </span><span class="hs-special">'</span><span class="hs-special">[</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726338"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726339"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726337"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-470"></span><span>         </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726338"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726339"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726337"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-471"></span><span>       </span><span class="hs-special">]</span><span>
</span><span id="line-472"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameters"><span class="hs-identifier hs-type">Parameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726344"><span class="hs-identifier hs-type">layers</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-473"></span><span>          </span><span class="annot"><a href="Torch.HList.html#%2B%2B"><span class="hs-operator hs-type">++</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726338"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726339"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726337"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-474"></span><span>                </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726338"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726339"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726337"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-475"></span><span>              </span><span class="hs-special">]</span><span>
</span><span id="line-476"></span><span>      </span><span class="hs-special">)</span><span>
</span><span id="line-477"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-478"></span><span>  </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameterized"><span class="hs-identifier hs-type">Parameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726343"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726341"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726340"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726336"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726337"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726342"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726339"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726338"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-479"></span><span>
</span><span id="line-480"></span><span class="hs-keyword">data</span><span>
</span><span id="line-481"></span><span>  </span><span id="FoldLayers"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span></span><span>
</span><span id="line-482"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726827"><span class="annot"><a href="#local-6989586621679726827"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-483"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726826"><span class="annot"><a href="#local-6989586621679726826"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-484"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726825"><span class="annot"><a href="#local-6989586621679726825"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-485"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679726824"><span class="annot"><a href="#local-6989586621679726824"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span id="FoldLayers"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span></span><span>
</span><span id="line-486"></span><span>  </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-487"></span><span>    </span><span id="flTrain"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device -&gt; Bool
</span><a href="Torch.Typed.NN.Transformer.html#flTrain"><span class="hs-identifier hs-var hs-var">flTrain</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span class="hs-special">,</span><span>
</span><span id="line-488"></span><span>    </span><span class="hs-comment">-- | optional attention mask</span><span>
</span><span id="line-489"></span><span>    </span><span id="flAttentionMask"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#flAttentionMask"><span class="hs-identifier hs-var hs-var">flAttentionMask</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726824"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726825"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726827"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726826"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726826"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-490"></span><span>    </span><span class="hs-comment">-- | optional key padding mask</span><span>
</span><span id="line-491"></span><span>    </span><span id="flKeyPaddingMask"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#flKeyPaddingMask"><span class="hs-identifier hs-var hs-var">flKeyPaddingMask</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726824"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726827"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726826"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-492"></span><span>  </span><span class="hs-special">}</span><span>
</span><span id="line-493"></span><span>
</span><span id="line-494"></span><span id="local-6989586621679726320"><span id="local-6989586621679726321"><span id="local-6989586621679726322"><span id="local-6989586621679726323"><span id="local-6989586621679726324"><span id="local-6989586621679726325"><span id="local-6989586621679726326"><span id="local-6989586621679726327"><span class="hs-keyword">instance</span><span>
</span><span id="line-495"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726327"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-496"></span><span>    </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679726325"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679726327"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-497"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726327"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726324"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726323"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726325"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-498"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#IsSuffixOf"><span class="hs-identifier hs-type">IsSuffixOf</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726323"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726324"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-499"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-500"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-501"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-502"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-503"></span><span>    </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-504"></span><span>    </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-505"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-506"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-507"></span><span>  </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span>
</span><span id="line-508"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726323"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726324"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-509"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726327"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726320"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-510"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726323"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726324"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-511"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-512"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726321"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726322"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726323"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726324"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726326"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-513"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-514"></span><span>  </span><span id="local-6989586621679726317"><span class="annot"><span class="annottext">apply' :: FoldLayers batchSize seqLen dtype device
-&gt; (TransformerLayer
      embedDim embedDim embedDim numHeads ffnDim dtype device,
    IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726313"><span id="local-6989586621679726314"><span id="local-6989586621679726315"><span class="annot"><span class="annottext">Bool
Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
Maybe (Tensor device 'Bool '[batchSize, seqLen])
flKeyPaddingMask :: Maybe (Tensor device 'Bool '[batchSize, seqLen])
flAttentionMask :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
flTrain :: Bool
flKeyPaddingMask :: forall (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
FoldLayers batchSize seqLen dtype device
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
flAttentionMask :: forall (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
FoldLayers batchSize seqLen dtype device
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
flTrain :: forall (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
FoldLayers batchSize seqLen dtype device -&gt; Bool
</span><a href="#local-6989586621679726313"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679726312"><span class="annot"><span class="annottext">TransformerLayer
  embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679726312"><span class="hs-identifier hs-var">layer</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679726311"><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="#local-6989586621679726311"><span class="hs-identifier hs-var">mx</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="#local-6989586621679726311"><span class="hs-identifier hs-var">mx</span></a></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, embedDim]
    -&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679726310"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726310"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">TransformerLayer
  embedDim embedDim embedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (numHeads :: Nat) (ffnDim :: Nat) (embedDim :: Nat)
       (kEmbedDim :: Nat) (vEmbedDim :: Nat) (headDim :: Nat)
       (seqLen :: Nat) (seqLen' :: Nat) (batchSize :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
(1 &lt;= numHeads, embedDim ~ (headDim * numHeads),
 All
   KnownNat
   '[embedDim, kEmbedDim, vEmbedDim, numHeads, seqLen, seqLen',
     batchSize, headDim],
 IsSuffixOf '[embedDim] '[batchSize, seqLen', embedDim],
 KnownDType dtype, dtype ~ SumDType dtype,
 StandardFloatingPointDTypeValidation device dtype,
 MatMulDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype,
 SumDTypeIsValid device dtype, KnownDevice device) =&gt;
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-var">transformerLayer</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLayer
  embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679726312"><span class="hs-identifier hs-var">layer</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726315"><span class="hs-identifier hs-var">flTrain</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="#local-6989586621679726314"><span class="hs-identifier hs-var">flAttentionMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726313"><span class="hs-identifier hs-var">flKeyPaddingMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
forall a. Maybe a
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
forall a. Maybe a
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726310"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726310"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726310"><span class="hs-identifier hs-var">x</span></a></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-515"></span><span>
</span><span id="line-516"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-type">transformerLM</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-517"></span><span>  </span><span class="hs-keyword">forall</span><span>
</span><span id="line-518"></span><span>    </span><span id="local-6989586621679726718"><span class="annot"><a href="#local-6989586621679726718"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span>
</span><span id="line-519"></span><span>    </span><span id="local-6989586621679726717"><span class="annot"><a href="#local-6989586621679726717"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span>
</span><span id="line-520"></span><span>    </span><span id="local-6989586621679726716"><span class="annot"><a href="#local-6989586621679726716"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span>
</span><span id="line-521"></span><span>    </span><span id="local-6989586621679726725"><span class="annot"><a href="#local-6989586621679726725"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span>
</span><span id="line-522"></span><span>    </span><span id="local-6989586621679726721"><span class="annot"><a href="#local-6989586621679726721"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span>
</span><span id="line-523"></span><span>    </span><span id="local-6989586621679726724"><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span>
</span><span id="line-524"></span><span>    </span><span id="local-6989586621679726723"><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span>
</span><span id="line-525"></span><span>    </span><span id="local-6989586621679726722"><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span>
</span><span id="line-526"></span><span>    </span><span id="local-6989586621679726720"><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span></span><span>
</span><span id="line-527"></span><span>    </span><span id="local-6989586621679726719"><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-528"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726725"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-529"></span><span>    </span><span class="annot"><a href="#local-6989586621679726725"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726721"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-530"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-531"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HFoldrM"><span class="hs-identifier hs-type">HFoldrM</span></a></span><span>
</span><span id="line-532"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-533"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-534"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-535"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726718"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726717"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726716"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-536"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-537"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-538"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-539"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-540"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-541"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-542"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-543"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726718"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726717"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726716"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726725"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726721"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-544"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-545"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-546"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726721"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-547"></span><span id="transformerLM"><span class="annot"><span class="annottext">transformerLM :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var hs-var">transformerLM</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726304"><span id="local-6989586621679726305"><span id="local-6989586621679726306"><span id="local-6989586621679726307"><span id="local-6989586621679726308"><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayer
        embedDim embedDim embedDim numHeads ffnDim dtype device))
Embedding 'Nothing 2048 embedDim 'Constant dtype device
Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
Linear embedDim numEmbeds dtype device
Dropout
tProj :: Linear embedDim numEmbeds dtype device
tLayers :: HList
  (HReplicateR
     numAttnLayers
     (TransformerLayer
        embedDim embedDim embedDim numHeads ffnDim dtype device))
tDropout :: Dropout
tPosEmbedding :: Embedding 'Nothing 2048 embedDim 'Constant dtype device
tEmbedding :: Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
tProj :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Linear embedDim numEmbeds dtype device
tLayers :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
tDropout :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Dropout
tPosEmbedding :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
tEmbedding :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding
     ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="#local-6989586621679726304"><span class="hs-glyph hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">..</span></a></span></span></span></span></span></span><span class="hs-special">}</span><span> </span><span id="local-6989586621679726303"><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726303"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679726302"><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726302"><span class="hs-identifier hs-var">xTokens</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-548"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726301"><span class="annot"><span class="annottext">x :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726301"><span class="hs-identifier hs-var hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (paddingIdx :: Maybe Nat) (shape :: [Nat])
       (numEmbeds :: Nat) (embedSize :: Nat)
       (embeddingType :: EmbeddingType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape' :: [Nat]).
(KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds,
 shape' ~ Reverse (embedSize : Reverse shape)) =&gt;
Embedding paddingIdx numEmbeds embedSize embeddingType dtype device
-&gt; Tensor device 'Int64 shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.NN.Sparse.html#embed"><span class="hs-identifier hs-var">embed</span></a></span><span> </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="#local-6989586621679726308"><span class="hs-identifier hs-var">tEmbedding</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726302"><span class="hs-identifier hs-var">xTokens</span></a></span><span>
</span><span id="line-549"></span><span>      </span><span id="local-6989586621679726299"><span class="annot"><span class="annottext">positions :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726299"><span class="hs-identifier hs-var hs-var">positions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-550"></span><span>        </span><span class="annot"><span class="annottext">Bool
-&gt; Tensor device dtype '[seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', shape' ~ Broadcast shape shape') =&gt;
Bool -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#expand"><span class="hs-identifier hs-var">expand</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726724"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span>
</span><span id="line-551"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Int -&gt; Tensor device dtype '[seqLen, embedDim])
-&gt; Int
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Tensor device 'Int64 '[seqLen]
-&gt; Tensor device dtype '[seqLen, embedDim]
forall (paddingIdx :: Maybe Nat) (shape :: [Nat])
       (numEmbeds :: Nat) (embedSize :: Nat)
       (embeddingType :: EmbeddingType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape' :: [Nat]).
(KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds,
 shape' ~ Reverse (embedSize : Reverse shape)) =&gt;
Embedding paddingIdx numEmbeds embedSize embeddingType dtype device
-&gt; Tensor device 'Int64 shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.NN.Sparse.html#embed"><span class="hs-identifier hs-var">embed</span></a></span><span> </span><span class="annot"><span class="annottext">Embedding 'Nothing 2048 embedDim 'Constant dtype device
</span><a href="#local-6989586621679726307"><span class="hs-identifier hs-var">tPosEmbedding</span></a></span><span>
</span><span id="line-552"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Int64 '[seqLen]
 -&gt; Tensor device dtype '[seqLen, embedDim])
-&gt; (Int -&gt; Tensor device 'Int64 '[seqLen])
-&gt; Int
-&gt; Tensor device dtype '[seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (dtype :: DType) (device :: (DeviceType, Nat))
       (shape :: [Nat]).
KnownDType 'Int64 =&gt;
Tensor device dtype shape -&gt; Tensor device 'Int64 shape
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span>
</span><span id="line-553"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[seqLen] -&gt; Tensor device 'Int64 '[seqLen])
-&gt; (Int -&gt; Tensor device 'Float '[seqLen])
-&gt; Int
-&gt; Tensor device 'Int64 '[seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[seqLen]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-554"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Int -&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (seqLen - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-555"></span><span>  </span><span id="local-6989586621679726295"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726295"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679726306"><span class="hs-identifier hs-var">tDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726303"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726301"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726299"><span class="hs-identifier hs-var">positions</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-556"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726294"><span class="annot"><span class="annottext">attentionMask :: Tensor device 'Bool '[1, seqLen, seqLen]
</span><a href="#local-6989586621679726294"><span class="hs-identifier hs-var hs-var">attentionMask</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-557"></span><span>        </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 0, shape' ~ Unsqueeze shape 0) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-558"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[1, seqLen, seqLen])
-&gt; (Tensor device 'Int8 '[seqLen, seqLen]
    -&gt; Tensor device 'Bool '[seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[1, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (dtype :: DType) (device :: (DeviceType, Nat))
       (shape :: [Nat]).
KnownDType 'Bool =&gt;
Tensor device dtype shape -&gt; Tensor device 'Bool shape
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span>
</span><span id="line-559"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Int8 '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[seqLen, seqLen])
-&gt; (Tensor device 'Int8 '[seqLen, seqLen]
    -&gt; Tensor device 'Int8 '[seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(shape ~ MatrixOrMatrixBatch shape) =&gt;
Int -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#triu"><span class="hs-identifier hs-var">triu</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">1</span></span><span>
</span><span id="line-560"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Int8 '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[1, seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[1, seqLen, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TensorOptions '[seqLen, seqLen] 'Int8 device =&gt;
Tensor device 'Int8 '[seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#ones"><span class="hs-identifier hs-var">ones</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Int8"><span class="hs-identifier hs-type">D.Int8</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-561"></span><span>      </span><span id="local-6989586621679726291"><span class="annot"><span class="annottext">attentionMask' :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="#local-6989586621679726291"><span class="hs-identifier hs-var hs-var">attentionMask'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-562"></span><span>        </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen]
 -&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen]))
-&gt; (Tensor device dtype '[batchSize, seqLen, seqLen]
    -&gt; Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[1, seqLen, seqLen]
-&gt; Double
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
forall a (shape :: [Nat]) (shape' :: [Nat]) (shape'' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(Scalar a, shape'' ~ Broadcast shape shape') =&gt;
Tensor device 'Bool shape'
-&gt; a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Functional.html#maskedFill"><span class="hs-identifier hs-var">maskedFill</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[1, seqLen, seqLen]
</span><a href="#local-6989586621679726294"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen]
 -&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen]))
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-563"></span><span>          </span><span class="annot"><span class="annottext">TensorOptions '[batchSize, seqLen, seqLen] dtype device =&gt;
Tensor device dtype '[batchSize, seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726722"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726723"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726720"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-564"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726289"><span class="annot"><span class="annottext">keyPaddingMask :: Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726289"><span class="hs-identifier hs-var hs-var">keyPaddingMask</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, seqLen]
 -&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen]))
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726302"><span class="hs-identifier hs-var">xTokens</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device 'Int64 '[]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ Broadcast shape shape',
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device 'Bool shape''
</span><a href="Torch.Typed.Tensor.html#%3D%3D."><span class="hs-operator hs-var">==.</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; Tensor device 'Int64 '[]
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Tensor device 'Int64 '[])
-&gt; (Proxy paddingIdx -&gt; Integer)
-&gt; Proxy paddingIdx
-&gt; Tensor device 'Int64 '[]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Proxy paddingIdx -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; Type).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy paddingIdx -&gt; Tensor device 'Int64 '[])
-&gt; Proxy paddingIdx -&gt; Tensor device 'Int64 '[]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy paddingIdx
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726725"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726719"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-565"></span><span>  </span><span id="local-6989586621679726285"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726285"><span class="hs-identifier hs-var">y</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall k k (m :: k -&gt; Type) f acc (xs :: [k]) (res :: k).
HFoldrM m f acc xs res =&gt;
f -&gt; acc -&gt; HList xs -&gt; m res
</span><a href="Torch.HList.html#hfoldrM"><span class="hs-identifier hs-var">hfoldrM</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; FoldLayers batchSize seqLen dtype device
forall (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; FoldLayers batchSize seqLen dtype device
</span><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679726303"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="#local-6989586621679726291"><span class="hs-identifier hs-var">attentionMask'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679726289"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726295"><span class="hs-identifier hs-var">x'</span></a></span><span> </span><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayer
        embedDim embedDim embedDim numHeads ffnDim dtype device))
</span><a href="#local-6989586621679726305"><span class="hs-identifier hs-var">tLayers</span></a></span><span>
</span><span id="line-566"></span><span>  </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numEmbeds]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (m :: Type -&gt; Type) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numEmbeds]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds]))
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim numEmbeds dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim numEmbeds dtype device
</span><a href="#local-6989586621679726304"><span class="hs-identifier hs-var">tProj</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679726285"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-567"></span><span>
</span><span id="line-568"></span><span id="local-6989586621679726274"><span id="local-6989586621679726275"><span id="local-6989586621679726276"><span id="local-6989586621679726277"><span id="local-6989586621679726278"><span id="local-6989586621679726279"><span id="local-6989586621679726280"><span id="local-6989586621679726281"><span id="local-6989586621679726282"><span id="local-6989586621679726283"><span class="hs-keyword">instance</span><span>
</span><span id="line-569"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726283"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726280"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-570"></span><span>    </span><span class="annot"><a href="#local-6989586621679726283"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726279"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-571"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-572"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HFoldrM"><span class="hs-identifier hs-type">HFoldrM</span></a></span><span>
</span><span id="line-573"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-574"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726280"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-575"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726280"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-576"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726276"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726275"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726274"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-577"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726280"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-578"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-579"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-580"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-581"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-582"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-583"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-584"></span><span>  </span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726276"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726275"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726274"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726283"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726279"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726282"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726280"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726277"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726278"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726280"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726281"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726279"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-585"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-586"></span><span>  </span><span id="local-6989586621679726270"><span class="annot"><span class="annottext">forward :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
</span><a href="#local-6989586621679726270"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span id="local-6989586621679726269"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679726269"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679726268"><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726268"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
 -&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(All KnownNat '[paddingIdx, embedDim, seqLen, batchSize],
 (paddingIdx + 1) &lt;= numEmbeds, 1 &lt;= seqLen,
 HFoldrM
   IO
   (FoldLayers batchSize seqLen dtype device)
   (Tensor device dtype '[batchSize, seqLen, embedDim])
   (HReplicateR
      numAttnLayers
      (TransformerLayer
         embedDim embedDim embedDim numHeads ffnDim dtype device))
   (Tensor device dtype '[batchSize, seqLen, embedDim]),
 BasicArithmeticDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device 'Int64, KnownDType dtype,
 KnownDevice device) =&gt;
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var">transformerLM</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679726269"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726268"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-587"></span><span>  </span><span id="local-6989586621679726267"><span class="annot"><span class="annottext">forwardStoch :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.NN.html#forwardStoch"><span class="hs-identifier hs-var hs-var hs-var hs-var">forwardStoch</span></a></span></span><span> </span><span id="local-6989586621679726265"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679726265"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679726264"><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726264"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(All KnownNat '[paddingIdx, embedDim, seqLen, batchSize],
 (paddingIdx + 1) &lt;= numEmbeds, 1 &lt;= seqLen,
 HFoldrM
   IO
   (FoldLayers batchSize seqLen dtype device)
   (Tensor device dtype '[batchSize, seqLen, embedDim])
   (HReplicateR
      numAttnLayers
      (TransformerLayer
         embedDim embedDim embedDim numHeads ffnDim dtype device))
   (Tensor device dtype '[batchSize, seqLen, embedDim]),
 BasicArithmeticDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device 'Int64, KnownDType dtype,
 KnownDevice device) =&gt;
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var">transformerLM</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679726265"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679726264"><span class="hs-identifier hs-var">input</span></a></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-588"></span><span>
</span><span id="line-589"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-type">sinusoidal</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-590"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679726672"><span class="annot"><a href="#local-6989586621679726672"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679726671"><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679726670"><span class="annot"><a href="#local-6989586621679726670"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-591"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726672"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-592"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726672"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-593"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">,</span><span>
</span><span id="line-594"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-595"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726670"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-596"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726670"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-597"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726670"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-598"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-599"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726670"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726672"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-600"></span><span id="sinusoidal"><span class="annot"><span class="annottext">sinusoidal :: Tensor device 'Float '[numEmbeds, embedDim]
</span><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-var hs-var">sinusoidal</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-601"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679726262"><span class="annot"><span class="annottext">positions :: Tensor device 'Float '[numEmbeds, 1]
</span><a href="#local-6989586621679726262"><span class="hs-identifier hs-var hs-var">positions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-602"></span><span>        </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ Unsqueeze shape 1) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-603"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[numEmbeds]
 -&gt; Tensor device 'Float '[numEmbeds, 1])
-&gt; (Int -&gt; Tensor device 'Float '[numEmbeds])
-&gt; Int
-&gt; Tensor device 'Float '[numEmbeds, 1]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[numEmbeds]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726672"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-604"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device 'Float '[numEmbeds, 1])
-&gt; Int -&gt; Tensor device 'Float '[numEmbeds, 1]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (numEmbeds - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679726672"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-605"></span><span>      </span><span id="local-6989586621679726261"><span class="annot"><span class="annottext">scalingFactors :: Tensor device 'Float '[Div embedDim 2]
</span><a href="#local-6989586621679726261"><span class="hs-identifier hs-var hs-var">scalingFactors</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-606"></span><span>        </span><span class="annot"><span class="annottext">Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#exp"><span class="hs-identifier hs-var">exp</span></a></span><span>
</span><span id="line-607"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[Div embedDim 2]
 -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; (Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int
-&gt; Tensor device 'Float '[Div embedDim 2]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Double
-&gt; Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[Div embedDim 2]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#mulScalar"><span class="hs-identifier hs-var">mulScalar</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">log</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double
</span><span class="hs-number">10000</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; Double
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Double)
-&gt; (Proxy (Div embedDim 2) -&gt; Integer)
-&gt; Proxy (Div embedDim 2)
-&gt; Double
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Proxy (Div embedDim 2) -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; Type).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy (Div embedDim 2) -&gt; Double)
-&gt; Proxy (Div embedDim 2) -&gt; Double
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy (Div embedDim 2)
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-608"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[Div embedDim 2]
 -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; (Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int
-&gt; Tensor device 'Float '[Div embedDim 2]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[Div embedDim 2]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-609"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int -&gt; Tensor device 'Float '[Div embedDim 2]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (Div embedDim 2 - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679726671"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-610"></span><span>      </span><span id="local-6989586621679726257"><span class="annot"><span class="annottext">radians :: Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679726257"><span class="hs-identifier hs-var hs-var">radians</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, 1]
-&gt; Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#mul"><span class="hs-identifier hs-var">mul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, 1]
</span><a href="#local-6989586621679726262"><span class="hs-identifier hs-var">positions</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[Div embedDim 2]
</span><a href="#local-6989586621679726261"><span class="hs-identifier hs-var">scalingFactors</span></a></span><span>
</span><span id="line-611"></span><span>      </span><span id="local-6989586621679726255"><span class="annot"><span class="annottext">weights :: Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
</span><a href="#local-6989586621679726255"><span class="hs-identifier hs-var hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList
  '[Tensor device 'Float '[numEmbeds, Div embedDim 2],
    Tensor device 'Float '[numEmbeds, Div embedDim 2]]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
forall k (dim :: Nat) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)) (tensors :: [k]).
(KnownNat dim, '(shape, dtype, device) ~ Stack dim tensors,
 Castable (HList tensors) [ATenTensor]) =&gt;
HList tensors -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#stack"><span class="hs-identifier hs-var">stack</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#sin"><span class="hs-identifier hs-var">sin</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679726257"><span class="hs-identifier hs-var">radians</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; HList '[Tensor device 'Float '[numEmbeds, Div embedDim 2]]
-&gt; HList
     '[Tensor device 'Float '[numEmbeds, Div embedDim 2],
       Tensor device 'Float '[numEmbeds, Div embedDim 2]]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#cos"><span class="hs-identifier hs-var">cos</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679726257"><span class="hs-identifier hs-var">radians</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; HList '[]
-&gt; HList '[Tensor device 'Float '[numEmbeds, Div embedDim 2]]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
forall k. HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-612"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
-&gt; Tensor device 'Float '[numEmbeds, embedDim]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
</span><a href="#local-6989586621679726255"><span class="hs-identifier hs-var">weights</span></a></span><span>
</span><span id="line-613"></span><span>
</span><span id="line-614"></span><span id="local-6989586621679726242"><span id="local-6989586621679726243"><span id="local-6989586621679726244"><span id="local-6989586621679726245"><span id="local-6989586621679726246"><span id="local-6989586621679726247"><span id="local-6989586621679726248"><span id="local-6989586621679726249"><span class="hs-keyword">instance</span><span>
</span><span id="line-615"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-616"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-617"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-618"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-619"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679726246"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-620"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HReplicate"><span class="hs-identifier hs-type">HReplicate</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726245"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726244"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726246"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-621"></span><span>    </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-622"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726245"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726244"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726246"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-623"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726245"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726244"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726246"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-624"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-625"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-626"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-627"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-628"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-629"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-630"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-631"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726245"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726244"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726246"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-632"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726245"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726244"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726246"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726248"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726247"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726243"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679726242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-633"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-634"></span><span>  </span><span id="local-6989586621679726240"><span class="annot"><span class="annottext">sample :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; IO
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
</span><a href="#local-6989586621679726240"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="hs-special">{</span><span id="local-6989586621679726238"><span id="local-6989586621679726239"><span class="annot"><span class="annottext">DropoutSpec
TransformerLayerSpec
  embedDim embedDim embedDim numHeads ffnDim dtype device
lmLayerSpec :: TransformerLayerSpec
  embedDim embedDim embedDim numHeads ffnDim dtype device
lmDropoutSpec :: DropoutSpec
lmLayerSpec :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLayerSpec
     embedDim embedDim embedDim numHeads ffnDim dtype device
lmDropoutSpec :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; DropoutSpec
</span><a href="#local-6989586621679726238"><span class="hs-glyph hs-var hs-var hs-var hs-var">..</span></a></span></span></span><span class="hs-special">}</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-635"></span><span>    </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Dropout
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; Linear embedDim numEmbeds dtype device
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Dropout
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; Linear embedDim numEmbeds dtype device
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span><span>
</span><span id="line-636"></span><span>      </span><span class="annot"><span class="annottext">(Embedding
   ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
 -&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
 -&gt; Dropout
 -&gt; HList
      (HReplicateR
         numAttnLayers
         (TransformerLayer
            embedDim embedDim embedDim numHeads ffnDim dtype device))
 -&gt; Linear embedDim numEmbeds dtype device
 -&gt; TransformerLM
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device)
-&gt; IO
     (Embedding
        ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device)
-&gt; IO
     (Embedding 'Nothing 2048 embedDim 'Constant dtype device
      -&gt; Dropout
      -&gt; HList
           (HReplicateR
              numAttnLayers
              (TransformerLayer
                 embedDim embedDim embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">EmbeddingSpec
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; IO
     (Embedding
        ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">forall (paddingIdx :: Maybe Nat) (numEmbeds :: Nat)
       (embedSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
EmbeddingSpec paddingIdx numEmbeds embedSize 'Learned dtype device
forall (numEmbeds :: Nat) (embedSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
EmbeddingSpec
  ('Just paddingIdx) numEmbeds embedSize 'Learned dtype device
</span><a href="Torch.Typed.NN.Sparse.html#LearnedEmbeddingWithRandomInitSpec"><span class="hs-identifier hs-var">LearnedEmbeddingWithRandomInitSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679726249"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-637"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Embedding 'Nothing 2048 embedDim 'Constant dtype device
   -&gt; Dropout
   -&gt; HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO (Embedding 'Nothing 2048 embedDim 'Constant dtype device)
-&gt; IO
     (Dropout
      -&gt; HList
           (HReplicateR
              numAttnLayers
              (TransformerLayer
                 embedDim embedDim embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">EmbeddingSpec 'Nothing 2048 embedDim 'Constant dtype device
-&gt; IO (Embedding 'Nothing 2048 embedDim 'Constant dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[2048, embedDim]
-&gt; EmbeddingSpec 'Nothing 2048 embedDim 'Constant dtype device
forall (paddingIdx :: Maybe Nat) (numEmbeds :: Nat)
       (embedSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Tensor device dtype '[numEmbeds, embedSize]
-&gt; EmbeddingSpec
     paddingIdx numEmbeds embedSize 'Constant dtype device
</span><a href="Torch.Typed.NN.Sparse.html#ConstEmbeddingSpec"><span class="hs-identifier hs-var">ConstEmbeddingSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device 'Float '[2048, embedDim]
-&gt; Tensor device dtype '[2048, embedDim]
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[2048, embedDim]
forall (numEmbeds :: Nat) (embedDim :: Nat)
       (device :: (DeviceType, Nat)).
(All KnownNat '[numEmbeds, embedDim], 1 &lt;= numEmbeds,
 1 &lt;= Div embedDim 2, (Div embedDim 2 * 2) ~ embedDim,
 StandardFloatingPointDTypeValidation device 'Float,
 BasicArithmeticDTypeIsValid device 'Float, KnownDevice device) =&gt;
Tensor device 'Float '[numEmbeds, embedDim]
</span><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-var">sinusoidal</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-638"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO Dropout
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679726239"><span class="hs-identifier hs-var">lmDropoutSpec</span></a></span><span>
</span><span id="line-639"></span><span>      </span><span class="annot"><span class="annottext">IO
  (HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device)))
-&gt; IO
     (Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayerSpec
        embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device)))
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim embedDim embedDim numHeads ffnDim dtype device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayerSpec
           embedDim embedDim embedDim numHeads ffnDim dtype device))
forall (n :: Nat) e. HReplicate n e =&gt; e -&gt; HList (HReplicateR n e)
</span><a href="Torch.HList.html#hreplicate"><span class="hs-identifier hs-var">hreplicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679726245"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679726238"><span class="hs-identifier hs-var">lmLayerSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-640"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO (Linear embedDim numEmbeds dtype device)
-&gt; IO
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim numEmbeds dtype device
-&gt; IO (Linear embedDim numEmbeds dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim numEmbeds dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-641"></span></pre></body></html>