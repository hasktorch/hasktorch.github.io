<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE NoStarIsType #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE DeriveGeneric #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# OPTIONS_GHC -fconstraint-solver-iterations=0 #-}</span><span>
</span><span id="line-15"></span><span>
</span><span id="line-16"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Typed.NN.Transformer</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-17"></span><span>
</span><span id="line-18"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad</span></span><span>
</span><span id="line-19"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Proxy</span></span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.Generics</span></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">System.IO.Unsafe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">unsafePerformIO</span></span><span class="hs-special">)</span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.DType.html"><span class="hs-identifier">Torch.DType</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Device.html"><span class="hs-identifier">Torch.Device</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.HList.html"><span class="hs-identifier">Torch.HList</span></a></span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">A</span></span><span>
</span><span id="line-27"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier">HasForward</span></a></span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-28"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html"><span class="hs-identifier">Torch.Typed.Aux</span></a></span><span>
</span><span id="line-29"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html"><span class="hs-identifier">Torch.Typed.Factories</span></a></span><span>
</span><span id="line-30"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html"><span class="hs-identifier">Torch.Typed.Functional</span></a></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#linear"><span class="hs-identifier">linear</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#log"><span class="hs-identifier">log</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-31"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html"><span class="hs-identifier">Torch.Typed.Tensor</span></a></span><span>
</span><span id="line-32"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html"><span class="hs-identifier">Torch.Typed.NN.Dropout</span></a></span><span>
</span><span id="line-33"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html"><span class="hs-identifier">Torch.Typed.NN.Linear</span></a></span><span>
</span><span id="line-34"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html"><span class="hs-identifier">Torch.Typed.NN.Normalization</span></a></span><span>
</span><span id="line-35"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html"><span class="hs-identifier">Torch.Typed.NN.Sparse</span></a></span><span>
</span><span id="line-36"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">cos</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">exp</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">sin</span></span><span class="hs-special">)</span><span>
</span><span id="line-37"></span><span>
</span><span id="line-38"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-39"></span><span class="hs-comment">-- Relation-Aware Multi-Headed Attention Layer</span><span>
</span><span id="line-40"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-41"></span><span>
</span><span id="line-42"></span><span class="hs-keyword">data</span><span>
</span><span id="line-43"></span><span>  </span><span id="MultiheadAttentionSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-var">MultiheadAttentionSpec</span></a></span></span><span>
</span><span id="line-44"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756728"><span class="annot"><a href="#local-6989586621679756728"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-45"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756727"><span class="annot"><a href="#local-6989586621679756727"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-46"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756726"><span class="annot"><a href="#local-6989586621679756726"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-47"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756725"><span class="annot"><a href="#local-6989586621679756725"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-48"></span><span>  </span><span id="local-6989586621679757179"><span id="local-6989586621679757180"><span id="local-6989586621679757181"><span id="local-6989586621679757182"><span id="MultiheadAttentionSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-var">MultiheadAttentionSpec</span></a></span></span><span>
</span><span id="line-49"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">{</span><span> </span><span id="mhaDropoutSpec"><span class="annot"><span class="annottext">MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#mhaDropoutSpec"><span class="hs-identifier hs-var hs-var">mhaDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span> </span><span class="hs-special">}</span><span>
</span><span id="line-50"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757182"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757181"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757180"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757179"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span><span>
</span><span id="line-51"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756717"><span id="local-6989586621679756719"><span id="local-6989586621679756721"><span class="annot"><span class="annottext">Int
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; ShowS
[MultiheadAttentionSpec embedDim numHeads dtype device] -&gt; ShowS
MultiheadAttentionSpec embedDim numHeads dtype device -&gt; String
(Int
 -&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; ShowS)
-&gt; (MultiheadAttentionSpec embedDim numHeads dtype device
    -&gt; String)
-&gt; ([MultiheadAttentionSpec embedDim numHeads dtype device]
    -&gt; ShowS)
-&gt; Show (MultiheadAttentionSpec embedDim numHeads dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[MultiheadAttentionSpec embedDim numHeads dtype device] -&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttentionSpec embedDim numHeads dtype device -&gt; String
showList :: [MultiheadAttentionSpec embedDim numHeads dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[MultiheadAttentionSpec embedDim numHeads dtype device] -&gt; ShowS
show :: MultiheadAttentionSpec embedDim numHeads dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttentionSpec embedDim numHeads dtype device -&gt; String
showsPrec :: Int
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679756712"><span id="local-6989586621679756714"><span class="annot"><span class="annottext">MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool
(MultiheadAttentionSpec embedDim numHeads dtype device
 -&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool)
-&gt; (MultiheadAttentionSpec embedDim numHeads dtype device
    -&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool)
-&gt; Eq (MultiheadAttentionSpec embedDim numHeads dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool
/= :: MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool
$c/= :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool
== :: MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool
$c== :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device -&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-52"></span><span>
</span><span id="line-53"></span><span id="local-6989586621679756709"><span id="local-6989586621679756710"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-54"></span><span>  </span><span id="MultiheadAttention"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span></span><span>
</span><span id="line-55"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756708"><span class="annot"><a href="#local-6989586621679756708"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-56"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756707"><span class="annot"><a href="#local-6989586621679756707"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-57"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756706"><span class="annot"><a href="#local-6989586621679756706"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-58"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756705"><span class="annot"><a href="#local-6989586621679756705"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-59"></span><span>  </span><span id="local-6989586621679757295"><span id="local-6989586621679757296"><span id="local-6989586621679757297"><span id="local-6989586621679757298"><span id="MultiheadAttention"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span></span><span>
</span><span id="line-60"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">{</span><span> </span><span id="mhaInProj"><span class="annot"><span class="annottext">MultiheadAttention embedDim numHeads dtype device
-&gt; Linear embedDim (embedDim * 3) dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaInProj"><span class="hs-identifier hs-var hs-var">mhaInProj</span></a></span></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757298"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679757298"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679757296"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757295"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-61"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="mhaOutProj"><span class="annot"><span class="annottext">MultiheadAttention embedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaOutProj"><span class="hs-identifier hs-var hs-var">mhaOutProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757298"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757298"><span class="hs-identifier hs-type">embedDim</span></a></span><span>       </span><span class="annot"><a href="#local-6989586621679757296"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757295"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-62"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="mhaDropout"><span class="annot"><span class="annottext">MultiheadAttention embedDim numHeads dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#mhaDropout"><span class="hs-identifier hs-var hs-var">mhaDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-63"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-64"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757298"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757297"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757296"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757295"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span><span>
</span><span id="line-65"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756695"><span id="local-6989586621679756697"><span id="local-6989586621679756699"><span class="annot"><span class="annottext">Int -&gt; MultiheadAttention embedDim numHeads dtype device -&gt; ShowS
[MultiheadAttention embedDim numHeads dtype device] -&gt; ShowS
MultiheadAttention embedDim numHeads dtype device -&gt; String
(Int -&gt; MultiheadAttention embedDim numHeads dtype device -&gt; ShowS)
-&gt; (MultiheadAttention embedDim numHeads dtype device -&gt; String)
-&gt; ([MultiheadAttention embedDim numHeads dtype device] -&gt; ShowS)
-&gt; Show (MultiheadAttention embedDim numHeads dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; MultiheadAttention embedDim numHeads dtype device -&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[MultiheadAttention embedDim numHeads dtype device] -&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttention embedDim numHeads dtype device -&gt; String
showList :: [MultiheadAttention embedDim numHeads dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[MultiheadAttention embedDim numHeads dtype device] -&gt; ShowS
show :: MultiheadAttention embedDim numHeads dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttention embedDim numHeads dtype device -&gt; String
showsPrec :: Int -&gt; MultiheadAttention embedDim numHeads dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; MultiheadAttention embedDim numHeads dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 MultiheadAttention embedDim numHeads dtype device
 -&gt; Rep (MultiheadAttention embedDim numHeads dtype device) x)
-&gt; (forall x.
    Rep (MultiheadAttention embedDim numHeads dtype device) x
    -&gt; MultiheadAttention embedDim numHeads dtype device)
-&gt; Generic (MultiheadAttention embedDim numHeads dtype device)
forall x.
Rep (MultiheadAttention embedDim numHeads dtype device) x
-&gt; MultiheadAttention embedDim numHeads dtype device
forall x.
MultiheadAttention embedDim numHeads dtype device
-&gt; Rep (MultiheadAttention embedDim numHeads dtype device) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (MultiheadAttention embedDim numHeads dtype device) x
-&gt; MultiheadAttention embedDim numHeads dtype device
forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
MultiheadAttention embedDim numHeads dtype device
-&gt; Rep (MultiheadAttention embedDim numHeads dtype device) x
$cto :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (MultiheadAttention embedDim numHeads dtype device) x
-&gt; MultiheadAttention embedDim numHeads dtype device
$cfrom :: forall (embedDim :: Nat) (numHeads :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
MultiheadAttention embedDim numHeads dtype device
-&gt; Rep (MultiheadAttention embedDim numHeads dtype device) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-66"></span><span>
</span><span id="line-67"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-type">multiheadAttention</span></a></span><span>
</span><span id="line-68"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679757022"><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679757024"><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679757020"><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679757019"><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679757021"><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span></span><span> </span><span id="local-6989586621679757018"><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679757017"><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-69"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-70"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-71"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Mod</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-72"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-73"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-74"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-75"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-76"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-77"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-78"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-79"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-80"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-81"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-82"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-83"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-84"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-85"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-86"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-87"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-88"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-89"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-90"></span><span>        </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757017"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757018"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-91"></span><span>        </span><span class="hs-special">)</span><span>
</span><span id="line-92"></span><span id="multiheadAttention"><span class="annot"><span class="annottext">multiheadAttention :: MultiheadAttention embedDim numHeads dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen, embedDim],
      Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-var hs-var">multiheadAttention</span></a></span></span><span> </span><span id="local-6989586621679756688"><span id="local-6989586621679756689"><span id="local-6989586621679756690"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span><span> </span><span id="local-6989586621679756687"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679756687"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679756686"><span class="annot"><span class="annottext">attentionMask :: Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756686"><span class="hs-identifier hs-var">attentionMask</span></a></span></span><span> </span><span id="local-6989586621679756685"><span class="annot"><span class="annottext">keyPaddingMask :: Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756685"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span></span><span> </span><span id="local-6989586621679756684"><span class="annot"><span class="annottext">maybeRelationsK :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756684"><span class="hs-identifier hs-var">maybeRelationsK</span></a></span></span><span> </span><span id="local-6989586621679756683"><span class="annot"><span class="annottext">maybeRelationsV :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756683"><span class="hs-identifier hs-var">maybeRelationsV</span></a></span></span><span> </span><span id="local-6989586621679756682"><span class="annot"><span class="annottext">x :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756682"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-93"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756681"><span class="annot"><span class="annottext">q :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756681"><span class="hs-identifier hs-var">q</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679756679"><span class="annot"><span class="annottext">k :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756679"><span class="hs-identifier hs-var">k</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679756678"><span class="annot"><span class="annottext">v :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756678"><span class="hs-identifier hs-var">v</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span class="annot"><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-type">HNil</span></a></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)) (tensorChunks :: [Type]).
(KnownNat 3, KnownNat 2,
 tensorChunks ~ Chunk 3 2 shape dtype device,
 Castable (HList tensorChunks) [ATenTensor]) =&gt;
Tensor device dtype shape -&gt; HList tensorChunks
forall k (chunks :: Nat) (dim :: Nat) (shape :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat))
       (tensorChunks :: [k]).
(KnownNat chunks, KnownNat dim,
 tensorChunks ~ Chunk chunks dim shape dtype device,
 Castable (HList tensorChunks) [ATenTensor]) =&gt;
Tensor device dtype shape -&gt; HList tensorChunks
</span><a href="Torch.Typed.Functional.html#chunk"><span class="hs-identifier hs-var">chunk</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim * 3]
 -&gt; HList
      '[Tensor device dtype '[batchSize, seqLen, embedDim],
        Tensor device dtype '[batchSize, seqLen, embedDim],
        Tensor device dtype '[batchSize, seqLen, embedDim]])
-&gt; (Tensor device dtype '[batchSize, seqLen, embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, embedDim * 3])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; HList
     '[Tensor device dtype '[batchSize, seqLen, embedDim],
       Tensor device dtype '[batchSize, seqLen, embedDim],
       Tensor device dtype '[batchSize, seqLen, embedDim]]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim (embedDim * 3) dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim * 3]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim (embedDim * 3) dtype device
</span><a href="#local-6989586621679756690"><span class="hs-identifier hs-var">mhaInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; HList
      '[Tensor device dtype '[batchSize, seqLen, embedDim],
        Tensor device dtype '[batchSize, seqLen, embedDim],
        Tensor device dtype '[batchSize, seqLen, embedDim]])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; HList
     '[Tensor device dtype '[batchSize, seqLen, embedDim],
       Tensor device dtype '[batchSize, seqLen, embedDim],
       Tensor device dtype '[batchSize, seqLen, embedDim]]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756682"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-94"></span><span>      </span><span id="local-6989586621679756673"><span class="annot"><span class="annottext">q' :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756673"><span class="hs-identifier hs-var hs-var">q'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756672"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756681"><span class="hs-identifier hs-var">q</span></a></span><span>
</span><span id="line-95"></span><span>      </span><span id="local-6989586621679756671"><span class="annot"><span class="annottext">k' :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756671"><span class="hs-identifier hs-var hs-var">k'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756672"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756679"><span class="hs-identifier hs-var">k</span></a></span><span>
</span><span id="line-96"></span><span>      </span><span id="local-6989586621679756670"><span class="annot"><span class="annottext">v' :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756670"><span class="hs-identifier hs-var hs-var">v'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756672"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756678"><span class="hs-identifier hs-var">v</span></a></span><span>
</span><span id="line-97"></span><span>  </span><span id="local-6989586621679756669"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756669"><span class="hs-identifier hs-var">coefficients</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
</span><a href="#local-6989586621679756668"><span class="hs-identifier hs-var">weightCoefficients</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756673"><span class="hs-identifier hs-var">q'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756671"><span class="hs-identifier hs-var">k'</span></a></span><span>
</span><span id="line-98"></span><span>  </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim],
 Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen, embedDim],
      Tensor device dtype '[batchSize, seqLen, seqLen])
forall (m :: Type -&gt; Type) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756667"><span class="hs-identifier hs-var">attention</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756669"><span class="hs-identifier hs-var">coefficients</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756670"><span class="hs-identifier hs-var">v'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756666"><span class="hs-identifier hs-var">averageOverHeads</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756669"><span class="hs-identifier hs-var">coefficients</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-99"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-100"></span><span>    </span><span id="local-6989586621679756668"><span class="annot"><span class="annottext">weightCoefficients :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
</span><a href="#local-6989586621679756668"><span class="hs-identifier hs-var hs-var">weightCoefficients</span></a></span></span><span> </span><span id="local-6989586621679756665"><span class="annot"><span class="annottext">q :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756665"><span class="hs-identifier hs-var">q</span></a></span></span><span> </span><span id="local-6989586621679756664"><span class="annot"><span class="annottext">k :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756664"><span class="hs-identifier hs-var">k</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-101"></span><span>      </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679756688"><span class="hs-identifier hs-var">mhaDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756687"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-102"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
 -&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]))
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 3, DimOutOfBoundCheck shape 3, KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
forall (dim :: Nat) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat dim, DimOutOfBoundCheck shape dim, KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#softmax"><span class="hs-identifier hs-var">softmax</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span>
</span><span id="line-103"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756661"><span class="hs-identifier hs-var">maskKeyPaddings</span></a></span><span>
</span><span id="line-104"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756660"><span class="hs-identifier hs-var">maskAttention</span></a></span><span>
</span><span id="line-105"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Double
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#divScalar"><span class="hs-identifier hs-var">divScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679756658"><span class="hs-identifier hs-var">scaling</span></a></span><span>
</span><span id="line-106"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
 -&gt; (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
     -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
 -&gt; Maybe
      (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m (a -&gt; b) -&gt; m a -&gt; m b
</span><span class="hs-identifier hs-var">ap</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b a. b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b
</span><span class="hs-identifier hs-var">maybe</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-identifier hs-var">add</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, headDim, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756665"><span class="hs-identifier hs-var">q</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, headDim, seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
    -&gt; Tensor device dtype '[batchSize, numHeads, headDim, seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 2, KnownNat 3, shape' ~ Transpose shape 2 3) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756664"><span class="hs-identifier hs-var">k</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-107"></span><span>        </span><span class="annot"><span class="annottext">(Maybe (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Maybe
         (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]))
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-identifier hs-var">fmap</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numHeads, seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
-&gt; (Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, headDim, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756665"><span class="hs-identifier hs-var">q</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, headDim, seqLen]
 -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, seqLen])
-&gt; (Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, headDim, seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 2, KnownNat 3, shape' ~ Transpose shape 2 3) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span>
</span><span id="line-108"></span><span>        </span><span class="annot"><span class="annottext">(Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
 -&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]))
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756684"><span class="hs-identifier hs-var">maybeRelationsK</span></a></span><span>
</span><span id="line-109"></span><span>    </span><span id="local-6989586621679756667"><span class="annot"><span class="annottext">attention :: Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756667"><span class="hs-identifier hs-var hs-var">attention</span></a></span></span><span> </span><span id="local-6989586621679756652"><span class="annot"><span class="annottext">coefficients :: Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756652"><span class="hs-identifier hs-var">coefficients</span></a></span></span><span> </span><span id="local-6989586621679756651"><span class="annot"><span class="annottext">v :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756651"><span class="hs-identifier hs-var">v</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-110"></span><span>      </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
</span><a href="#local-6989586621679756689"><span class="hs-identifier hs-var">mhaOutProj</span></a></span><span>
</span><span id="line-111"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape '[batchSize, seqLen, embedDim],
 Numel shape ~ Numel '[batchSize, seqLen, embedDim]) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757022"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-112"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
 -&gt; (Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
     -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
 -&gt; Maybe
      (Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
 -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m (a -&gt; b) -&gt; m a -&gt; m b
</span><span class="hs-identifier hs-var">ap</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
-&gt; (Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall b a. b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b
</span><span class="hs-identifier hs-var">maybe</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-identifier hs-var">add</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756652"><span class="hs-identifier hs-var">coefficients</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756651"><span class="hs-identifier hs-var">v</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-113"></span><span>        </span><span class="annot"><span class="annottext">(Maybe
   (Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
 -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; (Maybe
      (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
    -&gt; Maybe
         (Tensor device dtype '[batchSize, seqLen, numHeads, headDim]))
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-identifier hs-var">fmap</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numHeads, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756652"><span class="hs-identifier hs-var">coefficients</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-114"></span><span>        </span><span class="annot"><span class="annottext">(Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
 -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756683"><span class="hs-identifier hs-var">maybeRelationsV</span></a></span><span>
</span><span id="line-115"></span><span>    </span><span id="local-6989586621679756666"><span class="annot"><span class="annottext">averageOverHeads :: Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756666"><span class="hs-identifier hs-var hs-var">averageOverHeads</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-116"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756649"><span class="annot"><span class="annottext">numHeads' :: Int
</span><a href="#local-6989586621679756649"><span class="hs-identifier hs-var hs-var">numHeads'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">KnownNat numHeads =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-117"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Int
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#divScalar"><span class="hs-identifier hs-var">divScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679756649"><span class="hs-identifier hs-var">numHeads'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen]
 -&gt; Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
    -&gt; Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (dtype' :: DType) (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ DropValue shape 1,
 SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape'
forall (d :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (device :: (DeviceType, Nat)).
(KnownNat d, shape' ~ DropValue shape d,
 SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape'
</span><a href="Torch.Typed.Functional.html#sumDim"><span class="hs-identifier hs-var">sumDim</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-118"></span><span>    </span><span id="local-6989586621679756660"><span class="annot"><span class="annottext">maskAttention :: Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756660"><span class="hs-identifier hs-var hs-var">maskAttention</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, 1, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-identifier hs-var">add</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, 1, seqLen, seqLen]
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756686"><span class="hs-identifier hs-var">attentionMask</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-119"></span><span>    </span><span id="local-6989586621679756661"><span class="annot"><span class="annottext">maskKeyPaddings :: Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
</span><a href="#local-6989586621679756661"><span class="hs-identifier hs-var hs-var">maskKeyPaddings</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-120"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756645"><span class="annot"><span class="annottext">keyPaddingMask' :: Tensor device 'Bool '[batchSize, 1, 1, seqLen]
</span><a href="#local-6989586621679756645"><span class="hs-identifier hs-var hs-var">keyPaddingMask'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 2, shape' ~ Unsqueeze shape 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, 1, seqLen]
 -&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen])
-&gt; (Tensor device 'Bool '[batchSize, seqLen]
    -&gt; Tensor device 'Bool '[batchSize, 1, seqLen])
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ Unsqueeze shape 1) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, seqLen]
 -&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen])
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756685"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span>
</span><span id="line-121"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, 1, 1, seqLen]
-&gt; Double
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, seqLen]
forall a (shape :: [Nat]) (shape' :: [Nat]) (shape'' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(Scalar a, shape'' ~ Broadcast shape shape') =&gt;
Tensor device 'Bool shape'
-&gt; a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Functional.html#maskedFill"><span class="hs-identifier hs-var">maskedFill</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, 1, 1, seqLen]
</span><a href="#local-6989586621679756645"><span class="hs-identifier hs-var">keyPaddingMask'</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span>
</span><span id="line-122"></span><span>    </span><span id="local-6989586621679756672"><span class="annot"><span class="annottext">reshape' :: Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679756672"><span class="hs-identifier hs-var hs-var">reshape'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape '[batchSize, seqLen, numHeads, headDim],
 Numel shape ~ Numel '[batchSize, seqLen, numHeads, headDim]) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype '[batchSize, seqLen, numHeads, headDim]
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757019"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757020"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757024"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-123"></span><span>    </span><span id="local-6989586621679756658"><span class="annot"><span class="annottext">scaling :: Double
</span><a href="#local-6989586621679756658"><span class="hs-identifier hs-var hs-var">scaling</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">Prelude.sqrt</span></span><span> </span><span class="annot"><span class="annottext">(Double -&gt; Double) -&gt; (Int -&gt; Double) -&gt; Int -&gt; Double
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Double) -&gt; Int -&gt; Double
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat headDim =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679757021"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-124"></span><span>
</span><span id="line-125"></span><span id="local-6989586621679756638"><span id="local-6989586621679756639"><span id="local-6989586621679756640"><span id="local-6989586621679756641"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756641"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756640"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-126"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756639"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-127"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756638"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-128"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756638"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756639"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-129"></span><span>         </span><span class="hs-special">)</span><span>
</span><span id="line-130"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756641"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756640"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756639"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756638"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-131"></span><span>                    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679756641"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756640"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756639"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756638"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-132"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-133"></span><span>  </span><span id="local-6989586621679756635"><span class="annot"><span class="annottext">sample :: MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; IO (MultiheadAttention embedDim numHeads dtype device)
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679756633"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-134"></span><span>    </span><span class="annot"><span class="annottext">Linear embedDim (embedDim * 3) dtype device
-&gt; Linear embedDim embedDim dtype device
-&gt; Dropout
-&gt; MultiheadAttention embedDim numHeads dtype device
forall (embedDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) (numHeads :: Nat).
Linear embedDim (embedDim * 3) dtype device
-&gt; Linear embedDim embedDim dtype device
-&gt; Dropout
-&gt; MultiheadAttention embedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span><span>
</span><span id="line-135"></span><span>      </span><span class="annot"><span class="annottext">(Linear embedDim (embedDim * 3) dtype device
 -&gt; Linear embedDim embedDim dtype device
 -&gt; Dropout
 -&gt; MultiheadAttention embedDim numHeads dtype device)
-&gt; IO (Linear embedDim (embedDim * 3) dtype device)
-&gt; IO
     (Linear embedDim embedDim dtype device
      -&gt; Dropout -&gt; MultiheadAttention embedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim (embedDim * 3) dtype device
-&gt; IO (Linear embedDim (embedDim * 3) dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim (embedDim * 3) dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-136"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear embedDim embedDim dtype device
   -&gt; Dropout -&gt; MultiheadAttention embedDim numHeads dtype device)
-&gt; IO (Linear embedDim embedDim dtype device)
-&gt; IO
     (Dropout -&gt; MultiheadAttention embedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
-&gt; IO (Linear embedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-137"></span><span>      </span><span class="annot"><span class="annottext">IO (Dropout -&gt; MultiheadAttention embedDim numHeads dtype device)
-&gt; IO Dropout
-&gt; IO (MultiheadAttention embedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679756633"><span class="hs-identifier hs-var">mhaDropoutSpec</span></a></span></span></span></span></span><span>
</span><span id="line-138"></span><span>
</span><span id="line-139"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-140"></span><span class="hs-comment">-- Transformer MLP Layer</span><span>
</span><span id="line-141"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-142"></span><span>
</span><span id="line-143"></span><span class="hs-keyword">data</span><span>
</span><span id="line-144"></span><span>  </span><span id="TransformerMLPSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-var">TransformerMLPSpec</span></a></span></span><span>
</span><span id="line-145"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756630"><span class="annot"><a href="#local-6989586621679756630"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-146"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756629"><span class="annot"><a href="#local-6989586621679756629"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-147"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756628"><span class="annot"><a href="#local-6989586621679756628"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-148"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756627"><span class="annot"><a href="#local-6989586621679756627"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-149"></span><span>  </span><span id="TransformerMLPSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-var">TransformerMLPSpec</span></a></span></span><span>
</span><span id="line-150"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679757106"><span class="annot"><a href="#local-6989586621679757106"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679757105"><span class="annot"><a href="#local-6989586621679757105"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679757104"><span class="annot"><a href="#local-6989586621679757104"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679757103"><span class="annot"><a href="#local-6989586621679757103"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-151"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="dropout0Spec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#dropout0Spec"><span class="hs-identifier hs-var hs-var">dropout0Spec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span>
</span><span id="line-152"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="dropout1Spec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#dropout1Spec"><span class="hs-identifier hs-var hs-var">dropout1Spec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span>
</span><span id="line-153"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-154"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757106"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757105"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757104"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757103"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-155"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span id="local-6989586621679756618"><span id="local-6989586621679756620"><span id="local-6989586621679756622"><span class="annot"><span class="annottext">Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
(Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerMLPSpec embedDim ffnDim dtype device -&gt; String)
-&gt; ([TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS)
-&gt; Show (TransformerMLPSpec embedDim ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
showList :: [TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
show :: TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
showsPrec :: Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span>
</span><span id="line-156"></span><span>
</span><span id="line-157"></span><span id="local-6989586621679756616"><span id="local-6989586621679756617"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-158"></span><span>  </span><span id="TransformerMLP"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span></span><span>
</span><span id="line-159"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756615"><span class="annot"><a href="#local-6989586621679756615"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-160"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756614"><span class="annot"><a href="#local-6989586621679756614"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-161"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756613"><span class="annot"><a href="#local-6989586621679756613"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-162"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756612"><span class="annot"><a href="#local-6989586621679756612"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-163"></span><span>  </span><span id="TransformerMLP"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span></span><span>
</span><span id="line-164"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679757121"><span class="annot"><a href="#local-6989586621679757121"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679757120"><span class="annot"><a href="#local-6989586621679757120"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679757119"><span class="annot"><a href="#local-6989586621679757119"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679757118"><span class="annot"><a href="#local-6989586621679757118"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-165"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="linear0"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Linear embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#linear0"><span class="hs-identifier hs-var hs-var">linear0</span></a></span></span><span>     </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757121"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757120"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757119"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757118"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-166"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="linear1"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#linear1"><span class="hs-identifier hs-var hs-var">linear1</span></a></span></span><span>     </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757120"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757121"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757119"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757118"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-167"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="dropout0"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#dropout0"><span class="hs-identifier hs-var hs-var">dropout0</span></a></span></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-168"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="dropout1"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#dropout1"><span class="hs-identifier hs-var hs-var">dropout1</span></a></span></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-169"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-170"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757121"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757120"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757119"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757118"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-171"></span><span> </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756601"><span id="local-6989586621679756603"><span id="local-6989586621679756605"><span class="annot"><span class="annottext">Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
TransformerMLP embedDim ffnDim dtype device -&gt; String
(Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerMLP embedDim ffnDim dtype device -&gt; String)
-&gt; ([TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS)
-&gt; Show (TransformerMLP embedDim ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; String
showList :: [TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
show :: TransformerMLP embedDim ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; String
showsPrec :: Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TransformerMLP embedDim ffnDim dtype device
 -&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x)
-&gt; (forall x.
    Rep (TransformerMLP embedDim ffnDim dtype device) x
    -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; Generic (TransformerMLP embedDim ffnDim dtype device)
forall x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
forall x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
$cto :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
$cfrom :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-172"></span><span>
</span><span id="line-173"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-type">transformerMLP</span></a></span><span>
</span><span id="line-174"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679757014"><span class="annot"><a href="#local-6989586621679757014"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679757013"><span class="annot"><a href="#local-6989586621679757013"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679757012"><span class="annot"><a href="#local-6989586621679757012"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679757011"><span class="annot"><a href="#local-6989586621679757011"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679757015"><span class="annot"><a href="#local-6989586621679757015"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679757016"><span class="annot"><a href="#local-6989586621679757016"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-175"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757016"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757015"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-176"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757014"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757013"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757015"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757016"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-177"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-178"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757016"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757015"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757012"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757011"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757014"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-179"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757016"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757015"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757012"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757011"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679757014"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-180"></span><span id="transformerMLP"><span class="annot"><span class="annottext">transformerMLP :: TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-var hs-var">transformerMLP</span></a></span></span><span> </span><span id="local-6989586621679756594"><span id="local-6989586621679756595"><span id="local-6989586621679756596"><span id="local-6989586621679756597"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span><span> </span><span id="local-6989586621679756593"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679756593"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679756592"><span class="annot"><span class="annottext">input :: Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679756592"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-181"></span><span>  </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679756594"><span class="hs-identifier hs-var">dropout1</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756593"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-182"></span><span>    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, ffnDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span>   </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#relu"><span class="hs-identifier hs-var">relu</span></a></span><span>
</span><span id="line-183"></span><span>    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; (Tensor device dtype '[seqLen, batchSize, ffnDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span>   </span><span class="annot"><span class="annottext">Linear ffnDim embedDim dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear ffnDim embedDim dtype device
</span><a href="#local-6989586621679756596"><span class="hs-identifier hs-var">linear1</span></a></span><span>
</span><span id="line-184"></span><span>    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679756595"><span class="hs-identifier hs-var">dropout0</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756593"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-185"></span><span>    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span>   </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#relu"><span class="hs-identifier hs-var">relu</span></a></span><span>
</span><span id="line-186"></span><span>    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span>   </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
</span><a href="#local-6989586621679756597"><span class="hs-identifier hs-var">linear0</span></a></span><span>
</span><span id="line-187"></span><span>    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim]))
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679756592"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-188"></span><span>
</span><span id="line-189"></span><span id="local-6989586621679756586"><span id="local-6989586621679756587"><span id="local-6989586621679756588"><span id="local-6989586621679756589"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756589"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756588"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-190"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756587"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-191"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756586"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-192"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756586"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756587"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-193"></span><span>         </span><span class="hs-special">)</span><span>
</span><span id="line-194"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756589"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756588"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756587"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756586"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-195"></span><span>                    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679756589"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756588"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756587"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756586"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-196"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-197"></span><span>  </span><span id="local-6989586621679756584"><span class="annot"><span class="annottext">sample :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
</span><a href="#local-6989586621679756584"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679756582"><span id="local-6989586621679756583"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-198"></span><span>    </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
-&gt; Dropout
-&gt; Dropout
-&gt; TransformerMLP embedDim ffnDim dtype device
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Linear embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
-&gt; Dropout
-&gt; Dropout
-&gt; TransformerMLP embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span><span>
</span><span id="line-199"></span><span>      </span><span class="annot"><span class="annottext">(Linear embedDim ffnDim dtype device
 -&gt; Linear ffnDim embedDim dtype device
 -&gt; Dropout
 -&gt; Dropout
 -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (Linear embedDim ffnDim dtype device)
-&gt; IO
     (Linear ffnDim embedDim dtype device
      -&gt; Dropout
      -&gt; Dropout
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim ffnDim dtype device
-&gt; IO (Linear embedDim ffnDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim ffnDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-200"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear ffnDim embedDim dtype device
   -&gt; Dropout
   -&gt; Dropout
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (Linear ffnDim embedDim dtype device)
-&gt; IO
     (Dropout -&gt; Dropout -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec ffnDim embedDim dtype device
-&gt; IO (Linear ffnDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec ffnDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-201"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout -&gt; Dropout -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO (Dropout -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679756583"><span class="hs-identifier hs-var">dropout0Spec</span></a></span><span>
</span><span id="line-202"></span><span>      </span><span class="annot"><span class="annottext">IO (Dropout -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO Dropout -&gt; IO (TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679756582"><span class="hs-identifier hs-var">dropout1Spec</span></a></span></span></span></span></span><span>
</span><span id="line-203"></span><span>
</span><span id="line-204"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-205"></span><span class="hs-comment">-- Relation-Aware Transformer Layer</span><span>
</span><span id="line-206"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-207"></span><span>
</span><span id="line-208"></span><span class="hs-keyword">data</span><span>
</span><span id="line-209"></span><span>  </span><span id="TransformerLayerSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-var">TransformerLayerSpec</span></a></span></span><span>
</span><span id="line-210"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756581"><span class="annot"><a href="#local-6989586621679756581"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-211"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756580"><span class="annot"><a href="#local-6989586621679756580"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-212"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756579"><span class="annot"><a href="#local-6989586621679756579"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-213"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756578"><span class="annot"><a href="#local-6989586621679756578"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-214"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756577"><span class="annot"><a href="#local-6989586621679756577"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-215"></span><span>  </span><span id="TransformerLayerSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-var">TransformerLayerSpec</span></a></span></span><span>
</span><span id="line-216"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679757005"><span class="annot"><a href="#local-6989586621679757005"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679757004"><span class="annot"><a href="#local-6989586621679757004"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679757003"><span class="annot"><a href="#local-6989586621679757003"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679757002"><span class="annot"><a href="#local-6989586621679757002"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679757001"><span class="annot"><a href="#local-6989586621679757001"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-217"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="mhaSpec"><span class="annot"><span class="annottext">TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; MultiheadAttentionSpec embedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaSpec"><span class="hs-identifier hs-var hs-var">mhaSpec</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757005"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757004"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757002"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757001"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-218"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="attnDropoutSpec"><span class="annot"><span class="annottext">TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#attnDropoutSpec"><span class="hs-identifier hs-var hs-var">attnDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span>
</span><span id="line-219"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="epsSpec"><span class="annot"><span class="annottext">TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; Double
</span><a href="Torch.Typed.NN.Transformer.html#epsSpec"><span class="hs-identifier hs-var hs-var">epsSpec</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-220"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="mlpSpec"><span class="annot"><span class="annottext">TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mlpSpec"><span class="hs-identifier hs-var hs-var">mlpSpec</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757005"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757003"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757002"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757001"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-221"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-222"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757005"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757004"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757003"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757002"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757001"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-223"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756566"><span id="local-6989586621679756568"><span id="local-6989586621679756570"><span class="annot"><span class="annottext">Int
-&gt; TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; ShowS
[TransformerLayerSpec embedDim numHeads ffnDim dtype device]
-&gt; ShowS
TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; String
(Int
 -&gt; TransformerLayerSpec embedDim numHeads ffnDim dtype device
 -&gt; ShowS)
-&gt; (TransformerLayerSpec embedDim numHeads ffnDim dtype device
    -&gt; String)
-&gt; ([TransformerLayerSpec embedDim numHeads ffnDim dtype device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLayerSpec embedDim numHeads ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLayerSpec embedDim numHeads ffnDim dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; String
showList :: [TransformerLayerSpec embedDim numHeads ffnDim dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLayerSpec embedDim numHeads ffnDim dtype device]
-&gt; ShowS
show :: TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; String
showsPrec :: Int
-&gt; TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-224"></span><span>
</span><span id="line-225"></span><span id="local-6989586621679756564"><span id="local-6989586621679756565"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-226"></span><span>  </span><span id="TransformerLayer"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span></span><span>
</span><span id="line-227"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756563"><span class="annot"><a href="#local-6989586621679756563"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-228"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756562"><span class="annot"><a href="#local-6989586621679756562"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-229"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756561"><span class="annot"><a href="#local-6989586621679756561"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-230"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756560"><span class="annot"><a href="#local-6989586621679756560"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-231"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756559"><span class="annot"><a href="#local-6989586621679756559"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-232"></span><span>  </span><span id="TransformerLayer"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span></span><span>
</span><span id="line-233"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679757029"><span class="annot"><a href="#local-6989586621679757029"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679757028"><span class="annot"><a href="#local-6989586621679757028"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679757027"><span class="annot"><a href="#local-6989586621679757027"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679757026"><span class="annot"><a href="#local-6989586621679757026"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679757025"><span class="annot"><a href="#local-6989586621679757025"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-234"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="transformerLayer_mha"><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; MultiheadAttention embedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_mha"><span class="hs-identifier hs-var hs-var">transformerLayer_mha</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757029"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757028"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757026"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757025"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-235"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="transformerLayer_attnDropout"><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_attnDropout"><span class="hs-identifier hs-var hs-var">transformerLayer_attnDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-236"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="transformerLayer_ln0"><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_ln0"><span class="hs-identifier hs-var hs-var">transformerLayer_ln0</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html#LayerNorm"><span class="hs-identifier hs-type">LayerNorm</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757029"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679757026"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757025"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-237"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="transformerLayer_ln1"><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_ln1"><span class="hs-identifier hs-var hs-var">transformerLayer_ln1</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html#LayerNorm"><span class="hs-identifier hs-type">LayerNorm</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679757029"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679757026"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757025"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-238"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="transformerLayer_mlp"><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_mlp"><span class="hs-identifier hs-var hs-var">transformerLayer_mlp</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757029"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757027"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757026"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757025"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-239"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-240"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757029"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757028"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757027"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757026"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679757025"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-241"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756547"><span id="local-6989586621679756549"><span id="local-6989586621679756551"><span class="annot"><span class="annottext">Int
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device -&gt; ShowS
[TransformerLayer embedDim numHeads ffnDim dtype device] -&gt; ShowS
TransformerLayer embedDim numHeads ffnDim dtype device -&gt; String
(Int
 -&gt; TransformerLayer embedDim numHeads ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerLayer embedDim numHeads ffnDim dtype device
    -&gt; String)
-&gt; ([TransformerLayer embedDim numHeads ffnDim dtype device]
    -&gt; ShowS)
-&gt; Show (TransformerLayer embedDim numHeads ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLayer embedDim numHeads ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLayer embedDim numHeads ffnDim dtype device -&gt; String
showList :: [TransformerLayer embedDim numHeads ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLayer embedDim numHeads ffnDim dtype device] -&gt; ShowS
show :: TransformerLayer embedDim numHeads ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLayer embedDim numHeads ffnDim dtype device -&gt; String
showsPrec :: Int
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TransformerLayer embedDim numHeads ffnDim dtype device
 -&gt; Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x)
-&gt; (forall x.
    Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
    -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
-&gt; Generic (TransformerLayer embedDim numHeads ffnDim dtype device)
forall x.
Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device
forall x.
TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
$cto :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device
$cfrom :: forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; Rep (TransformerLayer embedDim numHeads ffnDim dtype device) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-242"></span><span>
</span><span id="line-243"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-type">transformerLayer</span></a></span><span>
</span><span id="line-244"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679756884"><span class="annot"><a href="#local-6989586621679756884"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679756877"><span class="annot"><a href="#local-6989586621679756877"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679756883"><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679756882"><span class="annot"><a href="#local-6989586621679756882"><span class="hs-identifier hs-type">headDim</span></a></span></span><span> </span><span id="local-6989586621679756881"><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679756880"><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679756879"><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679756878"><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-245"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756884"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-246"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756882"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679756884"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-247"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Mod</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-248"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-249"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756884"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756882"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-250"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#EndsWith"><span class="hs-identifier hs-type">EndsWith</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-251"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-252"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-253"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-254"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-255"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-256"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-257"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-258"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-259"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756884"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756877"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-260"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-261"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-262"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-263"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756882"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-264"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756882"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-265"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-266"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756878"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756879"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756880"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756881"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756883"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-267"></span><span id="transformerLayer"><span class="annot"><span class="annottext">transformerLayer :: TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-var hs-var">transformerLayer</span></a></span></span><span> </span><span id="local-6989586621679756539"><span id="local-6989586621679756540"><span id="local-6989586621679756541"><span id="local-6989586621679756542"><span id="local-6989586621679756543"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span></span><span> </span><span id="local-6989586621679756538"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679756538"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679756537"><span class="annot"><span class="annottext">attentionMask :: Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756537"><span class="hs-identifier hs-var">attentionMask</span></a></span></span><span> </span><span id="local-6989586621679756536"><span class="annot"><span class="annottext">keyPaddingMask :: Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756536"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span></span><span> </span><span id="local-6989586621679756535"><span class="annot"><span class="annottext">maybeRelationsK :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756535"><span class="hs-identifier hs-var">maybeRelationsK</span></a></span></span><span> </span><span id="local-6989586621679756534"><span class="annot"><span class="annottext">maybeRelationsV :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756534"><span class="hs-identifier hs-var">maybeRelationsV</span></a></span></span><span> </span><span id="local-6989586621679756533"><span class="annot"><span class="annottext">x :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756533"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-268"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679756532"><span class="annot"><span class="annottext">z :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756532"><span class="hs-identifier hs-var">z</span></a></span></span><span class="hs-special">,</span><span> </span><span class="hs-identifier">_</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">MultiheadAttention embedDim numHeads dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen, embedDim],
      Tensor device dtype '[batchSize, seqLen, seqLen])
forall (embedDim :: Nat) (numHeads :: Nat) (seqLen :: Nat)
       (batchSize :: Nat) (headDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(1 &lt;= numHeads, embedDim ~ (headDim * numHeads),
 Mod (embedDim * 3) 3 ~ 0, Div (embedDim * 3) 3 ~ embedDim,
 All KnownNat '[embedDim, numHeads, seqLen, batchSize, headDim],
 KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype,
 MatMulDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype, dtype ~ SumDType dtype,
 SumDTypeIsValid device dtype, KnownDevice device) =&gt;
MultiheadAttention embedDim numHeads dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen, embedDim],
      Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-var">multiheadAttention</span></a></span><span> </span><span class="annot"><span class="annottext">MultiheadAttention embedDim numHeads dtype device
</span><a href="#local-6989586621679756543"><span class="hs-identifier hs-var">transformerLayer_mha</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756538"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756537"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756536"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756535"><span class="hs-identifier hs-var">maybeRelationsK</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
</span><a href="#local-6989586621679756534"><span class="hs-identifier hs-var">maybeRelationsV</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756533"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-269"></span><span>  </span><span id="local-6989586621679756531"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756531"><span class="hs-identifier hs-var">z'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679756542"><span class="hs-identifier hs-var">transformerLayer_attnDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756538"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756532"><span class="hs-identifier hs-var">z</span></a></span><span>
</span><span id="line-270"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756530"><span class="annot"><span class="annottext">y :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756530"><span class="hs-identifier hs-var hs-var">y</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
</span><a href="#local-6989586621679756541"><span class="hs-identifier hs-var">transformerLayer_ln0</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756533"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756531"><span class="hs-identifier hs-var">z'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-271"></span><span>  </span><span id="local-6989586621679756529"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756529"><span class="hs-identifier hs-var">y'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (embedDim :: Nat) (ffnDim :: Nat) (seqLen :: Nat)
       (batchSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-var">transformerMLP</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
</span><a href="#local-6989586621679756539"><span class="hs-identifier hs-var">transformerLayer_mlp</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756538"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756530"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-272"></span><span>  </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (m :: Type -&gt; Type) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
</span><a href="#local-6989586621679756540"><span class="hs-identifier hs-var">transformerLayer_ln1</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756530"><span class="hs-identifier hs-var">y</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756529"><span class="hs-identifier hs-var">y'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-273"></span><span>
</span><span id="line-274"></span><span id="local-6989586621679756524"><span id="local-6989586621679756525"><span id="local-6989586621679756526"><span id="local-6989586621679756527"><span id="local-6989586621679756528"><span class="hs-keyword">instance</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756528"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756527"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756526"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-275"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756525"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-276"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756524"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-277"></span><span>         </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756524"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756525"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-278"></span><span>         </span><span class="hs-special">)</span><span>
</span><span id="line-279"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756528"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756527"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756526"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756525"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756524"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-280"></span><span>                    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679756528"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756527"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756526"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756525"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756524"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-281"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-282"></span><span>  </span><span id="local-6989586621679756522"><span class="annot"><span class="annottext">sample :: TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; IO (TransformerLayer embedDim numHeads ffnDim dtype device)
</span><a href="#local-6989586621679756522"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679756518"><span id="local-6989586621679756519"><span id="local-6989586621679756520"><span id="local-6989586621679756521"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-283"></span><span>    </span><span class="annot"><span class="annottext">MultiheadAttention embedDim numHeads dtype device
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device
forall (embedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention embedDim numHeads dtype device
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
-&gt; TransformerLayer embedDim numHeads ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span><span>
</span><span id="line-284"></span><span>      </span><span class="annot"><span class="annottext">(MultiheadAttention embedDim numHeads dtype device
 -&gt; Dropout
 -&gt; LayerNorm '[embedDim] dtype device
 -&gt; LayerNorm '[embedDim] dtype device
 -&gt; TransformerMLP embedDim ffnDim dtype device
 -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
-&gt; IO (MultiheadAttention embedDim numHeads dtype device)
-&gt; IO
     (Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">MultiheadAttentionSpec embedDim numHeads dtype device
-&gt; IO (MultiheadAttention embedDim numHeads dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">MultiheadAttentionSpec embedDim numHeads dtype device
</span><a href="#local-6989586621679756521"><span class="hs-identifier hs-var">mhaSpec</span></a></span><span>
</span><span id="line-285"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (LayerNorm '[embedDim] dtype device
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679756520"><span class="hs-identifier hs-var">attnDropoutSpec</span></a></span><span>
</span><span id="line-286"></span><span>      </span><span class="annot"><span class="annottext">IO
  (LayerNorm '[embedDim] dtype device
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
-&gt; IO (LayerNorm '[embedDim] dtype device)
-&gt; IO
     (LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayerNormSpec '[embedDim] dtype device
-&gt; IO (LayerNorm '[embedDim] dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double -&gt; LayerNormSpec '[embedDim] dtype device
forall (normalizedShape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Double -&gt; LayerNormSpec normalizedShape dtype device
</span><a href="Torch.Typed.NN.Normalization.html#LayerNormSpec"><span class="hs-identifier hs-var">LayerNormSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679756519"><span class="hs-identifier hs-var">epsSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-287"></span><span>      </span><span class="annot"><span class="annottext">IO
  (LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
-&gt; IO (LayerNorm '[embedDim] dtype device)
-&gt; IO
     (TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayerNormSpec '[embedDim] dtype device
-&gt; IO (LayerNorm '[embedDim] dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double -&gt; LayerNormSpec '[embedDim] dtype device
forall (normalizedShape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Double -&gt; LayerNormSpec normalizedShape dtype device
</span><a href="Torch.Typed.NN.Normalization.html#LayerNormSpec"><span class="hs-identifier hs-var">LayerNormSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679756519"><span class="hs-identifier hs-var">epsSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-288"></span><span>      </span><span class="annot"><span class="annottext">IO
  (TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer embedDim numHeads ffnDim dtype device)
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (TransformerLayer embedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
</span><a href="#local-6989586621679756518"><span class="hs-identifier hs-var">mlpSpec</span></a></span></span></span></span></span></span><span>
</span><span id="line-289"></span><span>
</span><span id="line-290"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-291"></span><span class="hs-comment">-- Transformer Language Model (GPT-2)</span><span>
</span><span id="line-292"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-293"></span><span>
</span><span id="line-294"></span><span class="hs-keyword">data</span><span>
</span><span id="line-295"></span><span>  </span><span id="TransformerLMSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-var">TransformerLMSpec</span></a></span></span><span>
</span><span id="line-296"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756516"><span class="annot"><a href="#local-6989586621679756516"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-297"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756515"><span class="annot"><a href="#local-6989586621679756515"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-298"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756514"><span class="annot"><a href="#local-6989586621679756514"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-299"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756513"><span class="annot"><a href="#local-6989586621679756513"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-300"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756512"><span class="annot"><a href="#local-6989586621679756512"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-301"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756511"><span class="annot"><a href="#local-6989586621679756511"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-302"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756510"><span class="annot"><a href="#local-6989586621679756510"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-303"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756509"><span class="annot"><a href="#local-6989586621679756509"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-304"></span><span>  </span><span id="TransformerLMSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-var">TransformerLMSpec</span></a></span></span><span>
</span><span id="line-305"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679756758"><span class="annot"><a href="#local-6989586621679756758"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span id="local-6989586621679756757"><span class="annot"><a href="#local-6989586621679756757"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679756756"><span class="annot"><a href="#local-6989586621679756756"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679756755"><span class="annot"><a href="#local-6989586621679756755"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span id="local-6989586621679756754"><span class="annot"><a href="#local-6989586621679756754"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679756753"><span class="annot"><a href="#local-6989586621679756753"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679756752"><span class="annot"><a href="#local-6989586621679756752"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679756751"><span class="annot"><a href="#local-6989586621679756751"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-306"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="lmDropoutSpec"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#lmDropoutSpec"><span class="hs-identifier hs-var hs-var">lmDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span>
</span><span id="line-307"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="lmLayerSpec"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLayerSpec embedDim numHeads ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#lmLayerSpec"><span class="hs-identifier hs-var hs-var">lmLayerSpec</span></a></span></span><span>   </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756753"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756757"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756756"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756752"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756751"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-308"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-309"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756758"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756757"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756756"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756755"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756754"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756753"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756752"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756751"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-310"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756500"><span id="local-6989586621679756502"><span id="local-6989586621679756504"><span class="annot"><span class="annottext">Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
(Int
 -&gt; TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
 -&gt; ShowS)
-&gt; (TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
    -&gt; String)
-&gt; ([TransformerLMSpec
       numAttnLayers
       numHeads
       ffnDim
       paddingIdx
       numEmbeds
       embedDim
       dtype
       device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLMSpec
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
showList :: [TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
$cshowList :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
show :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
$cshow :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
showsPrec :: Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
$cshowsPrec :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-311"></span><span>
</span><span id="line-312"></span><span id="local-6989586621679756498"><span id="local-6989586621679756499"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-313"></span><span>  </span><span id="TransformerLM"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span></span><span>
</span><span id="line-314"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756497"><span class="annot"><a href="#local-6989586621679756497"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-315"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756496"><span class="annot"><a href="#local-6989586621679756496"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-316"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756495"><span class="annot"><a href="#local-6989586621679756495"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-317"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756494"><span class="annot"><a href="#local-6989586621679756494"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-318"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756493"><span class="annot"><a href="#local-6989586621679756493"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-319"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756492"><span class="annot"><a href="#local-6989586621679756492"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-320"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756491"><span class="annot"><a href="#local-6989586621679756491"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-321"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756490"><span class="annot"><a href="#local-6989586621679756490"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-322"></span><span>  </span><span id="TransformerLM"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span></span><span>
</span><span id="line-323"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679756864"><span class="annot"><a href="#local-6989586621679756864"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span id="local-6989586621679756863"><span class="annot"><a href="#local-6989586621679756863"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679756862"><span class="annot"><a href="#local-6989586621679756862"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679756861"><span class="annot"><a href="#local-6989586621679756861"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span id="local-6989586621679756860"><span class="annot"><a href="#local-6989586621679756860"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679756859"><span class="annot"><a href="#local-6989586621679756859"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679756858"><span class="annot"><a href="#local-6989586621679756858"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679756857"><span class="annot"><a href="#local-6989586621679756857"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-324"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="tEmbedding"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding
     ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tEmbedding"><span class="hs-identifier hs-var hs-var">tEmbedding</span></a></span></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Embedding"><span class="hs-identifier hs-type">Embedding</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679756861"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679756860"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756859"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Learned"><span class="hs-identifier hs-type">Learned</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756858"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756857"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-325"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="tPosEmbedding"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tPosEmbedding"><span class="hs-identifier hs-var hs-var">tPosEmbedding</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Embedding"><span class="hs-identifier hs-type">Embedding</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span>           </span><span class="annot"><span class="hs-number">2048</span></span><span>      </span><span class="annot"><a href="#local-6989586621679756859"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Constant"><span class="hs-identifier hs-type">Constant</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756858"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756857"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-326"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="tDropout"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#tDropout"><span class="hs-identifier hs-var hs-var">tDropout</span></a></span></span><span>      </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-327"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="tLayers"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer embedDim numHeads ffnDim dtype device))
</span><a href="Torch.Typed.NN.Transformer.html#tLayers"><span class="hs-identifier hs-var hs-var">tLayers</span></a></span></span><span>       </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756864"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756859"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756863"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756862"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756858"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756857"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-328"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="tProj"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Linear embedDim numEmbeds dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tProj"><span class="hs-identifier hs-var hs-var">tProj</span></a></span></span><span>         </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756859"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756860"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756858"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756857"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-329"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-330"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756864"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756863"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756862"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756861"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756860"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756859"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756858"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756857"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-331"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(forall x.
 TransformerLM
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device
 -&gt; Rep
      (TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
      x)
-&gt; (forall x.
    Rep
      (TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
      x
    -&gt; TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
-&gt; Generic
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
$cto :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
$cfrom :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-332"></span><span>
</span><span id="line-333"></span><span class="hs-keyword">data</span><span>
</span><span id="line-334"></span><span>  </span><span id="FoldLayers"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span></span><span>
</span><span id="line-335"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756890"><span class="annot"><a href="#local-6989586621679756890"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-336"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756889"><span class="annot"><a href="#local-6989586621679756889"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-337"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756888"><span class="annot"><a href="#local-6989586621679756888"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-338"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679756887"><span class="annot"><a href="#local-6989586621679756887"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-339"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span id="FoldLayers"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span></span><span>
</span><span id="line-340"></span><span>      </span><span class="hs-special">{</span><span> </span><span id="flTrain"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device -&gt; Bool
</span><a href="Torch.Typed.NN.Transformer.html#flTrain"><span class="hs-identifier hs-var hs-var">flTrain</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-341"></span><span>      </span><span class="hs-special">,</span><span> </span><span id="flAttentionMask"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="Torch.Typed.NN.Transformer.html#flAttentionMask"><span class="hs-identifier hs-var hs-var">flAttentionMask</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756887"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756888"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756890"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756889"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756889"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-342"></span><span>      </span><span class="hs-special">,</span><span> </span><span id="flKeyPaddingMask"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Tensor device 'Bool '[batchSize, seqLen]
</span><a href="Torch.Typed.NN.Transformer.html#flKeyPaddingMask"><span class="hs-identifier hs-var hs-var">flKeyPaddingMask</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756887"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756890"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756889"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-343"></span><span>      </span><span class="hs-special">}</span><span>
</span><span id="line-344"></span><span>
</span><span id="line-345"></span><span id="local-6989586621679756470"><span id="local-6989586621679756471"><span id="local-6989586621679756472"><span id="local-6989586621679756473"><span id="local-6989586621679756474"><span id="local-6989586621679756475"><span id="local-6989586621679756476"><span id="local-6989586621679756477"><span class="hs-keyword">instance</span><span>
</span><span id="line-346"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756477"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-347"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756475"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679756477"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-348"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Mod</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-349"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-350"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756477"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756474"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756473"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756475"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-351"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#EndsWith"><span class="hs-identifier hs-type">EndsWith</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756473"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756474"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-352"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-353"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-354"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-355"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-356"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-357"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-358"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-359"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756473"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756474"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756477"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756470"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756473"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756474"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756471"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756472"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756473"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756474"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756476"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-360"></span><span>  </span><span id="local-6989586621679756467"><span class="annot"><span class="annottext">apply' :: FoldLayers batchSize seqLen dtype device
-&gt; (TransformerLayer embedDim numHeads ffnDim dtype device,
    IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span id="local-6989586621679756463"><span id="local-6989586621679756464"><span id="local-6989586621679756465"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679756462"><span class="annot"><span class="annottext">layer :: TransformerLayer embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679756462"><span class="hs-identifier hs-var">layer</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679756461"><span class="annot"><span class="annottext">mx :: IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="#local-6989586621679756461"><span class="hs-identifier hs-var">mx</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="#local-6989586621679756461"><span class="hs-identifier hs-var">mx</span></a></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, embedDim]
    -&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (numHeads :: Nat) (ffnDim :: Nat) (embedDim :: Nat)
       (headDim :: Nat) (seqLen :: Nat) (batchSize :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
(1 &lt;= numHeads, embedDim ~ (headDim * numHeads),
 Mod (embedDim * 3) 3 ~ 0, Div (embedDim * 3) 3 ~ embedDim,
 All KnownNat '[embedDim, numHeads, seqLen, batchSize, headDim],
 EndsWith '[batchSize, seqLen, embedDim] '[embedDim],
 KnownDType dtype, dtype ~ SumDType dtype,
 StandardFloatingPointDTypeValidation device dtype,
 MatMulDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype,
 SumDTypeIsValid device dtype, KnownDevice device) =&gt;
TransformerLayer embedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-var">transformerLayer</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLayer embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679756462"><span class="hs-identifier hs-var">layer</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756465"><span class="hs-identifier hs-var">flTrain</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756464"><span class="hs-identifier hs-var">flAttentionMask</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756463"><span class="hs-identifier hs-var">flKeyPaddingMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
forall a. Maybe a
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
forall a. Maybe a
</span><span class="hs-identifier hs-var">Nothing</span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-361"></span><span>
</span><span id="line-362"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-type">transformerLM</span></a></span><span>
</span><span id="line-363"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span>
</span><span id="line-364"></span><span>       </span><span id="local-6989586621679756783"><span class="annot"><a href="#local-6989586621679756783"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span>
</span><span id="line-365"></span><span>       </span><span id="local-6989586621679756782"><span class="annot"><a href="#local-6989586621679756782"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span>
</span><span id="line-366"></span><span>       </span><span id="local-6989586621679756781"><span class="annot"><a href="#local-6989586621679756781"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span>
</span><span id="line-367"></span><span>       </span><span id="local-6989586621679756790"><span class="annot"><a href="#local-6989586621679756790"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span>
</span><span id="line-368"></span><span>       </span><span id="local-6989586621679756786"><span class="annot"><a href="#local-6989586621679756786"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span>
</span><span id="line-369"></span><span>       </span><span id="local-6989586621679756789"><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span>
</span><span id="line-370"></span><span>       </span><span id="local-6989586621679756788"><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span>
</span><span id="line-371"></span><span>       </span><span id="local-6989586621679756787"><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span>
</span><span id="line-372"></span><span>       </span><span id="local-6989586621679756785"><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span></span><span>
</span><span id="line-373"></span><span>       </span><span id="local-6989586621679756784"><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-374"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756790"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-375"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756790"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756786"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span>
</span><span id="line-376"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span>
</span><span id="line-377"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.HList.html#HFoldrM"><span class="hs-identifier hs-type">HFoldrM</span></a></span><span>
</span><span id="line-378"></span><span>         </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-379"></span><span>         </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-380"></span><span>         </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-381"></span><span>         </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756783"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756782"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756781"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-382"></span><span>         </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-383"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-384"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-385"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span>
</span><span id="line-386"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-387"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-388"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-389"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756783"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756782"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756781"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756790"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756786"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-390"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-391"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-392"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756786"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-393"></span><span id="transformerLM"><span class="annot"><span class="annottext">transformerLM :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var hs-var">transformerLM</span></a></span></span><span> </span><span id="local-6989586621679756455"><span id="local-6989586621679756456"><span id="local-6989586621679756457"><span id="local-6989586621679756458"><span id="local-6989586621679756459"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span></span><span> </span><span id="local-6989586621679756454"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679756454"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679756453"><span class="annot"><span class="annottext">xTokens :: Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756453"><span class="hs-identifier hs-var">xTokens</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-394"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756452"><span class="annot"><span class="annottext">x :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756452"><span class="hs-identifier hs-var hs-var">x</span></a></span></span><span>         </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (paddingIdx :: Maybe Nat) (shape :: [Nat])
       (numEmbeds :: Nat) (embedSize :: Nat)
       (embeddingType :: EmbeddingType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape' :: [Nat]).
(KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds,
 shape' ~ Reverse (embedSize : Reverse shape)) =&gt;
Embedding paddingIdx numEmbeds embedSize embeddingType dtype device
-&gt; Tensor device 'Int64 shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.NN.Sparse.html#embed"><span class="hs-identifier hs-var">embed</span></a></span><span> </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="#local-6989586621679756459"><span class="hs-identifier hs-var">tEmbedding</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756453"><span class="hs-identifier hs-var">xTokens</span></a></span><span>
</span><span id="line-395"></span><span>      </span><span id="local-6989586621679756450"><span class="annot"><span class="annottext">positions :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756450"><span class="hs-identifier hs-var hs-var">positions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Bool
-&gt; Tensor device dtype '[seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', shape' ~ Broadcast shape shape') =&gt;
Bool -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#expand"><span class="hs-identifier hs-var">expand</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756789"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span>
</span><span id="line-396"></span><span>                    </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Int -&gt; Tensor device dtype '[seqLen, embedDim])
-&gt; Int
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Tensor device 'Int64 '[seqLen]
-&gt; Tensor device dtype '[seqLen, embedDim]
forall (paddingIdx :: Maybe Nat) (shape :: [Nat])
       (numEmbeds :: Nat) (embedSize :: Nat)
       (embeddingType :: EmbeddingType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape' :: [Nat]).
(KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds,
 shape' ~ Reverse (embedSize : Reverse shape)) =&gt;
Embedding paddingIdx numEmbeds embedSize embeddingType dtype device
-&gt; Tensor device 'Int64 shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.NN.Sparse.html#embed"><span class="hs-identifier hs-var">embed</span></a></span><span> </span><span class="annot"><span class="annottext">Embedding 'Nothing 2048 embedDim 'Constant dtype device
</span><a href="#local-6989586621679756458"><span class="hs-identifier hs-var">tPosEmbedding</span></a></span><span>
</span><span id="line-397"></span><span>                    </span><span class="annot"><span class="annottext">(Tensor device 'Int64 '[seqLen]
 -&gt; Tensor device dtype '[seqLen, embedDim])
-&gt; (Int -&gt; Tensor device 'Int64 '[seqLen])
-&gt; Int
-&gt; Tensor device dtype '[seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (dtype :: DType) (device :: (DeviceType, Nat))
       (shape :: [Nat]).
KnownDType 'Int64 =&gt;
Tensor device dtype shape -&gt; Tensor device 'Int64 shape
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span>
</span><span id="line-398"></span><span>                    </span><span class="annot"><span class="annottext">(Tensor device 'Float '[seqLen] -&gt; Tensor device 'Int64 '[seqLen])
-&gt; (Int -&gt; Tensor device 'Float '[seqLen])
-&gt; Int
-&gt; Tensor device 'Int64 '[seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[seqLen]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-399"></span><span>                    </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Int -&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (seqLen - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-400"></span><span>  </span><span id="local-6989586621679756446"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756446"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679756457"><span class="hs-identifier hs-var">tDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756454"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756452"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756450"><span class="hs-identifier hs-var">positions</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-401"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756445"><span class="annot"><span class="annottext">attentionMask :: Tensor device 'Bool '[1, seqLen, seqLen]
</span><a href="#local-6989586621679756445"><span class="hs-identifier hs-var hs-var">attentionMask</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 0, shape' ~ Unsqueeze shape 0) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-402"></span><span>                        </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[1, seqLen, seqLen])
-&gt; (Tensor device 'Int8 '[seqLen, seqLen]
    -&gt; Tensor device 'Bool '[seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[1, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (dtype :: DType) (device :: (DeviceType, Nat))
       (shape :: [Nat]).
KnownDType 'Bool =&gt;
Tensor device dtype shape -&gt; Tensor device 'Bool shape
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span>
</span><span id="line-403"></span><span>                        </span><span class="annot"><span class="annottext">(Tensor device 'Int8 '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[seqLen, seqLen])
-&gt; (Tensor device 'Int8 '[seqLen, seqLen]
    -&gt; Tensor device 'Int8 '[seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(shape ~ MatrixOrMatrixBatch shape) =&gt;
Int -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#triu"><span class="hs-identifier hs-var">triu</span></a></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-404"></span><span>                        </span><span class="annot"><span class="annottext">(Tensor device 'Int8 '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[1, seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[1, seqLen, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TensorOptions '[seqLen, seqLen] 'Int8 device =&gt;
Tensor device 'Int8 '[seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#ones"><span class="hs-identifier hs-var">ones</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Int8"><span class="hs-identifier hs-type">D.Int8</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-405"></span><span>      </span><span id="local-6989586621679756442"><span class="annot"><span class="annottext">attentionMask' :: Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756442"><span class="hs-identifier hs-var hs-var">attentionMask'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[1, seqLen, seqLen]
-&gt; Double
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
forall a (shape :: [Nat]) (shape' :: [Nat]) (shape'' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(Scalar a, shape'' ~ Broadcast shape shape') =&gt;
Tensor device 'Bool shape'
-&gt; a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Functional.html#maskedFill"><span class="hs-identifier hs-var">maskedFill</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[1, seqLen, seqLen]
</span><a href="#local-6989586621679756445"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span>
</span><span id="line-406"></span><span>                         </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen]
 -&gt; Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TensorOptions '[batchSize, seqLen, seqLen] dtype device =&gt;
Tensor device dtype '[batchSize, seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756787"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756788"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756785"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-407"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756440"><span class="annot"><span class="annottext">keyPaddingMask :: Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756440"><span class="hs-identifier hs-var hs-var">keyPaddingMask</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756453"><span class="hs-identifier hs-var">xTokens</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device 'Int64 '[]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ Broadcast shape shape',
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device 'Bool shape''
</span><a href="Torch.Typed.Tensor.html#%3D%3D."><span class="hs-operator hs-var">==.</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; Tensor device 'Int64 '[]
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Tensor device 'Int64 '[])
-&gt; (Proxy paddingIdx -&gt; Integer)
-&gt; Proxy paddingIdx
-&gt; Tensor device 'Int64 '[]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Proxy paddingIdx -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; Type).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy paddingIdx -&gt; Tensor device 'Int64 '[])
-&gt; Proxy paddingIdx -&gt; Tensor device 'Int64 '[]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy paddingIdx
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756790"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756784"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-408"></span><span>  </span><span id="local-6989586621679756436"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756436"><span class="hs-identifier hs-var">y</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer embedDim numHeads ffnDim dtype device))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall k k (m :: k -&gt; Type) f acc (xs :: [k]) (res :: k).
HFoldrM m f acc xs res =&gt;
f -&gt; acc -&gt; HList xs -&gt; m res
</span><a href="Torch.HList.html#hfoldrM"><span class="hs-identifier hs-var">hfoldrM</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; FoldLayers batchSize seqLen dtype device
forall (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Bool
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; FoldLayers batchSize seqLen dtype device
</span><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679756454"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
</span><a href="#local-6989586621679756442"><span class="hs-identifier hs-var">attentionMask'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679756440"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756446"><span class="hs-identifier hs-var">x'</span></a></span><span> </span><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayer embedDim numHeads ffnDim dtype device))
</span><a href="#local-6989586621679756456"><span class="hs-identifier hs-var">tLayers</span></a></span><span>
</span><span id="line-409"></span><span>  </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numEmbeds]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (m :: Type -&gt; Type) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numEmbeds]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds]))
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim numEmbeds dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim numEmbeds dtype device
</span><a href="#local-6989586621679756455"><span class="hs-identifier hs-var">tProj</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679756436"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-410"></span><span>
</span><span id="line-411"></span><span id="local-6989586621679756425"><span id="local-6989586621679756426"><span id="local-6989586621679756427"><span id="local-6989586621679756428"><span id="local-6989586621679756429"><span id="local-6989586621679756430"><span id="local-6989586621679756431"><span id="local-6989586621679756432"><span id="local-6989586621679756433"><span id="local-6989586621679756434"><span class="hs-keyword">instance</span><span>
</span><span id="line-412"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756434"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756433"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756431"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-413"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756434"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756430"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span>
</span><span id="line-414"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span>
</span><span id="line-415"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.HList.html#HFoldrM"><span class="hs-identifier hs-type">HFoldrM</span></a></span><span>
</span><span id="line-416"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-417"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756431"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-418"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756431"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756433"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-419"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756427"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756433"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756426"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756425"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-420"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756431"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756433"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-421"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-422"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-423"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span>
</span><span id="line-424"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-425"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-426"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756427"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756426"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756425"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756434"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756430"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756433"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756431"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756428"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756429"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756431"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756432"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756430"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-427"></span><span>  </span><span id="local-6989586621679756421"><span class="annot"><span class="annottext">forward :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
</span><a href="#local-6989586621679756421"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span id="local-6989586621679756420"><span class="annot"><span class="annottext">model :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679756420"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679756419"><span class="annot"><span class="annottext">input :: Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756419"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
 -&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(All KnownNat '[paddingIdx, embedDim, seqLen, batchSize],
 (paddingIdx + 1) &lt;= numEmbeds, 1 &lt;= seqLen,
 HFoldrM
   IO
   (FoldLayers batchSize seqLen dtype device)
   (Tensor device dtype '[batchSize, seqLen, embedDim])
   (HReplicateR
      numAttnLayers
      (TransformerLayer embedDim numHeads ffnDim dtype device))
   (Tensor device dtype '[batchSize, seqLen, embedDim]),
 BasicArithmeticDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device 'Int64, KnownDType dtype,
 KnownDevice device) =&gt;
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var">transformerLM</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679756420"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756419"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-428"></span><span>  </span><span id="local-6989586621679756418"><span class="annot"><span class="annottext">forwardStoch :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.NN.html#forwardStoch"><span class="hs-identifier hs-var hs-var hs-var hs-var">forwardStoch</span></a></span></span><span> </span><span id="local-6989586621679756416"><span class="annot"><span class="annottext">model :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679756416"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679756415"><span class="annot"><span class="annottext">input :: Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756415"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(All KnownNat '[paddingIdx, embedDim, seqLen, batchSize],
 (paddingIdx + 1) &lt;= numEmbeds, 1 &lt;= seqLen,
 HFoldrM
   IO
   (FoldLayers batchSize seqLen dtype device)
   (Tensor device dtype '[batchSize, seqLen, embedDim])
   (HReplicateR
      numAttnLayers
      (TransformerLayer embedDim numHeads ffnDim dtype device))
   (Tensor device dtype '[batchSize, seqLen, embedDim]),
 BasicArithmeticDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device 'Int64, KnownDType dtype,
 KnownDevice device) =&gt;
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var">transformerLM</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679756416"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679756415"><span class="hs-identifier hs-var">input</span></a></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-429"></span><span>
</span><span id="line-430"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-type">sinusoidal</span></a></span><span>
</span><span id="line-431"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679756739"><span class="annot"><a href="#local-6989586621679756739"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679756738"><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679756737"><span class="annot"><a href="#local-6989586621679756737"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-432"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756739"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-433"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756739"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span>
</span><span id="line-434"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span>
</span><span id="line-435"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-436"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756737"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span>
</span><span id="line-437"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756737"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span>
</span><span id="line-438"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756737"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-439"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-440"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756737"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756739"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-441"></span><span id="sinusoidal"><span class="annot"><span class="annottext">sinusoidal :: Tensor device 'Float '[numEmbeds, embedDim]
</span><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-var hs-var">sinusoidal</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-442"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679756413"><span class="annot"><span class="annottext">positions :: Tensor device 'Float '[numEmbeds, 1]
</span><a href="#local-6989586621679756413"><span class="hs-identifier hs-var hs-var">positions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-443"></span><span>        </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ Unsqueeze shape 1) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-444"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[numEmbeds]
 -&gt; Tensor device 'Float '[numEmbeds, 1])
-&gt; (Int -&gt; Tensor device 'Float '[numEmbeds])
-&gt; Int
-&gt; Tensor device 'Float '[numEmbeds, 1]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[numEmbeds]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756739"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-445"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device 'Float '[numEmbeds, 1])
-&gt; Int -&gt; Tensor device 'Float '[numEmbeds, 1]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (numEmbeds - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756739"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-446"></span><span>      </span><span id="local-6989586621679756412"><span class="annot"><span class="annottext">scalingFactors :: Tensor device 'Float '[Div embedDim 2]
</span><a href="#local-6989586621679756412"><span class="hs-identifier hs-var hs-var">scalingFactors</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-447"></span><span>        </span><span class="annot"><span class="annottext">Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#exp"><span class="hs-identifier hs-var">exp</span></a></span><span> </span><span>
</span><span id="line-448"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[Div embedDim 2]
 -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; (Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int
-&gt; Tensor device 'Float '[Div embedDim 2]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Double
-&gt; Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[Div embedDim 2]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#mulScalar"><span class="hs-identifier hs-var">mulScalar</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">log</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">10000</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; Double
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Double)
-&gt; (Proxy (Div embedDim 2) -&gt; Integer)
-&gt; Proxy (Div embedDim 2)
-&gt; Double
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Proxy (Div embedDim 2) -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; Type).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy (Div embedDim 2) -&gt; Double)
-&gt; Proxy (Div embedDim 2) -&gt; Double
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy (Div embedDim 2)
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-449"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[Div embedDim 2]
 -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; (Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int
-&gt; Tensor device 'Float '[Div embedDim 2]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[Div embedDim 2]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-450"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int -&gt; Tensor device 'Float '[Div embedDim 2]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (Div embedDim 2 - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679756738"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-451"></span><span>      </span><span id="local-6989586621679756408"><span class="annot"><span class="annottext">radians :: Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679756408"><span class="hs-identifier hs-var hs-var">radians</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, 1]
-&gt; Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#mul"><span class="hs-identifier hs-var">mul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, 1]
</span><a href="#local-6989586621679756413"><span class="hs-identifier hs-var">positions</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[Div embedDim 2]
</span><a href="#local-6989586621679756412"><span class="hs-identifier hs-var">scalingFactors</span></a></span><span>
</span><span id="line-452"></span><span>      </span><span id="local-6989586621679756406"><span class="annot"><span class="annottext">weights :: Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
</span><a href="#local-6989586621679756406"><span class="hs-identifier hs-var hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList
  '[Tensor device 'Float '[numEmbeds, Div embedDim 2],
    Tensor device 'Float '[numEmbeds, Div embedDim 2]]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
forall k (dim :: Nat) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)) (tensors :: [k]).
(KnownNat dim, '(shape, dtype, device) ~ Stack dim tensors,
 Castable (HList tensors) [ATenTensor]) =&gt;
HList tensors -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#stack"><span class="hs-identifier hs-var">stack</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#sin"><span class="hs-identifier hs-var">sin</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679756408"><span class="hs-identifier hs-var">radians</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; HList '[Tensor device 'Float '[numEmbeds, Div embedDim 2]]
-&gt; HList
     '[Tensor device 'Float '[numEmbeds, Div embedDim 2],
       Tensor device 'Float '[numEmbeds, Div embedDim 2]]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#cos"><span class="hs-identifier hs-var">cos</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679756408"><span class="hs-identifier hs-var">radians</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; HList '[]
-&gt; HList '[Tensor device 'Float '[numEmbeds, Div embedDim 2]]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
forall k. HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-453"></span><span>  </span><span class="hs-keyword">in</span><span>  </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
-&gt; Tensor device 'Float '[numEmbeds, embedDim]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
</span><a href="#local-6989586621679756406"><span class="hs-identifier hs-var">weights</span></a></span><span>
</span><span id="line-454"></span><span>
</span><span id="line-455"></span><span id="local-6989586621679756395"><span id="local-6989586621679756396"><span id="local-6989586621679756397"><span id="local-6989586621679756398"><span id="local-6989586621679756399"><span id="local-6989586621679756400"><span id="local-6989586621679756401"><span id="local-6989586621679756402"><span class="hs-keyword">instance</span><span>
</span><span id="line-456"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span>
</span><span id="line-457"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span>
</span><span id="line-458"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span>
</span><span id="line-459"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-460"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679756399"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-461"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.HList.html#HReplicate"><span class="hs-identifier hs-type">HReplicate</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756398"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756397"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756399"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-462"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756398"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756397"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756399"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-463"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756398"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756397"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756399"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-464"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-465"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-466"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span>
</span><span id="line-467"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span>
</span><span id="line-468"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-469"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756398"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756397"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756399"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-470"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679756398"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756397"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756399"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756401"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756400"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756396"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679756395"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-471"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-472"></span><span>  </span><span id="local-6989586621679756393"><span class="annot"><span class="annottext">sample :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; IO
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
</span><a href="#local-6989586621679756393"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679756391"><span id="local-6989586621679756392"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-473"></span><span>    </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Dropout
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer embedDim numHeads ffnDim dtype device))
-&gt; Linear embedDim numEmbeds dtype device
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Dropout
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer embedDim numHeads ffnDim dtype device))
-&gt; Linear embedDim numEmbeds dtype device
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span><span>
</span><span id="line-474"></span><span>      </span><span class="annot"><span class="annottext">(Embedding
   ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
 -&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
 -&gt; Dropout
 -&gt; HList
      (HReplicateR
         numAttnLayers
         (TransformerLayer embedDim numHeads ffnDim dtype device))
 -&gt; Linear embedDim numEmbeds dtype device
 -&gt; TransformerLM
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device)
-&gt; IO
     (Embedding
        ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device)
-&gt; IO
     (Embedding 'Nothing 2048 embedDim 'Constant dtype device
      -&gt; Dropout
      -&gt; HList
           (HReplicateR
              numAttnLayers
              (TransformerLayer embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">EmbeddingSpec
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; IO
     (Embedding
        ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">forall (paddingIdx :: Maybe Nat) (numEmbeds :: Nat)
       (embedSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
EmbeddingSpec paddingIdx numEmbeds embedSize 'Learned dtype device
forall (numEmbeds :: Nat) (embedSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
EmbeddingSpec
  ('Just paddingIdx) numEmbeds embedSize 'Learned dtype device
</span><a href="Torch.Typed.NN.Sparse.html#LearnedEmbeddingWithRandomInitSpec"><span class="hs-identifier hs-var">LearnedEmbeddingWithRandomInitSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679756402"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-475"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Embedding 'Nothing 2048 embedDim 'Constant dtype device
   -&gt; Dropout
   -&gt; HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO (Embedding 'Nothing 2048 embedDim 'Constant dtype device)
-&gt; IO
     (Dropout
      -&gt; HList
           (HReplicateR
              numAttnLayers
              (TransformerLayer embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">EmbeddingSpec 'Nothing 2048 embedDim 'Constant dtype device
-&gt; IO (Embedding 'Nothing 2048 embedDim 'Constant dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[2048, embedDim]
-&gt; EmbeddingSpec 'Nothing 2048 embedDim 'Constant dtype device
forall (paddingIdx :: Maybe Nat) (numEmbeds :: Nat)
       (embedSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Tensor device dtype '[numEmbeds, embedSize]
-&gt; EmbeddingSpec
     paddingIdx numEmbeds embedSize 'Constant dtype device
</span><a href="Torch.Typed.NN.Sparse.html#ConstEmbeddingSpec"><span class="hs-identifier hs-var">ConstEmbeddingSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device 'Float '[2048, embedDim]
-&gt; Tensor device dtype '[2048, embedDim]
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[2048, embedDim]
forall (numEmbeds :: Nat) (embedDim :: Nat)
       (device :: (DeviceType, Nat)).
(All KnownNat '[numEmbeds, embedDim], 1 &lt;= numEmbeds,
 1 &lt;= Div embedDim 2, (Div embedDim 2 * 2) ~ embedDim,
 StandardFloatingPointDTypeValidation device 'Float,
 BasicArithmeticDTypeIsValid device 'Float, KnownDevice device) =&gt;
Tensor device 'Float '[numEmbeds, embedDim]
</span><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-var">sinusoidal</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-476"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO Dropout
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679756392"><span class="hs-identifier hs-var">lmDropoutSpec</span></a></span><span>
</span><span id="line-477"></span><span>      </span><span class="annot"><span class="annottext">IO
  (HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer embedDim numHeads ffnDim dtype device)))
-&gt; IO
     (Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayerSpec embedDim numHeads ffnDim dtype device))
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer embedDim numHeads ffnDim dtype device)))
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">TransformerLayerSpec embedDim numHeads ffnDim dtype device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayerSpec embedDim numHeads ffnDim dtype device))
forall (n :: Nat) e. HReplicate n e =&gt; e -&gt; HList (HReplicateR n e)
</span><a href="Torch.HList.html#hreplicate"><span class="hs-identifier hs-var">hreplicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679756398"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLayerSpec embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679756391"><span class="hs-identifier hs-var">lmLayerSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-478"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO (Linear embedDim numEmbeds dtype device)
-&gt; IO
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim numEmbeds dtype device
-&gt; IO (Linear embedDim numEmbeds dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim numEmbeds dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-479"></span></pre></body></html>