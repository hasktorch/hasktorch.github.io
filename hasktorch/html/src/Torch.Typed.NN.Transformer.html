<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE DeriveAnyClass #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE DeriveGeneric #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE StandaloneDeriving #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-15"></span><span class="hs-pragma">{-# LANGUAGE NoStarIsType #-}</span><span>
</span><span id="line-16"></span><span class="hs-pragma">{-# OPTIONS_GHC -fconstraint-solver-iterations=0 #-}</span><span>
</span><span id="line-17"></span><span>
</span><span id="line-18"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Typed.NN.Transformer</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-19"></span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Control.Monad</span></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Data.Proxy</span></span><span>
</span><span id="line-22"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.Generics</span></span><span>
</span><span id="line-23"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span>
</span><span id="line-24"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">System.IO.Unsafe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">unsafePerformIO</span></span><span class="hs-special">)</span><span>
</span><span id="line-25"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.DType.html"><span class="hs-identifier">Torch.DType</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-26"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Device.html"><span class="hs-identifier">Torch.Device</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-27"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.HList.html"><span class="hs-identifier">Torch.HList</span></a></span><span>
</span><span id="line-28"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">..</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-29"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">A</span></span><span>
</span><span id="line-30"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Aux.html"><span class="hs-identifier">Torch.Typed.Aux</span></a></span><span>
</span><span id="line-31"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html"><span class="hs-identifier">Torch.Typed.Factories</span></a></span><span>
</span><span id="line-32"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html"><span class="hs-identifier">Torch.Typed.Functional</span></a></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#linear"><span class="hs-identifier">linear</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#log"><span class="hs-identifier">log</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-33"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html"><span class="hs-identifier">Torch.Typed.NN.Dropout</span></a></span><span>
</span><span id="line-34"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html"><span class="hs-identifier">Torch.Typed.NN.Linear</span></a></span><span>
</span><span id="line-35"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html"><span class="hs-identifier">Torch.Typed.NN.Normalization</span></a></span><span>
</span><span id="line-36"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html"><span class="hs-identifier">Torch.Typed.NN.Sparse</span></a></span><span>
</span><span id="line-37"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html"><span class="hs-identifier">Torch.Typed.Parameter</span></a></span><span>
</span><span id="line-38"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html"><span class="hs-identifier">Torch.Typed.Tensor</span></a></span><span>
</span><span id="line-39"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span> </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier">cos</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">exp</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier">sin</span></span><span class="hs-special">)</span><span>
</span><span id="line-40"></span><span>
</span><span id="line-41"></span><span id="residual"><span class="annot"><span class="annottext">residual :: (Tensor device dtype shape -&gt; m (Tensor device dtype' shape'))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
      (CheckBroadcast
         shape
         shape'
         (ComputeBroadcast
            (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
    -&gt; m b)
-&gt; Tensor device dtype shape
-&gt; m b
</span><a href="Torch.Typed.NN.Transformer.html#residual"><span class="hs-identifier hs-var hs-var">residual</span></a></span></span><span> </span><span id="local-6989586621679818010"><span class="annot"><span class="annottext">f :: Tensor device dtype shape -&gt; m (Tensor device dtype' shape')
</span><a href="#local-6989586621679818010"><span class="hs-identifier hs-var">f</span></a></span></span><span> </span><span id="local-6989586621679818009"><span class="annot"><span class="annottext">g :: Tensor
  device
  (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
  (CheckBroadcast
     shape
     shape'
     (ComputeBroadcast
        (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
-&gt; m b
</span><a href="#local-6989586621679818009"><span class="hs-identifier hs-var">g</span></a></span></span><span> </span><span id="local-6989586621679818008"><span class="annot"><span class="annottext">x :: Tensor device dtype shape
</span><a href="#local-6989586621679818008"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype shape -&gt; m (Tensor device dtype' shape')
</span><a href="#local-6989586621679818010"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype shape
</span><a href="#local-6989586621679818008"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">m (Tensor device dtype' shape')
-&gt; (Tensor device dtype' shape' -&gt; m b) -&gt; m b
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">\</span><span id="local-6989586621679818007"><span class="annot"><span class="annottext">x' :: Tensor device dtype' shape'
</span><a href="#local-6989586621679818007"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
  (CheckBroadcast
     shape
     shape'
     (ComputeBroadcast
        (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
-&gt; m b
</span><a href="#local-6989586621679818009"><span class="hs-identifier hs-var">g</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype shape
</span><a href="#local-6989586621679818008"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype shape
-&gt; Tensor device dtype' shape'
-&gt; Tensor
     device
     (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
     (CheckBroadcast
        shape
        shape'
        (ComputeBroadcast
           (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype' shape'
</span><a href="#local-6989586621679818007"><span class="hs-identifier hs-var">x'</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-42"></span><span>
</span><span id="line-43"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-44"></span><span class="hs-comment">-- Relation-Aware Multi-Headed Attention Layer</span><span>
</span><span id="line-45"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-46"></span><span>
</span><span id="line-47"></span><span class="hs-keyword">data</span><span>
</span><span id="line-48"></span><span>  </span><span id="MultiheadAttentionSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-var">MultiheadAttentionSpec</span></a></span></span><span>
</span><span id="line-49"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818005"><span class="annot"><a href="#local-6989586621679818005"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-50"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818004"><span class="annot"><a href="#local-6989586621679818004"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-51"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818003"><span class="annot"><a href="#local-6989586621679818003"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-52"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818002"><span class="annot"><a href="#local-6989586621679818002"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-53"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818001"><span class="annot"><a href="#local-6989586621679818001"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-54"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818000"><span class="annot"><a href="#local-6989586621679818000"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-55"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-56"></span><span>  </span><span id="local-6989586621679817994"><span id="local-6989586621679817995"><span id="local-6989586621679817996"><span id="local-6989586621679817997"><span id="local-6989586621679817998"><span id="local-6989586621679817999"><span id="MultiheadAttentionSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-var">MultiheadAttentionSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-57"></span><span>    </span><span class="hs-comment">-- | spec for dropout</span><span>
</span><span id="line-58"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-59"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817999"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817998"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817997"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817996"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817995"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817994"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-60"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817987"><span id="local-6989586621679817989"><span id="local-6989586621679817991"><span class="annot"><span class="annottext">Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
[MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
(Int
 -&gt; MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; ShowS)
-&gt; (MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; String)
-&gt; ([MultiheadAttentionSpec
       embedDim kEmbedDim vEmbedDim numHeads dtype device]
    -&gt; ShowS)
-&gt; Show
     (MultiheadAttentionSpec
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showList :: [MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
show :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showsPrec :: Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817982"><span id="local-6989586621679817984"><span class="annot"><span class="annottext">MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
(MultiheadAttentionSpec
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; Bool)
-&gt; (MultiheadAttentionSpec
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; MultiheadAttentionSpec
         embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; Bool)
-&gt; Eq
     (MultiheadAttentionSpec
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
/= :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
$c/= :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
== :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
$c== :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-61"></span><span>
</span><span id="line-62"></span><span id="local-6989586621679817979"><span id="local-6989586621679817980"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-63"></span><span>  </span><span id="MultiheadAttention"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span></span><span>
</span><span id="line-64"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817978"><span class="annot"><a href="#local-6989586621679817978"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-65"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817977"><span class="annot"><a href="#local-6989586621679817977"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-66"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817976"><span class="annot"><a href="#local-6989586621679817976"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-67"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817975"><span class="annot"><a href="#local-6989586621679817975"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-68"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817974"><span class="annot"><a href="#local-6989586621679817974"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-69"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817973"><span class="annot"><a href="#local-6989586621679817973"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-70"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-71"></span><span>  </span><span id="local-6989586621679818641"><span id="local-6989586621679818642"><span id="local-6989586621679818643"><span id="local-6989586621679818644"><span id="local-6989586621679818645"><span id="local-6989586621679818646"><span id="MultiheadAttention"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-72"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | in-projection for query</span><span>
</span><span id="line-73"></span><span>      </span><span id="mhaQInProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaQInProj"><span class="hs-identifier hs-var hs-var">mhaQInProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818642"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818641"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-74"></span><span>      </span><span class="hs-comment">-- | in-projection for key</span><span>
</span><span id="line-75"></span><span>      </span><span id="mhaKInProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear kEmbedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaKInProj"><span class="hs-identifier hs-var hs-var">mhaKInProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818645"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818642"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818641"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-76"></span><span>      </span><span class="hs-comment">-- | in-projection for value</span><span>
</span><span id="line-77"></span><span>      </span><span id="mhaVInProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear vEmbedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaVInProj"><span class="hs-identifier hs-var hs-var">mhaVInProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818644"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818642"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818641"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-78"></span><span>      </span><span class="hs-comment">-- | out-projection</span><span>
</span><span id="line-79"></span><span>      </span><span id="mhaOutProj"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Linear embedDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaOutProj"><span class="hs-identifier hs-var hs-var">mhaOutProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818642"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818641"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-80"></span><span>      </span><span class="hs-comment">-- | dropout</span><span>
</span><span id="line-81"></span><span>      </span><span id="mhaDropout"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#mhaDropout"><span class="hs-identifier hs-var hs-var">mhaDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-82"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-83"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818646"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818645"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818644"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818643"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818642"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818641"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-84"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817961"><span id="local-6989586621679817963"><span id="local-6989586621679817965"><span class="annot"><span class="annottext">Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
[MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
(Int
 -&gt; MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; ShowS)
-&gt; (MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; String)
-&gt; ([MultiheadAttention
       embedDim kEmbedDim vEmbedDim numHeads dtype device]
    -&gt; ShowS)
-&gt; Show
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showList :: [MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
[MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device]
-&gt; ShowS
show :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; String
showsPrec :: Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; Rep
      (MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
      x)
-&gt; (forall x.
    Rep
      (MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
      x
    -&gt; MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; Generic
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall x.
Rep
  (MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device)
  x
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
forall x.
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Rep
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device)
  x
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Rep
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
     x
$cto :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device)
  x
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
$cfrom :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) x.
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Rep
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817954"><span id="local-6989586621679817956"><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
(MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; HList
      (Parameters
         (MultiheadAttention
            embedDim kEmbedDim vEmbedDim numHeads dtype device)))
-&gt; (MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device
    -&gt; HList
         (Parameters
            (MultiheadAttention
               embedDim kEmbedDim vEmbedDim numHeads dtype device))
    -&gt; MultiheadAttention
         embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; Parameterized
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall f.
(f -&gt; HList (Parameters f))
-&gt; (f -&gt; HList (Parameters f) -&gt; f) -&gt; Parameterized f
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
replaceParameters :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
$creplaceParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
flattenParameters :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
$cflattenParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; HList
     (Parameters
        (MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device))
</span><a href="Torch.Typed.Parameter.html#C%3AParameterized"><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Parameterized</span></a></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-85"></span><span>
</span><span id="line-86"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-type">multiheadAttention</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-87"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818342"><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818340"><span class="annot"><a href="#local-6989586621679818340"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679818339"><span class="annot"><a href="#local-6989586621679818339"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679818344"><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679818338"><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679818337"><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span></span><span> </span><span id="local-6989586621679818336"><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679818341"><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span></span><span> </span><span id="local-6989586621679818335"><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818334"><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-88"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-89"></span><span>    </span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-90"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818340"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818339"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-91"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-92"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-93"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-94"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-95"></span><span>    </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-96"></span><span>    </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-97"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-98"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-99"></span><span>  </span><span class="hs-comment">-- | multi-head attention model ADT</span><span>
</span><span id="line-100"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818340"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818339"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-101"></span><span>  </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-102"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-103"></span><span>  </span><span class="hs-comment">-- | optional attention mask</span><span>
</span><span id="line-104"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-105"></span><span>  </span><span class="hs-comment">-- | optional key padding mask</span><span>
</span><span id="line-106"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-107"></span><span>  </span><span class="hs-comment">-- | optional key relations</span><span>
</span><span id="line-108"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-109"></span><span>  </span><span class="hs-comment">-- | optional value relations</span><span>
</span><span id="line-110"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-111"></span><span>  </span><span class="hs-comment">-- | query representation</span><span>
</span><span id="line-112"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-113"></span><span>  </span><span class="hs-comment">-- | key representation</span><span>
</span><span id="line-114"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818340"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-115"></span><span>  </span><span class="hs-comment">-- | value representation</span><span>
</span><span id="line-116"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818339"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-117"></span><span>  </span><span class="hs-comment">-- | attention and attention averaged over heads</span><span>
</span><span id="line-118"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-119"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-120"></span><span>      </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818338"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-121"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-122"></span><span id="multiheadAttention"><span class="annot"><span class="annottext">multiheadAttention :: MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-var hs-var">multiheadAttention</span></a></span></span><span> </span><span id="local-6989586621679817947"><span id="local-6989586621679817948"><span id="local-6989586621679817949"><span id="local-6989586621679817950"><span id="local-6989586621679817951"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span></span><span> </span><span id="local-6989586621679817946"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679817946"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679817945"><span class="annot"><span class="annottext">attentionMask :: Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679817945"><span class="hs-identifier hs-var">attentionMask</span></a></span></span><span> </span><span id="local-6989586621679817944"><span class="annot"><span class="annottext">keyPaddingMask :: Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817944"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span></span><span> </span><span id="local-6989586621679817943"><span class="annot"><span class="annottext">keyRelations :: Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817943"><span class="hs-identifier hs-var">keyRelations</span></a></span></span><span> </span><span id="local-6989586621679817942"><span class="annot"><span class="annottext">valueRelations :: Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817942"><span class="hs-identifier hs-var">valueRelations</span></a></span></span><span> </span><span id="local-6989586621679817941"><span class="annot"><span class="annottext">query :: Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817941"><span class="hs-identifier hs-var">query</span></a></span></span><span> </span><span id="local-6989586621679817940"><span class="annot"><span class="annottext">key :: Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679817940"><span class="hs-identifier hs-var">key</span></a></span></span><span> </span><span id="local-6989586621679817939"><span class="annot"><span class="annottext">value :: Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679817939"><span class="hs-identifier hs-var">value</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-123"></span><span>  </span><span id="local-6989586621679817938"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817938"><span class="hs-identifier hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span>
</span><span id="line-124"></span><span>    </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679817947"><span class="hs-identifier hs-var">mhaDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817946"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-125"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; IO
      (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]))
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 3, DimOutOfBoundCheck shape 3, KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
forall (dim :: Nat) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat dim, DimOutOfBoundCheck shape dim, KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#softmax"><span class="hs-identifier hs-var">softmax</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span>
</span><span id="line-126"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817934"><span class="hs-identifier hs-var">_maskKeyPaddings</span></a></span><span>
</span><span id="line-127"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817933"><span class="hs-identifier hs-var">_maskAttention</span></a></span><span>
</span><span id="line-128"></span><span>      </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
 -&gt; IO
      (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]))
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; IO (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817932"><span class="hs-identifier hs-var">_attentionWeights</span></a></span><span>
</span><span id="line-129"></span><span>  </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim],
 Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817931"><span class="hs-identifier hs-var">_attention</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817938"><span class="hs-identifier hs-var">weights</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679817930"><span class="hs-identifier hs-var">averageOverHeads</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817938"><span class="hs-identifier hs-var">weights</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-130"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-131"></span><span>    </span><span id="local-6989586621679817932"><span class="annot"><span class="annottext">_attentionWeights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817932"><span class="hs-identifier hs-var hs-var">_attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-132"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817929"><span class="annot"><span class="annottext">scaling :: Double
</span><a href="#local-6989586621679817929"><span class="hs-identifier hs-var hs-var">scaling</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">Prelude.sqrt</span></span><span> </span><span class="annot"><span class="annottext">(Double -&gt; Double) -&gt; (Int -&gt; Double) -&gt; Int -&gt; Double
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Double
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">(Int -&gt; Double) -&gt; Int -&gt; Double
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat headDim =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-133"></span><span>          </span><span id="local-6989586621679817926"><span class="annot"><span class="annottext">q :: Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
</span><a href="#local-6989586621679817926"><span class="hs-identifier hs-var hs-var">q</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall (seqLen'' :: Nat).
KnownNat seqLen'' =&gt;
Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679817925"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Double
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#divScalar"><span class="hs-identifier hs-var">divScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679817929"><span class="hs-identifier hs-var">scaling</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
</span><a href="#local-6989586621679817951"><span class="hs-identifier hs-var">mhaQInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817941"><span class="hs-identifier hs-var">query</span></a></span><span>
</span><span id="line-134"></span><span>          </span><span id="local-6989586621679817922"><span class="annot"><span class="annottext">k :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679817922"><span class="hs-identifier hs-var hs-var">k</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall (seqLen'' :: Nat).
KnownNat seqLen'' =&gt;
Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679817925"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, kEmbedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear kEmbedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear kEmbedDim embedDim dtype device
</span><a href="#local-6989586621679817950"><span class="hs-identifier hs-var">mhaKInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, kEmbedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679817940"><span class="hs-identifier hs-var">key</span></a></span><span>
</span><span id="line-135"></span><span>          </span><span id="local-6989586621679817921"><span class="annot"><span class="annottext">weights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817921"><span class="hs-identifier hs-var hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, headDim, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
</span><a href="#local-6989586621679817926"><span class="hs-identifier hs-var">q</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, headDim, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679817922"><span class="hs-identifier hs-var">k</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-136"></span><span>          </span><span id="local-6989586621679817918"><span class="annot"><span class="annottext">weights' :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817918"><span class="hs-identifier hs-var hs-var">weights'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817943"><span class="hs-identifier hs-var">keyRelations</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-137"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817921"><span class="hs-identifier hs-var">weights</span></a></span><span>
</span><span id="line-138"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679817917"><span class="annot"><span class="annottext">kr :: Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679817917"><span class="hs-identifier hs-var">kr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817921"><span class="hs-identifier hs-var">weights</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
</span><a href="#local-6989586621679817926"><span class="hs-identifier hs-var">q</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', headDim, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-operator hs-var">`matmul`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', headDim, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679817917"><span class="hs-identifier hs-var">kr</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-139"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817918"><span class="hs-identifier hs-var">weights'</span></a></span><span>
</span><span id="line-140"></span><span>    </span><span id="local-6989586621679817933"><span class="annot"><span class="annottext">_maskAttention :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817933"><span class="hs-identifier hs-var hs-var">_maskAttention</span></a></span></span><span> </span><span id="local-6989586621679817916"><span class="annot"><span class="annottext">attentionWeights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817916"><span class="hs-identifier hs-var">attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-141"></span><span>      </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679817945"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-142"></span><span>        </span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817916"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span>
</span><span id="line-143"></span><span>        </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679817915"><span class="annot"><span class="annottext">am :: Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679817915"><span class="hs-identifier hs-var">am</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817916"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, 1, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, 1, seqLen', seqLen]
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679817915"><span class="hs-identifier hs-var">am</span></a></span><span>
</span><span id="line-144"></span><span>    </span><span id="local-6989586621679817934"><span class="annot"><span class="annottext">_maskKeyPaddings :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817934"><span class="hs-identifier hs-var hs-var">_maskKeyPaddings</span></a></span></span><span> </span><span id="local-6989586621679817913"><span class="annot"><span class="annottext">attentionWeights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817913"><span class="hs-identifier hs-var">attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-145"></span><span>      </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817944"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-146"></span><span>        </span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817913"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span>
</span><span id="line-147"></span><span>        </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679817912"><span class="annot"><span class="annottext">kpm :: Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679817912"><span class="hs-identifier hs-var">kpm</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-148"></span><span>          </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817911"><span class="annot"><span class="annottext">keyPaddingMask' :: Tensor device 'Bool '[batchSize, 1, 1, seqLen]
</span><a href="#local-6989586621679817911"><span class="hs-identifier hs-var hs-var">keyPaddingMask'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 2, shape' ~ Unsqueeze shape 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, 1, seqLen]
 -&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen])
-&gt; (Tensor device 'Bool '[batchSize, seqLen]
    -&gt; Tensor device 'Bool '[batchSize, 1, seqLen])
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ Unsqueeze shape 1) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, seqLen]
 -&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen])
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Tensor device 'Bool '[batchSize, 1, 1, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
</span><a href="#local-6989586621679817912"><span class="hs-identifier hs-var">kpm</span></a></span><span>
</span><span id="line-149"></span><span>           </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, 1, 1, seqLen]
-&gt; Double
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
forall a (shape :: [Nat]) (shape' :: [Nat]) (shape'' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(Scalar a, shape'' ~ Broadcast shape shape') =&gt;
Tensor device 'Bool shape'
-&gt; a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Functional.html#maskedFill"><span class="hs-identifier hs-var">maskedFill</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, 1, 1, seqLen]
</span><a href="#local-6989586621679817911"><span class="hs-identifier hs-var">keyPaddingMask'</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817913"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span>
</span><span id="line-150"></span><span>    </span><span id="local-6989586621679817931"><span class="annot"><span class="annottext">_attention :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817931"><span class="hs-identifier hs-var hs-var">_attention</span></a></span></span><span> </span><span id="local-6989586621679817908"><span class="annot"><span class="annottext">attentionWeights :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817908"><span class="hs-identifier hs-var">attentionWeights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-151"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817907"><span class="annot"><span class="annottext">v :: Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679817907"><span class="hs-identifier hs-var hs-var">v</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall (seqLen'' :: Nat).
KnownNat seqLen'' =&gt;
Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679817925"><span class="hs-identifier hs-var">reshape'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, vEmbedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear vEmbedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear vEmbedDim embedDim dtype device
</span><a href="#local-6989586621679817949"><span class="hs-identifier hs-var">mhaVInProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, vEmbedDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679817939"><span class="hs-identifier hs-var">value</span></a></span><span>
</span><span id="line-152"></span><span>          </span><span id="local-6989586621679817906"><span class="annot"><span class="annottext">attention :: Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679817906"><span class="hs-identifier hs-var hs-var">attention</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817908"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen, headDim]
</span><a href="#local-6989586621679817907"><span class="hs-identifier hs-var">v</span></a></span><span>
</span><span id="line-153"></span><span>          </span><span id="local-6989586621679817905"><span class="annot"><span class="annottext">attention' :: Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679817905"><span class="hs-identifier hs-var hs-var">attention'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">case</span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817942"><span class="hs-identifier hs-var">valueRelations</span></a></span><span> </span><span class="hs-keyword">of</span><span>
</span><span id="line-154"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679817906"><span class="hs-identifier hs-var">attention</span></a></span><span>
</span><span id="line-155"></span><span>            </span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621679817904"><span class="annot"><span class="annottext">vr :: Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679817904"><span class="hs-identifier hs-var">vr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679817906"><span class="hs-identifier hs-var">attention</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype shape' -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Tensor.html#matmul"><span class="hs-identifier hs-var">matmul</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, seqLen]
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
</span><a href="#local-6989586621679817908"><span class="hs-identifier hs-var">attentionWeights</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', seqLen, headDim]
</span><a href="#local-6989586621679817904"><span class="hs-identifier hs-var">vr</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-156"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
</span><a href="#local-6989586621679817948"><span class="hs-identifier hs-var">mhaOutProj</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape '[batchSize, seqLen', embedDim],
 Numel shape ~ Numel '[batchSize, seqLen', embedDim]) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818337"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
 -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', numHeads, headDim]
</span><a href="#local-6989586621679817905"><span class="hs-identifier hs-var">attention'</span></a></span><span>
</span><span id="line-157"></span><span>    </span><span id="local-6989586621679817930"><span class="annot"><span class="annottext">averageOverHeads :: Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
</span><a href="#local-6989586621679817930"><span class="hs-identifier hs-var hs-var">averageOverHeads</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-158"></span><span>      </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817902"><span class="annot"><span class="annottext">numHeads' :: Int
</span><a href="#local-6989586621679817902"><span class="hs-identifier hs-var hs-var">numHeads'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">KnownNat numHeads =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-159"></span><span>       </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Int
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#divScalar"><span class="hs-identifier hs-var">divScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679817902"><span class="hs-identifier hs-var">numHeads'</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', seqLen]
 -&gt; Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; (Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
    -&gt; Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen', seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen', seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (dtype' :: DType) (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ DropValue shape 1,
 SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape'
forall (d :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (device :: (DeviceType, Nat)).
(KnownNat d, shape' ~ DropValue shape d,
 SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape'
</span><a href="Torch.Typed.Functional.html#sumDim"><span class="hs-identifier hs-var">sumDim</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-160"></span><span>    </span><span class="annot"><a href="#local-6989586621679817925"><span class="hs-identifier hs-type">reshape'</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-161"></span><span>      </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818618"><span class="annot"><a href="#local-6989586621679818618"><span class="hs-identifier hs-type">seqLen''</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-162"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679818618"><span class="hs-identifier hs-type">seqLen''</span></a></span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-163"></span><span>      </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818618"><span class="hs-identifier hs-type">seqLen''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818342"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-164"></span><span>      </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818334"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818335"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818618"><span class="hs-identifier hs-type">seqLen''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-165"></span><span>    </span><span id="local-6989586621679817925"><span class="annot"><span class="annottext">reshape' :: Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
</span><a href="#local-6989586621679817925"><span class="hs-identifier hs-var hs-var">reshape'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, KnownNat 2, shape' ~ Transpose shape 1 2) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (n :: Nat) (m :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat n, KnownNat m, shape' ~ Transpose shape n m) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#transpose"><span class="hs-identifier hs-var">transpose</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen'', numHeads, headDim]
 -&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim])
-&gt; (Tensor device dtype '[batchSize, seqLen'', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen'', numHeads, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen'', embedDim]
-&gt; Tensor device dtype '[batchSize, numHeads, seqLen'', headDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape '[batchSize, seqLen'', numHeads, headDim],
 Numel shape ~ Numel '[batchSize, seqLen'', numHeads, headDim]) =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype '[batchSize, seqLen'', numHeads, headDim]
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818336"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818618"><span class="hs-identifier hs-type">seqLen''</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818344"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818341"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-166"></span><span>
</span><span id="line-167"></span><span id="local-6989586621679817895"><span id="local-6989586621679817896"><span id="local-6989586621679817897"><span id="local-6989586621679817898"><span id="local-6989586621679817899"><span id="local-6989586621679817900"><span class="hs-keyword">instance</span><span>
</span><span id="line-168"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817900"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817899"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817898"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817897"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-169"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817896"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-170"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817895"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-171"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817895"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817896"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-172"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-173"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-174"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817900"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817899"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817898"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817897"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817896"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817895"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-175"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817900"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817899"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817898"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817897"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817896"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817895"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-176"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-177"></span><span>  </span><span id="local-6989586621679817892"><span class="annot"><span class="annottext">sample :: MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span id="local-6989586621679817890"><span class="annot"><span class="annottext">mhaDropoutSpec :: DropoutSpec
</span><a href="#local-6989586621679817890"><span class="hs-identifier hs-var">mhaDropoutSpec</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-178"></span><span>    </span><span class="annot"><span class="annottext">Linear embedDim embedDim dtype device
-&gt; Linear kEmbedDim embedDim dtype device
-&gt; Linear vEmbedDim embedDim dtype device
-&gt; Linear embedDim embedDim dtype device
-&gt; Dropout
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
forall (embedDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat).
Linear embedDim embedDim dtype device
-&gt; Linear kEmbedDim embedDim dtype device
-&gt; Linear vEmbedDim embedDim dtype device
-&gt; Linear embedDim embedDim dtype device
-&gt; Dropout
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-var">MultiheadAttention</span></a></span><span>
</span><span id="line-179"></span><span>      </span><span class="annot"><span class="annottext">(Linear embedDim embedDim dtype device
 -&gt; Linear kEmbedDim embedDim dtype device
 -&gt; Linear vEmbedDim embedDim dtype device
 -&gt; Linear embedDim embedDim dtype device
 -&gt; Dropout
 -&gt; MultiheadAttention
      embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear embedDim embedDim dtype device)
-&gt; IO
     (Linear kEmbedDim embedDim dtype device
      -&gt; Linear vEmbedDim embedDim dtype device
      -&gt; Linear embedDim embedDim dtype device
      -&gt; Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
-&gt; IO (Linear embedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-180"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear kEmbedDim embedDim dtype device
   -&gt; Linear vEmbedDim embedDim dtype device
   -&gt; Linear embedDim embedDim dtype device
   -&gt; Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear kEmbedDim embedDim dtype device)
-&gt; IO
     (Linear vEmbedDim embedDim dtype device
      -&gt; Linear embedDim embedDim dtype device
      -&gt; Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec kEmbedDim embedDim dtype device
-&gt; IO (Linear kEmbedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec kEmbedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-181"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear vEmbedDim embedDim dtype device
   -&gt; Linear embedDim embedDim dtype device
   -&gt; Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear vEmbedDim embedDim dtype device)
-&gt; IO
     (Linear embedDim embedDim dtype device
      -&gt; Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec vEmbedDim embedDim dtype device
-&gt; IO (Linear vEmbedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec vEmbedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-182"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear embedDim embedDim dtype device
   -&gt; Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO (Linear embedDim embedDim dtype device)
-&gt; IO
     (Dropout
      -&gt; MultiheadAttention
           embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
-&gt; IO (Linear embedDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-183"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO Dropout
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679817890"><span class="hs-identifier hs-var">mhaDropoutSpec</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-184"></span><span>
</span><span id="line-185"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-186"></span><span class="hs-comment">-- Transformer MLP Layer</span><span>
</span><span id="line-187"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-188"></span><span>
</span><span id="line-189"></span><span class="hs-keyword">data</span><span>
</span><span id="line-190"></span><span>  </span><span id="TransformerMLPSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-var">TransformerMLPSpec</span></a></span></span><span>
</span><span id="line-191"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817887"><span class="annot"><a href="#local-6989586621679817887"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-192"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817886"><span class="annot"><a href="#local-6989586621679817886"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-193"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817885"><span class="annot"><a href="#local-6989586621679817885"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-194"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817884"><span class="annot"><a href="#local-6989586621679817884"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-195"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-196"></span><span>  </span><span id="TransformerMLPSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-var">TransformerMLPSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-197"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818465"><span class="annot"><a href="#local-6989586621679818465"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818464"><span class="annot"><a href="#local-6989586621679818464"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818463"><span class="annot"><a href="#local-6989586621679818463"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818462"><span class="annot"><a href="#local-6989586621679818462"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-198"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | spec for relu dropout</span><span>
</span><span id="line-199"></span><span>      </span><span id="dropout0Spec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#dropout0Spec"><span class="hs-identifier hs-var hs-var">dropout0Spec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-200"></span><span>      </span><span class="hs-comment">-- | spec for other dropout</span><span>
</span><span id="line-201"></span><span>      </span><span id="dropout1Spec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#dropout1Spec"><span class="hs-identifier hs-var hs-var">dropout1Spec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-202"></span><span>      </span><span class="hs-comment">-- | epsilon for layer norm</span><span>
</span><span id="line-203"></span><span>      </span><span id="epsSpec"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device -&gt; Double
</span><a href="Torch.Typed.NN.Transformer.html#epsSpec"><span class="hs-identifier hs-var hs-var">epsSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span>
</span><span id="line-204"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-205"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818465"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818464"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818463"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818462"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-206"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817874"><span id="local-6989586621679817876"><span id="local-6989586621679817878"><span class="annot"><span class="annottext">Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
(Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerMLPSpec embedDim ffnDim dtype device -&gt; String)
-&gt; ([TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS)
-&gt; Show (TransformerMLPSpec embedDim ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
showList :: [TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLPSpec embedDim ffnDim dtype device] -&gt; ShowS
show :: TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device -&gt; String
showsPrec :: Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817870"><span id="local-6989586621679817872"><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
(TransformerMLPSpec embedDim ffnDim dtype device
 -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool)
-&gt; (TransformerMLPSpec embedDim ffnDim dtype device
    -&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool)
-&gt; Eq (TransformerMLPSpec embedDim ffnDim dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
/= :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
$c/= :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
== :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
$c== :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLPSpec embedDim ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device -&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-207"></span><span>
</span><span id="line-208"></span><span id="local-6989586621679817868"><span id="local-6989586621679817869"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-209"></span><span>  </span><span id="TransformerMLP"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span></span><span>
</span><span id="line-210"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817867"><span class="annot"><a href="#local-6989586621679817867"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-211"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817866"><span class="annot"><a href="#local-6989586621679817866"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-212"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817865"><span class="annot"><a href="#local-6989586621679817865"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-213"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817864"><span class="annot"><a href="#local-6989586621679817864"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-214"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-215"></span><span>  </span><span id="TransformerMLP"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-216"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818479"><span class="annot"><a href="#local-6989586621679818479"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818478"><span class="annot"><a href="#local-6989586621679818478"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818477"><span class="annot"><a href="#local-6989586621679818477"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818476"><span class="annot"><a href="#local-6989586621679818476"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-217"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | first fully connected layer</span><span>
</span><span id="line-218"></span><span>      </span><span id="linear0"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Linear embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#linear0"><span class="hs-identifier hs-var hs-var">linear0</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818479"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818478"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818477"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818476"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-219"></span><span>      </span><span class="hs-comment">-- | second fully connected layer</span><span>
</span><span id="line-220"></span><span>      </span><span id="linear1"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#linear1"><span class="hs-identifier hs-var hs-var">linear1</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818478"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818479"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818477"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818476"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-221"></span><span>      </span><span class="hs-comment">-- | relu dropout</span><span>
</span><span id="line-222"></span><span>      </span><span id="dropout0"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#dropout0"><span class="hs-identifier hs-var hs-var">dropout0</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-223"></span><span>      </span><span class="hs-comment">-- | other dropout</span><span>
</span><span id="line-224"></span><span>      </span><span id="dropout1"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device -&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#dropout1"><span class="hs-identifier hs-var hs-var">dropout1</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-225"></span><span>      </span><span class="hs-comment">-- | layer norm</span><span>
</span><span id="line-226"></span><span>      </span><span id="ln"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
</span><a href="Torch.Typed.NN.Transformer.html#ln"><span class="hs-identifier hs-var hs-var">ln</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html#LayerNorm"><span class="hs-identifier hs-type">LayerNorm</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818479"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679818477"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818476"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-227"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-228"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818479"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818478"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818477"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818476"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-229"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817852"><span id="local-6989586621679817854"><span id="local-6989586621679817856"><span class="annot"><span class="annottext">Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
TransformerMLP embedDim ffnDim dtype device -&gt; String
(Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS)
-&gt; (TransformerMLP embedDim ffnDim dtype device -&gt; String)
-&gt; ([TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS)
-&gt; Show (TransformerMLP embedDim ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; String
showList :: [TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerMLP embedDim ffnDim dtype device] -&gt; ShowS
show :: TransformerMLP embedDim ffnDim dtype device -&gt; String
$cshow :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device -&gt; String
showsPrec :: Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int -&gt; TransformerMLP embedDim ffnDim dtype device -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TransformerMLP embedDim ffnDim dtype device
 -&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x)
-&gt; (forall x.
    Rep (TransformerMLP embedDim ffnDim dtype device) x
    -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; Generic (TransformerMLP embedDim ffnDim dtype device)
forall x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
forall x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
$cto :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep (TransformerMLP embedDim ffnDim dtype device) x
-&gt; TransformerMLP embedDim ffnDim dtype device
$cfrom :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerMLP embedDim ffnDim dtype device
-&gt; Rep (TransformerMLP embedDim ffnDim dtype device) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817846"><span id="local-6989586621679817848"><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
(TransformerMLP embedDim ffnDim dtype device
 -&gt; HList
      (Parameters (TransformerMLP embedDim ffnDim dtype device)))
-&gt; (TransformerMLP embedDim ffnDim dtype device
    -&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
    -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; Parameterized (TransformerMLP embedDim ffnDim dtype device)
forall f.
(f -&gt; HList (Parameters f))
-&gt; (f -&gt; HList (Parameters f) -&gt; f) -&gt; Parameterized f
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
replaceParameters :: TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
$creplaceParameters :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
-&gt; TransformerMLP embedDim ffnDim dtype device
flattenParameters :: TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
$cflattenParameters :: forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerMLP embedDim ffnDim dtype device
-&gt; HList (Parameters (TransformerMLP embedDim ffnDim dtype device))
</span><a href="#local-6989586621679817846"><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Parameterized</span></a></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-230"></span><span>
</span><span id="line-231"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-type">transformerMLP</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-232"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818329"><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818325"><span class="annot"><a href="#local-6989586621679818325"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818328"><span class="annot"><a href="#local-6989586621679818328"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span id="local-6989586621679818327"><span class="annot"><a href="#local-6989586621679818327"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span id="local-6989586621679818330"><span class="annot"><a href="#local-6989586621679818330"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818331"><span class="annot"><a href="#local-6989586621679818331"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-233"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818331"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818330"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-234"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818331"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818330"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-235"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-236"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#IsSuffixOf"><span class="hs-identifier hs-type">IsSuffixOf</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818328"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818327"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-237"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-238"></span><span>  </span><span class="hs-comment">-- | MLP model ADT for transformer</span><span>
</span><span id="line-239"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818325"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818330"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818331"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-240"></span><span>  </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-241"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-242"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818331"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818330"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818328"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818327"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-comment">-- input</span><span>
</span><span id="line-243"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818331"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818330"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818328"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818327"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818329"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-comment">-- output</span><span>
</span><span id="line-244"></span><span id="transformerMLP"><span class="annot"><span class="annottext">transformerMLP :: TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-var hs-var">transformerMLP</span></a></span></span><span> </span><span id="local-6989586621679817840"><span id="local-6989586621679817841"><span id="local-6989586621679817842"><span id="local-6989586621679817843"><span id="local-6989586621679817844"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span></span><span> </span><span id="local-6989586621679817839"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679817839"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679817838"><span class="annot"><span class="annottext">input :: Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679817838"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-245"></span><span>  </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype (CmpDType dtype dtype))
      (CheckBroadcast
         '[seqLen, batchSize, embedDim]
         '[seqLen, batchSize, embedDim]
         (ComputeBroadcast
            (ReverseImpl '[seqLen, batchSize, embedDim] '[])
            (ReverseImpl '[seqLen, batchSize, embedDim] '[])))
    -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (dtype' :: DType) (m :: Type -&gt; Type) (shape :: [Nat])
       (shape' :: [Nat]) b.
(BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid
   device (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')),
 Monad m) =&gt;
(Tensor device dtype shape -&gt; m (Tensor device dtype' shape'))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
      (CheckBroadcast
         shape
         shape'
         (ComputeBroadcast
            (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
    -&gt; m b)
-&gt; Tensor device dtype shape
-&gt; m b
</span><a href="Torch.Typed.NN.Transformer.html#residual"><span class="hs-identifier hs-var">residual</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="#local-6989586621679817837"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
</span><a href="#local-6989586621679817840"><span class="hs-identifier hs-var">ln</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679817838"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-246"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-247"></span><span>    </span><span id="local-6989586621679817837"><span class="annot"><span class="annottext">f :: Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="#local-6989586621679817837"><span class="hs-identifier hs-var hs-var">f</span></a></span></span><span> </span><span id="local-6989586621679817836"><span class="annot"><span class="annottext">x :: Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679817836"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-248"></span><span>      </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679817841"><span class="hs-identifier hs-var">dropout1</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817839"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-249"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, ffnDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear ffnDim embedDim dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear ffnDim embedDim dtype device
</span><a href="#local-6989586621679817843"><span class="hs-identifier hs-var">linear1</span></a></span><span>
</span><span id="line-250"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim]))
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679817842"><span class="hs-identifier hs-var">dropout0</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817839"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-251"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim]))
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, ffnDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#relu"><span class="hs-identifier hs-var">relu</span></a></span><span>
</span><span id="line-252"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, ffnDim]
 -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; (Tensor device dtype '[seqLen, batchSize, embedDim]
    -&gt; Tensor device dtype '[seqLen, batchSize, ffnDim])
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; Tensor device dtype '[seqLen, batchSize, ffnDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
</span><a href="#local-6989586621679817844"><span class="hs-identifier hs-var">linear0</span></a></span><span>
</span><span id="line-253"></span><span>        </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, batchSize, embedDim]
 -&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim]))
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
-&gt; IO (Tensor device dtype '[seqLen, batchSize, ffnDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[seqLen, batchSize, embedDim]
</span><a href="#local-6989586621679817836"><span class="hs-identifier hs-var">x</span></a></span><span>
</span><span id="line-254"></span><span>
</span><span id="line-255"></span><span id="local-6989586621679817830"><span id="local-6989586621679817831"><span id="local-6989586621679817832"><span id="local-6989586621679817833"><span class="hs-keyword">instance</span><span>
</span><span id="line-256"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817833"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817832"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-257"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817831"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-258"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817830"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-259"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817830"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817831"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-260"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-261"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-262"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817833"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817832"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817831"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817830"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-263"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817833"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817832"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817831"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817830"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-264"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-265"></span><span>  </span><span id="local-6989586621679817828"><span class="annot"><span class="annottext">sample :: TransformerMLPSpec embedDim ffnDim dtype device
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
</span><a href="#local-6989586621679817828"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679817825"><span id="local-6989586621679817826"><span id="local-6989586621679817827"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-266"></span><span>    </span><span class="annot"><span class="annottext">Linear embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
-&gt; Dropout
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
forall (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Linear embedDim ffnDim dtype device
-&gt; Linear ffnDim embedDim dtype device
-&gt; Dropout
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-var">TransformerMLP</span></a></span><span>
</span><span id="line-267"></span><span>      </span><span class="annot"><span class="annottext">(Linear embedDim ffnDim dtype device
 -&gt; Linear ffnDim embedDim dtype device
 -&gt; Dropout
 -&gt; Dropout
 -&gt; LayerNorm '[embedDim] dtype device
 -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (Linear embedDim ffnDim dtype device)
-&gt; IO
     (Linear ffnDim embedDim dtype device
      -&gt; Dropout
      -&gt; Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim ffnDim dtype device
-&gt; IO (Linear embedDim ffnDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim ffnDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-268"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear ffnDim embedDim dtype device
   -&gt; Dropout
   -&gt; Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (Linear ffnDim embedDim dtype device)
-&gt; IO
     (Dropout
      -&gt; Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec ffnDim embedDim dtype device
-&gt; IO (Linear ffnDim embedDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec ffnDim embedDim dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span><span>
</span><span id="line-269"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679817827"><span class="hs-identifier hs-var">dropout0Spec</span></a></span><span>
</span><span id="line-270"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679817826"><span class="hs-identifier hs-var">dropout1Spec</span></a></span><span>
</span><span id="line-271"></span><span>      </span><span class="annot"><span class="annottext">IO
  (LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device)
-&gt; IO (LayerNorm '[embedDim] dtype device)
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayerNormSpec '[embedDim] dtype device
-&gt; IO (LayerNorm '[embedDim] dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double -&gt; LayerNormSpec '[embedDim] dtype device
forall (normalizedShape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Double -&gt; LayerNormSpec normalizedShape dtype device
</span><a href="Torch.Typed.NN.Normalization.html#LayerNormSpec"><span class="hs-identifier hs-var">LayerNormSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679817825"><span class="hs-identifier hs-var">epsSpec</span></a></span><span class="hs-special">)</span></span></span></span></span><span>
</span><span id="line-272"></span><span>
</span><span id="line-273"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-274"></span><span class="hs-comment">-- Relation-Aware Transformer Layer</span><span>
</span><span id="line-275"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-276"></span><span>
</span><span id="line-277"></span><span class="hs-keyword">data</span><span>
</span><span id="line-278"></span><span>  </span><span id="TransformerLayerSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-var">TransformerLayerSpec</span></a></span></span><span>
</span><span id="line-279"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817823"><span class="annot"><a href="#local-6989586621679817823"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-280"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817822"><span class="annot"><a href="#local-6989586621679817822"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-281"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817821"><span class="annot"><a href="#local-6989586621679817821"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-282"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817820"><span class="annot"><a href="#local-6989586621679817820"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-283"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817819"><span class="annot"><a href="#local-6989586621679817819"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-284"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817818"><span class="annot"><a href="#local-6989586621679817818"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-285"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817817"><span class="annot"><a href="#local-6989586621679817817"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-286"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-287"></span><span>  </span><span id="TransformerLayerSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-var">TransformerLayerSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-288"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818317"><span class="annot"><a href="#local-6989586621679818317"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818316"><span class="annot"><a href="#local-6989586621679818316"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679818315"><span class="annot"><a href="#local-6989586621679818315"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679818314"><span class="annot"><a href="#local-6989586621679818314"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679818313"><span class="annot"><a href="#local-6989586621679818313"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818312"><span class="annot"><a href="#local-6989586621679818312"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818311"><span class="annot"><a href="#local-6989586621679818311"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-289"></span><span>    </span><span class="hs-special">{</span><span> </span><span id="mhaSpec"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; MultiheadAttentionSpec
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mhaSpec"><span class="hs-identifier hs-var hs-var">mhaSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttentionSpec"><span class="hs-identifier hs-type">MultiheadAttentionSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818317"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818316"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818315"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818314"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818312"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818311"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-290"></span><span>      </span><span id="attnDropoutSpec"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#attnDropoutSpec"><span class="hs-identifier hs-var hs-var">attnDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-291"></span><span>      </span><span id="epsSpec%27"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Double
</span><a href="Torch.Typed.NN.Transformer.html#epsSpec%27"><span class="hs-identifier hs-var hs-var">epsSpec'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">,</span><span>
</span><span id="line-292"></span><span>      </span><span id="mlpSpec"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerMLPSpec embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#mlpSpec"><span class="hs-identifier hs-var hs-var">mlpSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLPSpec"><span class="hs-identifier hs-type">TransformerMLPSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818317"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818313"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818312"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818311"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-293"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-294"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818317"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818316"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818315"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818314"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818313"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818312"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818311"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-295"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817806"><span id="local-6989586621679817808"><span id="local-6989586621679817810"><span class="annot"><span class="annottext">Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
[TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
(Int
 -&gt; TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; ShowS)
-&gt; (TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; String)
-&gt; ([TransformerLayerSpec
       embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLayerSpec
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showList :: [TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
show :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showsPrec :: Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817802"><span id="local-6989586621679817804"><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
(TransformerLayerSpec
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; Bool)
-&gt; (TransformerLayerSpec
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; TransformerLayerSpec
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; Bool)
-&gt; Eq
     (TransformerLayerSpec
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
/= :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
$c/= :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
== :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
$c== :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerLayerSpec
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-296"></span><span>
</span><span id="line-297"></span><span id="local-6989586621679817800"><span id="local-6989586621679817801"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-298"></span><span>  </span><span id="TransformerLayer"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span></span><span>
</span><span id="line-299"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817799"><span class="annot"><a href="#local-6989586621679817799"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-300"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817798"><span class="annot"><a href="#local-6989586621679817798"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-301"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817797"><span class="annot"><a href="#local-6989586621679817797"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-302"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817796"><span class="annot"><a href="#local-6989586621679817796"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-303"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817795"><span class="annot"><a href="#local-6989586621679817795"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-304"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817794"><span class="annot"><a href="#local-6989586621679817794"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-305"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817793"><span class="annot"><a href="#local-6989586621679817793"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-306"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-307"></span><span>  </span><span id="TransformerLayer"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-308"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818351"><span class="annot"><a href="#local-6989586621679818351"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818350"><span class="annot"><a href="#local-6989586621679818350"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679818349"><span class="annot"><a href="#local-6989586621679818349"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span id="local-6989586621679818348"><span class="annot"><a href="#local-6989586621679818348"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679818347"><span class="annot"><a href="#local-6989586621679818347"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818346"><span class="annot"><a href="#local-6989586621679818346"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818345"><span class="annot"><a href="#local-6989586621679818345"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-309"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | multi-head attention</span><span>
</span><span id="line-310"></span><span>      </span><span id="transformerLayer_mha"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; MultiheadAttention
     embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_mha"><span class="hs-identifier hs-var hs-var">transformerLayer_mha</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#MultiheadAttention"><span class="hs-identifier hs-type">MultiheadAttention</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818351"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818350"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818349"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818348"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818346"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818345"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-311"></span><span>      </span><span class="hs-comment">-- | dropout</span><span>
</span><span id="line-312"></span><span>      </span><span id="transformerLayer_attnDropout"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_attnDropout"><span class="hs-identifier hs-var hs-var">transformerLayer_attnDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-313"></span><span>      </span><span class="hs-comment">-- | layer norm</span><span>
</span><span id="line-314"></span><span>      </span><span id="transformerLayer_ln"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; LayerNorm '[embedDim] dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_ln"><span class="hs-identifier hs-var hs-var">transformerLayer_ln</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Normalization.html#LayerNorm"><span class="hs-identifier hs-type">LayerNorm</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818351"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><a href="#local-6989586621679818346"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818345"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-315"></span><span>      </span><span class="hs-comment">-- | MLP</span><span>
</span><span id="line-316"></span><span>      </span><span id="transformerLayer_mlp"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer_mlp"><span class="hs-identifier hs-var hs-var">transformerLayer_mlp</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerMLP"><span class="hs-identifier hs-type">TransformerMLP</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818351"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818347"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818346"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818345"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-317"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-318"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818351"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818350"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818349"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818348"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818347"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818346"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818345"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-319"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817782"><span id="local-6989586621679817784"><span id="local-6989586621679817786"><span class="annot"><span class="annottext">Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
[TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
(Int
 -&gt; TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; ShowS)
-&gt; (TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; String)
-&gt; ([TransformerLayer
       embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showList :: [TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
$cshowList :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device]
-&gt; ShowS
show :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
$cshow :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; String
showsPrec :: Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
$cshowsPrec :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; Rep
      (TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
      x)
-&gt; (forall x.
    Rep
      (TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
      x
    -&gt; TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; Generic
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall x.
Rep
  (TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
  x
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
forall x.
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Rep
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
  x
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Rep
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
     x
$cto :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
  x
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
$cfrom :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Rep
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817776"><span id="local-6989586621679817778"><span class="annot"><span class="annottext">TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
(TransformerLayer
   embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
 -&gt; HList
      (Parameters
         (TransformerLayer
            embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)))
-&gt; (TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
    -&gt; HList
         (Parameters
            (TransformerLayer
               embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
    -&gt; TransformerLayer
         embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; Parameterized
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall f.
(f -&gt; HList (Parameters f))
-&gt; (f -&gt; HList (Parameters f) -&gt; f) -&gt; Parameterized f
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
replaceParameters :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
$creplaceParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
flattenParameters :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
$cflattenParameters :: forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; HList
     (Parameters
        (TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device))
</span><a href="#local-6989586621679817776"><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Parameterized</span></a></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-320"></span><span>
</span><span id="line-321"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-type">transformerLayer</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-322"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818173"><span class="annot"><a href="#local-6989586621679818173"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818163"><span class="annot"><a href="#local-6989586621679818163"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818172"><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818170"><span class="annot"><a href="#local-6989586621679818170"><span class="hs-identifier hs-type">kEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818169"><span class="annot"><a href="#local-6989586621679818169"><span class="hs-identifier hs-type">vEmbedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818171"><span class="annot"><a href="#local-6989586621679818171"><span class="hs-identifier hs-type">headDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818168"><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818167"><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679818166"><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679818165"><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818164"><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-323"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679818173"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-324"></span><span>    </span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679818171"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679818173"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-325"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818170"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818169"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818173"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818171"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-326"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#IsSuffixOf"><span class="hs-identifier hs-type">IsSuffixOf</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-327"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-328"></span><span>    </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-329"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-330"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-331"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-332"></span><span>    </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-333"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-334"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-335"></span><span>  </span><span class="hs-comment">-- | transformer layer model ADT</span><span>
</span><span id="line-336"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818170"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818169"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818173"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818163"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-337"></span><span>  </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-338"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-339"></span><span>  </span><span class="hs-comment">-- | optional attention mask</span><span>
</span><span id="line-340"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-341"></span><span>  </span><span class="hs-comment">-- | optional key padding mask</span><span>
</span><span id="line-342"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-343"></span><span>  </span><span class="hs-comment">-- | optional key relations</span><span>
</span><span id="line-344"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818171"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-345"></span><span>  </span><span class="hs-comment">-- | optional value relations</span><span>
</span><span id="line-346"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818171"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-347"></span><span>  </span><span class="hs-comment">-- | query representation</span><span>
</span><span id="line-348"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-349"></span><span>  </span><span class="hs-comment">-- | key representation</span><span>
</span><span id="line-350"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818170"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-351"></span><span>  </span><span class="hs-comment">-- | value representation</span><span>
</span><span id="line-352"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818168"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818169"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-353"></span><span>  </span><span class="hs-comment">-- | transformer layer output representation</span><span>
</span><span id="line-354"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818164"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818165"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818166"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818167"><span class="hs-identifier hs-type">seqLen'</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818172"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-355"></span><span id="transformerLayer"><span class="annot"><span class="annottext">transformerLayer :: TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-var hs-var">transformerLayer</span></a></span></span><span> </span><span id="local-6989586621679817771"><span id="local-6989586621679817772"><span id="local-6989586621679817773"><span id="local-6989586621679817774"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span><span> </span><span id="local-6989586621679817770"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679817770"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679817769"><span class="annot"><span class="annottext">attentionMask :: Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679817769"><span class="hs-identifier hs-var">attentionMask</span></a></span></span><span> </span><span id="local-6989586621679817768"><span class="annot"><span class="annottext">keyPaddingMask :: Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817768"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span></span><span> </span><span id="local-6989586621679817767"><span class="annot"><span class="annottext">keyRelations :: Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817767"><span class="hs-identifier hs-var">keyRelations</span></a></span></span><span> </span><span id="local-6989586621679817766"><span class="annot"><span class="annottext">valueRelations :: Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817766"><span class="hs-identifier hs-var">valueRelations</span></a></span></span><span> </span><span id="local-6989586621679817765"><span class="annot"><span class="annottext">query :: Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817765"><span class="hs-identifier hs-var">query</span></a></span></span><span> </span><span id="local-6989586621679817764"><span class="annot"><span class="annottext">key :: Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679817764"><span class="hs-identifier hs-var">key</span></a></span></span><span> </span><span id="local-6989586621679817763"><span class="annot"><span class="annottext">value :: Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679817763"><span class="hs-identifier hs-var">value</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-356"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817762"><span class="annot"><span class="annottext">f :: Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="#local-6989586621679817762"><span class="hs-identifier hs-var hs-var">f</span></a></span></span><span> </span><span id="local-6989586621679817761"><span class="annot"><span class="annottext">query' :: Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817761"><span class="hs-identifier hs-var">query'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-357"></span><span>        </span><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (seqLen :: Nat) (seqLen' :: Nat)
       (batchSize :: Nat) (headDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(1 &lt;= numHeads, embedDim ~ (headDim * numHeads),
 All
   KnownNat
   '[embedDim, kEmbedDim, vEmbedDim, numHeads, seqLen, seqLen',
     batchSize, headDim],
 KnownDType dtype,
 StandardFloatingPointDTypeValidation device dtype,
 MatMulDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype, dtype ~ SumDType dtype,
 SumDTypeIsValid device dtype, KnownDevice device) =&gt;
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO
     (Tensor device dtype '[batchSize, seqLen', embedDim],
      Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#multiheadAttention"><span class="hs-identifier hs-var">multiheadAttention</span></a></span><span> </span><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="#local-6989586621679817774"><span class="hs-identifier hs-var">transformerLayer_mha</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817770"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
</span><a href="#local-6989586621679817769"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817768"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817767"><span class="hs-identifier hs-var">keyRelations</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
</span><a href="#local-6989586621679817766"><span class="hs-identifier hs-var">valueRelations</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817761"><span class="hs-identifier hs-var">query'</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, kEmbedDim]
</span><a href="#local-6989586621679817764"><span class="hs-identifier hs-var">key</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, vEmbedDim]
</span><a href="#local-6989586621679817763"><span class="hs-identifier hs-var">value</span></a></span><span>
</span><span id="line-358"></span><span>          </span><span class="annot"><span class="annottext">IO
  (Tensor device dtype '[batchSize, seqLen', embedDim],
   Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; ((Tensor device dtype '[batchSize, seqLen', embedDim],
     Tensor device dtype '[batchSize, seqLen', seqLen])
    -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679817773"><span class="hs-identifier hs-var">transformerLayer_attnDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817770"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; ((Tensor device dtype '[batchSize, seqLen', embedDim],
     Tensor device dtype '[batchSize, seqLen', seqLen])
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim],
    Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim],
 Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall a b. (a, b) -&gt; a
</span><span class="hs-identifier hs-var">fst</span></span><span>
</span><span id="line-359"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype (CmpDType dtype dtype))
      (CheckBroadcast
         '[batchSize, seqLen', embedDim]
         '[batchSize, seqLen', embedDim]
         (ComputeBroadcast
            (ReverseImpl '[batchSize, seqLen', embedDim] '[])
            (ReverseImpl '[batchSize, seqLen', embedDim] '[])))
    -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (dtype' :: DType) (m :: Type -&gt; Type) (shape :: [Nat])
       (shape' :: [Nat]) b.
(BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid
   device (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')),
 Monad m) =&gt;
(Tensor device dtype shape -&gt; m (Tensor device dtype' shape'))
-&gt; (Tensor
      device
      (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype'))
      (CheckBroadcast
         shape
         shape'
         (ComputeBroadcast
            (ReverseImpl shape '[]) (ReverseImpl shape' '[])))
    -&gt; m b)
-&gt; Tensor device dtype shape
-&gt; m b
</span><a href="Torch.Typed.NN.Transformer.html#residual"><span class="hs-identifier hs-var">residual</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="#local-6989586621679817762"><span class="hs-identifier hs-var">f</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen', embedDim]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">LayerNorm '[embedDim] dtype device
</span><a href="#local-6989586621679817772"><span class="hs-identifier hs-var">transformerLayer_ln</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen', embedDim]
</span><a href="#local-6989586621679817765"><span class="hs-identifier hs-var">query</span></a></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen', embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen', embedDim]
    -&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
forall (embedDim :: Nat) (ffnDim :: Nat) (seqLen :: Nat)
       (batchSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
(BasicArithmeticDTypeIsValid device dtype,
 StandardFloatingPointDTypeValidation device dtype,
 KnownNat embedDim,
 IsSuffixOf '[embedDim] '[seqLen, batchSize, embedDim]) =&gt;
TransformerMLP embedDim ffnDim dtype device
-&gt; Bool
-&gt; Tensor device dtype '[seqLen, batchSize, embedDim]
-&gt; IO (Tensor device dtype '[seqLen, batchSize, embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerMLP"><span class="hs-identifier hs-var">transformerMLP</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerMLP embedDim ffnDim dtype device
</span><a href="#local-6989586621679817771"><span class="hs-identifier hs-var">transformerLayer_mlp</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817770"><span class="hs-identifier hs-var">train</span></a></span><span>
</span><span id="line-360"></span><span>
</span><span id="line-361"></span><span id="local-6989586621679817754"><span id="local-6989586621679817755"><span id="local-6989586621679817756"><span id="local-6989586621679817757"><span id="local-6989586621679817758"><span id="local-6989586621679817759"><span id="local-6989586621679817760"><span class="hs-keyword">instance</span><span>
</span><span id="line-362"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817760"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817759"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817758"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817757"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817756"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-363"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817755"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-364"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817754"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-365"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817754"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817755"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-366"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-367"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-368"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817760"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817759"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817758"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817757"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817756"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817755"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817754"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-369"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817760"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817759"><span class="hs-identifier hs-type">kEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817758"><span class="hs-identifier hs-type">vEmbedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817757"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817756"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817755"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817754"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-370"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-371"></span><span>  </span><span id="local-6989586621679817752"><span class="annot"><span class="annottext">sample :: TransformerLayerSpec
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; IO
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
</span><a href="#local-6989586621679817752"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679817748"><span id="local-6989586621679817749"><span id="local-6989586621679817750"><span id="local-6989586621679817751"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-372"></span><span>    </span><span class="annot"><span class="annottext">MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
forall (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat)
       (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
MultiheadAttention
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; Dropout
-&gt; LayerNorm '[embedDim] dtype device
-&gt; TransformerMLP embedDim ffnDim dtype device
-&gt; TransformerLayer
     embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-var">TransformerLayer</span></a></span><span>
</span><span id="line-373"></span><span>      </span><span class="annot"><span class="annottext">(MultiheadAttention
   embedDim kEmbedDim vEmbedDim numHeads dtype device
 -&gt; Dropout
 -&gt; LayerNorm '[embedDim] dtype device
 -&gt; TransformerMLP embedDim ffnDim dtype device
 -&gt; TransformerLayer
      embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
-&gt; IO
     (Dropout
      -&gt; LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
-&gt; IO
     (MultiheadAttention
        embedDim kEmbedDim vEmbedDim numHeads dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">MultiheadAttentionSpec
  embedDim kEmbedDim vEmbedDim numHeads dtype device
</span><a href="#local-6989586621679817751"><span class="hs-identifier hs-var">mhaSpec</span></a></span><span>
</span><span id="line-374"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO Dropout
-&gt; IO
     (LayerNorm '[embedDim] dtype device
      -&gt; TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679817750"><span class="hs-identifier hs-var">attnDropoutSpec</span></a></span><span>
</span><span id="line-375"></span><span>      </span><span class="annot"><span class="annottext">IO
  (LayerNorm '[embedDim] dtype device
   -&gt; TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO (LayerNorm '[embedDim] dtype device)
-&gt; IO
     (TransformerMLP embedDim ffnDim dtype device
      -&gt; TransformerLayer
           embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LayerNormSpec '[embedDim] dtype device
-&gt; IO (LayerNorm '[embedDim] dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Double -&gt; LayerNormSpec '[embedDim] dtype device
forall (normalizedShape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Double -&gt; LayerNormSpec normalizedShape dtype device
</span><a href="Torch.Typed.NN.Normalization.html#LayerNormSpec"><span class="hs-identifier hs-var">LayerNormSpec</span></a></span><span> </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679817749"><span class="hs-identifier hs-var">epsSpec'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-376"></span><span>      </span><span class="annot"><span class="annottext">IO
  (TransformerMLP embedDim ffnDim dtype device
   -&gt; TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
-&gt; IO
     (TransformerLayer
        embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
-&gt; IO (TransformerMLP embedDim ffnDim dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerMLPSpec embedDim ffnDim dtype device
</span><a href="#local-6989586621679817748"><span class="hs-identifier hs-var">mlpSpec</span></a></span></span></span></span></span></span></span></span><span>
</span><span id="line-377"></span><span>
</span><span id="line-378"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-379"></span><span class="hs-comment">-- Transformer Language Model (GPT-2)</span><span>
</span><span id="line-380"></span><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><span id="line-381"></span><span>
</span><span id="line-382"></span><span class="hs-keyword">data</span><span>
</span><span id="line-383"></span><span>  </span><span id="TransformerLMSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-var">TransformerLMSpec</span></a></span></span><span>
</span><span id="line-384"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817747"><span class="annot"><a href="#local-6989586621679817747"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-385"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817746"><span class="annot"><a href="#local-6989586621679817746"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-386"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817745"><span class="annot"><a href="#local-6989586621679817745"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-387"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817744"><span class="annot"><a href="#local-6989586621679817744"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-388"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817743"><span class="annot"><a href="#local-6989586621679817743"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-389"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817742"><span class="annot"><a href="#local-6989586621679817742"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-390"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817741"><span class="annot"><a href="#local-6989586621679817741"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-391"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817740"><span class="annot"><a href="#local-6989586621679817740"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-392"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-393"></span><span>  </span><span id="TransformerLMSpec"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-var">TransformerLMSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-394"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818041"><span class="annot"><a href="#local-6989586621679818041"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span id="local-6989586621679818040"><span class="annot"><a href="#local-6989586621679818040"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679818039"><span class="annot"><a href="#local-6989586621679818039"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818038"><span class="annot"><a href="#local-6989586621679818038"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span id="local-6989586621679818037"><span class="annot"><a href="#local-6989586621679818037"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679818036"><span class="annot"><a href="#local-6989586621679818036"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818035"><span class="annot"><a href="#local-6989586621679818035"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818034"><span class="annot"><a href="#local-6989586621679818034"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-395"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | dropout spec</span><span>
</span><span id="line-396"></span><span>      </span><span id="lmDropoutSpec"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; DropoutSpec
</span><a href="Torch.Typed.NN.Transformer.html#lmDropoutSpec"><span class="hs-identifier hs-var hs-var">lmDropoutSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-397"></span><span>      </span><span class="hs-comment">-- | spec for each and every transformer layer</span><span>
</span><span id="line-398"></span><span>      </span><span id="lmLayerSpec"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLayerSpec
     embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="Torch.Typed.NN.Transformer.html#lmLayerSpec"><span class="hs-identifier hs-var hs-var">lmLayerSpec</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818036"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818036"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818036"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818040"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818039"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818035"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818034"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-399"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-400"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818041"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818040"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818039"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818038"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818037"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818036"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818035"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818034"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-401"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817731"><span id="local-6989586621679817733"><span id="local-6989586621679817735"><span class="annot"><span class="annottext">Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
(Int
 -&gt; TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
 -&gt; ShowS)
-&gt; (TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
    -&gt; String)
-&gt; ([TransformerLMSpec
       numAttnLayers
       numHeads
       ffnDim
       paddingIdx
       numEmbeds
       embedDim
       dtype
       device]
    -&gt; ShowS)
-&gt; Show
     (TransformerLMSpec
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
showList :: [TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
$cshowList :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
[TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device]
-&gt; ShowS
show :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
$cshow :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; String
showsPrec :: Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
$cshowsPrec :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Int
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817727"><span id="local-6989586621679817729"><span class="annot"><span class="annottext">TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
(TransformerLMSpec
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device
 -&gt; TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
 -&gt; Bool)
-&gt; (TransformerLMSpec
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device
    -&gt; TransformerLMSpec
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device
    -&gt; Bool)
-&gt; Eq
     (TransformerLMSpec
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
/= :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
$c/= :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
== :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
$c== :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; TransformerLMSpec
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-402"></span><span>
</span><span id="line-403"></span><span id="local-6989586621679817725"><span id="local-6989586621679817726"></span></span><span class="hs-keyword">data</span><span>
</span><span id="line-404"></span><span>  </span><span id="TransformerLM"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span></span><span>
</span><span id="line-405"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817724"><span class="annot"><a href="#local-6989586621679817724"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-406"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817723"><span class="annot"><a href="#local-6989586621679817723"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-407"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817722"><span class="annot"><a href="#local-6989586621679817722"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-408"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817721"><span class="annot"><a href="#local-6989586621679817721"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-409"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817720"><span class="annot"><a href="#local-6989586621679817720"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-410"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817719"><span class="annot"><a href="#local-6989586621679817719"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-411"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817718"><span class="annot"><a href="#local-6989586621679817718"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-412"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679817717"><span class="annot"><a href="#local-6989586621679817717"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-413"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-414"></span><span>  </span><span id="TransformerLM"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-415"></span><span>    </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818150"><span class="annot"><a href="#local-6989586621679818150"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span> </span><span id="local-6989586621679818149"><span class="annot"><a href="#local-6989586621679818149"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span> </span><span id="local-6989586621679818148"><span class="annot"><a href="#local-6989586621679818148"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span> </span><span id="local-6989586621679818147"><span class="annot"><a href="#local-6989586621679818147"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span> </span><span id="local-6989586621679818146"><span class="annot"><a href="#local-6989586621679818146"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679818145"><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818144"><span class="annot"><a href="#local-6989586621679818144"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679818143"><span class="annot"><a href="#local-6989586621679818143"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-416"></span><span>    </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | token embedding</span><span>
</span><span id="line-417"></span><span>      </span><span id="tEmbedding"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding
     ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tEmbedding"><span class="hs-identifier hs-var hs-var">tEmbedding</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Embedding"><span class="hs-identifier hs-type">Embedding</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679818147"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679818146"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Learned"><span class="hs-identifier hs-type">Learned</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818144"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818143"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-418"></span><span>      </span><span class="hs-comment">-- | positional embedding</span><span>
</span><span id="line-419"></span><span>      </span><span id="tPosEmbedding"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tPosEmbedding"><span class="hs-identifier hs-var hs-var">tPosEmbedding</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Embedding"><span class="hs-identifier hs-type">Embedding</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="annot"><span class="hs-number">2048</span></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Sparse.html#Constant"><span class="hs-identifier hs-type">Constant</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818144"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818143"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-420"></span><span>      </span><span class="hs-comment">-- | transformer dropout</span><span>
</span><span id="line-421"></span><span>      </span><span id="tDropout"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Transformer.html#tDropout"><span class="hs-identifier hs-var hs-var">tDropout</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Dropout.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-422"></span><span>      </span><span class="hs-comment">-- | transformer layers</span><span>
</span><span id="line-423"></span><span>      </span><span id="tLayers"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
</span><a href="Torch.Typed.NN.Transformer.html#tLayers"><span class="hs-identifier hs-var hs-var">tLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818150"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818149"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818148"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818144"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818143"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-424"></span><span>      </span><span class="hs-comment">-- | final output projection</span><span>
</span><span id="line-425"></span><span>      </span><span id="tProj"><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Linear embedDim numEmbeds dtype device
</span><a href="Torch.Typed.NN.Transformer.html#tProj"><span class="hs-identifier hs-var hs-var">tProj</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Linear.html#Linear"><span class="hs-identifier hs-type">Linear</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818146"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818144"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818143"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-426"></span><span>    </span><span class="hs-special">}</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-427"></span><span>    </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818150"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818149"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818148"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818147"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818146"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818145"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818144"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818143"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-428"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(forall x.
 TransformerLM
   numAttnLayers
   numHeads
   ffnDim
   paddingIdx
   numEmbeds
   embedDim
   dtype
   device
 -&gt; Rep
      (TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
      x)
-&gt; (forall x.
    Rep
      (TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
      x
    -&gt; TransformerLM
         numAttnLayers
         numHeads
         ffnDim
         paddingIdx
         numEmbeds
         embedDim
         dtype
         device)
-&gt; Generic
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
$cto :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
Rep
  (TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device)
  x
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
$cfrom :: forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)) x.
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Rep
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-429"></span><span>
</span><span id="line-430"></span><span id="local-6989586621679817695"><span id="local-6989586621679817697"><span id="local-6989586621679817699"><span id="local-6989586621679817701"><span id="local-6989586621679817702"><span id="local-6989586621679817703"><span id="local-6989586621679817704"><span id="local-6989586621679817705"><span id="local-6989586621679817706"><span id="local-6989586621679817707"><span id="local-6989586621679817708"><span class="hs-keyword">deriving</span><span> </span><span class="hs-keyword">instance</span><span>
</span><span id="line-431"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span>
</span><span id="line-432"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span>
</span><span id="line-433"></span><span>          </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span>
</span><span id="line-434"></span><span>              </span><span class="annot"><a href="#local-6989586621679817708"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span>
</span><span id="line-435"></span><span>              </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span>
</span><span id="line-436"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817707"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-437"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817707"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-438"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817707"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-439"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817706"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-440"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817705"><span class="hs-identifier hs-type">ffnDim</span></a></span><span>
</span><span id="line-441"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817704"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-442"></span><span>                  </span><span class="annot"><a href="#local-6989586621679817703"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-443"></span><span>              </span><span class="hs-special">)</span><span>
</span><span id="line-444"></span><span>          </span><span class="hs-special">)</span><span>
</span><span id="line-445"></span><span>      </span><span class="hs-special">)</span><span>
</span><span id="line-446"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-447"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817708"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817706"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817705"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817702"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817701"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817707"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817704"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817703"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-448"></span><span>
</span><span id="line-449"></span><span id="local-6989586621679817686"><span id="local-6989586621679817687"><span id="local-6989586621679817688"><span id="local-6989586621679817689"><span id="local-6989586621679817690"><span id="local-6989586621679817691"><span id="local-6989586621679817692"><span id="local-6989586621679817693"><span id="local-6989586621679817694"><span class="hs-keyword">instance</span><span>
</span><span id="line-450"></span><span>  </span><span id="local-6989586621679817682"><span id="local-6989586621679817684"><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679817694"><span class="hs-identifier hs-type">layers</span></a></span><span>
</span><span id="line-451"></span><span>      </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span>
</span><span id="line-452"></span><span>            </span><span class="annot"><a href="#local-6989586621679817693"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span>
</span><span id="line-453"></span><span>            </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span>
</span><span id="line-454"></span><span>                </span><span class="annot"><a href="#local-6989586621679817692"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-455"></span><span>                </span><span class="annot"><a href="#local-6989586621679817692"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-456"></span><span>                </span><span class="annot"><a href="#local-6989586621679817692"><span class="hs-identifier hs-type">embedDim</span></a></span><span>
</span><span id="line-457"></span><span>                </span><span class="annot"><a href="#local-6989586621679817691"><span class="hs-identifier hs-type">numHeads</span></a></span><span>
</span><span id="line-458"></span><span>                </span><span class="annot"><a href="#local-6989586621679817690"><span class="hs-identifier hs-type">ffnDim</span></a></span><span>
</span><span id="line-459"></span><span>                </span><span class="annot"><a href="#local-6989586621679817689"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-460"></span><span>                </span><span class="annot"><a href="#local-6989586621679817688"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-461"></span><span>            </span><span class="hs-special">)</span><span>
</span><span id="line-462"></span><span>        </span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-463"></span><span>    </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameterized"><span class="hs-identifier hs-type">Parameterized</span></a></span><span>
</span><span id="line-464"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span>
</span><span id="line-465"></span><span>          </span><span class="annot"><a href="#local-6989586621679817694"><span class="hs-identifier hs-type">layers</span></a></span><span>
</span><span id="line-466"></span><span>      </span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-467"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HAppendFD"><span class="hs-identifier hs-type">HAppendFD</span></a></span><span>
</span><span id="line-468"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameters"><span class="hs-identifier hs-type">Parameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817694"><span class="hs-identifier hs-type">layers</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-469"></span><span>      </span><span class="hs-special">'</span><span class="hs-special">[</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817688"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817689"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817687"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817692"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-470"></span><span>         </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817688"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817689"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817687"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-471"></span><span>       </span><span class="hs-special">]</span><span>
</span><span id="line-472"></span><span>      </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameters"><span class="hs-identifier hs-type">Parameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817694"><span class="hs-identifier hs-type">layers</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-473"></span><span>          </span><span class="annot"><a href="Torch.HList.html#%2B%2B"><span class="hs-operator hs-type">++</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817688"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817689"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817687"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817692"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-474"></span><span>                </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817688"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817689"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817687"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-475"></span><span>              </span><span class="hs-special">]</span><span>
</span><span id="line-476"></span><span>      </span><span class="hs-special">)</span><span>
</span><span id="line-477"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-478"></span><span>  </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameterized"><span class="hs-identifier hs-type">Parameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817693"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817691"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817690"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817686"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817687"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817692"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817689"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817688"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-479"></span><span>
</span><span id="line-480"></span><span class="hs-keyword">data</span><span>
</span><span id="line-481"></span><span>  </span><span id="FoldLayers"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span></span><span>
</span><span id="line-482"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818177"><span class="annot"><a href="#local-6989586621679818177"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-483"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818176"><span class="annot"><a href="#local-6989586621679818176"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-484"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818175"><span class="annot"><a href="#local-6989586621679818175"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-485"></span><span>    </span><span class="hs-special">(</span><span id="local-6989586621679818174"><span class="annot"><a href="#local-6989586621679818174"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span id="FoldLayers"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span></span><span>
</span><span id="line-486"></span><span>  </span><span class="hs-special">{</span><span> </span><span class="hs-comment">-- | switch between training mode and evaluation mode (turns random dropout on and off)</span><span>
</span><span id="line-487"></span><span>    </span><span id="flTrain"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device -&gt; Bool
</span><a href="Torch.Typed.NN.Transformer.html#flTrain"><span class="hs-identifier hs-var hs-var">flTrain</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span class="hs-special">,</span><span>
</span><span id="line-488"></span><span>    </span><span class="hs-comment">-- | optional attention mask</span><span>
</span><span id="line-489"></span><span>    </span><span id="flAttentionMask"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#flAttentionMask"><span class="hs-identifier hs-var hs-var">flAttentionMask</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818174"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818175"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818177"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818176"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818176"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-490"></span><span>    </span><span class="hs-comment">-- | optional key padding mask</span><span>
</span><span id="line-491"></span><span>    </span><span id="flKeyPaddingMask"><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="Torch.Typed.NN.Transformer.html#flKeyPaddingMask"><span class="hs-identifier hs-var hs-var">flKeyPaddingMask</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818174"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818177"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818176"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-492"></span><span>  </span><span class="hs-special">}</span><span>
</span><span id="line-493"></span><span>
</span><span id="line-494"></span><span id="local-6989586621679817670"><span id="local-6989586621679817671"><span id="local-6989586621679817672"><span id="local-6989586621679817673"><span id="local-6989586621679817674"><span id="local-6989586621679817675"><span id="local-6989586621679817676"><span id="local-6989586621679817677"><span class="hs-keyword">instance</span><span>
</span><span id="line-495"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679817677"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-496"></span><span>    </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679817675"><span class="hs-identifier hs-type">headDim</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679817677"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-497"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817677"><span class="hs-identifier hs-type">numHeads</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817674"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817673"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817675"><span class="hs-identifier hs-type">headDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-498"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#IsSuffixOf"><span class="hs-identifier hs-type">IsSuffixOf</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817673"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817674"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-499"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-500"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-501"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#MatMulDTypeIsValid"><span class="hs-identifier hs-type">MatMulDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-502"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-503"></span><span>    </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDType"><span class="hs-identifier hs-type">SumDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-504"></span><span>    </span><span class="annot"><a href="Torch.Typed.Functional.html#SumDTypeIsValid"><span class="hs-identifier hs-type">SumDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-505"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-506"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-507"></span><span>  </span><span class="annot"><a href="Torch.HList.html#Apply%27"><span class="hs-identifier hs-type">Apply'</span></a></span><span>
</span><span id="line-508"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817673"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817674"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-509"></span><span>    </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817677"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817670"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-510"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817673"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817674"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-511"></span><span>    </span><span class="hs-special">)</span><span>
</span><span id="line-512"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817671"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817672"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817673"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817674"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817676"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-513"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-514"></span><span>  </span><span id="local-6989586621679817667"><span class="annot"><span class="annottext">apply' :: FoldLayers batchSize seqLen dtype device
-&gt; (TransformerLayer
      embedDim embedDim embedDim numHeads ffnDim dtype device,
    IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="Torch.HList.html#apply%27"><span class="hs-identifier hs-var hs-var hs-var hs-var">apply'</span></a></span></span><span> </span><span id="local-6989586621679817663"><span id="local-6989586621679817664"><span id="local-6989586621679817665"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679817662"><span class="annot"><span class="annottext">layer :: TransformerLayer
  embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679817662"><span class="hs-identifier hs-var">layer</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679817661"><span class="annot"><span class="annottext">mx :: IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="#local-6989586621679817661"><span class="hs-identifier hs-var">mx</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
</span><a href="#local-6989586621679817661"><span class="hs-identifier hs-var">mx</span></a></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Tensor device dtype '[batchSize, seqLen, embedDim]
    -&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim]))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
</span><span class="hs-operator hs-var">&gt;&gt;=</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621679817660"><span class="annot"><span class="annottext">x :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817660"><span class="hs-identifier hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="annottext">TransformerLayer
  embedDim embedDim embedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (numHeads :: Nat) (ffnDim :: Nat) (embedDim :: Nat)
       (kEmbedDim :: Nat) (vEmbedDim :: Nat) (headDim :: Nat)
       (seqLen :: Nat) (seqLen' :: Nat) (batchSize :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
(1 &lt;= numHeads, embedDim ~ (headDim * numHeads),
 All
   KnownNat
   '[embedDim, kEmbedDim, vEmbedDim, numHeads, seqLen, seqLen',
     batchSize, headDim],
 IsSuffixOf '[embedDim] '[batchSize, seqLen', embedDim],
 KnownDType dtype, dtype ~ SumDType dtype,
 StandardFloatingPointDTypeValidation device dtype,
 MatMulDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype,
 SumDTypeIsValid device dtype, KnownDevice device) =&gt;
TransformerLayer
  embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
-&gt; Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen', seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Maybe
     (Tensor device dtype '[batchSize, seqLen', seqLen, headDim])
-&gt; Tensor device dtype '[batchSize, seqLen', embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, kEmbedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, vEmbedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen', embedDim])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLayer"><span class="hs-identifier hs-var">transformerLayer</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLayer
  embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679817662"><span class="hs-identifier hs-var">layer</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817665"><span class="hs-identifier hs-var">flTrain</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="#local-6989586621679817664"><span class="hs-identifier hs-var">flAttentionMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817663"><span class="hs-identifier hs-var">flKeyPaddingMask</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
forall a. Maybe a
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen, headDim])
forall a. Maybe a
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817660"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817660"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817660"><span class="hs-identifier hs-var">x</span></a></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-515"></span><span>
</span><span id="line-516"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-type">transformerLM</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-517"></span><span>  </span><span class="hs-keyword">forall</span><span>
</span><span id="line-518"></span><span>    </span><span id="local-6989586621679818068"><span class="annot"><a href="#local-6989586621679818068"><span class="hs-identifier hs-type">numAttnLayers</span></a></span></span><span>
</span><span id="line-519"></span><span>    </span><span id="local-6989586621679818067"><span class="annot"><a href="#local-6989586621679818067"><span class="hs-identifier hs-type">numHeads</span></a></span></span><span>
</span><span id="line-520"></span><span>    </span><span id="local-6989586621679818066"><span class="annot"><a href="#local-6989586621679818066"><span class="hs-identifier hs-type">ffnDim</span></a></span></span><span>
</span><span id="line-521"></span><span>    </span><span id="local-6989586621679818075"><span class="annot"><a href="#local-6989586621679818075"><span class="hs-identifier hs-type">paddingIdx</span></a></span></span><span>
</span><span id="line-522"></span><span>    </span><span id="local-6989586621679818071"><span class="annot"><a href="#local-6989586621679818071"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span>
</span><span id="line-523"></span><span>    </span><span id="local-6989586621679818074"><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span>
</span><span id="line-524"></span><span>    </span><span id="local-6989586621679818073"><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span>
</span><span id="line-525"></span><span>    </span><span id="local-6989586621679818072"><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span>
</span><span id="line-526"></span><span>    </span><span id="local-6989586621679818070"><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span></span><span>
</span><span id="line-527"></span><span>    </span><span id="local-6989586621679818069"><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-528"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818075"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-529"></span><span>    </span><span class="annot"><a href="#local-6989586621679818075"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679818071"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-530"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-531"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HFoldrM"><span class="hs-identifier hs-type">HFoldrM</span></a></span><span>
</span><span id="line-532"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-533"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-534"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-535"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818068"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818067"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818066"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-536"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-537"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-538"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-539"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-540"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-541"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-542"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-543"></span><span>  </span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818068"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818067"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818066"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818075"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818071"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-544"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-545"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-546"></span><span>  </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818071"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-547"></span><span id="transformerLM"><span class="annot"><span class="annottext">transformerLM :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var hs-var">transformerLM</span></a></span></span><span> </span><span id="local-6989586621679817654"><span id="local-6989586621679817655"><span id="local-6989586621679817656"><span id="local-6989586621679817657"><span id="local-6989586621679817658"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span></span></span></span><span> </span><span id="local-6989586621679817653"><span class="annot"><span class="annottext">train :: Bool
</span><a href="#local-6989586621679817653"><span class="hs-identifier hs-var">train</span></a></span></span><span> </span><span id="local-6989586621679817652"><span class="annot"><span class="annottext">xTokens :: Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817652"><span class="hs-identifier hs-var">xTokens</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-548"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817651"><span class="annot"><span class="annottext">x :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817651"><span class="hs-identifier hs-var hs-var">x</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (paddingIdx :: Maybe Nat) (shape :: [Nat])
       (numEmbeds :: Nat) (embedSize :: Nat)
       (embeddingType :: EmbeddingType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape' :: [Nat]).
(KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds,
 shape' ~ Reverse (embedSize : Reverse shape)) =&gt;
Embedding paddingIdx numEmbeds embedSize embeddingType dtype device
-&gt; Tensor device 'Int64 shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.NN.Sparse.html#embed"><span class="hs-identifier hs-var">embed</span></a></span><span> </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
</span><a href="#local-6989586621679817658"><span class="hs-identifier hs-var">tEmbedding</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817652"><span class="hs-identifier hs-var">xTokens</span></a></span><span>
</span><span id="line-549"></span><span>      </span><span id="local-6989586621679817649"><span class="annot"><span class="annottext">positions :: Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817649"><span class="hs-identifier hs-var hs-var">positions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-550"></span><span>        </span><span class="annot"><span class="annottext">Bool
-&gt; Tensor device dtype '[seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', shape' ~ Broadcast shape shape') =&gt;
Bool -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#expand"><span class="hs-identifier hs-var">expand</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818074"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span>
</span><span id="line-551"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device dtype '[seqLen, embedDim]
 -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; (Int -&gt; Tensor device dtype '[seqLen, embedDim])
-&gt; Int
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Tensor device 'Int64 '[seqLen]
-&gt; Tensor device dtype '[seqLen, embedDim]
forall (paddingIdx :: Maybe Nat) (shape :: [Nat])
       (numEmbeds :: Nat) (embedSize :: Nat)
       (embeddingType :: EmbeddingType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape' :: [Nat]).
(KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds,
 shape' ~ Reverse (embedSize : Reverse shape)) =&gt;
Embedding paddingIdx numEmbeds embedSize embeddingType dtype device
-&gt; Tensor device 'Int64 shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.NN.Sparse.html#embed"><span class="hs-identifier hs-var">embed</span></a></span><span> </span><span class="annot"><span class="annottext">Embedding 'Nothing 2048 embedDim 'Constant dtype device
</span><a href="#local-6989586621679817657"><span class="hs-identifier hs-var">tPosEmbedding</span></a></span><span>
</span><span id="line-552"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Int64 '[seqLen]
 -&gt; Tensor device dtype '[seqLen, embedDim])
-&gt; (Int -&gt; Tensor device 'Int64 '[seqLen])
-&gt; Int
-&gt; Tensor device dtype '[seqLen, embedDim]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (dtype :: DType) (device :: (DeviceType, Nat))
       (shape :: [Nat]).
KnownDType 'Int64 =&gt;
Tensor device dtype shape -&gt; Tensor device 'Int64 shape
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span>
</span><span id="line-553"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[seqLen] -&gt; Tensor device 'Int64 '[seqLen])
-&gt; (Int -&gt; Tensor device 'Float '[seqLen])
-&gt; Int
-&gt; Tensor device 'Int64 '[seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[seqLen]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-554"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device dtype '[batchSize, seqLen, embedDim])
-&gt; Int -&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (seqLen - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-555"></span><span>  </span><span id="local-6989586621679817645"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817645"><span class="hs-identifier hs-var">x'</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">Dropout
-&gt; Bool
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Dropout
-&gt; Bool
-&gt; Tensor device dtype shape
-&gt; IO (Tensor device dtype shape)
</span><a href="Torch.Typed.NN.Dropout.html#dropoutForward"><span class="hs-identifier hs-var">dropoutForward</span></a></span><span> </span><span class="annot"><span class="annottext">Dropout
</span><a href="#local-6989586621679817656"><span class="hs-identifier hs-var">tDropout</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817653"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817651"><span class="hs-identifier hs-var">x</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#add"><span class="hs-operator hs-var">`add`</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817649"><span class="hs-identifier hs-var">positions</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-556"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817644"><span class="annot"><span class="annottext">attentionMask :: Tensor device 'Bool '[1, seqLen, seqLen]
</span><a href="#local-6989586621679817644"><span class="hs-identifier hs-var hs-var">attentionMask</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-557"></span><span>        </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 0, shape' ~ Unsqueeze shape 0) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-558"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[1, seqLen, seqLen])
-&gt; (Tensor device 'Int8 '[seqLen, seqLen]
    -&gt; Tensor device 'Bool '[seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[1, seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">forall (dtype :: DType) (device :: (DeviceType, Nat))
       (shape :: [Nat]).
KnownDType 'Bool =&gt;
Tensor device dtype shape -&gt; Tensor device 'Bool shape
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Bool"><span class="hs-identifier hs-type">D.Bool</span></a></span><span>
</span><span id="line-559"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Int8 '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[seqLen, seqLen])
-&gt; (Tensor device 'Int8 '[seqLen, seqLen]
    -&gt; Tensor device 'Int8 '[seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[seqLen, seqLen]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(shape ~ MatrixOrMatrixBatch shape) =&gt;
Int -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#triu"><span class="hs-identifier hs-var">triu</span></a></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-560"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Int8 '[seqLen, seqLen]
 -&gt; Tensor device 'Bool '[1, seqLen, seqLen])
-&gt; Tensor device 'Int8 '[seqLen, seqLen]
-&gt; Tensor device 'Bool '[1, seqLen, seqLen]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TensorOptions '[seqLen, seqLen] 'Int8 device =&gt;
Tensor device 'Int8 '[seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#ones"><span class="hs-identifier hs-var">ones</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="Torch.DType.html#Int8"><span class="hs-identifier hs-type">D.Int8</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-561"></span><span>      </span><span id="local-6989586621679817641"><span class="annot"><span class="annottext">attentionMask' :: Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="#local-6989586621679817641"><span class="hs-identifier hs-var hs-var">attentionMask'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-562"></span><span>        </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen]
 -&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen]))
-&gt; (Tensor device dtype '[batchSize, seqLen, seqLen]
    -&gt; Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[1, seqLen, seqLen]
-&gt; Double
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
forall a (shape :: [Nat]) (shape' :: [Nat]) (shape'' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(Scalar a, shape'' ~ Broadcast shape shape') =&gt;
Tensor device 'Bool shape'
-&gt; a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape''
</span><a href="Torch.Typed.Functional.html#maskedFill"><span class="hs-identifier hs-var">maskedFill</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[1, seqLen, seqLen]
</span><a href="#local-6989586621679817644"><span class="hs-identifier hs-var">attentionMask</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, seqLen]
 -&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen]))
-&gt; Tensor device dtype '[batchSize, seqLen, seqLen]
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span>
</span><span id="line-563"></span><span>          </span><span class="annot"><span class="annottext">TensorOptions '[batchSize, seqLen, seqLen] dtype device =&gt;
Tensor device dtype '[batchSize, seqLen, seqLen]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818072"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818073"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818070"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-564"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817639"><span class="annot"><span class="annottext">keyPaddingMask :: Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817639"><span class="hs-identifier hs-var hs-var">keyPaddingMask</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device 'Bool '[batchSize, seqLen]
 -&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen]))
-&gt; Tensor device 'Bool '[batchSize, seqLen]
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817652"><span class="hs-identifier hs-var">xTokens</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device 'Int64 '[]
-&gt; Tensor device 'Bool '[batchSize, seqLen]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (device :: (DeviceType, Nat)).
(shape'' ~ Broadcast shape shape',
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device 'Bool shape''
</span><a href="Torch.Typed.Tensor.html#%3D%3D."><span class="hs-operator hs-var">==.</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; Tensor device 'Int64 '[]
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Tensor device 'Int64 '[])
-&gt; (Proxy paddingIdx -&gt; Integer)
-&gt; Proxy paddingIdx
-&gt; Tensor device 'Int64 '[]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Proxy paddingIdx -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; Type).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy paddingIdx -&gt; Tensor device 'Int64 '[])
-&gt; Proxy paddingIdx -&gt; Tensor device 'Int64 '[]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy paddingIdx
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818075"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818069"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-565"></span><span>  </span><span id="local-6989586621679817635"><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817635"><span class="hs-identifier hs-var">y</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">FoldLayers batchSize seqLen dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; IO (Tensor device dtype '[batchSize, seqLen, embedDim])
forall k k (m :: k -&gt; Type) f acc (xs :: [k]) (res :: k).
HFoldrM m f acc xs res =&gt;
f -&gt; acc -&gt; HList xs -&gt; m res
</span><a href="Torch.HList.html#hfoldrM"><span class="hs-identifier hs-var">hfoldrM</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; FoldLayers batchSize seqLen dtype device
forall (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Bool
-&gt; Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
-&gt; Maybe (Tensor device 'Bool '[batchSize, seqLen])
-&gt; FoldLayers batchSize seqLen dtype device
</span><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-var">FoldLayers</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679817653"><span class="hs-identifier hs-var">train</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])
</span><a href="#local-6989586621679817641"><span class="hs-identifier hs-var">attentionMask'</span></a></span><span> </span><span class="annot"><span class="annottext">Maybe (Tensor device 'Bool '[batchSize, seqLen])
</span><a href="#local-6989586621679817639"><span class="hs-identifier hs-var">keyPaddingMask</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817645"><span class="hs-identifier hs-var">x'</span></a></span><span> </span><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayer
        embedDim embedDim embedDim numHeads ffnDim dtype device))
</span><a href="#local-6989586621679817655"><span class="hs-identifier hs-var">tLayers</span></a></span><span>
</span><span id="line-566"></span><span>  </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, numEmbeds]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (m :: Type -&gt; Type) a. Monad m =&gt; a -&gt; m a
</span><span class="hs-identifier hs-var">return</span></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype '[batchSize, seqLen, numEmbeds]
 -&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds]))
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Linear embedDim numEmbeds dtype device
-&gt; Tensor device dtype '[batchSize, seqLen, embedDim]
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall f a b. HasForward f a b =&gt; f -&gt; a -&gt; b
</span><a href="Torch.NN.html#forward"><span class="hs-identifier hs-var">forward</span></a></span><span> </span><span class="annot"><span class="annottext">Linear embedDim numEmbeds dtype device
</span><a href="#local-6989586621679817654"><span class="hs-identifier hs-var">tProj</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[batchSize, seqLen, embedDim]
</span><a href="#local-6989586621679817635"><span class="hs-identifier hs-var">y</span></a></span><span>
</span><span id="line-567"></span><span>
</span><span id="line-568"></span><span id="local-6989586621679817624"><span id="local-6989586621679817625"><span id="local-6989586621679817626"><span id="local-6989586621679817627"><span id="local-6989586621679817628"><span id="local-6989586621679817629"><span id="local-6989586621679817630"><span id="local-6989586621679817631"><span id="local-6989586621679817632"><span id="local-6989586621679817633"><span class="hs-keyword">instance</span><span>
</span><span id="line-569"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817633"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817630"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-570"></span><span>    </span><span class="annot"><a href="#local-6989586621679817633"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679817629"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-571"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-572"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HFoldrM"><span class="hs-identifier hs-type">HFoldrM</span></a></span><span>
</span><span id="line-573"></span><span>      </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span>
</span><span id="line-574"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#FoldLayers"><span class="hs-identifier hs-type">FoldLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817630"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-575"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817630"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-576"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817626"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817625"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817624"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-577"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817630"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-578"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-579"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-580"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#ComparisonDTypeIsValid"><span class="hs-identifier hs-type">ComparisonDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-581"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-582"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-583"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-584"></span><span>  </span><span class="annot"><a href="Torch.NN.html#HasForward"><span class="hs-identifier hs-type">HasForward</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817626"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817625"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817624"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817633"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817629"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817632"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Int64"><span class="hs-identifier hs-type">D.Int64</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817630"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817627"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817628"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817630"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817631"><span class="hs-identifier hs-type">seqLen</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817629"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-585"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-586"></span><span>  </span><span id="local-6989586621679817620"><span class="annot"><span class="annottext">forward :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
</span><a href="#local-6989586621679817620"><span class="hs-identifier hs-var hs-var hs-var hs-var">forward</span></a></span></span><span> </span><span id="local-6989586621679817619"><span class="annot"><span class="annottext">model :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679817619"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679817618"><span class="annot"><span class="annottext">input :: Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817618"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall a. IO a -&gt; a
</span><span class="hs-identifier hs-var">unsafePerformIO</span></span><span> </span><span class="annot"><span class="annottext">(IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
 -&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
-&gt; Tensor device dtype '[batchSize, seqLen, numEmbeds]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(All KnownNat '[paddingIdx, embedDim, seqLen, batchSize],
 (paddingIdx + 1) &lt;= numEmbeds, 1 &lt;= seqLen,
 HFoldrM
   IO
   (FoldLayers batchSize seqLen dtype device)
   (Tensor device dtype '[batchSize, seqLen, embedDim])
   (HReplicateR
      numAttnLayers
      (TransformerLayer
         embedDim embedDim embedDim numHeads ffnDim dtype device))
   (Tensor device dtype '[batchSize, seqLen, embedDim]),
 BasicArithmeticDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device 'Int64, KnownDType dtype,
 KnownDevice device) =&gt;
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var">transformerLM</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679817619"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817618"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-587"></span><span>  </span><span id="local-6989586621679817617"><span class="annot"><span class="annottext">forwardStoch :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.NN.html#forwardStoch"><span class="hs-identifier hs-var hs-var hs-var hs-var">forwardStoch</span></a></span></span><span> </span><span id="local-6989586621679817615"><span class="annot"><span class="annottext">model :: TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679817615"><span class="hs-identifier hs-var">model</span></a></span></span><span> </span><span id="local-6989586621679817614"><span class="annot"><span class="annottext">input :: Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817614"><span class="hs-identifier hs-var">input</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(All KnownNat '[paddingIdx, embedDim, seqLen, batchSize],
 (paddingIdx + 1) &lt;= numEmbeds, 1 &lt;= seqLen,
 HFoldrM
   IO
   (FoldLayers batchSize seqLen dtype device)
   (Tensor device dtype '[batchSize, seqLen, embedDim])
   (HReplicateR
      numAttnLayers
      (TransformerLayer
         embedDim embedDim embedDim numHeads ffnDim dtype device))
   (Tensor device dtype '[batchSize, seqLen, embedDim]),
 BasicArithmeticDTypeIsValid device dtype,
 ComparisonDTypeIsValid device dtype,
 ComparisonDTypeIsValid device 'Int64, KnownDType dtype,
 KnownDevice device) =&gt;
TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; Bool
-&gt; Tensor device 'Int64 '[batchSize, seqLen]
-&gt; IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
</span><a href="Torch.Typed.NN.Transformer.html#transformerLM"><span class="hs-identifier hs-var">transformerLM</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLM
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
</span><a href="#local-6989586621679817615"><span class="hs-identifier hs-var">model</span></a></span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Int64 '[batchSize, seqLen]
</span><a href="#local-6989586621679817614"><span class="hs-identifier hs-var">input</span></a></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-588"></span><span>
</span><span id="line-589"></span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-type">sinusoidal</span></a></span><span> </span><span class="hs-glyph">::</span><span>
</span><span id="line-590"></span><span>  </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679818022"><span class="annot"><a href="#local-6989586621679818022"><span class="hs-identifier hs-type">numEmbeds</span></a></span></span><span> </span><span id="local-6989586621679818021"><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span></span><span> </span><span id="local-6989586621679818020"><span class="annot"><a href="#local-6989586621679818020"><span class="hs-identifier hs-type">device</span></a></span></span><span class="hs-operator">.</span><span>
</span><span id="line-591"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818022"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-592"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679818022"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-593"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">,</span><span>
</span><span id="line-594"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-595"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818020"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-596"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818020"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-597"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818020"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-598"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-599"></span><span>  </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679818020"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679818022"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-600"></span><span id="sinusoidal"><span class="annot"><span class="annottext">sinusoidal :: Tensor device 'Float '[numEmbeds, embedDim]
</span><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-var hs-var">sinusoidal</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-601"></span><span>  </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679817612"><span class="annot"><span class="annottext">positions :: Tensor device 'Float '[numEmbeds, 1]
</span><a href="#local-6989586621679817612"><span class="hs-identifier hs-var hs-var">positions</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-602"></span><span>        </span><span class="annot"><span class="annottext">forall (shape :: [Nat]) (shape' :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownNat 1, shape' ~ Unsqueeze shape 1) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (dim :: Nat) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat dim, shape' ~ Unsqueeze shape dim) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#unsqueeze"><span class="hs-identifier hs-var">unsqueeze</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-603"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[numEmbeds]
 -&gt; Tensor device 'Float '[numEmbeds, 1])
-&gt; (Int -&gt; Tensor device 'Float '[numEmbeds])
-&gt; Int
-&gt; Tensor device 'Float '[numEmbeds, 1]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[numEmbeds]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679818022"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-604"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device 'Float '[numEmbeds, 1])
-&gt; Int -&gt; Tensor device 'Float '[numEmbeds, 1]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (numEmbeds - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679818022"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-605"></span><span>      </span><span id="local-6989586621679817611"><span class="annot"><span class="annottext">scalingFactors :: Tensor device 'Float '[Div embedDim 2]
</span><a href="#local-6989586621679817611"><span class="hs-identifier hs-var hs-var">scalingFactors</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-606"></span><span>        </span><span class="annot"><span class="annottext">Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#exp"><span class="hs-identifier hs-var">exp</span></a></span><span>
</span><span id="line-607"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[Div embedDim 2]
 -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; (Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int
-&gt; Tensor device 'Float '[Div embedDim 2]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Double
-&gt; Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[Div embedDim 2]
forall a (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Scalar a =&gt;
a -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#mulScalar"><span class="hs-identifier hs-var">mulScalar</span></a></span><span> </span><span class="hs-special">(</span><span class="hs-glyph">-</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">log</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">10000</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Double</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Double -&gt; Double -&gt; Double
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Integer -&gt; Double
forall a. Num a =&gt; Integer -&gt; a
</span><span class="hs-identifier hs-var">fromInteger</span></span><span> </span><span class="annot"><span class="annottext">(Integer -&gt; Double)
-&gt; (Proxy (Div embedDim 2) -&gt; Integer)
-&gt; Proxy (Div embedDim 2)
-&gt; Double
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Proxy (Div embedDim 2) -&gt; Integer
forall (n :: Nat) (proxy :: Nat -&gt; Type).
KnownNat n =&gt;
proxy n -&gt; Integer
</span><span class="hs-identifier hs-var">natVal</span></span><span> </span><span class="annot"><span class="annottext">(Proxy (Div embedDim 2) -&gt; Double)
-&gt; Proxy (Div embedDim 2) -&gt; Double
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Proxy (Div embedDim 2)
forall k (t :: k). Proxy t
</span><span class="hs-identifier hs-var">Proxy</span></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-608"></span><span>          </span><span class="annot"><span class="annottext">(Tensor device 'Float '[Div embedDim 2]
 -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; (Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int
-&gt; Tensor device 'Float '[Div embedDim 2]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Tensor device 'Float '[Div embedDim 2]
forall (steps :: Nat) (device :: (DeviceType, Nat)) start end.
(Scalar start, Scalar end, KnownNat steps,
 TensorOptions '[steps] 'Float device) =&gt;
start -&gt; end -&gt; Tensor device 'Float '[steps]
</span><a href="Torch.Typed.Factories.html#linspace"><span class="hs-identifier hs-var">linspace</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">0</span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-609"></span><span>          </span><span class="annot"><span class="annottext">(Int -&gt; Tensor device 'Float '[Div embedDim 2])
-&gt; Int -&gt; Tensor device 'Float '[Div embedDim 2]
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">KnownNat (Div embedDim 2 - 1) =&gt; Int
forall (n :: Nat). KnownNat n =&gt; Int
</span><a href="Torch.Typed.Aux.html#natValI"><span class="hs-identifier hs-var">natValI</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679818021"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span>
</span><span id="line-610"></span><span>      </span><span id="local-6989586621679817607"><span class="annot"><span class="annottext">radians :: Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679817607"><span class="hs-identifier hs-var hs-var">radians</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, 1]
-&gt; Tensor device 'Float '[Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape'' :: [Nat]) (shape :: [Nat]) (shape' :: [Nat])
       (dtype :: DType) (dtype' :: DType) (dtype'' :: DType)
       (device :: (DeviceType, Nat)).
(dtype'' ~ DTypePromotion dtype dtype',
 shape'' ~ Broadcast shape shape',
 BasicArithmeticDTypeIsValid device dtype,
 BasicArithmeticDTypeIsValid device dtype',
 BasicArithmeticDTypeIsValid device dtype'') =&gt;
Tensor device dtype shape
-&gt; Tensor device dtype' shape' -&gt; Tensor device dtype'' shape''
</span><a href="Torch.Typed.Tensor.html#mul"><span class="hs-identifier hs-var">mul</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, 1]
</span><a href="#local-6989586621679817612"><span class="hs-identifier hs-var">positions</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[Div embedDim 2]
</span><a href="#local-6989586621679817611"><span class="hs-identifier hs-var">scalingFactors</span></a></span><span>
</span><span id="line-611"></span><span>      </span><span id="local-6989586621679817605"><span class="annot"><span class="annottext">weights :: Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
</span><a href="#local-6989586621679817605"><span class="hs-identifier hs-var hs-var">weights</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList
  '[Tensor device 'Float '[numEmbeds, Div embedDim 2],
    Tensor device 'Float '[numEmbeds, Div embedDim 2]]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
forall k (dim :: Nat) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)) (tensors :: [k]).
(KnownNat dim, '(shape, dtype, device) ~ Stack dim tensors,
 Castable (HList tensors) [ATenTensor]) =&gt;
HList tensors -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#stack"><span class="hs-identifier hs-var">stack</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#sin"><span class="hs-identifier hs-var">sin</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679817607"><span class="hs-identifier hs-var">radians</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; HList '[Tensor device 'Float '[numEmbeds, Div embedDim 2]]
-&gt; HList
     '[Tensor device 'Float '[numEmbeds, Div embedDim 2],
       Tensor device 'Float '[numEmbeds, Div embedDim 2]]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; Tensor device 'Float '[numEmbeds, Div embedDim 2]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
StandardFloatingPointDTypeValidation device dtype =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Functional.html#cos"><span class="hs-identifier hs-var">cos</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
</span><a href="#local-6989586621679817607"><span class="hs-identifier hs-var">radians</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2]
-&gt; HList '[]
-&gt; HList '[Tensor device 'Float '[numEmbeds, Div embedDim 2]]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
forall k. HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-612"></span><span>   </span><span class="hs-keyword">in</span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
-&gt; Tensor device 'Float '[numEmbeds, embedDim]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[numEmbeds, Div embedDim 2, 2]
</span><a href="#local-6989586621679817605"><span class="hs-identifier hs-var">weights</span></a></span><span>
</span><span id="line-613"></span><span>
</span><span id="line-614"></span><span id="local-6989586621679817592"><span id="local-6989586621679817593"><span id="local-6989586621679817594"><span id="local-6989586621679817595"><span id="local-6989586621679817596"><span id="local-6989586621679817597"><span id="local-6989586621679817598"><span id="local-6989586621679817599"><span class="hs-keyword">instance</span><span>
</span><span id="line-615"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-616"></span><span>    </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-617"></span><span>    </span><span class="hs-special">(</span><span class="hs-special">(</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">+</span></span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-618"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Div</span></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-619"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#All"><span class="hs-identifier hs-type">All</span></a></span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679817596"><span class="hs-identifier hs-type">ffnDim</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span class="hs-special">]</span><span class="hs-special">,</span><span>
</span><span id="line-620"></span><span>    </span><span class="annot"><a href="Torch.HList.html#HReplicate"><span class="hs-identifier hs-type">HReplicate</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817595"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817594"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817596"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-621"></span><span>    </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-622"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817595"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayerSpec"><span class="hs-identifier hs-type">TransformerLayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817594"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817596"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-623"></span><span>      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HReplicateR"><span class="hs-identifier hs-type">HReplicateR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817595"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLayer"><span class="hs-identifier hs-type">TransformerLayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817594"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817596"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">,</span><span>
</span><span id="line-624"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-625"></span><span>    </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-626"></span><span>    </span><span class="annot"><a href="Torch.Typed.Aux.html#StandardFloatingPointDTypeValidation"><span class="hs-identifier hs-type">StandardFloatingPointDTypeValidation</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-627"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#BasicArithmeticDTypeIsValid"><span class="hs-identifier hs-type">BasicArithmeticDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.DType.html#Float"><span class="hs-identifier hs-type">D.Float</span></a></span><span class="hs-special">,</span><span>
</span><span id="line-628"></span><span>    </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-629"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span>
</span><span id="line-630"></span><span>  </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span>
</span><span id="line-631"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817595"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817594"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817596"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-632"></span><span>    </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-type">TransformerLM</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817595"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817594"><span class="hs-identifier hs-type">numHeads</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817596"><span class="hs-identifier hs-type">ffnDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817598"><span class="hs-identifier hs-type">numEmbeds</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817597"><span class="hs-identifier hs-type">embedDim</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817593"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679817592"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-633"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-634"></span><span>  </span><span id="local-6989586621679817590"><span class="annot"><span class="annottext">sample :: TransformerLMSpec
  numAttnLayers
  numHeads
  ffnDim
  paddingIdx
  numEmbeds
  embedDim
  dtype
  device
-&gt; IO
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
</span><a href="#local-6989586621679817590"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679817588"><span id="local-6989586621679817589"><span class="annot"><a href="Torch.Typed.NN.Transformer.html#TransformerLMSpec"><span class="hs-identifier hs-type">TransformerLMSpec</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-635"></span><span>    </span><span class="annot"><span class="annottext">Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Dropout
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; Linear embedDim numEmbeds dtype device
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
forall (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat)
       (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
Embedding
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
-&gt; Dropout
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; Linear embedDim numEmbeds dtype device
-&gt; TransformerLM
     numAttnLayers
     numHeads
     ffnDim
     paddingIdx
     numEmbeds
     embedDim
     dtype
     device
</span><a href="Torch.Typed.NN.Transformer.html#TransformerLM"><span class="hs-identifier hs-var">TransformerLM</span></a></span><span>
</span><span id="line-636"></span><span>      </span><span class="annot"><span class="annottext">(Embedding
   ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
 -&gt; Embedding 'Nothing 2048 embedDim 'Constant dtype device
 -&gt; Dropout
 -&gt; HList
      (HReplicateR
         numAttnLayers
         (TransformerLayer
            embedDim embedDim embedDim numHeads ffnDim dtype device))
 -&gt; Linear embedDim numEmbeds dtype device
 -&gt; TransformerLM
      numAttnLayers
      numHeads
      ffnDim
      paddingIdx
      numEmbeds
      embedDim
      dtype
      device)
-&gt; IO
     (Embedding
        ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device)
-&gt; IO
     (Embedding 'Nothing 2048 embedDim 'Constant dtype device
      -&gt; Dropout
      -&gt; HList
           (HReplicateR
              numAttnLayers
              (TransformerLayer
                 embedDim embedDim embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">EmbeddingSpec
  ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device
-&gt; IO
     (Embedding
        ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">forall (paddingIdx :: Maybe Nat) (numEmbeds :: Nat)
       (embedSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
EmbeddingSpec paddingIdx numEmbeds embedSize 'Learned dtype device
forall (numEmbeds :: Nat) (embedSize :: Nat) (dtype :: DType)
       (device :: (DeviceType, Nat)).
EmbeddingSpec
  ('Just paddingIdx) numEmbeds embedSize 'Learned dtype device
</span><a href="Torch.Typed.NN.Sparse.html#LearnedEmbeddingWithRandomInitSpec"><span class="hs-identifier hs-var">LearnedEmbeddingWithRandomInitSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span class="annot"><a href="#local-6989586621679817599"><span class="hs-identifier hs-type">paddingIdx</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-637"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Embedding 'Nothing 2048 embedDim 'Constant dtype device
   -&gt; Dropout
   -&gt; HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO (Embedding 'Nothing 2048 embedDim 'Constant dtype device)
-&gt; IO
     (Dropout
      -&gt; HList
           (HReplicateR
              numAttnLayers
              (TransformerLayer
                 embedDim embedDim embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">EmbeddingSpec 'Nothing 2048 embedDim 'Constant dtype device
-&gt; IO (Embedding 'Nothing 2048 embedDim 'Constant dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[2048, embedDim]
-&gt; EmbeddingSpec 'Nothing 2048 embedDim 'Constant dtype device
forall (paddingIdx :: Maybe Nat) (numEmbeds :: Nat)
       (embedSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)).
Tensor device dtype '[numEmbeds, embedSize]
-&gt; EmbeddingSpec
     paddingIdx numEmbeds embedSize 'Constant dtype device
</span><a href="Torch.Typed.NN.Sparse.html#ConstEmbeddingSpec"><span class="hs-identifier hs-var">ConstEmbeddingSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="annot"><span class="hs-identifier hs-type">Nothing</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device 'Float '[2048, embedDim]
-&gt; Tensor device dtype '[2048, embedDim]
forall (dtype' :: DType) (dtype :: DType)
       (device :: (DeviceType, Nat)) (shape :: [Nat]).
KnownDType dtype' =&gt;
Tensor device dtype shape -&gt; Tensor device dtype' shape
</span><a href="Torch.Typed.Tensor.html#toDType"><span class="hs-identifier hs-var">Torch.Typed.Tensor.toDType</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device 'Float '[2048, embedDim]
forall (numEmbeds :: Nat) (embedDim :: Nat)
       (device :: (DeviceType, Nat)).
(All KnownNat '[numEmbeds, embedDim], 1 &lt;= numEmbeds,
 1 &lt;= Div embedDim 2, (Div embedDim 2 * 2) ~ embedDim,
 StandardFloatingPointDTypeValidation device 'Float,
 BasicArithmeticDTypeIsValid device 'Float, KnownDevice device) =&gt;
Tensor device 'Float '[numEmbeds, embedDim]
</span><a href="Torch.Typed.NN.Transformer.html#sinusoidal"><span class="hs-identifier hs-var">sinusoidal</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-638"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO Dropout
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device))
      -&gt; Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679817589"><span class="hs-identifier hs-var">lmDropoutSpec</span></a></span><span>
</span><span id="line-639"></span><span>      </span><span class="annot"><span class="annottext">IO
  (HList
     (HReplicateR
        numAttnLayers
        (TransformerLayer
           embedDim embedDim embedDim numHeads ffnDim dtype device))
   -&gt; Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device)))
-&gt; IO
     (Linear embedDim numEmbeds dtype device
      -&gt; TransformerLM
           numAttnLayers
           numHeads
           ffnDim
           paddingIdx
           numEmbeds
           embedDim
           dtype
           device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">HList
  (HReplicateR
     numAttnLayers
     (TransformerLayerSpec
        embedDim embedDim embedDim numHeads ffnDim dtype device))
-&gt; IO
     (HList
        (HReplicateR
           numAttnLayers
           (TransformerLayer
              embedDim embedDim embedDim numHeads ffnDim dtype device)))
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim embedDim embedDim numHeads ffnDim dtype device
-&gt; HList
     (HReplicateR
        numAttnLayers
        (TransformerLayerSpec
           embedDim embedDim embedDim numHeads ffnDim dtype device))
forall (n :: Nat) e. HReplicate n e =&gt; e -&gt; HList (HReplicateR n e)
</span><a href="Torch.HList.html#hreplicate"><span class="hs-identifier hs-var">hreplicate</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679817595"><span class="hs-identifier hs-type">numAttnLayers</span></a></span><span> </span><span class="annot"><span class="annottext">TransformerLayerSpec
  embedDim embedDim embedDim numHeads ffnDim dtype device
</span><a href="#local-6989586621679817588"><span class="hs-identifier hs-var">lmLayerSpec</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-640"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Linear embedDim numEmbeds dtype device
   -&gt; TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
-&gt; IO (Linear embedDim numEmbeds dtype device)
-&gt; IO
     (TransformerLM
        numAttnLayers
        numHeads
        ffnDim
        paddingIdx
        numEmbeds
        embedDim
        dtype
        device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim numEmbeds dtype device
-&gt; IO (Linear embedDim numEmbeds dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">LinearSpec embedDim numEmbeds dtype device
forall (inputFeatures :: Nat) (outputFeatures :: Nat)
       (dtype :: DType) (device :: (DeviceType, Nat)).
LinearSpec inputFeatures outputFeatures dtype device
</span><a href="Torch.Typed.NN.Linear.html#LinearSpec"><span class="hs-identifier hs-var">LinearSpec</span></a></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-641"></span></pre></body></html>