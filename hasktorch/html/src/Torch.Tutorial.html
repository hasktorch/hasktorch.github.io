<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# OPTIONS_GHC -fno-warn-unused-imports #-}</span><span>
</span><span id="line-2"></span><span>
</span><span id="line-3"></span><span class="hs-comment">-- | Hasktorch is a library for scientific computing and differentiable</span><span>
</span><span id="line-4"></span><span class="hs-comment">-- programming.</span><span>
</span><span id="line-5"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Tutorial</span><span>
</span><span id="line-6"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-comment">-- $tutorial</span></span><span>
</span><span id="line-7"></span><span>  </span><span class="hs-special">)</span><span>
</span><span id="line-8"></span><span class="hs-keyword">where</span><span>
</span><span id="line-9"></span><span>
</span><span id="line-10"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Type.Context</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">manual_seed_L</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-11"></span><span>
</span><span id="line-12"></span><span class="hs-comment">-- $setup</span><span>
</span><span id="line-13"></span><span class="hs-comment">-- &gt;&gt;&gt; manual_seed_L 123</span><span>
</span><span id="line-14"></span><span class="hs-comment">-- &gt;&gt;&gt; :set -XNoOverloadedLists</span><span>
</span><span id="line-15"></span><span>
</span><span id="line-16"></span><span class="hs-comment">-- $tutorial</span><span>
</span><span id="line-17"></span><span class="hs-comment">-- = What is Hasktorch?</span><span>
</span><span id="line-18"></span><span class="hs-comment">-- #introduction#</span><span>
</span><span id="line-19"></span><span class="hs-comment">--</span><span>
</span><span id="line-20"></span><span class="hs-comment">-- Hasktorch is a Haskell library for scientific computing and</span><span>
</span><span id="line-21"></span><span class="hs-comment">-- differentiable programming.  It leverages @libtorch@ (the backend</span><span>
</span><span id="line-22"></span><span class="hs-comment">-- library powering PyTorch) for efficient tensor manipulation and</span><span>
</span><span id="line-23"></span><span class="hs-comment">-- automatic differentiation, while bringing to bear Haskell's expressive</span><span>
</span><span id="line-24"></span><span class="hs-comment">-- type system and first-class support for for the functional programming</span><span>
</span><span id="line-25"></span><span class="hs-comment">-- paradigm.</span><span>
</span><span id="line-26"></span><span class="hs-comment">--</span><span>
</span><span id="line-27"></span><span class="hs-comment">-- == Goal of this tutorial</span><span>
</span><span id="line-28"></span><span class="hs-comment">--</span><span>
</span><span id="line-29"></span><span class="hs-comment">-- The sequence of topics and examples here is loosely based on the</span><span>
</span><span id="line-30"></span><span class="hs-comment">-- PyTorch tutorial [Deep Learning with PyTorch: A 60 Minute</span><span>
</span><span id="line-31"></span><span class="hs-comment">-- Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)</span><span>
</span><span id="line-32"></span><span class="hs-comment">-- by Soumith Chintala.</span><span>
</span><span id="line-33"></span><span class="hs-comment">--</span><span>
</span><span id="line-34"></span><span class="hs-comment">-- In this tutorial we will implement a simple machine learning</span><span>
</span><span id="line-35"></span><span class="hs-comment">-- model. Along the way, you will learn to</span><span>
</span><span id="line-36"></span><span class="hs-comment">--</span><span>
</span><span id="line-37"></span><span class="hs-comment">-- - create and manipulate tensors</span><span>
</span><span id="line-38"></span><span class="hs-comment">--</span><span>
</span><span id="line-39"></span><span class="hs-comment">-- - build computation graphs from tensors and compute gradients</span><span>
</span><span id="line-40"></span><span class="hs-comment">--</span><span>
</span><span id="line-41"></span><span class="hs-comment">-- - optimize parameters with respect to an objective function</span><span>
</span><span id="line-42"></span><span class="hs-comment">--</span><span>
</span><span id="line-43"></span><span class="hs-comment">-- = Usage</span><span>
</span><span id="line-44"></span><span class="hs-comment">-- #usage#</span><span>
</span><span id="line-45"></span><span class="hs-comment">--</span><span>
</span><span id="line-46"></span><span class="hs-comment">-- The reader is encouraged to follow along with the examples in a GHCi session.</span><span>
</span><span id="line-47"></span><span class="hs-comment">--</span><span>
</span><span id="line-48"></span><span class="hs-comment">-- To start, import 'Torch':</span><span>
</span><span id="line-49"></span><span class="hs-comment">--</span><span>
</span><span id="line-50"></span><span class="hs-comment">-- &gt;&gt;&gt; import Torch</span><span>
</span><span id="line-51"></span><span class="hs-comment">--</span><span>
</span><span id="line-52"></span><span class="hs-comment">-- == Tensors</span><span>
</span><span id="line-53"></span><span class="hs-comment">-- #tensors#</span><span>
</span><span id="line-54"></span><span class="hs-comment">--</span><span>
</span><span id="line-55"></span><span class="hs-comment">-- A `Tensor` in Hasktorch is multidimensional array with a fixed shape</span><span>
</span><span id="line-56"></span><span class="hs-comment">-- and element type.</span><span>
</span><span id="line-57"></span><span class="hs-comment">--</span><span>
</span><span id="line-58"></span><span class="hs-comment">-- For example, we can initialize a tensor with shape @[3, 4]@ and filled</span><span>
</span><span id="line-59"></span><span class="hs-comment">-- with zeros using</span><span>
</span><span id="line-60"></span><span class="hs-comment">--</span><span>
</span><span id="line-61"></span><span class="hs-comment">-- &gt;&gt;&gt; Torch.zeros' [3, 4]</span><span>
</span><span id="line-62"></span><span class="hs-comment">-- Tensor Float [3,4] [[ 0.0000,  0.0000,  0.0000,  0.0000],</span><span>
</span><span id="line-63"></span><span class="hs-comment">--                     [ 0.0000,  0.0000,  0.0000,  0.0000],</span><span>
</span><span id="line-64"></span><span class="hs-comment">--                     [ 0.0000,  0.0000,  0.0000,  0.0000]]</span><span>
</span><span id="line-65"></span><span class="hs-comment">--</span><span>
</span><span id="line-66"></span><span class="hs-comment">-- We can also initialize a tensor from a Haskell list using</span><span>
</span><span id="line-67"></span><span class="hs-comment">-- 'Torch.Tensor.asTensor':</span><span>
</span><span id="line-68"></span><span class="hs-comment">--</span><span>
</span><span id="line-69"></span><span class="hs-comment">-- &gt;&gt;&gt; asTensor ([[4, 3], [2, 1]] :: [[Float]])</span><span>
</span><span id="line-70"></span><span class="hs-comment">-- Tensor Float [2,2] [[ 4.0000   ,  3.0000   ],</span><span>
</span><span id="line-71"></span><span class="hs-comment">--                     [ 2.0000   ,  1.0000   ]]</span><span>
</span><span id="line-72"></span><span class="hs-comment">--</span><span>
</span><span id="line-73"></span><span class="hs-comment">-- Note that the numerical type of the tensor is inferred from the types of</span><span>
</span><span id="line-74"></span><span class="hs-comment">-- the values in the list.</span><span>
</span><span id="line-75"></span><span class="hs-comment">--</span><span>
</span><span id="line-76"></span><span class="hs-comment">-- Scalar values are represented in Hasktorch as tensors with shape @[]@:</span><span>
</span><span id="line-77"></span><span class="hs-comment">--</span><span>
</span><span id="line-78"></span><span class="hs-comment">-- &gt;&gt;&gt; asTensor 3.5</span><span>
</span><span id="line-79"></span><span class="hs-comment">-- Tensor Double []  3.5000</span><span>
</span><span id="line-80"></span><span class="hs-comment">--</span><span>
</span><span id="line-81"></span><span class="hs-comment">-- We can get the scalar value back out using 'Torch.Tensor.asValue':</span><span>
</span><span id="line-82"></span><span class="hs-comment">--</span><span>
</span><span id="line-83"></span><span class="hs-comment">-- &gt;&gt;&gt; asValue (asTensor 3.5)</span><span>
</span><span id="line-84"></span><span class="hs-comment">-- 3.5</span><span>
</span><span id="line-85"></span><span class="hs-comment">--</span><span>
</span><span id="line-86"></span><span class="hs-comment">-- === Specifying Tensor parameters</span><span>
</span><span id="line-87"></span><span class="hs-comment">--</span><span>
</span><span id="line-88"></span><span class="hs-comment">-- In the previous section we initialized a tensor filled with zeros</span><span>
</span><span id="line-89"></span><span class="hs-comment">-- using @zeros'@ (note the prime suffix). Hasktorch functions use a</span><span>
</span><span id="line-90"></span><span class="hs-comment">-- convention where default versions of functions use a prime suffix. The</span><span>
</span><span id="line-91"></span><span class="hs-comment">-- unprimed versions of these functions expect an additional parameter</span><span>
</span><span id="line-92"></span><span class="hs-comment">-- specifying tensor parameters. For example:</span><span>
</span><span id="line-93"></span><span class="hs-comment">--</span><span>
</span><span id="line-94"></span><span class="hs-comment">-- @</span><span>
</span><span id="line-95"></span><span class="hs-comment">--   zeros :: [Int] -&gt; 'Torch.TensorOptions' -&gt; Tensor</span><span>
</span><span id="line-96"></span><span class="hs-comment">-- @</span><span>
</span><span id="line-97"></span><span class="hs-comment">--</span><span>
</span><span id="line-98"></span><span class="hs-comment">-- @TensorOptions@ are typically specified by starting with</span><span>
</span><span id="line-99"></span><span class="hs-comment">-- 'Torch.TensorOptions.defaultOpts' and modifying using one or more of</span><span>
</span><span id="line-100"></span><span class="hs-comment">-- the following:</span><span>
</span><span id="line-101"></span><span class="hs-comment">--</span><span>
</span><span id="line-102"></span><span class="hs-comment">-- - 'Torch.TensorOptions.withDType' configures the data type of the elements</span><span>
</span><span id="line-103"></span><span class="hs-comment">-- - 'Torch.TensorOptions.withDevice' configures on which device the tensor is to be used</span><span>
</span><span id="line-104"></span><span class="hs-comment">-- - others (see 'Torch.TensorOptions')</span><span>
</span><span id="line-105"></span><span class="hs-comment">--</span><span>
</span><span id="line-106"></span><span class="hs-comment">-- For example, to construct a matrix filled with zeros of dtype @Int64@:</span><span>
</span><span id="line-107"></span><span class="hs-comment">--</span><span>
</span><span id="line-108"></span><span class="hs-comment">-- &gt;&gt;&gt; zeros [4, 4] (withDType Int64 defaultOpts)</span><span>
</span><span id="line-109"></span><span class="hs-comment">-- Tensor Int64 [4,4] [[ 0,  0,  0,  0],</span><span>
</span><span id="line-110"></span><span class="hs-comment">--                     [ 0,  0,  0,  0],</span><span>
</span><span id="line-111"></span><span class="hs-comment">--                     [ 0,  0,  0,  0],</span><span>
</span><span id="line-112"></span><span class="hs-comment">--                     [ 0,  0,  0,  0]]</span><span>
</span><span id="line-113"></span><span class="hs-comment">--</span><span>
</span><span id="line-114"></span><span class="hs-comment">-- === Tensor factories</span><span>
</span><span id="line-115"></span><span class="hs-comment">--</span><span>
</span><span id="line-116"></span><span class="hs-comment">-- Hasktorch comes with many &quot;factory&quot; functions similar to @zeros@ and</span><span>
</span><span id="line-117"></span><span class="hs-comment">-- @zeros'@ useful for initializing common kinds of tensors. For example,</span><span>
</span><span id="line-118"></span><span class="hs-comment">-- 'Torch.TensorFactories.ones',</span><span>
</span><span id="line-119"></span><span class="hs-comment">-- 'Torch.TensorFactories.full',</span><span>
</span><span id="line-120"></span><span class="hs-comment">-- 'Torch.TensorFactories.eye',</span><span>
</span><span id="line-121"></span><span class="hs-comment">-- and the primed versions of these. See 'Torch.TensorFactories' for a</span><span>
</span><span id="line-122"></span><span class="hs-comment">-- complete list.</span><span>
</span><span id="line-123"></span><span class="hs-comment">--</span><span>
</span><span id="line-124"></span><span class="hs-comment">-- One useful class of factory functions are those suffixed with &quot;-like&quot;</span><span>
</span><span id="line-125"></span><span class="hs-comment">-- (e.g. 'Torch.TensorFactories.onesLike'), which initialize a tensor</span><span>
</span><span id="line-126"></span><span class="hs-comment">-- with the same dimensions as their argument. For example:</span><span>
</span><span id="line-127"></span><span class="hs-comment">--</span><span>
</span><span id="line-128"></span><span class="hs-comment">-- &gt;&gt;&gt; let x = zeros' [3, 2]</span><span>
</span><span id="line-129"></span><span class="hs-comment">-- &gt;&gt;&gt; onesLike x</span><span>
</span><span id="line-130"></span><span class="hs-comment">-- Tensor Float [3,2] [[ 1.0000   ,  1.0000   ],</span><span>
</span><span id="line-131"></span><span class="hs-comment">--                     [ 1.0000   ,  1.0000   ],</span><span>
</span><span id="line-132"></span><span class="hs-comment">--                     [ 1.0000   ,  1.0000   ]]</span><span>
</span><span id="line-133"></span><span class="hs-comment">--</span><span>
</span><span id="line-134"></span><span class="hs-comment">-- == Operations</span><span>
</span><span id="line-135"></span><span class="hs-comment">-- #operations#</span><span>
</span><span id="line-136"></span><span class="hs-comment">--</span><span>
</span><span id="line-137"></span><span class="hs-comment">-- Most operations are pure functions, similar to Haskell standard library</span><span>
</span><span id="line-138"></span><span class="hs-comment">-- math operations.</span><span>
</span><span id="line-139"></span><span class="hs-comment">--</span><span>
</span><span id="line-140"></span><span class="hs-comment">-- Tensors implement the @Num@ typeclass:</span><span>
</span><span id="line-141"></span><span class="hs-comment">--</span><span>
</span><span id="line-142"></span><span class="hs-comment">-- &gt;&gt;&gt; let x = ones' [4]</span><span>
</span><span id="line-143"></span><span class="hs-comment">-- &gt;&gt;&gt; x + x</span><span>
</span><span id="line-144"></span><span class="hs-comment">-- Tensor Float [4] [ 2.0000   ,  2.0000   ,  2.0000   ,  2.0000   ]</span><span>
</span><span id="line-145"></span><span class="hs-comment">--</span><span>
</span><span id="line-146"></span><span class="hs-comment">-- Some operations transform a tensor:</span><span>
</span><span id="line-147"></span><span class="hs-comment">--</span><span>
</span><span id="line-148"></span><span class="hs-comment">-- &gt;&gt;&gt; Torch.relu (asTensor ([-1.0, -0.5, 0.5, 1] :: [Float]))</span><span>
</span><span id="line-149"></span><span class="hs-comment">-- Tensor Float [4] [ 0.0000,  0.0000,  0.5000   ,  1.0000   ]</span><span>
</span><span id="line-150"></span><span class="hs-comment">--</span><span>
</span><span id="line-151"></span><span class="hs-comment">-- 'Torch.Tensor.selectDim' slices out a selection by specifying a dimension and index:</span><span>
</span><span id="line-152"></span><span class="hs-comment">--</span><span>
</span><span id="line-153"></span><span class="hs-comment">-- &gt;&gt;&gt; let x = asTensor [[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]], [[10, 11, 12]]]</span><span>
</span><span id="line-154"></span><span class="hs-comment">-- &gt;&gt;&gt; shape x</span><span>
</span><span id="line-155"></span><span class="hs-comment">-- [4,1,3]</span><span>
</span><span id="line-156"></span><span class="hs-comment">--</span><span>
</span><span id="line-157"></span><span class="hs-comment">-- &gt;&gt;&gt; select 2 1 x</span><span>
</span><span id="line-158"></span><span class="hs-comment">-- Tensor Double [4,1] [[ 2.0000   ],</span><span>
</span><span id="line-159"></span><span class="hs-comment">--                      [ 5.0000   ],</span><span>
</span><span id="line-160"></span><span class="hs-comment">--                      [ 8.0000   ],</span><span>
</span><span id="line-161"></span><span class="hs-comment">--                      [ 11.0000   ]]</span><span>
</span><span id="line-162"></span><span class="hs-comment">--</span><span>
</span><span id="line-163"></span><span class="hs-comment">-- &gt;&gt;&gt; let y = asTensor [1, 2, 3]</span><span>
</span><span id="line-164"></span><span class="hs-comment">-- &gt;&gt;&gt; Torch.select 0 1 y</span><span>
</span><span id="line-165"></span><span class="hs-comment">-- Tensor Double []  2.0000</span><span>
</span><span id="line-166"></span><span class="hs-comment">--</span><span>
</span><span id="line-167"></span><span class="hs-comment">-- Values can be extracted from a tensor using @asValue@ so long as the</span><span>
</span><span id="line-168"></span><span class="hs-comment">-- dtype matches the Haskell type:</span><span>
</span><span id="line-169"></span><span class="hs-comment">--</span><span>
</span><span id="line-170"></span><span class="hs-comment">-- &gt;&gt;&gt; let x = asTensor ([2] :: [Int])</span><span>
</span><span id="line-171"></span><span class="hs-comment">-- &gt;&gt;&gt; let y = asValue x :: Int</span><span>
</span><span id="line-172"></span><span class="hs-comment">-- &gt;&gt;&gt; y</span><span>
</span><span id="line-173"></span><span class="hs-comment">-- 2</span><span>
</span><span id="line-174"></span><span class="hs-comment">--</span><span>
</span><span id="line-175"></span><span class="hs-comment">-- == Randomness</span><span>
</span><span id="line-176"></span><span class="hs-comment">--</span><span>
</span><span id="line-177"></span><span class="hs-comment">-- Create a randomly initialized matrix:</span><span>
</span><span id="line-178"></span><span class="hs-comment">--</span><span>
</span><span id="line-179"></span><span class="hs-comment">-- &gt;&gt;&gt; x &lt;-randIO' [2, 2]</span><span>
</span><span id="line-180"></span><span class="hs-comment">-- &gt;&gt;&gt; x</span><span>
</span><span id="line-181"></span><span class="hs-comment">-- Tensor Float [2,2] [[ 0.2961   ,  0.5166   ],</span><span>
</span><span id="line-182"></span><span class="hs-comment">--                     [ 0.2517   ,  0.6886   ]]</span><span>
</span><span id="line-183"></span><span class="hs-comment">--</span><span>
</span><span id="line-184"></span><span class="hs-comment">-- Note that since random initialization returns a different result each</span><span>
</span><span id="line-185"></span><span class="hs-comment">-- time, unlike other tensor constructors, is monadic reflecting the</span><span>
</span><span id="line-186"></span><span class="hs-comment">-- context of an underlying random number generator (RNG) changing state.</span><span>
</span><span id="line-187"></span><span class="hs-comment">--</span><span>
</span><span id="line-188"></span><span class="hs-comment">-- Hasktorch includes variations of random tensor initializers in which the</span><span>
</span><span id="line-189"></span><span class="hs-comment">-- RNG object is threaded explicitly rather than implicitly. See the</span><span>
</span><span id="line-190"></span><span class="hs-comment">-- @Torch.Random@ module functions for details and variations. Samplers for</span><span>
</span><span id="line-191"></span><span class="hs-comment">-- which the RNG is not explicit such as @randIO\'@ example above use the</span><span>
</span><span id="line-192"></span><span class="hs-comment">-- @-IO@ suffix.</span><span>
</span><span id="line-193"></span><span class="hs-comment">--</span><span>
</span><span id="line-194"></span><span class="hs-comment">--</span><span>
</span><span id="line-195"></span><span class="hs-comment">-- == Automatic Differentiation</span><span>
</span><span id="line-196"></span><span class="hs-comment">-- #automatic-differentiation#</span><span>
</span><span id="line-197"></span><span class="hs-comment">--</span><span>
</span><span id="line-198"></span><span class="hs-comment">-- Automatic differentiation is achieved through the use of two primary</span><span>
</span><span id="line-199"></span><span class="hs-comment">-- functions in the 'Torch.Autograd' module,</span><span>
</span><span id="line-200"></span><span class="hs-comment">-- 'Torch.Autograd.makeIndependent' and 'Torch.Autograd.grad'.</span><span>
</span><span id="line-201"></span><span class="hs-comment">--</span><span>
</span><span id="line-202"></span><span class="hs-comment">-- === Independent Tensors</span><span>
</span><span id="line-203"></span><span class="hs-comment">-- #independent-tensors#</span><span>
</span><span id="line-204"></span><span class="hs-comment">--</span><span>
</span><span id="line-205"></span><span class="hs-comment">-- @makeIndependent@ is used to instantiate an independent tensor variable</span><span>
</span><span id="line-206"></span><span class="hs-comment">-- from which a compute graph is constructed for differentiation, while</span><span>
</span><span id="line-207"></span><span class="hs-comment">-- @grad@ uses compute graph to compute gradients.</span><span>
</span><span id="line-208"></span><span class="hs-comment">--</span><span>
</span><span id="line-209"></span><span class="hs-comment">-- @makeIndependent@ takes a tensor as input and returns an IO action</span><span>
</span><span id="line-210"></span><span class="hs-comment">-- which produces an 'Torch.Autograd.IndependentTensor':</span><span>
</span><span id="line-211"></span><span class="hs-comment">--</span><span>
</span><span id="line-212"></span><span class="hs-comment">-- &gt;  makeIndependent :: Tensor -&gt; IO IndependentTensor</span><span>
</span><span id="line-213"></span><span class="hs-comment">--</span><span>
</span><span id="line-214"></span><span class="hs-comment">-- What is the definition of the @IndependentTensor@ type produced by the</span><span>
</span><span id="line-215"></span><span class="hs-comment">-- @makeIndependent@ action? It&#8217;s defined in the Hasktorch library as:</span><span>
</span><span id="line-216"></span><span class="hs-comment">--</span><span>
</span><span id="line-217"></span><span class="hs-comment">-- &gt;  newtype IndependentTensor = IndependentTensor { toDependent :: Tensor }</span><span>
</span><span id="line-218"></span><span class="hs-comment">-- &gt;  deriving (Show)</span><span>
</span><span id="line-219"></span><span class="hs-comment">--</span><span>
</span><span id="line-220"></span><span class="hs-comment">-- Thus @IndependentTensor@ is simply a wrapper around the underlying</span><span>
</span><span id="line-221"></span><span class="hs-comment">-- Tensor that is passed in as the argument to @makeIndependent@. Building</span><span>
</span><span id="line-222"></span><span class="hs-comment">-- up computations using ops applied to the @toDependent@ tensor of an</span><span>
</span><span id="line-223"></span><span class="hs-comment">-- @IndependentTensor@ will implicitly construct a compute graph to which</span><span>
</span><span id="line-224"></span><span class="hs-comment">-- @grad@ can be applied.</span><span>
</span><span id="line-225"></span><span class="hs-comment">--</span><span>
</span><span id="line-226"></span><span class="hs-comment">-- All tensors have an underlying property that can be retrieved using</span><span>
</span><span id="line-227"></span><span class="hs-comment">-- the 'Torch.Autograd.requiresGrad' function which indicates whether</span><span>
</span><span id="line-228"></span><span class="hs-comment">-- they are a differentiable value in a compute graph. &lt;#notes [1]&gt;</span><span>
</span><span id="line-229"></span><span class="hs-comment">--</span><span>
</span><span id="line-230"></span><span class="hs-comment">-- &gt;&gt;&gt; let x = asTensor [1, 2, 3]</span><span>
</span><span id="line-231"></span><span class="hs-comment">-- &gt;&gt;&gt; y &lt;- makeIndependent (asTensor [4, 5, 6])</span><span>
</span><span id="line-232"></span><span class="hs-comment">-- &gt;&gt;&gt; let y' = toDependent y</span><span>
</span><span id="line-233"></span><span class="hs-comment">-- &gt;&gt;&gt; let z = x + y'</span><span>
</span><span id="line-234"></span><span class="hs-comment">-- &gt;&gt;&gt; requiresGrad x</span><span>
</span><span id="line-235"></span><span class="hs-comment">-- False</span><span>
</span><span id="line-236"></span><span class="hs-comment">--</span><span>
</span><span id="line-237"></span><span class="hs-comment">-- &gt;&gt;&gt; requiresGrad y'</span><span>
</span><span id="line-238"></span><span class="hs-comment">-- True</span><span>
</span><span id="line-239"></span><span class="hs-comment">--</span><span>
</span><span id="line-240"></span><span class="hs-comment">-- &gt;&gt;&gt; requiresGrad z</span><span>
</span><span id="line-241"></span><span class="hs-comment">-- True</span><span>
</span><span id="line-242"></span><span class="hs-comment">--</span><span>
</span><span id="line-243"></span><span class="hs-comment">-- In summary, tensors that are computations of values derived from tensor</span><span>
</span><span id="line-244"></span><span class="hs-comment">-- constructors (e.g. @ones@, @zeros@, @fill@, @randIO@ etc.) outside the</span><span>
</span><span id="line-245"></span><span class="hs-comment">-- context of a @IndependentTensor@ are not differentiable. Tensors that</span><span>
</span><span id="line-246"></span><span class="hs-comment">-- are derived from computations on the @toDependent@ value of an</span><span>
</span><span id="line-247"></span><span class="hs-comment">-- @IndependentTensor@ are differentiable, as the above example</span><span>
</span><span id="line-248"></span><span class="hs-comment">-- illustrates.</span><span>
</span><span id="line-249"></span><span class="hs-comment">--</span><span>
</span><span id="line-250"></span><span class="hs-comment">-- === Gradients</span><span>
</span><span id="line-251"></span><span class="hs-comment">-- #gradients#</span><span>
</span><span id="line-252"></span><span class="hs-comment">--</span><span>
</span><span id="line-253"></span><span class="hs-comment">-- Once a computation graph is constructed by applying ops and computing</span><span>
</span><span id="line-254"></span><span class="hs-comment">-- derived quantities stemming from a @toDependent@ value of an</span><span>
</span><span id="line-255"></span><span class="hs-comment">-- @IndependentTensor@, a gradient can be taken by using the @grad@</span><span>
</span><span id="line-256"></span><span class="hs-comment">-- function specifying in the first argument tensor corresponding to</span><span>
</span><span id="line-257"></span><span class="hs-comment">-- function value of interest and a list of @Independent@ tensor variables</span><span>
</span><span id="line-258"></span><span class="hs-comment">-- that the the derivative is taken with respect to:</span><span>
</span><span id="line-259"></span><span class="hs-comment">--</span><span>
</span><span id="line-260"></span><span class="hs-comment">-- &gt;  grad :: Tensor -&gt; [IndependentTensor] -&gt; [Tensor]</span><span>
</span><span id="line-261"></span><span class="hs-comment">--</span><span>
</span><span id="line-262"></span><span class="hs-comment">-- Let&#8217;s demonstrate this with a concrete example. We create a tensor and</span><span>
</span><span id="line-263"></span><span class="hs-comment">-- derive an @IndependentTensor@ from it:</span><span>
</span><span id="line-264"></span><span class="hs-comment">--</span><span>
</span><span id="line-265"></span><span class="hs-comment">-- &gt;&gt;&gt; x &lt;- makeIndependent (ones' [2, 2])</span><span>
</span><span id="line-266"></span><span class="hs-comment">-- &gt;&gt;&gt; let x' = toDependent x</span><span>
</span><span id="line-267"></span><span class="hs-comment">-- &gt;&gt;&gt; x'</span><span>
</span><span id="line-268"></span><span class="hs-comment">-- Tensor Float [2,2] [[ 1.0000   ,  1.0000   ],</span><span>
</span><span id="line-269"></span><span class="hs-comment">--                     [ 1.0000   ,  1.0000   ]]</span><span>
</span><span id="line-270"></span><span class="hs-comment">--</span><span>
</span><span id="line-271"></span><span class="hs-comment">-- Now do some computations on the dependent tensor:</span><span>
</span><span id="line-272"></span><span class="hs-comment">--</span><span>
</span><span id="line-273"></span><span class="hs-comment">-- &gt;&gt;&gt; let y = x' + 2</span><span>
</span><span id="line-274"></span><span class="hs-comment">-- &gt;&gt;&gt; y</span><span>
</span><span id="line-275"></span><span class="hs-comment">-- Tensor Float [2,2] [[ 3.0000   ,  3.0000   ],</span><span>
</span><span id="line-276"></span><span class="hs-comment">--                     [ 3.0000   ,  3.0000   ]]</span><span>
</span><span id="line-277"></span><span class="hs-comment">--</span><span>
</span><span id="line-278"></span><span class="hs-comment">-- Since y is dependent on the x independent tensor, it is differentiable:</span><span>
</span><span id="line-279"></span><span class="hs-comment">--</span><span>
</span><span id="line-280"></span><span class="hs-comment">-- &gt;&gt;&gt; requiresGrad y</span><span>
</span><span id="line-281"></span><span class="hs-comment">-- True</span><span>
</span><span id="line-282"></span><span class="hs-comment">--</span><span>
</span><span id="line-283"></span><span class="hs-comment">-- Applying more operations:</span><span>
</span><span id="line-284"></span><span class="hs-comment">--</span><span>
</span><span id="line-285"></span><span class="hs-comment">-- &gt;&gt;&gt; let z = y * y * 3</span><span>
</span><span id="line-286"></span><span class="hs-comment">-- &gt;&gt;&gt; let out = mean z</span><span>
</span><span id="line-287"></span><span class="hs-comment">-- &gt;&gt;&gt; z</span><span>
</span><span id="line-288"></span><span class="hs-comment">-- Tensor Float [2,2] [[ 27.0000   ,  27.0000   ],</span><span>
</span><span id="line-289"></span><span class="hs-comment">--                     [ 27.0000   ,  27.0000   ]]</span><span>
</span><span id="line-290"></span><span class="hs-comment">--</span><span>
</span><span id="line-291"></span><span class="hs-comment">-- Now retrieve the gradient:</span><span>
</span><span id="line-292"></span><span class="hs-comment">--</span><span>
</span><span id="line-293"></span><span class="hs-comment">-- &gt;&gt;&gt; grad out [x]</span><span>
</span><span id="line-294"></span><span class="hs-comment">-- [Tensor Float [2,2] [[ 4.5000   ,  4.5000   ],</span><span>
</span><span id="line-295"></span><span class="hs-comment">--                     [ 4.5000   ,  4.5000   ]]]</span><span>
</span><span id="line-296"></span><span class="hs-comment">--</span><span>
</span><span id="line-297"></span><span class="hs-comment">-- == Differentiable Programs (Neural Networks)</span><span>
</span><span id="line-298"></span><span class="hs-comment">-- #differentiable-programs-neural-networks#</span><span>
</span><span id="line-299"></span><span class="hs-comment">--</span><span>
</span><span id="line-300"></span><span class="hs-comment">-- From a functional programming perspective, a neural network is</span><span>
</span><span id="line-301"></span><span class="hs-comment">-- represented by data and functions, much like any other functional</span><span>
</span><span id="line-302"></span><span class="hs-comment">-- program. The only distinction that differentiates neural networks from</span><span>
</span><span id="line-303"></span><span class="hs-comment">-- any other functional program is that it implements a small interface</span><span>
</span><span id="line-304"></span><span class="hs-comment">-- surface to support differentiation. Thus, we can consider neural</span><span>
</span><span id="line-305"></span><span class="hs-comment">-- networks to be \&quot;differentiable functional programming\&quot;.</span><span>
</span><span id="line-306"></span><span class="hs-comment">--</span><span>
</span><span id="line-307"></span><span class="hs-comment">-- The data in neural networks are the values to be fitted that</span><span>
</span><span id="line-308"></span><span class="hs-comment">-- parameterize the functions which carry out the inference operation and</span><span>
</span><span id="line-309"></span><span class="hs-comment">-- are modified based on gradients of through those functions.</span><span>
</span><span id="line-310"></span><span class="hs-comment">--</span><span>
</span><span id="line-311"></span><span class="hs-comment">-- As with a regular Haskell program, this data is represented by an</span><span>
</span><span id="line-312"></span><span class="hs-comment">-- algebraic data type (ADT). The ADT can take on any shape that&#8217;s needed</span><span>
</span><span id="line-313"></span><span class="hs-comment">-- to model the domain of interest, allowing a great deal of flexibility</span><span>
</span><span id="line-314"></span><span class="hs-comment">-- and enabling all of Haskell&#8217;s strenghts in data modeling - can use sum</span><span>
</span><span id="line-315"></span><span class="hs-comment">-- or product types, nest types, etc. The ADT can implement various</span><span>
</span><span id="line-316"></span><span class="hs-comment">-- typeclasses to take on other functionality.</span><span>
</span><span id="line-317"></span><span class="hs-comment">--</span><span>
</span><span id="line-318"></span><span class="hs-comment">-- The core interface that defines capability specific to differentiable</span><span>
</span><span id="line-319"></span><span class="hs-comment">-- programming is the 'Torch.NN.Parameterized' typeclass:</span><span>
</span><span id="line-320"></span><span class="hs-comment">--</span><span>
</span><span id="line-321"></span><span class="hs-comment">-- &gt;  class Parameterized f where</span><span>
</span><span id="line-322"></span><span class="hs-comment">-- &gt;    flattenParameters :: f -&gt; [Parameter]</span><span>
</span><span id="line-323"></span><span class="hs-comment">-- &gt;    default flattenParameters :: (Generic f, Parameterized' (Rep f)) =&gt; f -&gt; [Parameter]</span><span>
</span><span id="line-324"></span><span class="hs-comment">-- &gt;    flattenParameters f = flattenParameters' (from f)</span><span>
</span><span id="line-325"></span><span class="hs-comment">-- &gt;</span><span>
</span><span id="line-326"></span><span class="hs-comment">-- &gt;    replaceOwnParameters :: f -&gt; ParamStream f</span><span>
</span><span id="line-327"></span><span class="hs-comment">-- &gt;    default replaceOwnParameters :: (Generic f, Parameterized' (Rep f)) =&gt; f -&gt; ParamStream f</span><span>
</span><span id="line-328"></span><span class="hs-comment">-- &gt;    replaceOwnParameters f = to &lt;$&gt; replaceOwnParameters' (from f)</span><span>
</span><span id="line-329"></span><span class="hs-comment">--</span><span>
</span><span id="line-330"></span><span class="hs-comment">-- Note @Parameter@ is simply a type alias for @IndependentTensor@ in the</span><span>
</span><span id="line-331"></span><span class="hs-comment">-- context of neural networks (i.e. @type Parameter = IndependentTensor@).</span><span>
</span><span id="line-332"></span><span class="hs-comment">--</span><span>
</span><span id="line-333"></span><span class="hs-comment">-- The role of @flattenParameters@ is to unroll any arbitrary ADT</span><span>
</span><span id="line-334"></span><span class="hs-comment">-- representation of a neural network into a standard flattened</span><span>
</span><span id="line-335"></span><span class="hs-comment">-- representation consisting a list of @IndependentTensor@ which is used to</span><span>
</span><span id="line-336"></span><span class="hs-comment">-- compute gradients.</span><span>
</span><span id="line-337"></span><span class="hs-comment">--</span><span>
</span><span id="line-338"></span><span class="hs-comment">-- @replaceOwnParameters@ is used to update parameters. ParamStream is a</span><span>
</span><span id="line-339"></span><span class="hs-comment">-- type alias for a State type with state represented by a @Parameter@ list</span><span>
</span><span id="line-340"></span><span class="hs-comment">-- and a value parameter corresponding to the ADT defining the model.</span><span>
</span><span id="line-341"></span><span class="hs-comment">--</span><span>
</span><span id="line-342"></span><span class="hs-comment">-- &gt;  type ParamStream a = State [Parameter] a</span><span>
</span><span id="line-343"></span><span class="hs-comment">--</span><span>
</span><span id="line-344"></span><span class="hs-comment">-- Note the use of generics. Generics allow the compiler to usually</span><span>
</span><span id="line-345"></span><span class="hs-comment">-- automatically derive @flattenParameters@ and @replaceOwnParameter@</span><span>
</span><span id="line-346"></span><span class="hs-comment">-- instances without any code if your type is built up on tensors,</span><span>
</span><span id="line-347"></span><span class="hs-comment">-- containers of tensors, or other types that are built from tensor values</span><span>
</span><span id="line-348"></span><span class="hs-comment">-- (for example, layer modules provided in @Torch.NN@. In many cases, as</span><span>
</span><span id="line-349"></span><span class="hs-comment">-- you&#8217;ll see in the following examples, you will only need to add</span><span>
</span><span id="line-350"></span><span class="hs-comment">--</span><span>
</span><span id="line-351"></span><span class="hs-comment">-- &gt;  instance Parameterized MyNeuralNetwork</span><span>
</span><span id="line-352"></span><span class="hs-comment">--</span><span>
</span><span id="line-353"></span><span class="hs-comment">-- (where @MyNeuralNetwork@ is an ADT definition for your model) and the</span><span>
</span><span id="line-354"></span><span class="hs-comment">-- compiler will derive implementations for the @flattenParameters@ and</span><span>
</span><span id="line-355"></span><span class="hs-comment">-- @replaceOwnParameters@.</span><span>
</span><span id="line-356"></span><span class="hs-comment">--</span><span>
</span><span id="line-357"></span><span class="hs-comment">-- === Linear Regression</span><span>
</span><span id="line-358"></span><span class="hs-comment">-- #linear-regression#</span><span>
</span><span id="line-359"></span><span class="hs-comment">--</span><span>
</span><span id="line-360"></span><span class="hs-comment">-- Lets start with a simple example of linear regression. Here we generate</span><span>
</span><span id="line-361"></span><span class="hs-comment">-- random data with an underlying affine relationship between the inputs</span><span>
</span><span id="line-362"></span><span class="hs-comment">-- and outputs, then fit a linear regression to reproduce that</span><span>
</span><span id="line-363"></span><span class="hs-comment">-- relationship.</span><span>
</span><span id="line-364"></span><span class="hs-comment">--</span><span>
</span><span id="line-365"></span><span class="hs-comment">-- This example is adapted from</span><span>
</span><span id="line-366"></span><span class="hs-comment">-- &lt;https://github.com/hasktorch/hasktorch/tree/master/examples/regression&gt;.</span><span>
</span><span id="line-367"></span><span class="hs-comment">--</span><span>
</span><span id="line-368"></span><span class="hs-comment">-- In a standard supervised learning model, the neural network is</span><span>
</span><span id="line-369"></span><span class="hs-comment">-- initialized using a randomized initialization scheme. An iterative</span><span>
</span><span id="line-370"></span><span class="hs-comment">-- optimization is performed such that at each iteration a batch.</span><span>
</span><span id="line-371"></span><span class="hs-comment">--</span><span>
</span><span id="line-372"></span><span class="hs-comment">-- &gt;  module Main where</span><span>
</span><span id="line-373"></span><span class="hs-comment">-- &gt;</span><span>
</span><span id="line-374"></span><span class="hs-comment">-- &gt;  import Control.Monad (when)</span><span>
</span><span id="line-375"></span><span class="hs-comment">-- &gt;  import Torch</span><span>
</span><span id="line-376"></span><span class="hs-comment">-- &gt;</span><span>
</span><span id="line-377"></span><span class="hs-comment">-- &gt;  groundTruth :: Tensor -&gt; Tensor</span><span>
</span><span id="line-378"></span><span class="hs-comment">-- &gt;  groundTruth t = squeezeAll $ matmul t weight + bias</span><span>
</span><span id="line-379"></span><span class="hs-comment">-- &gt;    where</span><span>
</span><span id="line-380"></span><span class="hs-comment">-- &gt;      weight = asTensor ([42.0, 64.0, 96.0] :: [Float])</span><span>
</span><span id="line-381"></span><span class="hs-comment">-- &gt;      bias = full' [1] (3.14 :: Float)</span><span>
</span><span id="line-382"></span><span class="hs-comment">-- &gt;</span><span>
</span><span id="line-383"></span><span class="hs-comment">-- &gt;  model :: Linear -&gt; Tensor -&gt; Tensor</span><span>
</span><span id="line-384"></span><span class="hs-comment">-- &gt;  model state input = squeezeAll $ linear state input</span><span>
</span><span id="line-385"></span><span class="hs-comment">-- &gt;</span><span>
</span><span id="line-386"></span><span class="hs-comment">-- &gt;  main :: IO ()</span><span>
</span><span id="line-387"></span><span class="hs-comment">-- &gt;  main = do</span><span>
</span><span id="line-388"></span><span class="hs-comment">-- &gt;      init &lt;- sample $ LinearSpec{in_features = numFeatures, out_features = 1}</span><span>
</span><span id="line-389"></span><span class="hs-comment">-- &gt;      randGen &lt;- mkGenerator (Device CPU 0) 12345</span><span>
</span><span id="line-390"></span><span class="hs-comment">-- &gt;      (trained, _) &lt;- foldLoop (init, randGen) 2000 $ \(state, randGen) i -&gt; do</span><span>
</span><span id="line-391"></span><span class="hs-comment">-- &gt;          let (input, randGen') = randn' [batchSize, numFeatures] randGen</span><span>
</span><span id="line-392"></span><span class="hs-comment">-- &gt;              (y, y') = (groundTruth input, model state input)</span><span>
</span><span id="line-393"></span><span class="hs-comment">-- &gt;              loss = mseLoss y y'</span><span>
</span><span id="line-394"></span><span class="hs-comment">-- &gt;          when (i `mod` 100 == 0) $ do</span><span>
</span><span id="line-395"></span><span class="hs-comment">-- &gt;              putStrLn $ &quot;Iteration: &quot; ++ show i ++ &quot; | Loss: &quot; ++ show loss</span><span>
</span><span id="line-396"></span><span class="hs-comment">-- &gt;          (newParam, _) &lt;- runStep state GD loss 5e-3</span><span>
</span><span id="line-397"></span><span class="hs-comment">-- &gt;          pure (replaceParameters state newParam, randGen')</span><span>
</span><span id="line-398"></span><span class="hs-comment">-- &gt;      pure ()</span><span>
</span><span id="line-399"></span><span class="hs-comment">-- &gt;    where</span><span>
</span><span id="line-400"></span><span class="hs-comment">-- &gt;      batchSize = 4</span><span>
</span><span id="line-401"></span><span class="hs-comment">-- &gt;      numFeatures = 3</span><span>
</span><span id="line-402"></span><span class="hs-comment">--</span><span>
</span><span id="line-403"></span><span class="hs-comment">-- Note the expression of the architecture in the 'Torch.NN.linear'</span><span>
</span><span id="line-404"></span><span class="hs-comment">-- function (a single linear layer, or alternatively a neural network</span><span>
</span><span id="line-405"></span><span class="hs-comment">-- with zero hidden layers), does not require an explicit representation</span><span>
</span><span id="line-406"></span><span class="hs-comment">-- of the compute graph, but is simply a composition of tensor</span><span>
</span><span id="line-407"></span><span class="hs-comment">-- ops. Because of the autodiff mechanism described in the previous</span><span>
</span><span id="line-408"></span><span class="hs-comment">-- section, the graph is constructed automatically as pure functional ops</span><span>
</span><span id="line-409"></span><span class="hs-comment">-- are applied, given a context of a set of independent variables.</span><span>
</span><span id="line-410"></span><span class="hs-comment">--</span><span>
</span><span id="line-411"></span><span class="hs-comment">-- The @init@ variable is initialized as a @Linear@ type (defined in</span><span>
</span><span id="line-412"></span><span class="hs-comment">-- @Torch.NN@) using @sample@ which randomly initializes a @Linear@ value.</span><span>
</span><span id="line-413"></span><span class="hs-comment">--</span><span>
</span><span id="line-414"></span><span class="hs-comment">-- @Linear@ is a built-in ADT implementing the @Parameterized@ typeclass</span><span>
</span><span id="line-415"></span><span class="hs-comment">-- and representing a fully connected linear layer, equivalent to linear</span><span>
</span><span id="line-416"></span><span class="hs-comment">-- regression when no hidden layers are present.</span><span>
</span><span id="line-417"></span><span class="hs-comment">--</span><span>
</span><span id="line-418"></span><span class="hs-comment">-- @init@ is passed into the 'Torch.Optim.foldLoop'@&lt;#notes [2]&gt; as the @state@</span><span>
</span><span id="line-419"></span><span class="hs-comment">-- variable.</span><span>
</span><span id="line-420"></span><span class="hs-comment">--</span><span>
</span><span id="line-421"></span><span class="hs-comment">-- A new list of @Parameter@ values is passed back from</span><span>
</span><span id="line-422"></span><span class="hs-comment">-- 'Torch.Optim.runStep' (which calls @grad@ to retrieve gradients, given</span><span>
</span><span id="line-423"></span><span class="hs-comment">-- a loss function, learning rate, and optimizer) and the typeclass</span><span>
</span><span id="line-424"></span><span class="hs-comment">-- function @replaceParameters@ is used to update the model at each</span><span>
</span><span id="line-425"></span><span class="hs-comment">-- iteration.</span><span>
</span><span id="line-426"></span><span class="hs-comment">--</span><span>
</span><span id="line-427"></span><span class="hs-comment">-- Initialization is discussed in more detail in the following section</span><span>
</span><span id="line-428"></span><span class="hs-comment">--</span><span>
</span><span id="line-429"></span><span class="hs-comment">-- === Weight Initialization</span><span>
</span><span id="line-430"></span><span class="hs-comment">-- #weight-initialization#</span><span>
</span><span id="line-431"></span><span class="hs-comment">--</span><span>
</span><span id="line-432"></span><span class="hs-comment">-- Random initialization of weights is not a pure function since two</span><span>
</span><span id="line-433"></span><span class="hs-comment">-- random initializations return different values. Initialization occurs</span><span>
</span><span id="line-434"></span><span class="hs-comment">-- by calling the 'Torch.NN.sample' function for an ADT (@spec@)</span><span>
</span><span id="line-435"></span><span class="hs-comment">-- implementing the 'Torch.NN.Randomizable' typeclass:</span><span>
</span><span id="line-436"></span><span class="hs-comment">--</span><span>
</span><span id="line-437"></span><span class="hs-comment">-- &gt;  class Randomizable spec f | spec -&gt; f where</span><span>
</span><span id="line-438"></span><span class="hs-comment">-- &gt;    sample :: spec -&gt; IO f</span><span>
</span><span id="line-439"></span><span class="hs-comment">--</span><span>
</span><span id="line-440"></span><span class="hs-comment">-- In a typical (but not required) usage, @f@ is an ADT that implements the</span><span>
</span><span id="line-441"></span><span class="hs-comment">-- @Parameterized@ typeclass, so that there&#8217;s a pair of types - a</span><span>
</span><span id="line-442"></span><span class="hs-comment">-- specification type implementing the @spec@ input to @sample@ and a type</span><span>
</span><span id="line-443"></span><span class="hs-comment">-- implementing @Parameterizable@ representing the model state.</span><span>
</span><span id="line-444"></span><span class="hs-comment">--</span><span>
</span><span id="line-445"></span><span class="hs-comment">-- For example, a linear fully connected layer is provided by the</span><span>
</span><span id="line-446"></span><span class="hs-comment">-- @Torch.NN@ module and defined therein as:</span><span>
</span><span id="line-447"></span><span class="hs-comment">--</span><span>
</span><span id="line-448"></span><span class="hs-comment">-- &gt;  data Linear = Linear { weight :: Parameter, bias :: Parameter } deriving (Show, Generic)</span><span>
</span><span id="line-449"></span><span class="hs-comment">--</span><span>
</span><span id="line-450"></span><span class="hs-comment">-- and is typically used with a specification type:</span><span>
</span><span id="line-451"></span><span class="hs-comment">--</span><span>
</span><span id="line-452"></span><span class="hs-comment">-- &gt;  data LinearSpec = LinearSpec { in_features :: Int, out_features :: Int }</span><span>
</span><span id="line-453"></span><span class="hs-comment">-- &gt;    deriving (Show, Eq)</span><span>
</span><span id="line-454"></span><span class="hs-comment">--</span><span>
</span><span id="line-455"></span><span class="hs-comment">-- Putting this together, in untyped tensor usage, the user can implement</span><span>
</span><span id="line-456"></span><span class="hs-comment">-- custom models or layers implementing the @Parameterizable@ typeclass</span><span>
</span><span id="line-457"></span><span class="hs-comment">-- built up from other ADTs implementing @Parameterizable@. The shape of</span><span>
</span><span id="line-458"></span><span class="hs-comment">-- the data required for initialization is described by a type implementing</span><span>
</span><span id="line-459"></span><span class="hs-comment">-- @Randomizable@&#8217;s @spec@ parameter, and the @sample@ implementation</span><span>
</span><span id="line-460"></span><span class="hs-comment">-- specifies the default weight initialization.</span><span>
</span><span id="line-461"></span><span class="hs-comment">--</span><span>
</span><span id="line-462"></span><span class="hs-comment">-- Note this initialization approach is specific to untyped tensors. One</span><span>
</span><span id="line-463"></span><span class="hs-comment">-- consequence of using typed tensors is that the information in these</span><span>
</span><span id="line-464"></span><span class="hs-comment">-- @spec@ types is reflected in the type itself and thus are not needed.</span><span>
</span><span id="line-465"></span><span class="hs-comment">--</span><span>
</span><span id="line-466"></span><span class="hs-comment">-- What if you want to use a custom initialization that differs from the</span><span>
</span><span id="line-467"></span><span class="hs-comment">-- default? You can define an alternative function with the same signature</span><span>
</span><span id="line-468"></span><span class="hs-comment">-- @spec -&gt; IO f@ and use the alternative function instead of @sample@.</span><span>
</span><span id="line-469"></span><span class="hs-comment">--</span><span>
</span><span id="line-470"></span><span class="hs-comment">-- === Optimizers</span><span>
</span><span id="line-471"></span><span class="hs-comment">-- #optimizers#</span><span>
</span><span id="line-472"></span><span class="hs-comment">--</span><span>
</span><span id="line-473"></span><span class="hs-comment">-- Optimization implementations are functions that take as input the</span><span>
</span><span id="line-474"></span><span class="hs-comment">-- current parameter values of a model, parameter gradient estimates of the</span><span>
</span><span id="line-475"></span><span class="hs-comment">-- loss function at those parameters for a single batch, and a</span><span>
</span><span id="line-476"></span><span class="hs-comment">-- characteristic learning describing how large a perturbation to make to</span><span>
</span><span id="line-477"></span><span class="hs-comment">-- the parameters in order to reduce the loss. Given those inputs, they</span><span>
</span><span id="line-478"></span><span class="hs-comment">-- output a new set of parameters.</span><span>
</span><span id="line-479"></span><span class="hs-comment">--</span><span>
</span><span id="line-480"></span><span class="hs-comment">-- In the simple case of stochastic gradient descent, the function to</span><span>
</span><span id="line-481"></span><span class="hs-comment">-- output a new set of parameters is to subtract from the current parameter</span><span>
</span><span id="line-482"></span><span class="hs-comment">-- \(\theta\), the gradient of the loss \(\nabla J\) scaled by the learning</span><span>
</span><span id="line-483"></span><span class="hs-comment">-- rate \(\eta\):</span><span>
</span><span id="line-484"></span><span class="hs-comment">--</span><span>
</span><span id="line-485"></span><span class="hs-comment">-- \[\theta_{i+1} = \theta_i - \eta \nabla J(\theta)\]</span><span>
</span><span id="line-486"></span><span class="hs-comment">--</span><span>
</span><span id="line-487"></span><span class="hs-comment">-- While stochastic gradient descent is a stateless function of the</span><span>
</span><span id="line-488"></span><span class="hs-comment">-- parameters, loss, and gradient, some optimizers have a notion of</span><span>
</span><span id="line-489"></span><span class="hs-comment">-- internal state that is propagated from one step to the step, for</span><span>
</span><span id="line-490"></span><span class="hs-comment">-- example, retaining and updating momentum between steps:</span><span>
</span><span id="line-491"></span><span class="hs-comment">--</span><span>
</span><span id="line-492"></span><span class="hs-comment">-- \[\begin{gathered}</span><span>
</span><span id="line-493"></span><span class="hs-comment">--     \Delta \theta_i = \alpha \Delta \theta_{i-1} - \eta \nabla J(\theta) \\</span><span>
</span><span id="line-494"></span><span class="hs-comment">--     \theta_{i+1} = \theta_i + \Delta \theta_i</span><span>
</span><span id="line-495"></span><span class="hs-comment">-- \end{gathered}\]</span><span>
</span><span id="line-496"></span><span class="hs-comment">--</span><span>
</span><span id="line-497"></span><span class="hs-comment">-- In this case, the momentum term \(\Delta \theta_i\) is carried forward</span><span>
</span><span id="line-498"></span><span class="hs-comment">-- as internal state of the optimizer that is propagated to the next step.</span><span>
</span><span id="line-499"></span><span class="hs-comment">-- \(\alpha\) is an optimizer parameter which determines a weighting on the</span><span>
</span><span id="line-500"></span><span class="hs-comment">-- momentum term relative to the gradient.</span><span>
</span><span id="line-501"></span><span class="hs-comment">--</span><span>
</span><span id="line-502"></span><span class="hs-comment">-- Implementation of an optimizer consists of defining an ADT describing</span><span>
</span><span id="line-503"></span><span class="hs-comment">-- the optimizer state and a @step@ function that implements a single step</span><span>
</span><span id="line-504"></span><span class="hs-comment">-- perturbation given the learning rate, loss gradients, current</span><span>
</span><span id="line-505"></span><span class="hs-comment">-- parameters, and optimizer state.</span><span>
</span><span id="line-506"></span><span class="hs-comment">--</span><span>
</span><span id="line-507"></span><span class="hs-comment">-- This function interface is described in the 'Torch.Optim.Optimizer'</span><span>
</span><span id="line-508"></span><span class="hs-comment">-- typeclass interface:</span><span>
</span><span id="line-509"></span><span class="hs-comment">--</span><span>
</span><span id="line-510"></span><span class="hs-comment">-- &gt;  class Optimizer o where</span><span>
</span><span id="line-511"></span><span class="hs-comment">-- &gt;      step :: LearningRate -&gt; Gradients -&gt; [Tensor] -&gt; o -&gt; ([Tensor], o)</span><span>
</span><span id="line-512"></span><span class="hs-comment">--</span><span>
</span><span id="line-513"></span><span class="hs-comment">-- @Gradients@ is a newtype wrapper around a list of tensors to make</span><span>
</span><span id="line-514"></span><span class="hs-comment">-- intent explicit: @newtype Gradients = Gradients [Tensor]@.</span><span>
</span><span id="line-515"></span><span class="hs-comment">--</span><span>
</span><span id="line-516"></span><span class="hs-comment">-- Hasktorch provides built-in optimizer implementations in @Torch.Optim@.</span><span>
</span><span id="line-517"></span><span class="hs-comment">-- Some illustrative example implementations follow.</span><span>
</span><span id="line-518"></span><span class="hs-comment">--</span><span>
</span><span id="line-519"></span><span class="hs-comment">-- Being stateless, stochastic gradient descent has an ADT that has only</span><span>
</span><span id="line-520"></span><span class="hs-comment">-- one constructor value:</span><span>
</span><span id="line-521"></span><span class="hs-comment">--</span><span>
</span><span id="line-522"></span><span class="hs-comment">-- &gt;  data GD = GD</span><span>
</span><span id="line-523"></span><span class="hs-comment">--</span><span>
</span><span id="line-524"></span><span class="hs-comment">-- and implements the step function as:</span><span>
</span><span id="line-525"></span><span class="hs-comment">--</span><span>
</span><span id="line-526"></span><span class="hs-comment">-- &gt;  instance Optimizer GD where</span><span>
</span><span id="line-527"></span><span class="hs-comment">-- &gt;      step lr gradients depParameters dummy = (gd lr gradients depParameters, dummy)</span><span>
</span><span id="line-528"></span><span class="hs-comment">-- &gt;          where</span><span>
</span><span id="line-529"></span><span class="hs-comment">-- &gt;          step p dp = p - (lr * dp)</span><span>
</span><span id="line-530"></span><span class="hs-comment">-- &gt;          gd lr (Gradients gradients) parameters = zipWith step parameters gradients</span><span>
</span><span id="line-531"></span><span class="hs-comment">--</span><span>
</span><span id="line-532"></span><span class="hs-comment">-- The use of an optimizer was illustrated in the linear regression example</span><span>
</span><span id="line-533"></span><span class="hs-comment">-- using the function @runStep@</span><span>
</span><span id="line-534"></span><span class="hs-comment">--</span><span>
</span><span id="line-535"></span><span class="hs-comment">-- &gt;  (newParam, _) &lt;- runStep state GD loss 5e-3</span><span>
</span><span id="line-536"></span><span class="hs-comment">--</span><span>
</span><span id="line-537"></span><span class="hs-comment">-- In this case the new optimizer state returned is ignored (as @_@) since</span><span>
</span><span id="line-538"></span><span class="hs-comment">-- gradient descent does not have any internal state. Under the hood,</span><span>
</span><span id="line-539"></span><span class="hs-comment">-- @runStep@ does a little bookkeeping making independent variables from a</span><span>
</span><span id="line-540"></span><span class="hs-comment">-- model, computing gradients, and passing values to the @step@ function.</span><span>
</span><span id="line-541"></span><span class="hs-comment">-- Usually a user can ignore the details and just pass model parameters and</span><span>
</span><span id="line-542"></span><span class="hs-comment">-- the optimizer to runStep as an abstracted interface which takes</span><span>
</span><span id="line-543"></span><span class="hs-comment">-- parameter values, the optimizer value, loss (a tensor), and learning</span><span>
</span><span id="line-544"></span><span class="hs-comment">-- rate as input and returns new parameters and an updated optimizer value.</span><span>
</span><span id="line-545"></span><span class="hs-comment">--</span><span>
</span><span id="line-546"></span><span class="hs-comment">-- &gt;  runStep :: (Parameterized p, Optimizer o) =&gt;</span><span>
</span><span id="line-547"></span><span class="hs-comment">-- &gt;          p -&gt; o -&gt; Tensor -&gt; LearningRate -&gt; IO ([Parameter], o)</span><span>
</span><span id="line-548"></span><span class="hs-comment">--</span><span>
</span><span id="line-549"></span><span class="hs-comment">-- = Typed Tensors</span><span>
</span><span id="line-550"></span><span class="hs-comment">-- #typed-tensors#</span><span>
</span><span id="line-551"></span><span class="hs-comment">--</span><span>
</span><span id="line-552"></span><span class="hs-comment">-- Typed tensors provide an alternative API for which tensor</span><span>
</span><span id="line-553"></span><span class="hs-comment">-- characteristics are encoded in the type of the tensor. The Hasktorch</span><span>
</span><span id="line-554"></span><span class="hs-comment">-- library is layered such that [ TODO ]</span><span>
</span><span id="line-555"></span><span class="hs-comment">--</span><span>
</span><span id="line-556"></span><span class="hs-comment">-- Using typed tensors increases the expressiveness of program invariants</span><span>
</span><span id="line-557"></span><span class="hs-comment">-- that can be automatically checked by GHC at the cost of needing to be</span><span>
</span><span id="line-558"></span><span class="hs-comment">-- more explicit in writing code and also requiring working with Haskell&#8217;s</span><span>
</span><span id="line-559"></span><span class="hs-comment">-- type-level machinery. Type-level Haskell programming has arisen through</span><span>
</span><span id="line-560"></span><span class="hs-comment">-- progressive compiler iteration leading to mechanisms that have a higher</span><span>
</span><span id="line-561"></span><span class="hs-comment">-- degree of complexity compared to value-level Haskell code or other</span><span>
</span><span id="line-562"></span><span class="hs-comment">-- languages such as Idris which were designed with type-level computations</span><span>
</span><span id="line-563"></span><span class="hs-comment">-- from inception. In spite of these compromises, Haskell offers enough</span><span>
</span><span id="line-564"></span><span class="hs-comment">-- capability to express powerful type-level representations of models that</span><span>
</span><span id="line-565"></span><span class="hs-comment">-- is matched by few other languages used for production applications.</span><span>
</span><span id="line-566"></span><span class="hs-comment">--</span><span>
</span><span id="line-567"></span><span class="hs-comment">-- Things we can do in typed Hasktorch:</span><span>
</span><span id="line-568"></span><span class="hs-comment">--</span><span>
</span><span id="line-569"></span><span class="hs-comment">-- -   specify, check, and infer tensor shapes at compile time</span><span>
</span><span id="line-570"></span><span class="hs-comment">--</span><span>
</span><span id="line-571"></span><span class="hs-comment">-- -   specify, check, and infer tensor data types at compile time</span><span>
</span><span id="line-572"></span><span class="hs-comment">--</span><span>
</span><span id="line-573"></span><span class="hs-comment">-- -   specify, check, and infer tensor compute devices at compile time</span><span>
</span><span id="line-574"></span><span class="hs-comment">--</span><span>
</span><span id="line-575"></span><span class="hs-comment">-- We can encode all Hasktorch tensor shapes on the type level using</span><span>
</span><span id="line-576"></span><span class="hs-comment">-- type-level lists and natural numbers:</span><span>
</span><span id="line-577"></span><span class="hs-comment">--</span><span>
</span><span id="line-578"></span><span class="hs-comment">-- &gt;  type EmptyShape = '[]</span><span>
</span><span id="line-579"></span><span class="hs-comment">-- &gt;  type OneDimensionalShape (a :: Nat) = '[a]</span><span>
</span><span id="line-580"></span><span class="hs-comment">-- &gt;  type TwoDimensionalShape (a :: Nat) (b :: Nat) = '[a, b]</span><span>
</span><span id="line-581"></span><span class="hs-comment">-- &gt;  ...</span><span>
</span><span id="line-582"></span><span class="hs-comment">--</span><span>
</span><span id="line-583"></span><span class="hs-comment">-- Tensor data types and compute device types are lifted to the type level</span><span>
</span><span id="line-584"></span><span class="hs-comment">-- using the @DataKinds@ language extension:</span><span>
</span><span id="line-585"></span><span class="hs-comment">--</span><span>
</span><span id="line-586"></span><span class="hs-comment">-- &gt;  type BooleanCPUTensor (shape :: [Nat]) = Tensor '(CPU,  0) 'Bool  shape</span><span>
</span><span id="line-587"></span><span class="hs-comment">-- &gt;  type IntCUDATensor    (shape :: [Nat]) = Tensor '(CUDA, 1) 'Int64 shape</span><span>
</span><span id="line-588"></span><span class="hs-comment">--</span><span>
</span><span id="line-589"></span><span class="hs-comment">-- Devices are represented as tuples consisting of a @DeviceType@ (here</span><span>
</span><span id="line-590"></span><span class="hs-comment">-- @CPU@ for the CPU and @CUDA@ for a CUDA device, respectively) and a</span><span>
</span><span id="line-591"></span><span class="hs-comment">-- device id (here the @Nat@s @0@ and @1@, respectively).</span><span>
</span><span id="line-592"></span><span class="hs-comment">--</span><span>
</span><span id="line-593"></span><span class="hs-comment">-- It is a common misconception that specifying tensor properties at</span><span>
</span><span id="line-594"></span><span class="hs-comment">-- compile time is only possible if all tensor properties are constants and</span><span>
</span><span id="line-595"></span><span class="hs-comment">-- are statically known. If this were the case, then we could only write</span><span>
</span><span id="line-596"></span><span class="hs-comment">-- functions over fully specified tensors, say,</span><span>
</span><span id="line-597"></span><span class="hs-comment">--</span><span>
</span><span id="line-598"></span><span class="hs-comment">-- &gt;  boring :: BooleanCPUTensor '[] -&gt; BooleanCPUTensor '[]</span><span>
</span><span id="line-599"></span><span class="hs-comment">-- &gt;  boring = id</span><span>
</span><span id="line-600"></span><span class="hs-comment">--</span><span>
</span><span id="line-601"></span><span class="hs-comment">-- Fortunately, Haskell has the ability to reason about type variables.</span><span>
</span><span id="line-602"></span><span class="hs-comment">-- This feature is called parametric polymorphism. Consider this simple</span><span>
</span><span id="line-603"></span><span class="hs-comment">-- example of a function:</span><span>
</span><span id="line-604"></span><span class="hs-comment">--</span><span>
</span><span id="line-605"></span><span class="hs-comment">-- &gt;  tensorNoOp</span><span>
</span><span id="line-606"></span><span class="hs-comment">-- &gt;    :: forall (shape :: [Nat]) (dtype :: DType) (device :: (DeviceType, Nat))</span><span>
</span><span id="line-607"></span><span class="hs-comment">-- &gt;     . Tensor device dtype shape</span><span>
</span><span id="line-608"></span><span class="hs-comment">-- &gt;    -&gt; Tensor device dtype shape</span><span>
</span><span id="line-609"></span><span class="hs-comment">-- &gt;  tensorNoOp = id</span><span>
</span><span id="line-610"></span><span class="hs-comment">--</span><span>
</span><span id="line-611"></span><span class="hs-comment">-- Here, @shape@, @dtype@, and @device@ are type variables that have been</span><span>
</span><span id="line-612"></span><span class="hs-comment">-- constrained to be of kind shape, data type, and device, respectively.</span><span>
</span><span id="line-613"></span><span class="hs-comment">-- The universal quantifier @forall@ implies that this function is</span><span>
</span><span id="line-614"></span><span class="hs-comment">-- well-defined for all inhabitants of the types that are compatible with</span><span>
</span><span id="line-615"></span><span class="hs-comment">-- the type variables and their constraints.</span><span>
</span><span id="line-616"></span><span class="hs-comment">--</span><span>
</span><span id="line-617"></span><span class="hs-comment">-- The @tensorNoOp@ function may seem trivial, and that is because its type</span><span>
</span><span id="line-618"></span><span class="hs-comment">-- is very strongly constrained: Given any typed tensor, it must return a</span><span>
</span><span id="line-619"></span><span class="hs-comment">-- tensor of the same shape and with the same data type and on the same</span><span>
</span><span id="line-620"></span><span class="hs-comment">-- device. Besides by means of the identity, @id@, there are not many ways</span><span>
</span><span id="line-621"></span><span class="hs-comment">-- in which this function can be implemented.</span><span>
</span><span id="line-622"></span><span class="hs-comment">--</span><span>
</span><span id="line-623"></span><span class="hs-comment">-- There is a connection between a type signature of a function and</span><span>
</span><span id="line-624"></span><span class="hs-comment">-- mathematical proofs. We can say that the fact that a function exists is</span><span>
</span><span id="line-625"></span><span class="hs-comment">-- witnessed by its implementation. The implementation, @tensorNoOp = id@,</span><span>
</span><span id="line-626"></span><span class="hs-comment">-- is the proof of the theorem stated by @tensorNoOp@&#8217;s type signature. And</span><span>
</span><span id="line-627"></span><span class="hs-comment">-- here we have a proof that we can run this function on any device and for</span><span>
</span><span id="line-628"></span><span class="hs-comment">-- any data type and tensor shape.</span><span>
</span><span id="line-629"></span><span class="hs-comment">--</span><span>
</span><span id="line-630"></span><span class="hs-comment">-- This is the essence of typed Hasktorch. As soon as the compiler gives</span><span>
</span><span id="line-631"></span><span class="hs-comment">-- its OK, we hold a proof that our program will run without shape or CUDA</span><span>
</span><span id="line-632"></span><span class="hs-comment">-- errors.</span><span>
</span><span id="line-633"></span><span class="hs-comment">--</span><span>
</span><span id="line-634"></span><span class="hs-comment">-- #notes#</span><span>
</span><span id="line-635"></span><span class="hs-comment">--</span><span>
</span><span id="line-636"></span><span class="hs-comment">-- 1.  PyTorch users will be familiar with this as the @requires_grad@</span><span>
</span><span id="line-637"></span><span class="hs-comment">--     member variable for the PyTorch tensor type. The Hasktorch mechanism</span><span>
</span><span id="line-638"></span><span class="hs-comment">--     is distinct from PyTorch&#8217;s mechanism - by only allowing gradients to</span><span>
</span><span id="line-639"></span><span class="hs-comment">--     be applied in the context of a set of @IndependentTensor@ variables,</span><span>
</span><span id="line-640"></span><span class="hs-comment">--     it allows ops to be semantically pure and preserve referential</span><span>
</span><span id="line-641"></span><span class="hs-comment">--     transparency.</span><span>
</span><span id="line-642"></span><span class="hs-comment">--</span><span>
</span><span id="line-643"></span><span class="hs-comment">-- 2.  @foldLoop@ is a convenience function defined in terms of @foldM@ as</span><span>
</span><span id="line-644"></span><span class="hs-comment">--     @foldLoop x count block = foldM block x ([1 .. count] :: [a])@</span><span>
</span><span id="line-645"></span></pre></body></html>