<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE AllowAmbiguousTypes #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE ConstraintKinds #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE DeriveGeneric #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE FlexibleContexts #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE FlexibleInstances #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE GADTs #-}</span><span>
</span><span id="line-8"></span><span class="hs-pragma">{-# LANGUAGE MultiParamTypeClasses #-}</span><span>
</span><span id="line-9"></span><span class="hs-pragma">{-# LANGUAGE NoStarIsType #-}</span><span>
</span><span id="line-10"></span><span class="hs-pragma">{-# LANGUAGE OverloadedLists #-}</span><span>
</span><span id="line-11"></span><span class="hs-pragma">{-# LANGUAGE PartialTypeSignatures #-}</span><span>
</span><span id="line-12"></span><span class="hs-pragma">{-# LANGUAGE PolyKinds #-}</span><span>
</span><span id="line-13"></span><span class="hs-pragma">{-# LANGUAGE QuantifiedConstraints #-}</span><span>
</span><span id="line-14"></span><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><span id="line-15"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-16"></span><span class="hs-pragma">{-# LANGUAGE StrictData #-}</span><span>
</span><span id="line-17"></span><span class="hs-pragma">{-# LANGUAGE TypeApplications #-}</span><span>
</span><span id="line-18"></span><span class="hs-pragma">{-# LANGUAGE TypeFamilies #-}</span><span>
</span><span id="line-19"></span><span class="hs-pragma">{-# LANGUAGE TypeOperators #-}</span><span>
</span><span id="line-20"></span><span class="hs-pragma">{-# LANGUAGE UndecidableInstances #-}</span><span>
</span><span id="line-21"></span><span class="hs-pragma">{-# LANGUAGE UndecidableSuperClasses #-}</span><span>
</span><span id="line-22"></span><span class="hs-pragma">{-# LANGUAGE StandaloneDeriving #-}</span><span>
</span><span id="line-23"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-24"></span><span class="hs-pragma">{-# OPTIONS_GHC -fplugin GHC.TypeLits.Normalise #-}</span><span>
</span><span id="line-25"></span><span class="hs-pragma">{-# OPTIONS_GHC -fplugin GHC.TypeLits.KnownNat.Solver #-}</span><span>
</span><span id="line-26"></span><span class="hs-pragma">{-# OPTIONS_GHC -fplugin GHC.TypeLits.Extra.Solver #-}</span><span>
</span><span id="line-27"></span><span class="hs-pragma">{-# OPTIONS_GHC -fno-warn-partial-type-signatures #-}</span><span>
</span><span id="line-28"></span><span>
</span><span id="line-29"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Typed.NN.Recurrent.GRU</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-30"></span><span>
</span><span id="line-31"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">Prelude</span></span><span>                 </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier">tanh</span></span><span> </span><span class="hs-special">)</span><span>
</span><span id="line-32"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">Data.Kind</span></span><span>
</span><span id="line-33"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.HList.html"><span class="hs-identifier">Torch.HList</span></a></span><span>
</span><span id="line-34"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">Foreign.ForeignPtr</span></span><span>
</span><span id="line-35"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">GHC.Generics</span></span><span>
</span><span id="line-36"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">GHC.TypeLits</span></span><span>
</span><span id="line-37"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="../file:///nix/store/4rdd355v4wjgdxhl3jckcl3acr69ynkg-ghc-typelits-extra-lib-ghc-typelits-extra-0.4-doc/share/doc/ghc-typelits-extra/html/src"><span class="hs-identifier">GHC.TypeLits.Extra</span></a></span><span>
</span><span id="line-38"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">System.Environment</span></span><span>
</span><span id="line-39"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><span class="hs-identifier">System.IO.Unsafe</span></span><span>
</span><span id="line-40"></span><span>
</span><span id="line-41"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Cast</span></a></span><span>                     </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-42"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Class</span></a></span><span>                    </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-43"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Managed.Type.Tensor</span></a></span><span>      </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-44"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier">Torch.Internal.Type</span></a></span><span>                     </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">ATen</span></span><span>
</span><span id="line-45"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Autograd.html"><span class="hs-identifier">Torch.Autograd</span></a></span><span>                </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">A</span></span><span>
</span><span id="line-46"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Device.html"><span class="hs-identifier">Torch.Device</span></a></span><span>                  </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-47"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.DType.html"><span class="hs-identifier">Torch.DType</span></a></span><span>                   </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-48"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Functional.html"><span class="hs-identifier">Torch.Functional</span></a></span><span>              </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-49"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.NN.html"><span class="hs-identifier">Torch.NN</span></a></span><span>                      </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">A</span></span><span>
</span><span id="line-50"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.Tensor.html"><span class="hs-identifier">Torch.Tensor</span></a></span><span>                  </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-51"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="Torch.TensorFactories.html"><span class="hs-identifier">Torch.TensorFactories</span></a></span><span>         </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">D</span></span><span>
</span><span id="line-52"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.Typed.html"><span class="hs-identifier">Torch.Typed</span></a></span><span>
</span><span id="line-53"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.Typed.Factories.html"><span class="hs-identifier">Torch.Typed.Factories</span></a></span><span>
</span><span id="line-54"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.Typed.Functional.html"><span class="hs-identifier">Torch.Typed.Functional</span></a></span><span>      </span><span class="hs-keyword">hiding</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#sqrt"><span class="hs-identifier">sqrt</span></a></span><span> </span><span class="hs-special">)</span><span>
</span><span id="line-55"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.Typed.Tensor.html"><span class="hs-identifier">Torch.Typed.Tensor</span></a></span><span>
</span><span id="line-56"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.Typed.Parameter.html"><span class="hs-identifier">Torch.Typed.Parameter</span></a></span><span>
</span><span id="line-57"></span><span class="hs-keyword">import</span><span>           </span><span class="annot"><a href="Torch.Typed.NN.html"><span class="hs-identifier">Torch.Typed.NN</span></a></span><span>
</span><span id="line-58"></span><span>
</span><span id="line-59"></span><span class="hs-keyword">data</span><span> </span><span id="GRULayerSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-var">GRULayerSpec</span></a></span></span><span>
</span><span id="line-60"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730725"><span class="annot"><a href="#local-6989586621679730725"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-61"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730724"><span class="annot"><a href="#local-6989586621679730724"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-62"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730723"><span class="annot"><a href="#local-6989586621679730723"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-63"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730722"><span class="annot"><a href="#local-6989586621679730722"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-64"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730721"><span class="annot"><a href="#local-6989586621679730721"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-65"></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="GRULayerSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-var">GRULayerSpec</span></a></span></span><span> </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730425"><span id="local-6989586621679730427"><span id="local-6989586621679730429"><span class="annot"><span class="annottext">Int
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; ShowS
[GRULayerSpec inputSize hiddenSize directionality dtype device]
-&gt; ShowS
GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; String
(Int
 -&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
 -&gt; ShowS)
-&gt; (GRULayerSpec inputSize hiddenSize directionality dtype device
    -&gt; String)
-&gt; ([GRULayerSpec inputSize hiddenSize directionality dtype device]
    -&gt; ShowS)
-&gt; Show
     (GRULayerSpec inputSize hiddenSize directionality dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRULayerSpec inputSize hiddenSize directionality dtype device]
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; String
showList :: [GRULayerSpec inputSize hiddenSize directionality dtype device]
-&gt; ShowS
$cshowList :: forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRULayerSpec inputSize hiddenSize directionality dtype device]
-&gt; ShowS
show :: GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; String
$cshow :: forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; String
showsPrec :: Int
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; ShowS
$cshowsPrec :: forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679730420"><span id="local-6989586621679730422"><span class="annot"><span class="annottext">GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; Bool
(GRULayerSpec inputSize hiddenSize directionality dtype device
 -&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
 -&gt; Bool)
-&gt; (GRULayerSpec inputSize hiddenSize directionality dtype device
    -&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
    -&gt; Bool)
-&gt; Eq
     (GRULayerSpec inputSize hiddenSize directionality dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; Bool
/= :: GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; Bool
$c/= :: forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; Bool
== :: GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; Bool
$c== :: forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-66"></span><span>
</span><span id="line-67"></span><span class="hs-keyword">data</span><span> </span><span id="GRULayer"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-var">GRULayer</span></a></span></span><span>
</span><span id="line-68"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730418"><span class="annot"><a href="#local-6989586621679730418"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-69"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730417"><span class="annot"><a href="#local-6989586621679730417"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-70"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730416"><span class="annot"><a href="#local-6989586621679730416"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-71"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730415"><span class="annot"><a href="#local-6989586621679730415"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-72"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730414"><span class="annot"><a href="#local-6989586621679730414"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-73"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-74"></span><span>  </span><span id="local-6989586621679730857"><span id="local-6989586621679730858"><span id="local-6989586621679730859"><span id="local-6989586621679730860"><span id="GRUUnidirectionalLayer"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUUnidirectionalLayer"><span class="hs-identifier hs-var">GRUUnidirectionalLayer</span></a></span></span><span>
</span><span id="line-75"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730860"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730859"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWIShape"><span class="hs-identifier hs-type">GRUWIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730858"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730857"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-76"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730860"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730859"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWHShape"><span class="hs-identifier hs-type">GRUWHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730858"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730857"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-77"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730860"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730859"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBIShape"><span class="hs-identifier hs-type">GRUBIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730858"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730857"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-78"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730860"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730859"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBHShape"><span class="hs-identifier hs-type">GRUBHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730858"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730857"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-79"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730857"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730858"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Unidirectional"><span class="hs-identifier hs-type">Unidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730859"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730860"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span><span>
</span><span id="line-80"></span><span>  </span><span id="local-6989586621679730841"><span id="local-6989586621679730842"><span id="local-6989586621679730843"><span id="local-6989586621679730844"><span id="GRUBidirectionalLayer"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUBidirectionalLayer"><span class="hs-identifier hs-var">GRUBidirectionalLayer</span></a></span></span><span>
</span><span id="line-81"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWIShape"><span class="hs-identifier hs-type">GRUWIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-82"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWHShape"><span class="hs-identifier hs-type">GRUWHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-83"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBIShape"><span class="hs-identifier hs-type">GRUBIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-84"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBHShape"><span class="hs-identifier hs-type">GRUBHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-85"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWIShape"><span class="hs-identifier hs-type">GRUWIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-86"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWHShape"><span class="hs-identifier hs-type">GRUWHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-87"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBIShape"><span class="hs-identifier hs-type">GRUBIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-88"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBHShape"><span class="hs-identifier hs-type">GRUBHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-89"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730841"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730842"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Bidirectional"><span class="hs-identifier hs-type">Bidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730843"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730844"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span><span>
</span><span id="line-90"></span><span>
</span><span id="line-91"></span><span id="local-6989586621679730400"><span id="local-6989586621679730402"><span id="local-6989586621679730404"><span id="local-6989586621679730406"><span id="local-6989586621679730407"><span id="local-6989586621679730408"><span id="local-6989586621679730409"><span id="local-6989586621679730410"><span class="hs-keyword">deriving</span><span> </span><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730410"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730409"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730408"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730407"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730406"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span><span>
</span><span id="line-92"></span><span class="hs-comment">-- deriving instance Generic (GRULayer inputSize hiddenSize directionality dtype device)</span><span>
</span><span id="line-93"></span><span>
</span><span id="line-94"></span><span id="local-6989586621679730391"><span id="local-6989586621679730392"><span id="local-6989586621679730393"><span id="local-6989586621679730394"><span id="local-6989586621679730395"><span id="local-6989586621679730396"><span id="local-6989586621679730397"><span id="local-6989586621679730398"><span id="local-6989586621679730399"><span class="hs-keyword">instance</span><span>
</span><span id="line-95"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679730399"><span class="hs-identifier hs-type">wiShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWIShape"><span class="hs-identifier hs-type">GRUWIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730398"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730397"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-96"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730396"><span class="hs-identifier hs-type">whShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWHShape"><span class="hs-identifier hs-type">GRUWHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730398"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730397"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-97"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730395"><span class="hs-identifier hs-type">biShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBIShape"><span class="hs-identifier hs-type">GRUBIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730398"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730397"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-98"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730394"><span class="hs-identifier hs-type">bhShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBHShape"><span class="hs-identifier hs-type">GRUBHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730398"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730397"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-99"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730393"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730392"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730391"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730399"><span class="hs-identifier hs-type">wiShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730392"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730391"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730396"><span class="hs-identifier hs-type">whShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730392"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730391"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730395"><span class="hs-identifier hs-type">biShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730392"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730391"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730394"><span class="hs-identifier hs-type">bhShape</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-100"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730397"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730398"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Unidirectional"><span class="hs-identifier hs-type">Unidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730391"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730392"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730393"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-101"></span><span>  </span><span id="local-6989586621679730387"><span class="annot"><span class="annottext">gFlattenParameters :: K1 R (GRULayer inputSize hiddenSize 'Unidirectional dtype device) a
-&gt; HList parameters
</span><a href="Torch.Typed.Parameter.html#gFlattenParameters"><span class="hs-identifier hs-var hs-var hs-var hs-var">gFlattenParameters</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUUnidirectionalLayer"><span class="hs-identifier hs-type">GRUUnidirectionalLayer</span></a></span><span> </span><span id="local-6989586621679730384"><span class="annot"><span class="annottext">wi :: Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730384"><span class="hs-identifier hs-var">wi</span></a></span></span><span> </span><span id="local-6989586621679730383"><span class="annot"><span class="annottext">wh :: Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730383"><span class="hs-identifier hs-var">wh</span></a></span></span><span> </span><span id="local-6989586621679730382"><span class="annot"><span class="annottext">bi :: Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730382"><span class="hs-identifier hs-var">bi</span></a></span></span><span> </span><span id="local-6989586621679730381"><span class="annot"><span class="annottext">bh :: Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730381"><span class="hs-identifier hs-var">bh</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-102"></span><span>    </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730384"><span class="hs-identifier hs-var">wi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730383"><span class="hs-identifier hs-var">wh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730382"><span class="hs-identifier hs-var">bi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730381"><span class="hs-identifier hs-var">bh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; HList '[]
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
forall k. HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span>
</span><span id="line-103"></span><span>  </span><span id="local-6989586621679730378"><span class="annot"><span class="annottext">gReplaceParameters :: K1 R (GRULayer inputSize hiddenSize 'Unidirectional dtype device) a
-&gt; HList parameters
-&gt; K1
     R (GRULayer inputSize hiddenSize 'Unidirectional dtype device) a
</span><a href="Torch.Typed.Parameter.html#gReplaceParameters"><span class="hs-identifier hs-var hs-var hs-var hs-var">gReplaceParameters</span></a></span></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730376"><span class="annot"><a href="#local-6989586621679730376"><span class="hs-identifier hs-var">wi</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730375"><span class="annot"><a href="#local-6989586621679730375"><span class="hs-identifier hs-var">wh</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730374"><span class="annot"><a href="#local-6989586621679730374"><span class="hs-identifier hs-var">bi</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730373"><span class="annot"><a href="#local-6989586621679730373"><span class="hs-identifier hs-var">bh</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span class="annot"><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-type">HNil</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-104"></span><span>    </span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize 'Unidirectional dtype device
-&gt; K1
     R (GRULayer inputSize hiddenSize 'Unidirectional dtype device) a
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (inputSize :: Nat).
Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUUnidirectionalLayer"><span class="hs-identifier hs-var">GRUUnidirectionalLayer</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730376"><span class="hs-identifier hs-var">wi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730375"><span class="hs-identifier hs-var">wh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730374"><span class="hs-identifier hs-var">bi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730373"><span class="hs-identifier hs-var">bh</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-105"></span><span>
</span><span id="line-106"></span><span id="local-6989586621679730364"><span id="local-6989586621679730365"><span id="local-6989586621679730366"><span id="local-6989586621679730367"><span id="local-6989586621679730368"><span id="local-6989586621679730369"><span id="local-6989586621679730370"><span id="local-6989586621679730371"><span id="local-6989586621679730372"><span class="hs-keyword">instance</span><span>
</span><span id="line-107"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679730372"><span class="hs-identifier hs-type">wiShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWIShape"><span class="hs-identifier hs-type">GRUWIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730371"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730370"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-108"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730369"><span class="hs-identifier hs-type">whShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUWHShape"><span class="hs-identifier hs-type">GRUWHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730371"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730370"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-109"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730368"><span class="hs-identifier hs-type">biShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBIShape"><span class="hs-identifier hs-type">GRUBIShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730371"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730370"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730367"><span class="hs-identifier hs-type">bhShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#GRUBHShape"><span class="hs-identifier hs-type">GRUBHShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730371"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730370"><span class="hs-identifier hs-type">inputSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-111"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730366"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730372"><span class="hs-identifier hs-type">wiShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730369"><span class="hs-identifier hs-type">whShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730368"><span class="hs-identifier hs-type">biShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730367"><span class="hs-identifier hs-type">bhShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730372"><span class="hs-identifier hs-type">wiShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730369"><span class="hs-identifier hs-type">whShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730368"><span class="hs-identifier hs-type">biShape</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730367"><span class="hs-identifier hs-type">bhShape</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-112"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730370"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730371"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Bidirectional"><span class="hs-identifier hs-type">Bidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730364"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730365"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730366"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-113"></span><span>  </span><span id="local-6989586621679730361"><span class="annot"><span class="annottext">gFlattenParameters :: K1 R (GRULayer inputSize hiddenSize 'Bidirectional dtype device) a
-&gt; HList parameters
</span><a href="#local-6989586621679730361"><span class="hs-identifier hs-var hs-var hs-var hs-var">gFlattenParameters</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUBidirectionalLayer"><span class="hs-identifier hs-type">GRUBidirectionalLayer</span></a></span><span> </span><span id="local-6989586621679730360"><span class="annot"><span class="annottext">wi :: Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730360"><span class="hs-identifier hs-var">wi</span></a></span></span><span> </span><span id="local-6989586621679730359"><span class="annot"><span class="annottext">wh :: Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730359"><span class="hs-identifier hs-var">wh</span></a></span></span><span> </span><span id="local-6989586621679730358"><span class="annot"><span class="annottext">bi :: Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730358"><span class="hs-identifier hs-var">bi</span></a></span></span><span> </span><span id="local-6989586621679730357"><span class="annot"><span class="annottext">bh :: Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730357"><span class="hs-identifier hs-var">bh</span></a></span></span><span> </span><span id="local-6989586621679730356"><span class="annot"><span class="annottext">wi' :: Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730356"><span class="hs-identifier hs-var">wi'</span></a></span></span><span> </span><span id="local-6989586621679730355"><span class="annot"><span class="annottext">wh' :: Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730355"><span class="hs-identifier hs-var">wh'</span></a></span></span><span> </span><span id="local-6989586621679730354"><span class="annot"><span class="annottext">bi' :: Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730354"><span class="hs-identifier hs-var">bi'</span></a></span></span><span> </span><span id="local-6989586621679730353"><span class="annot"><span class="annottext">bh' :: Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730353"><span class="hs-identifier hs-var">bh'</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-114"></span><span>    </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730360"><span class="hs-identifier hs-var">wi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730359"><span class="hs-identifier hs-var">wh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730358"><span class="hs-identifier hs-var">bi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730357"><span class="hs-identifier hs-var">bh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730356"><span class="hs-identifier hs-var">wi'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUWIShape hiddenSize inputSize),
       Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730355"><span class="hs-identifier hs-var">wh'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUWHShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730354"><span class="hs-identifier hs-var">bi'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize)]
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize),
       Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730353"><span class="hs-identifier hs-var">bh'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; HList '[]
-&gt; HList
     '[Parameter device dtype (GRUBIShape hiddenSize inputSize)]
forall x (xs :: [Type]). x -&gt; HList xs -&gt; HList (x : xs)
</span><a href="Torch.HList.html#%3A."><span class="hs-operator hs-var">:.</span></a></span><span> </span><span class="annot"><span class="annottext">HList '[]
forall k. HList '[]
</span><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-var">HNil</span></a></span><span>
</span><span id="line-115"></span><span>  </span><span id="local-6989586621679730352"><span class="annot"><span class="annottext">gReplaceParameters :: K1 R (GRULayer inputSize hiddenSize 'Bidirectional dtype device) a
-&gt; HList parameters
-&gt; K1
     R (GRULayer inputSize hiddenSize 'Bidirectional dtype device) a
</span><a href="#local-6989586621679730352"><span class="hs-identifier hs-var hs-var hs-var hs-var">gReplaceParameters</span></a></span></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730351"><span class="annot"><a href="#local-6989586621679730351"><span class="hs-identifier hs-var">wi</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730350"><span class="annot"><a href="#local-6989586621679730350"><span class="hs-identifier hs-var">wh</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730349"><span class="annot"><a href="#local-6989586621679730349"><span class="hs-identifier hs-var">bi</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730348"><span class="annot"><a href="#local-6989586621679730348"><span class="hs-identifier hs-var">bh</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730347"><span class="annot"><a href="#local-6989586621679730347"><span class="hs-identifier hs-var">wi'</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730346"><span class="annot"><a href="#local-6989586621679730346"><span class="hs-identifier hs-var">wh'</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730345"><span class="annot"><a href="#local-6989586621679730345"><span class="hs-identifier hs-var">bi'</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span id="local-6989586621679730344"><span class="annot"><a href="#local-6989586621679730344"><span class="hs-identifier hs-var">bh'</span></a></span></span><span> </span><span class="annot"><a href="Torch.HList.html#%3A."><span class="hs-operator hs-type">:.</span></a></span><span> </span><span class="annot"><a href="Torch.HList.html#HNil"><span class="hs-identifier hs-type">HNil</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-116"></span><span>    </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize 'Bidirectional dtype device
-&gt; K1
     R (GRULayer inputSize hiddenSize 'Bidirectional dtype device) a
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (inputSize :: Nat).
Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUBidirectionalLayer"><span class="hs-identifier hs-var">GRUBidirectionalLayer</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730351"><span class="hs-identifier hs-var">wi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730350"><span class="hs-identifier hs-var">wh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730349"><span class="hs-identifier hs-var">bi</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730348"><span class="hs-identifier hs-var">bh</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730347"><span class="hs-identifier hs-var">wi'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWHShape hiddenSize inputSize)
</span><a href="#local-6989586621679730346"><span class="hs-identifier hs-var">wh'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730345"><span class="hs-identifier hs-var">bi'</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter device dtype (GRUBIShape hiddenSize inputSize)
</span><a href="#local-6989586621679730344"><span class="hs-identifier hs-var">bh'</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-117"></span><span>
</span><span id="line-118"></span><span id="local-6989586621679730340"><span id="local-6989586621679730341"><span id="local-6989586621679730342"><span id="local-6989586621679730343"><span class="hs-keyword">instance</span><span>
</span><span id="line-119"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730343"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730342"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-120"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730341"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-121"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730340"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-122"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730342"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-123"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730343"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-124"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-type">GRULayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730341"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730340"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Unidirectional"><span class="hs-identifier hs-type">Unidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730342"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730343"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-125"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730341"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730340"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Unidirectional"><span class="hs-identifier hs-type">Unidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730342"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730343"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-126"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-127"></span><span>  </span><span id="local-6989586621679730337"><span class="annot"><span class="annottext">sample :: GRULayerSpec inputSize hiddenSize 'Unidirectional dtype device
-&gt; IO (GRULayer inputSize hiddenSize 'Unidirectional dtype device)
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-128"></span><span>    </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (inputSize :: Nat).
Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUUnidirectionalLayer"><span class="hs-identifier hs-var">GRUUnidirectionalLayer</span></a></span><span>
</span><span id="line-129"></span><span>      </span><span class="annot"><span class="annottext">(Parameter device dtype (GRUWIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUWIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUWIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUWIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype (GRUWIShape hiddenSize inputSize))
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (featureSize :: Nat).
(KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize,
 KnownDevice device, RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var">xavierUniformGRU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-130"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUWHShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUWHShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUWHShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype (GRUWHShape hiddenSize inputSize))
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (featureSize :: Nat).
(KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize,
 KnownDevice device, RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var">xavierUniformGRU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-131"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-132"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Unidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (GRULayer inputSize hiddenSize 'Unidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span class="hs-special">)</span></span></span></span></span><span>
</span><span id="line-133"></span><span>
</span><span id="line-134"></span><span id="local-6989586621679730327"><span id="local-6989586621679730328"><span id="local-6989586621679730329"><span id="local-6989586621679730330"><span class="hs-keyword">instance</span><span>
</span><span id="line-135"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730330"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730329"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-136"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730328"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-137"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730327"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-138"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730329"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-139"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730330"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-140"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-type">GRULayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730328"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730327"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Bidirectional"><span class="hs-identifier hs-type">Bidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730329"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730330"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-141"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730328"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730327"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.Functional.html#Bidirectional"><span class="hs-identifier hs-type">Bidirectional</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730329"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730330"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-142"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-143"></span><span>  </span><span id="local-6989586621679730325"><span class="annot"><span class="annottext">sample :: GRULayerSpec inputSize hiddenSize 'Bidirectional dtype device
-&gt; IO (GRULayer inputSize hiddenSize 'Bidirectional dtype device)
</span><a href="#local-6989586621679730325"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-144"></span><span>    </span><span class="annot"><span class="annottext">Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (inputSize :: Nat).
Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
-&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUBidirectionalLayer"><span class="hs-identifier hs-var">GRUBidirectionalLayer</span></a></span><span>
</span><span id="line-145"></span><span>      </span><span class="annot"><span class="annottext">(Parameter device dtype (GRUWIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUWIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUWIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUWIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype (GRUWIShape hiddenSize inputSize))
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (featureSize :: Nat).
(KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize,
 KnownDevice device, RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var">xavierUniformGRU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-146"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUWHShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUWHShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUWHShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype (GRUWHShape hiddenSize inputSize))
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (featureSize :: Nat).
(KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize,
 KnownDevice device, RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var">xavierUniformGRU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-147"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-148"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUWIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-149"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUWIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUWHShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUWIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUWIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUWIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUWIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype (GRUWIShape hiddenSize inputSize))
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (featureSize :: Nat).
(KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize,
 KnownDevice device, RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var">xavierUniformGRU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-150"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUWHShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUWHShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUWHShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUWHShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUWHShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype (GRUWHShape hiddenSize inputSize))
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (hiddenSize :: Nat) (featureSize :: Nat).
(KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize,
 KnownDevice device, RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var">xavierUniformGRU</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-151"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO
     (Parameter device dtype (GRUBIShape hiddenSize inputSize)
      -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-152"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter device dtype (GRUBIShape hiddenSize inputSize)
   -&gt; GRULayer inputSize hiddenSize 'Bidirectional dtype device)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (GRULayer inputSize hiddenSize 'Bidirectional dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor device dtype (GRUBIShape hiddenSize inputSize)
 -&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize)))
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
-&gt; IO (Parameter device dtype (GRUBIShape hiddenSize inputSize))
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
-&gt; IO (Tensor device dtype (GRUBIShape hiddenSize inputSize))
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype (GRUBIShape hiddenSize inputSize)
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span class="hs-special">)</span></span></span></span></span><span>
</span><span id="line-153"></span><span>
</span><span id="line-154"></span><span class="hs-keyword">data</span><span> </span><span id="GRULayerStackSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-var">GRULayerStackSpec</span></a></span></span><span>
</span><span id="line-155"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730714"><span class="annot"><a href="#local-6989586621679730714"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-156"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730713"><span class="annot"><a href="#local-6989586621679730713"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-157"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730712"><span class="annot"><a href="#local-6989586621679730712"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-158"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730711"><span class="annot"><a href="#local-6989586621679730711"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-159"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730710"><span class="annot"><a href="#local-6989586621679730710"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-160"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730709"><span class="annot"><a href="#local-6989586621679730709"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-161"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span id="GRULayerStackSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-var">GRULayerStackSpec</span></a></span></span><span> </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730318"><span id="local-6989586621679730320"><span id="local-6989586621679730322"><span class="annot"><span class="annottext">Int
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
[GRULayerStackSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; String
(Int
 -&gt; GRULayerStackSpec
      inputSize hiddenSize numLayers directionality dtype device
 -&gt; ShowS)
-&gt; (GRULayerStackSpec
      inputSize hiddenSize numLayers directionality dtype device
    -&gt; String)
-&gt; ([GRULayerStackSpec
       inputSize hiddenSize numLayers directionality dtype device]
    -&gt; ShowS)
-&gt; Show
     (GRULayerStackSpec
        inputSize hiddenSize numLayers directionality dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRULayerStackSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; String
showList :: [GRULayerStackSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
$cshowList :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRULayerStackSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
show :: GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; String
$cshow :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; String
showsPrec :: Int
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
$cshowsPrec :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679730314"><span id="local-6989586621679730316"><span class="annot"><span class="annottext">GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; Bool
(GRULayerStackSpec
   inputSize hiddenSize numLayers directionality dtype device
 -&gt; GRULayerStackSpec
      inputSize hiddenSize numLayers directionality dtype device
 -&gt; Bool)
-&gt; (GRULayerStackSpec
      inputSize hiddenSize numLayers directionality dtype device
    -&gt; GRULayerStackSpec
         inputSize hiddenSize numLayers directionality dtype device
    -&gt; Bool)
-&gt; Eq
     (GRULayerStackSpec
        inputSize hiddenSize numLayers directionality dtype device)
forall a. (a -&gt; a -&gt; Bool) -&gt; (a -&gt; a -&gt; Bool) -&gt; Eq a
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; Bool
/= :: GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; Bool
$c/= :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; Bool
== :: GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; Bool
$c== :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStackSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; Bool
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var">Eq</span></span></span></span><span class="hs-special">)</span><span>
</span><span id="line-162"></span><span>
</span><span id="line-163"></span><span class="hs-comment">-- Input-to-hidden, hidden-to-hidden, and bias parameters for a mulilayered</span><span>
</span><span id="line-164"></span><span class="hs-comment">-- (and optionally) bidirectional GRU.</span><span>
</span><span id="line-165"></span><span class="hs-comment">--</span><span>
</span><span id="line-166"></span><span class="hs-keyword">data</span><span> </span><span id="GRULayerStack"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-var">GRULayerStack</span></a></span></span><span>
</span><span id="line-167"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730313"><span class="annot"><a href="#local-6989586621679730313"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-168"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730312"><span class="annot"><a href="#local-6989586621679730312"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-169"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730311"><span class="annot"><a href="#local-6989586621679730311"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-170"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730310"><span class="annot"><a href="#local-6989586621679730310"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-171"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730309"><span class="annot"><a href="#local-6989586621679730309"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-172"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730308"><span class="annot"><a href="#local-6989586621679730308"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-173"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-174"></span><span>  </span><span id="local-6989586621679730770"><span id="local-6989586621679730771"><span id="local-6989586621679730772"><span id="local-6989586621679730773"><span id="local-6989586621679730774"><span id="GRULayer1"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer1"><span class="hs-identifier hs-var">GRULayer1</span></a></span></span><span>
</span><span id="line-175"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730774"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730773"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730772"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730771"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730770"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-176"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730774"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730773"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><a href="#local-6989586621679730772"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730771"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730770"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span></span><span>
</span><span id="line-177"></span><span>  </span><span id="local-6989586621679730734"><span id="local-6989586621679730735"><span id="local-6989586621679730736"><span id="local-6989586621679730737"><span id="local-6989586621679730738"><span id="local-6989586621679730739"><span id="GRULayerK"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerK"><span class="hs-identifier hs-var">GRULayerK</span></a></span></span><span>
</span><span id="line-178"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679730739"><span class="hs-identifier hs-type">numLayers</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-179"></span><span>    </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730738"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730737"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730739"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730736"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730735"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730734"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-180"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730737"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730736"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730737"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730736"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730735"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730734"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-181"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730738"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730737"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730739"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730736"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730735"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730734"><span class="hs-identifier hs-type">device</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-182"></span><span>
</span><span id="line-183"></span><span id="local-6989586621679730294"><span id="local-6989586621679730296"><span id="local-6989586621679730298"><span id="local-6989586621679730300"><span id="local-6989586621679730301"><span id="local-6989586621679730302"><span id="local-6989586621679730303"><span id="local-6989586621679730304"><span id="local-6989586621679730305"><span class="hs-keyword">deriving</span><span> </span><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730305"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730304"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730303"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730302"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730301"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730300"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-184"></span><span class="hs-comment">--  TODO: Generics? see https://gist.github.com/RyanGlScott/71d9f933e823b4a03f99de54d4b94d51</span><span>
</span><span id="line-185"></span><span class="hs-comment">-- deriving instance Generic (GRULayerStack inputSize hiddenSize numLayers directionality dtype device)</span><span>
</span><span id="line-186"></span><span>
</span><span id="line-187"></span><span id="local-6989586621679730287"><span id="local-6989586621679730288"><span id="local-6989586621679730289"><span id="local-6989586621679730290"><span id="local-6989586621679730291"><span id="local-6989586621679730292"><span id="local-6989586621679730293"><span class="hs-keyword">instance</span><span> </span><span class="hs-pragma">{-# OVERLAPS</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-188"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="#local-6989586621679730293"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730292"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730291"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730290"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730289"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730288"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-189"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730293"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730287"><span class="hs-identifier hs-type">parameters</span></a></span><span>
</span><span id="line-190"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730292"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730291"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><a href="#local-6989586621679730290"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730289"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730288"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730287"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-191"></span><span>  </span><span id="local-6989586621679730284"><span class="annot"><span class="annottext">gFlattenParameters :: K1
  R
  (GRULayerStack inputSize hiddenSize 1 directionality dtype device)
  a
-&gt; HList parameters
</span><a href="#local-6989586621679730284"><span class="hs-identifier hs-var hs-var hs-var hs-var">gFlattenParameters</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer1"><span class="hs-identifier hs-type">GRULayer1</span></a></span><span> </span><span id="local-6989586621679730283"><span class="annot"><span class="annottext">gruLayer :: GRULayer inputSize hiddenSize directionality dtype device
</span><a href="#local-6989586621679730283"><span class="hs-identifier hs-var">gruLayer</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-192"></span><span>    </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">layer Any -&gt; HList parameters
forall (f :: Type -&gt; Type) (as :: [Type]) a.
GParameterized f as =&gt;
f a -&gt; HList as
</span><a href="Torch.Typed.Parameter.html#gFlattenParameters"><span class="hs-identifier hs-var">gFlattenParameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize directionality dtype device
-&gt; K1
     R (GRULayer inputSize hiddenSize directionality dtype device) _
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize directionality dtype device
</span><a href="#local-6989586621679730283"><span class="hs-identifier hs-var">gruLayer</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679730293"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span>
</span><span id="line-193"></span><span>  </span><span id="local-6989586621679730282"><span class="annot"><span class="annottext">gReplaceParameters :: K1
  R
  (GRULayerStack inputSize hiddenSize 1 directionality dtype device)
  a
-&gt; HList parameters
-&gt; K1
     R
     (GRULayerStack inputSize hiddenSize 1 directionality dtype device)
     a
</span><a href="#local-6989586621679730282"><span class="hs-identifier hs-var hs-var hs-var hs-var">gReplaceParameters</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer1"><span class="hs-identifier hs-type">GRULayer1</span></a></span><span> </span><span id="local-6989586621679730281"><span class="annot"><span class="annottext">gruLayer :: GRULayer inputSize hiddenSize directionality dtype device
</span><a href="#local-6989586621679730281"><span class="hs-identifier hs-var">gruLayer</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679730280"><span class="annot"><span class="annottext">parameters :: HList parameters
</span><a href="#local-6989586621679730280"><span class="hs-identifier hs-var">parameters</span></a></span></span><span>
</span><span id="line-194"></span><span>    </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRULayerStack inputSize hiddenSize 1 directionality dtype device
-&gt; K1
     R
     (GRULayerStack inputSize hiddenSize 1 directionality dtype device)
     a
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize directionality dtype device
-&gt; GRULayerStack inputSize hiddenSize 1 directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayer inputSize hiddenSize directionality dtype device
-&gt; GRULayerStack inputSize hiddenSize 1 directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer1"><span class="hs-identifier hs-var">GRULayer1</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">K1
  R (GRULayer inputSize hiddenSize directionality dtype device) Any
-&gt; GRULayer inputSize hiddenSize directionality dtype device
forall i c k (p :: k). K1 i c p -&gt; c
</span><span class="hs-identifier hs-var hs-var">unK1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">layer Any -&gt; HList parameters -&gt; layer Any
forall (f :: Type -&gt; Type) (as :: [Type]) a.
GParameterized f as =&gt;
f a -&gt; HList as -&gt; f a
</span><a href="Torch.Typed.Parameter.html#gReplaceParameters"><span class="hs-identifier hs-var">gReplaceParameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize directionality dtype device
-&gt; K1
     R (GRULayer inputSize hiddenSize directionality dtype device) _
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize directionality dtype device
</span><a href="#local-6989586621679730281"><span class="hs-identifier hs-var">gruLayer</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679730293"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679730280"><span class="hs-identifier hs-var">parameters</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span class="hs-special">)</span></span></span></span></span></span></span></span><span>
</span><span id="line-195"></span><span>
</span><span id="line-196"></span><span id="local-6989586621679730268"><span id="local-6989586621679730269"><span id="local-6989586621679730270"><span id="local-6989586621679730271"><span id="local-6989586621679730272"><span id="local-6989586621679730273"><span id="local-6989586621679730274"><span id="local-6989586621679730275"><span id="local-6989586621679730276"><span id="local-6989586621679730277"><span id="local-6989586621679730278"><span class="hs-keyword">instance</span><span> </span><span class="hs-pragma">{-# OVERLAPPABLE</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-197"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679730278"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-198"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730277"><span class="hs-identifier hs-type">layerStack</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730276"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730275"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730278"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730274"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730273"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730272"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-199"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730271"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730275"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730274"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730275"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730274"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730273"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730272"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-200"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730277"><span class="hs-identifier hs-type">layerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730270"><span class="hs-identifier hs-type">parameters</span></a></span><span>
</span><span id="line-201"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730271"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730269"><span class="hs-identifier hs-type">parameters'</span></a></span><span>
</span><span id="line-202"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.HList.html#HAppendFD"><span class="hs-identifier hs-type">HAppendFD</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730270"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730269"><span class="hs-identifier hs-type">parameters'</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730268"><span class="hs-identifier hs-type">parameters''</span></a></span><span>
</span><span id="line-203"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730268"><span class="hs-identifier hs-type">parameters''</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730270"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="Torch.HList.html#%2B%2B"><span class="hs-operator hs-type">++</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730269"><span class="hs-identifier hs-type">parameters'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-204"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#GParameterized"><span class="hs-identifier hs-type">GParameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">R</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730276"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730275"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730278"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730274"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730273"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730272"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730268"><span class="hs-identifier hs-type">parameters''</span></a></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-205"></span><span>  </span><span id="local-6989586621679730265"><span class="annot"><span class="annottext">gFlattenParameters :: K1
  R
  (GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device)
  a
-&gt; HList parameters''
</span><a href="#local-6989586621679730265"><span class="hs-identifier hs-var hs-var hs-var hs-var">gFlattenParameters</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerK"><span class="hs-identifier hs-type">GRULayerK</span></a></span><span> </span><span id="local-6989586621679730264"><span class="annot"><span class="annottext">gruLayerStack :: GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
</span><a href="#local-6989586621679730264"><span class="hs-identifier hs-var">gruLayerStack</span></a></span></span><span> </span><span id="local-6989586621679730263"><span class="annot"><span class="annottext">gruLayer :: GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
</span><a href="#local-6989586621679730263"><span class="hs-identifier hs-var">gruLayer</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-206"></span><span>    </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">let</span><span> </span><span id="local-6989586621679730262"><span class="annot"><span class="annottext">parameters :: HList parameters
</span><a href="#local-6989586621679730262"><span class="hs-identifier hs-var hs-var">parameters</span></a></span></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">layerStack Any -&gt; HList parameters
forall (f :: Type -&gt; Type) (as :: [Type]) a.
GParameterized f as =&gt;
f a -&gt; HList as
</span><a href="Torch.Typed.Parameter.html#gFlattenParameters"><span class="hs-identifier hs-var">gFlattenParameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; K1
     R
     (GRULayerStack
        inputSize hiddenSize (numLayers - 1) directionality dtype device)
     _
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
</span><a href="#local-6989586621679730264"><span class="hs-identifier hs-var">gruLayerStack</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679730277"><span class="hs-identifier hs-type">layerStack</span></a></span><span> </span><span class="annot"><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span>
</span><span id="line-207"></span><span>          </span><span id="local-6989586621679730261"><span class="annot"><span class="annottext">parameters' :: HList parameters'
</span><a href="#local-6989586621679730261"><span class="hs-identifier hs-var hs-var">parameters'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">layer Any -&gt; HList parameters'
forall (f :: Type -&gt; Type) (as :: [Type]) a.
GParameterized f as =&gt;
f a -&gt; HList as
</span><a href="Torch.Typed.Parameter.html#gFlattenParameters"><span class="hs-identifier hs-var">gFlattenParameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
-&gt; K1
     R
     (GRULayer
        (hiddenSize * NumberOfDirections directionality)
        hiddenSize
        directionality
        dtype
        device)
     _
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
</span><a href="#local-6989586621679730263"><span class="hs-identifier hs-var">gruLayer</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679730271"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span>
</span><span id="line-208"></span><span>      </span><span class="hs-keyword">in</span><span>  </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679730262"><span class="hs-identifier hs-var">parameters</span></a></span><span> </span><span class="annot"><span class="annottext">HList parameters -&gt; HList parameters' -&gt; HList parameters''
forall k (a :: [k]) (b :: [k]) (ab :: [k]).
HAppendFD a b ab =&gt;
HList a -&gt; HList b -&gt; HList ab
</span><a href="Torch.HList.html#happendFD"><span class="hs-operator hs-var">`happendFD`</span></a></span><span> </span><span class="annot"><span class="annottext">HList parameters'
</span><a href="#local-6989586621679730261"><span class="hs-identifier hs-var">parameters'</span></a></span><span>
</span><span id="line-209"></span><span>  </span><span id="local-6989586621679730259"><span class="annot"><span class="annottext">gReplaceParameters :: K1
  R
  (GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device)
  a
-&gt; HList parameters''
-&gt; K1
     R
     (GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
     a
</span><a href="#local-6989586621679730259"><span class="hs-identifier hs-var hs-var hs-var hs-var">gReplaceParameters</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerK"><span class="hs-identifier hs-type">GRULayerK</span></a></span><span> </span><span id="local-6989586621679730258"><span class="annot"><span class="annottext">gruLayerStack :: GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
</span><a href="#local-6989586621679730258"><span class="hs-identifier hs-var">gruLayerStack</span></a></span></span><span> </span><span id="local-6989586621679730257"><span class="annot"><span class="annottext">gruLayer :: GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
</span><a href="#local-6989586621679730257"><span class="hs-identifier hs-var">gruLayer</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679730256"><span class="annot"><span class="annottext">parameters'' :: HList parameters''
</span><a href="#local-6989586621679730256"><span class="hs-identifier hs-var">parameters''</span></a></span></span><span>
</span><span id="line-210"></span><span>    </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">let</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730255"><span class="annot"><span class="annottext">parameters :: HList parameters
</span><a href="#local-6989586621679730255"><span class="hs-identifier hs-var">parameters</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679730254"><span class="annot"><span class="annottext">parameters' :: HList parameters'
</span><a href="#local-6989586621679730254"><span class="hs-identifier hs-var">parameters'</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList parameters'' -&gt; (HList parameters, HList parameters')
forall k (a :: [k]) (b :: [k]) (ab :: [k]).
HAppendFD a b ab =&gt;
HList ab -&gt; (HList a, HList b)
</span><a href="Torch.HList.html#hunappendFD"><span class="hs-identifier hs-var">hunappendFD</span></a></span><span> </span><span class="annot"><span class="annottext">HList parameters''
</span><a href="#local-6989586621679730256"><span class="hs-identifier hs-var">parameters''</span></a></span><span>
</span><span id="line-211"></span><span>          </span><span id="local-6989586621679730252"><span class="annot"><span class="annottext">gruLayerStack' :: GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
</span><a href="#local-6989586621679730252"><span class="hs-identifier hs-var hs-var">gruLayerStack'</span></a></span></span><span>           </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">K1
  R
  (GRULayerStack
     inputSize hiddenSize (numLayers - 1) directionality dtype device)
  Any
-&gt; GRULayerStack
     inputSize hiddenSize (numLayers - 1) directionality dtype device
forall i c k (p :: k). K1 i c p -&gt; c
</span><span class="hs-identifier hs-var hs-var">unK1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">layerStack Any -&gt; HList parameters -&gt; layerStack Any
forall (f :: Type -&gt; Type) (as :: [Type]) a.
GParameterized f as =&gt;
f a -&gt; HList as -&gt; f a
</span><a href="Torch.Typed.Parameter.html#gReplaceParameters"><span class="hs-identifier hs-var">gReplaceParameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; K1
     R
     (GRULayerStack
        inputSize hiddenSize (numLayers - 1) directionality dtype device)
     _
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
</span><a href="#local-6989586621679730258"><span class="hs-identifier hs-var">gruLayerStack</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679730277"><span class="hs-identifier hs-type">layerStack</span></a></span><span> </span><span class="annot"><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">HList parameters
</span><a href="#local-6989586621679730255"><span class="hs-identifier hs-var">parameters</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-212"></span><span>          </span><span id="local-6989586621679730251"><span class="annot"><span class="annottext">gruLayer' :: GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
</span><a href="#local-6989586621679730251"><span class="hs-identifier hs-var hs-var">gruLayer'</span></a></span></span><span>                </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">K1
  R
  (GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device)
  Any
-&gt; GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
forall i c k (p :: k). K1 i c p -&gt; c
</span><span class="hs-identifier hs-var hs-var">unK1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">layer Any -&gt; HList parameters' -&gt; layer Any
forall (f :: Type -&gt; Type) (as :: [Type]) a.
GParameterized f as =&gt;
f a -&gt; HList as -&gt; f a
</span><a href="Torch.Typed.Parameter.html#gReplaceParameters"><span class="hs-identifier hs-var">gReplaceParameters</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
-&gt; K1
     R
     (GRULayer
        (hiddenSize * NumberOfDirections directionality)
        hiddenSize
        directionality
        dtype
        device)
     _
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
</span><a href="#local-6989586621679730257"><span class="hs-identifier hs-var">gruLayer</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="#local-6989586621679730271"><span class="hs-identifier hs-type">layer</span></a></span><span> </span><span class="annot"><span class="hs-identifier">_</span></span><span class="hs-special">)</span><span>      </span><span class="annot"><span class="annottext">HList parameters'
</span><a href="#local-6989586621679730254"><span class="hs-identifier hs-var">parameters'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-213"></span><span>      </span><span class="hs-keyword">in</span><span>  </span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize numLayers directionality dtype device
-&gt; K1
     R
     (GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
     a
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
-&gt; GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device
forall (numLayers :: Nat) (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(2 &lt;= numLayers) =&gt;
GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
-&gt; GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerK"><span class="hs-identifier hs-var">GRULayerK</span></a></span><span> </span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
</span><a href="#local-6989586621679730252"><span class="hs-identifier hs-var">gruLayerStack'</span></a></span><span> </span><span class="annot"><span class="annottext">GRULayer
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
</span><a href="#local-6989586621679730251"><span class="hs-identifier hs-var">gruLayer'</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-214"></span><span>
</span><span id="line-215"></span><span id="local-6989586621679730246"><span id="local-6989586621679730247"><span id="local-6989586621679730248"><span id="local-6989586621679730249"><span id="local-6989586621679730250"><span class="hs-keyword">instance</span><span> </span><span class="hs-pragma">{-# OVERLAPS</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-216"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-217"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730248"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-218"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730247"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-219"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-220"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-221"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-type">GRULayerSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730248"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730247"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730246"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-222"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730248"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730247"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730246"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-223"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-type">GRULayerStackSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730248"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730247"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><a href="#local-6989586621679730246"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-224"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730248"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730247"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span> </span><span class="annot"><a href="#local-6989586621679730246"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-225"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-226"></span><span>  </span><span id="local-6989586621679730244"><span class="annot"><span class="annottext">sample :: GRULayerStackSpec
  inputSize hiddenSize 1 directionality dtype device
-&gt; IO
     (GRULayerStack inputSize hiddenSize 1 directionality dtype device)
</span><a href="#local-6989586621679730244"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRULayer inputSize hiddenSize directionality dtype device
-&gt; GRULayerStack inputSize hiddenSize 1 directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayer inputSize hiddenSize directionality dtype device
-&gt; GRULayerStack inputSize hiddenSize 1 directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer1"><span class="hs-identifier hs-var">GRULayer1</span></a></span><span> </span><span class="annot"><span class="annottext">(GRULayer inputSize hiddenSize directionality dtype device
 -&gt; GRULayerStack
      inputSize hiddenSize 1 directionality dtype device)
-&gt; IO (GRULayer inputSize hiddenSize directionality dtype device)
-&gt; IO
     (GRULayerStack inputSize hiddenSize 1 directionality dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; IO (GRULayer inputSize hiddenSize directionality dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">(GRULayerSpec inputSize hiddenSize directionality dtype device
 -&gt; IO (GRULayer inputSize hiddenSize directionality dtype device))
-&gt; GRULayerSpec inputSize hiddenSize directionality dtype device
-&gt; IO (GRULayer inputSize hiddenSize directionality dtype device)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">GRULayerSpec inputSize hiddenSize directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-var">GRULayerSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730248"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730247"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730246"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730249"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730250"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span><span>
</span><span id="line-227"></span><span>
</span><span id="line-228"></span><span id="local-6989586621679730238"><span id="local-6989586621679730239"><span id="local-6989586621679730240"><span id="local-6989586621679730241"><span id="local-6989586621679730242"><span id="local-6989586621679730243"><span class="hs-keyword">instance</span><span> </span><span class="hs-pragma">{-# OVERLAPPABLE</span><span> </span><span class="hs-pragma">#-}</span><span>
</span><span id="line-229"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-number">2</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">&lt;=</span></span><span> </span><span class="annot"><a href="#local-6989586621679730243"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-230"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-231"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730240"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-232"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-233"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-234"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-235"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-type">GRULayerStackSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730240"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730243"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-236"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730240"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730243"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-237"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-type">GRULayerSpec</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-238"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayer"><span class="hs-identifier hs-type">GRULayer</span></a></span><span>     </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-239"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-type">GRULayerStackSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730240"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730243"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-240"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730240"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730243"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-241"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-242"></span><span>  </span><span id="local-6989586621679730236"><span class="annot"><span class="annottext">sample :: GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
</span><a href="#local-6989586621679730236"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-243"></span><span>    </span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
-&gt; GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device
forall (numLayers :: Nat) (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(2 &lt;= numLayers) =&gt;
GRULayerStack
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
-&gt; GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerK"><span class="hs-identifier hs-var">GRULayerK</span></a></span><span>
</span><span id="line-244"></span><span>      </span><span class="annot"><span class="annottext">(GRULayerStack
   inputSize hiddenSize (numLayers - 1) directionality dtype device
 -&gt; GRULayer
      (hiddenSize * NumberOfDirections directionality)
      hiddenSize
      directionality
      dtype
      device
 -&gt; GRULayerStack
      inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize (numLayers - 1) directionality dtype device)
-&gt; IO
     (GRULayer
        (hiddenSize * NumberOfDirections directionality)
        hiddenSize
        directionality
        dtype
        device
      -&gt; GRULayerStack
           inputSize hiddenSize numLayers directionality dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerStackSpec
  inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize (numLayers - 1) directionality dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">(GRULayerStackSpec
   inputSize hiddenSize (numLayers - 1) directionality dtype device
 -&gt; IO
      (GRULayerStack
         inputSize hiddenSize (numLayers - 1) directionality dtype device))
-&gt; GRULayerStackSpec
     inputSize hiddenSize (numLayers - 1) directionality dtype device
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize (numLayers - 1) directionality dtype device)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">GRULayerStackSpec
  inputSize hiddenSize (numLayers - 1) directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-var">GRULayerStackSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730240"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730243"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">-</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-245"></span><span>      </span><span class="annot"><span class="annottext">IO
  (GRULayer
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
   -&gt; GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (GRULayer
        (hiddenSize * NumberOfDirections directionality)
        hiddenSize
        directionality
        dtype
        device)
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerSpec
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
-&gt; IO
     (GRULayer
        (hiddenSize * NumberOfDirections directionality)
        hiddenSize
        directionality
        dtype
        device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">(GRULayerSpec
   (hiddenSize * NumberOfDirections directionality)
   hiddenSize
   directionality
   dtype
   device
 -&gt; IO
      (GRULayer
         (hiddenSize * NumberOfDirections directionality)
         hiddenSize
         directionality
         dtype
         device))
-&gt; GRULayerSpec
     (hiddenSize * NumberOfDirections directionality)
     hiddenSize
     directionality
     dtype
     device
-&gt; IO
     (GRULayer
        (hiddenSize * NumberOfDirections directionality)
        hiddenSize
        directionality
        dtype
        device)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">GRULayerSpec
  (hiddenSize * NumberOfDirections directionality)
  hiddenSize
  directionality
  dtype
  device
forall (inputSize :: Nat) (hiddenSize :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerSpec inputSize hiddenSize directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerSpec"><span class="hs-identifier hs-var">GRULayerSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730239"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730238"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730241"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730242"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-246"></span><span>
</span><span id="line-247"></span><span id="local-6989586621679730234"><span id="local-6989586621679730235"></span></span><span class="hs-keyword">newtype</span><span> </span><span id="GRUSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-var">GRUSpec</span></a></span></span><span>
</span><span id="line-248"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730233"><span class="annot"><a href="#local-6989586621679730233"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-249"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730232"><span class="annot"><a href="#local-6989586621679730232"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-250"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730231"><span class="annot"><a href="#local-6989586621679730231"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-251"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730230"><span class="annot"><a href="#local-6989586621679730230"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-252"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730229"><span class="annot"><a href="#local-6989586621679730229"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-253"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730228"><span class="annot"><a href="#local-6989586621679730228"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-254"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span id="GRUSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-var">GRUSpec</span></a></span></span><span> </span><span class="annot"><a href="Torch.Typed.NN.html#DropoutSpec"><span class="hs-identifier hs-type">DropoutSpec</span></a></span><span>
</span><span id="line-255"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730221"><span id="local-6989586621679730223"><span id="local-6989586621679730225"><span class="annot"><span class="annottext">Int
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
[GRUSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; String
(Int
 -&gt; GRUSpec
      inputSize hiddenSize numLayers directionality dtype device
 -&gt; ShowS)
-&gt; (GRUSpec
      inputSize hiddenSize numLayers directionality dtype device
    -&gt; String)
-&gt; ([GRUSpec
       inputSize hiddenSize numLayers directionality dtype device]
    -&gt; ShowS)
-&gt; Show
     (GRUSpec
        inputSize hiddenSize numLayers directionality dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRUSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; String
showList :: [GRUSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
$cshowList :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRUSpec
   inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
show :: GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; String
$cshow :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; String
showsPrec :: Int
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
$cshowsPrec :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 GRUSpec inputSize hiddenSize numLayers directionality dtype device
 -&gt; Rep
      (GRUSpec
         inputSize hiddenSize numLayers directionality dtype device)
      x)
-&gt; (forall x.
    Rep
      (GRUSpec
         inputSize hiddenSize numLayers directionality dtype device)
      x
    -&gt; GRUSpec
         inputSize hiddenSize numLayers directionality dtype device)
-&gt; Generic
     (GRUSpec
        inputSize hiddenSize numLayers directionality dtype device)
forall x.
Rep
  (GRUSpec
     inputSize hiddenSize numLayers directionality dtype device)
  x
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
forall x.
GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; Rep
     (GRUSpec
        inputSize hiddenSize numLayers directionality dtype device)
     x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (GRUSpec
     inputSize hiddenSize numLayers directionality dtype device)
  x
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; Rep
     (GRUSpec
        inputSize hiddenSize numLayers directionality dtype device)
     x
$cto :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (GRUSpec
     inputSize hiddenSize numLayers directionality dtype device)
  x
-&gt; GRUSpec
     inputSize hiddenSize numLayers directionality dtype device
$cfrom :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; Rep
     (GRUSpec
        inputSize hiddenSize numLayers directionality dtype device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-256"></span><span>
</span><span id="line-257"></span><span id="local-6989586621679730216"><span id="local-6989586621679730217"></span></span><span class="hs-keyword">data</span><span> </span><span id="GRU"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-var">GRU</span></a></span></span><span>
</span><span id="line-258"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730625"><span class="annot"><a href="#local-6989586621679730625"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-259"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730624"><span class="annot"><a href="#local-6989586621679730624"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-260"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730623"><span class="annot"><a href="#local-6989586621679730623"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-261"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730622"><span class="annot"><a href="#local-6989586621679730622"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-262"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730621"><span class="annot"><a href="#local-6989586621679730621"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-263"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730620"><span class="annot"><a href="#local-6989586621679730620"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-264"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span id="GRU"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-var">GRU</span></a></span></span><span>
</span><span id="line-265"></span><span>      </span><span class="hs-special">{</span><span> </span><span id="gru_layer_stack"><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; GRULayerStack
     inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gru_layer_stack"><span class="hs-identifier hs-var hs-var">gru_layer_stack</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730625"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730624"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730623"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730622"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730621"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730620"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-266"></span><span>      </span><span class="hs-special">,</span><span> </span><span id="gru_dropout"><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Dropout
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gru_dropout"><span class="hs-identifier hs-var hs-var">gru_dropout</span></a></span></span><span>     </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span>
</span><span id="line-267"></span><span>      </span><span class="hs-special">}</span><span>
</span><span id="line-268"></span><span>  </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730207"><span id="local-6989586621679730209"><span id="local-6989586621679730211"><span class="annot"><span class="annottext">Int
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
[GRU inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; String
(Int
 -&gt; GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; ShowS)
-&gt; (GRU inputSize hiddenSize numLayers directionality dtype device
    -&gt; String)
-&gt; ([GRU
       inputSize hiddenSize numLayers directionality dtype device]
    -&gt; ShowS)
-&gt; Show
     (GRU inputSize hiddenSize numLayers directionality dtype device)
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRU inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; String
showList :: [GRU inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
$cshowList :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
[GRU inputSize hiddenSize numLayers directionality dtype device]
-&gt; ShowS
show :: GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; String
$cshow :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; String
showsPrec :: Int
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
$cshowsPrec :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Int
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x.
 GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; Rep
      (GRU inputSize hiddenSize numLayers directionality dtype device) x)
-&gt; (forall x.
    Rep
      (GRU inputSize hiddenSize numLayers directionality dtype device) x
    -&gt; GRU inputSize hiddenSize numLayers directionality dtype device)
-&gt; Generic
     (GRU inputSize hiddenSize numLayers directionality dtype device)
forall x.
Rep
  (GRU inputSize hiddenSize numLayers directionality dtype device) x
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
forall x.
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Rep
     (GRU inputSize hiddenSize numLayers directionality dtype device) x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (GRU inputSize hiddenSize numLayers directionality dtype device) x
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Rep
     (GRU inputSize hiddenSize numLayers directionality dtype device) x
$cto :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
Rep
  (GRU inputSize hiddenSize numLayers directionality dtype device) x
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
$cfrom :: forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)) x.
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Rep
     (GRU inputSize hiddenSize numLayers directionality dtype device) x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-269"></span><span>
</span><span id="line-270"></span><span class="hs-comment">-- TODO: when we have cannonical initializers do this correctly:</span><span>
</span><span id="line-271"></span><span class="hs-comment">-- https://github.com/pytorch/pytorch/issues/9221</span><span>
</span><span id="line-272"></span><span class="hs-comment">-- https://discuss.pytorch.org/t/initializing-rnn-gru-and-gru-correctly/23605</span><span>
</span><span id="line-273"></span><span>
</span><span id="line-274"></span><span class="hs-comment">-- | Helper to do xavier uniform initializations on weight matrices and</span><span>
</span><span id="line-275"></span><span class="hs-comment">-- orthagonal initializations for the gates. (When implemented.)</span><span>
</span><span id="line-276"></span><span class="hs-comment">--</span><span>
</span><span id="line-277"></span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-type">xavierUniformGRU</span></a></span><span>
</span><span id="line-278"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679730817"><span class="annot"><a href="#local-6989586621679730817"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span id="local-6989586621679730820"><span class="annot"><a href="#local-6989586621679730820"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679730819"><span class="annot"><a href="#local-6989586621679730819"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span id="local-6989586621679730818"><span class="annot"><a href="#local-6989586621679730818"><span class="hs-identifier hs-type">featureSize</span></a></span></span><span>
</span><span id="line-279"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730820"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-280"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730819"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-281"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730818"><span class="hs-identifier hs-type">featureSize</span></a></span><span>
</span><span id="line-282"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730817"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-283"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730817"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730820"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-284"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-285"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730817"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730820"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679730819"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730818"><span class="hs-identifier hs-type">featureSize</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-286"></span><span id="xavierUniformGRU"><span class="annot"><span class="annottext">xavierUniformGRU :: IO (Tensor device dtype '[3 * hiddenSize, featureSize])
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformGRU"><span class="hs-identifier hs-var hs-var">xavierUniformGRU</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">do</span><span>
</span><span id="line-287"></span><span>  </span><span id="local-6989586621679730204"><span class="annot"><span class="annottext">Tensor device dtype '[3 * hiddenSize, featureSize]
</span><a href="#local-6989586621679730204"><span class="hs-identifier hs-var">init</span></a></span></span><span> </span><span class="hs-glyph">&lt;-</span><span> </span><span class="annot"><span class="annottext">IO (Tensor device dtype '[3 * hiddenSize, featureSize])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(TensorOptions shape dtype device,
 RandDTypeIsValid device dtype) =&gt;
IO (Tensor device dtype shape)
</span><a href="Torch.Typed.Factories.html#randn"><span class="hs-identifier hs-var">randn</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730817"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730820"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679730819"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730818"><span class="hs-identifier hs-type">featureSize</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-288"></span><span>  </span><span class="annot"><span class="annottext">Tensor -&gt; Tensor device dtype '[3 * hiddenSize, featureSize]
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (shape :: [Nat]).
Tensor -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Tensor.html#UnsafeMkTensor"><span class="hs-identifier hs-var">UnsafeMkTensor</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor -&gt; Tensor device dtype '[3 * hiddenSize, featureSize])
-&gt; IO Tensor
-&gt; IO (Tensor device dtype '[3 * hiddenSize, featureSize])
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor -&gt; Float -&gt; [Int] -&gt; IO Tensor
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformFIXME"><span class="hs-identifier hs-var">xavierUniformFIXME</span></a></span><span>
</span><span id="line-289"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[3 * hiddenSize, featureSize] -&gt; Tensor
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (shape :: [Nat]).
Tensor device dtype shape -&gt; Tensor
</span><a href="Torch.Typed.Tensor.html#toDynamic"><span class="hs-identifier hs-var hs-var">toDynamic</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[3 * hiddenSize, featureSize]
</span><a href="#local-6989586621679730204"><span class="hs-identifier hs-var">init</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-290"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">5.0</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="annot"><span class="hs-number">3</span></span><span class="hs-special">)</span><span>
</span><span id="line-291"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor device dtype '[3 * hiddenSize, featureSize] -&gt; [Int]
forall (device :: (DeviceType, Nat)) (dtype :: DType)
       (shape :: [Nat]).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape -&gt; [Int]
</span><a href="Torch.Typed.Tensor.html#shape"><span class="hs-identifier hs-var">shape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730817"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730820"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><span class="hs-number">3</span></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="#local-6989586621679730819"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730818"><span class="hs-identifier hs-type">featureSize</span></a></span><span class="hs-special">]</span><span> </span><span class="annot"><span class="annottext">Tensor device dtype '[3 * hiddenSize, featureSize]
</span><a href="#local-6989586621679730204"><span class="hs-identifier hs-var">init</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-292"></span><span>
</span><span id="line-293"></span><span class="hs-comment">-- TODO: This is taken from the initializers example code and should be replaced with cannonical,</span><span>
</span><span id="line-294"></span><span class="hs-comment">-- tested versions. However, even a potentially incorrect implementation will likely perform</span><span>
</span><span id="line-295"></span><span class="hs-comment">-- better than an ad-hoc random-normal distribution.</span><span>
</span><span id="line-296"></span><span class="hs-comment">-- | Fan-in / Fan-out scaling calculation</span><span>
</span><span id="line-297"></span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#calculateFan"><span class="hs-identifier hs-type">calculateFan</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">)</span><span>
</span><span id="line-298"></span><span id="calculateFan"><span class="annot"><span class="annottext">calculateFan :: [Int] -&gt; (Int, Int)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#calculateFan"><span class="hs-identifier hs-var hs-var">calculateFan</span></a></span></span><span> </span><span id="local-6989586621679730196"><span class="annot"><span class="annottext">shape :: [Int]
</span><a href="#local-6989586621679730196"><span class="hs-identifier hs-var">shape</span></a></span></span><span>
</span><span id="line-299"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730195"><span class="hs-identifier hs-var">dimT</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Ord a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">&lt;</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span>
</span><span id="line-300"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">String -&gt; (Int, Int)
forall a. HasCallStack =&gt; String -&gt; a
</span><span class="hs-identifier hs-var">error</span></span><span>
</span><span id="line-301"></span><span>    </span><span class="annot"><span class="hs-string">&quot;Fan in and fan out can not be computed for tensor with fewer than 2 dimensions&quot;</span></span><span>
</span><span id="line-302"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730195"><span class="hs-identifier hs-var">dimT</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Bool
forall a. Eq a =&gt; a -&gt; a -&gt; Bool
</span><span class="hs-operator hs-var">==</span></span><span> </span><span class="annot"><span class="hs-number">2</span></span><span>
</span><span id="line-303"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730192"><span class="hs-identifier hs-var">numInputFmaps</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730191"><span class="hs-identifier hs-var">numOutputFmaps</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-304"></span><span>  </span><span class="hs-glyph">|</span><span> </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">otherwise</span></span><span>
</span><span id="line-305"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730192"><span class="hs-identifier hs-var">numInputFmaps</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730189"><span class="hs-identifier hs-var">receptiveFieldSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730191"><span class="hs-identifier hs-var">numOutputFmaps</span></a></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Int -&gt; Int
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730189"><span class="hs-identifier hs-var">receptiveFieldSize</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-306"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-307"></span><span>  </span><span id="local-6989586621679730195"><span class="annot"><span class="annottext">dimT :: Int
</span><a href="#local-6989586621679730195"><span class="hs-identifier hs-var hs-var">dimT</span></a></span></span><span>               </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: Type -&gt; Type) a. Foldable t =&gt; t a -&gt; Int
</span><span class="hs-identifier hs-var">length</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679730196"><span class="hs-identifier hs-var">shape</span></a></span><span>
</span><span id="line-308"></span><span>  </span><span id="local-6989586621679730192"><span class="annot"><span class="annottext">numInputFmaps :: Int
</span><a href="#local-6989586621679730192"><span class="hs-identifier hs-var hs-var">numInputFmaps</span></a></span></span><span>      </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679730196"><span class="hs-identifier hs-var">shape</span></a></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int -&gt; Int
forall a. [a] -&gt; Int -&gt; a
</span><span class="hs-operator hs-var">!!</span></span><span> </span><span class="annot"><span class="hs-number">1</span></span><span>
</span><span id="line-309"></span><span>  </span><span id="local-6989586621679730191"><span class="annot"><span class="annottext">numOutputFmaps :: Int
</span><a href="#local-6989586621679730191"><span class="hs-identifier hs-var hs-var">numOutputFmaps</span></a></span></span><span>     </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679730196"><span class="hs-identifier hs-var">shape</span></a></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int -&gt; Int
forall a. [a] -&gt; Int -&gt; a
</span><span class="hs-operator hs-var">!!</span></span><span> </span><span class="annot"><span class="hs-number">0</span></span><span>
</span><span id="line-310"></span><span>  </span><span id="local-6989586621679730189"><span class="annot"><span class="annottext">receptiveFieldSize :: Int
</span><a href="#local-6989586621679730189"><span class="hs-identifier hs-var hs-var">receptiveFieldSize</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; Int
forall (t :: Type -&gt; Type) a. (Foldable t, Num a) =&gt; t a -&gt; a
</span><span class="hs-identifier hs-var">product</span></span><span> </span><span class="annot"><span class="annottext">([Int] -&gt; Int) -&gt; [Int] -&gt; Int
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; [Int]
forall a. [a] -&gt; [a]
</span><span class="hs-identifier hs-var">tail</span></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679730196"><span class="hs-identifier hs-var">shape</span></a></span><span>
</span><span id="line-311"></span><span>
</span><span id="line-312"></span><span class="hs-comment">-- | Xavier Initialization - Uniform</span><span>
</span><span id="line-313"></span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformFIXME"><span class="hs-identifier hs-type">xavierUniformFIXME</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Tensor.html#Tensor"><span class="hs-identifier hs-type">D.Tensor</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Float</span></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">[</span><span class="annot"><span class="hs-identifier hs-type">Int</span></span><span class="hs-special">]</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="annot"><a href="Torch.Tensor.html#Tensor"><span class="hs-identifier hs-type">D.Tensor</span></a></span><span>
</span><span id="line-314"></span><span id="xavierUniformFIXME"><span class="annot"><span class="annottext">xavierUniformFIXME :: Tensor -&gt; Float -&gt; [Int] -&gt; IO Tensor
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#xavierUniformFIXME"><span class="hs-identifier hs-var hs-var">xavierUniformFIXME</span></a></span></span><span> </span><span id="local-6989586621679730184"><span class="annot"><span class="annottext">init :: Tensor
</span><a href="#local-6989586621679730184"><span class="hs-identifier hs-var">init</span></a></span></span><span> </span><span id="local-6989586621679730183"><span class="annot"><span class="annottext">gain :: Float
</span><a href="#local-6989586621679730183"><span class="hs-identifier hs-var">gain</span></a></span></span><span> </span><span id="local-6989586621679730182"><span class="annot"><span class="annottext">shape :: [Int]
</span><a href="#local-6989586621679730182"><span class="hs-identifier hs-var">shape</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Tensor -&gt; IO Tensor
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span>
</span><span id="line-315"></span><span>  </span><span class="annot"><span class="annottext">(Tensor -&gt; IO Tensor) -&gt; Tensor -&gt; IO Tensor
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Tensor -&gt; Tensor
forall a. Scalar a =&gt; a -&gt; Tensor -&gt; Tensor
</span><a href="Torch.Functional.html#subScalar"><span class="hs-identifier hs-var">D.subScalar</span></a></span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679730180"><span class="hs-identifier hs-var">bound</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor -&gt; Tensor) -&gt; Tensor -&gt; Tensor
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Tensor -&gt; Tensor
forall a. Scalar a =&gt; a -&gt; Tensor -&gt; Tensor
</span><a href="Torch.Functional.html#mulScalar"><span class="hs-identifier hs-var">D.mulScalar</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679730180"><span class="hs-identifier hs-var">bound</span></a></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="hs-number">2.0</span></span><span class="hs-special">)</span><span> </span><span class="annot"><span class="annottext">Tensor
</span><a href="#local-6989586621679730184"><span class="hs-identifier hs-var">init</span></a></span><span>
</span><span id="line-316"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-317"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730178"><span class="annot"><span class="annottext">fanIn :: Int
</span><a href="#local-6989586621679730178"><span class="hs-identifier hs-var">fanIn</span></a></span></span><span class="hs-special">,</span><span> </span><span id="local-6989586621679730177"><span class="annot"><span class="annottext">fanOut :: Int
</span><a href="#local-6989586621679730177"><span class="hs-identifier hs-var">fanOut</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">[Int] -&gt; (Int, Int)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#calculateFan"><span class="hs-identifier hs-var">calculateFan</span></a></span><span> </span><span class="annot"><span class="annottext">[Int]
</span><a href="#local-6989586621679730182"><span class="hs-identifier hs-var">shape</span></a></span><span>
</span><span id="line-318"></span><span>  </span><span id="local-6989586621679730176"><span class="annot"><span class="annottext">std :: Float
</span><a href="#local-6989586621679730176"><span class="hs-identifier hs-var hs-var">std</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679730183"><span class="hs-identifier hs-var">gain</span></a></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">sqrt</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-number">2.0</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Fractional a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">/</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Int -&gt; Float
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730178"><span class="hs-identifier hs-var">fanIn</span></a></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">+</span></span><span> </span><span class="annot"><span class="annottext">Int -&gt; Float
forall a b. (Integral a, Num b) =&gt; a -&gt; b
</span><span class="hs-identifier hs-var">fromIntegral</span></span><span> </span><span class="annot"><span class="annottext">Int
</span><a href="#local-6989586621679730177"><span class="hs-identifier hs-var">fanOut</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-319"></span><span>  </span><span id="local-6989586621679730180"><span class="annot"><span class="annottext">bound :: Float
</span><a href="#local-6989586621679730180"><span class="hs-identifier hs-var hs-var">bound</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float
forall a. Floating a =&gt; a -&gt; a
</span><span class="hs-identifier hs-var">sqrt</span></span><span> </span><span class="annot"><span class="hs-number">3.0</span></span><span> </span><span class="annot"><span class="annottext">Float -&gt; Float -&gt; Float
forall a. Num a =&gt; a -&gt; a -&gt; a
</span><span class="hs-operator hs-var">*</span></span><span> </span><span class="annot"><span class="annottext">Float
</span><a href="#local-6989586621679730176"><span class="hs-identifier hs-var">std</span></a></span><span>
</span><span id="line-320"></span><span>
</span><span id="line-321"></span><span id="local-6989586621679730168"><span id="local-6989586621679730169"><span id="local-6989586621679730170"><span id="local-6989586621679730171"><span id="local-6989586621679730172"><span id="local-6989586621679730173"><span class="hs-keyword">instance</span><span>
</span><span id="line-322"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-323"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-324"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730171"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-325"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730170"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-326"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730169"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-327"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Factories.html#RandDTypeIsValid"><span class="hs-identifier hs-type">RandDTypeIsValid</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-328"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-type">GRULayerStackSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730171"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730170"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730168"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730169"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-329"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStack"><span class="hs-identifier hs-type">GRULayerStack</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730171"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730170"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730168"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730169"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-330"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730171"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730170"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730168"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730169"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-331"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730171"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730170"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730168"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730169"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-332"></span><span>  </span><span id="local-6989586621679730166"><span class="annot"><span class="annottext">sample :: GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
</span><a href="#local-6989586621679730166"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span id="local-6989586621679730165"><span class="annot"><span class="annottext">dropoutSpec :: DropoutSpec
</span><a href="#local-6989586621679730165"><span class="hs-identifier hs-var">dropoutSpec</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-333"></span><span>    </span><span class="annot"><span class="annottext">GRULayerStack
  inputSize hiddenSize numLayers directionality dtype device
-&gt; Dropout
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStack
  inputSize hiddenSize numLayers directionality dtype device
-&gt; Dropout
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-var">GRU</span></a></span><span>
</span><span id="line-334"></span><span>      </span><span class="annot"><span class="annottext">(GRULayerStack
   inputSize hiddenSize numLayers directionality dtype device
 -&gt; Dropout
 -&gt; GRU inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (Dropout
      -&gt; GRU inputSize hiddenSize numLayers directionality dtype device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
-&gt; IO
     (GRULayerStack
        inputSize hiddenSize numLayers directionality dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRULayerStackSpec
  inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRULayerStackSpec"><span class="hs-identifier hs-var">GRULayerStackSpec</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730171"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730170"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730168"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730169"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730173"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730172"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-335"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Dropout
   -&gt; GRU inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO Dropout
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">DropoutSpec -&gt; IO Dropout
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">DropoutSpec
</span><a href="#local-6989586621679730165"><span class="hs-identifier hs-var">dropoutSpec</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-336"></span><span>
</span><span id="line-337"></span><span id="local-6989586621679730163"><span id="local-6989586621679730164"></span></span><span class="hs-keyword">data</span><span> </span><span id="RNNInitialization"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#RNNInitialization"><span class="hs-identifier hs-var">RNNInitialization</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span id="ConstantInitialization"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-var">ConstantInitialization</span></a></span></span><span> </span><span class="hs-glyph">|</span><span> </span><span id="LearnedInitialization"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-var">LearnedInitialization</span></a></span></span><span> </span><span class="hs-keyword">deriving</span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730157"><span id="local-6989586621679730159"><span id="local-6989586621679730161"><span class="annot"><span class="annottext">Int -&gt; RNNInitialization -&gt; ShowS
[RNNInitialization] -&gt; ShowS
RNNInitialization -&gt; String
(Int -&gt; RNNInitialization -&gt; ShowS)
-&gt; (RNNInitialization -&gt; String)
-&gt; ([RNNInitialization] -&gt; ShowS)
-&gt; Show RNNInitialization
forall a.
(Int -&gt; a -&gt; ShowS) -&gt; (a -&gt; String) -&gt; ([a] -&gt; ShowS) -&gt; Show a
showList :: [RNNInitialization] -&gt; ShowS
$cshowList :: [RNNInitialization] -&gt; ShowS
show :: RNNInitialization -&gt; String
$cshow :: RNNInitialization -&gt; String
showsPrec :: Int -&gt; RNNInitialization -&gt; ShowS
$cshowsPrec :: Int -&gt; RNNInitialization -&gt; ShowS
</span><span class="hs-identifier hs-var hs-var hs-var hs-var hs-var hs-var hs-var hs-var">Show</span></span></span></span></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="annottext">(forall x. RNNInitialization -&gt; Rep RNNInitialization x)
-&gt; (forall x. Rep RNNInitialization x -&gt; RNNInitialization)
-&gt; Generic RNNInitialization
forall x. Rep RNNInitialization x -&gt; RNNInitialization
forall x. RNNInitialization -&gt; Rep RNNInitialization x
forall a.
(forall x. a -&gt; Rep a x) -&gt; (forall x. Rep a x -&gt; a) -&gt; Generic a
$cto :: forall x. Rep RNNInitialization x -&gt; RNNInitialization
$cfrom :: forall x. RNNInitialization -&gt; Rep RNNInitialization x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">Generic</span></span><span class="hs-special">)</span><span>
</span><span id="line-338"></span><span>
</span><span id="line-339"></span><span class="hs-comment">-- | A specification for a long, short-term memory layer.</span><span>
</span><span id="line-340"></span><span class="hs-comment">--</span><span>
</span><span id="line-341"></span><span class="hs-keyword">data</span><span> </span><span id="GRUWithInitSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-var">GRUWithInitSpec</span></a></span></span><span>
</span><span id="line-342"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730154"><span class="annot"><a href="#local-6989586621679730154"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-343"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730153"><span class="annot"><a href="#local-6989586621679730153"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-344"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730152"><span class="annot"><a href="#local-6989586621679730152"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-345"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730151"><span class="annot"><a href="#local-6989586621679730151"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-346"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730150"><span class="annot"><a href="#local-6989586621679730150"><span class="hs-identifier hs-type">initialization</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#RNNInitialization"><span class="hs-identifier hs-type">RNNInitialization</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-347"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730149"><span class="annot"><a href="#local-6989586621679730149"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-348"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730148"><span class="annot"><a href="#local-6989586621679730148"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-349"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-350"></span><span>  </span><span class="hs-comment">-- | Weights drawn from Xavier-Uniform</span><span>
</span><span id="line-351"></span><span>  </span><span class="hs-comment">--   with zeros-value initialized biases and cell states.</span><span>
</span><span id="line-352"></span><span>  </span><span id="GRUWithZerosInitSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithZerosInitSpec"><span class="hs-identifier hs-var">GRUWithZerosInitSpec</span></a></span></span><span>
</span><span id="line-353"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679730146"><span class="annot"><a href="#local-6989586621679730146"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span id="local-6989586621679730145"><span class="annot"><a href="#local-6989586621679730145"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span id="local-6989586621679730144"><span class="annot"><a href="#local-6989586621679730144"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span id="local-6989586621679730143"><span class="annot"><a href="#local-6989586621679730143"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span id="local-6989586621679730142"><span class="annot"><a href="#local-6989586621679730142"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679730141"><span class="annot"><a href="#local-6989586621679730141"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-354"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730146"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730145"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730144"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730143"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730142"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730141"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-355"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-type">GRUWithInitSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730146"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730145"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730144"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730143"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730142"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730141"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-356"></span><span>  </span><span class="hs-comment">-- | Weights drawn from Xavier-Uniform</span><span>
</span><span id="line-357"></span><span>  </span><span class="hs-comment">--   with zeros-value initialized biases</span><span>
</span><span id="line-358"></span><span>  </span><span class="hs-comment">--   and user-provided cell states.</span><span>
</span><span id="line-359"></span><span>  </span><span id="GRUWithConstInitSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInitSpec"><span class="hs-identifier hs-var">GRUWithConstInitSpec</span></a></span></span><span>
</span><span id="line-360"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679730139"><span class="annot"><a href="#local-6989586621679730139"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span id="local-6989586621679730138"><span class="annot"><a href="#local-6989586621679730138"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span id="local-6989586621679730137"><span class="annot"><a href="#local-6989586621679730137"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span id="local-6989586621679730136"><span class="annot"><a href="#local-6989586621679730136"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span id="local-6989586621679730135"><span class="annot"><a href="#local-6989586621679730135"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679730134"><span class="annot"><a href="#local-6989586621679730134"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-361"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730139"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730138"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730137"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730136"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730135"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730134"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-362"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730134"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730135"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730137"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730136"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730138"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-comment">-- ^ The initial values of the hidden state</span><span>
</span><span id="line-363"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-type">GRUWithInitSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730139"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730138"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730137"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730136"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730135"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730134"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-364"></span><span>  </span><span class="hs-comment">-- | Weights drawn from Xavier-Uniform</span><span>
</span><span id="line-365"></span><span>  </span><span class="hs-comment">--   with zeros-value initialized biases</span><span>
</span><span id="line-366"></span><span>  </span><span class="hs-comment">--   and learned cell states.</span><span>
</span><span id="line-367"></span><span>  </span><span id="GRUWithLearnedInitSpec"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInitSpec"><span class="hs-identifier hs-var">GRUWithLearnedInitSpec</span></a></span></span><span>
</span><span id="line-368"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679730132"><span class="annot"><a href="#local-6989586621679730132"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span id="local-6989586621679730131"><span class="annot"><a href="#local-6989586621679730131"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span id="local-6989586621679730130"><span class="annot"><a href="#local-6989586621679730130"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span id="local-6989586621679730129"><span class="annot"><a href="#local-6989586621679730129"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span id="local-6989586621679730128"><span class="annot"><a href="#local-6989586621679730128"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679730127"><span class="annot"><a href="#local-6989586621679730127"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-369"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730132"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730131"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730130"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730129"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730128"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730127"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-370"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730127"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730128"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730130"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730129"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730131"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span> </span><span class="hs-comment">-- ^ The initial (learnable)</span><span>
</span><span id="line-371"></span><span>                                                                                        </span><span class="hs-comment">-- values of the hidden state</span><span>
</span><span id="line-372"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-type">GRUWithInitSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730132"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730131"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730130"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730129"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-type">LearnedInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730128"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730127"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-373"></span><span>
</span><span id="line-374"></span><span id="local-6989586621679730114"><span id="local-6989586621679730116"><span id="local-6989586621679730118"><span id="local-6989586621679730120"><span id="local-6989586621679730121"><span id="local-6989586621679730122"><span id="local-6989586621679730123"><span id="local-6989586621679730124"><span id="local-6989586621679730125"><span id="local-6989586621679730126"><span class="hs-keyword">deriving</span><span> </span><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-type">GRUWithInitSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730126"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730125"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730124"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730123"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730122"><span class="hs-identifier hs-type">initialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730121"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730120"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-375"></span><span class="hs-comment">-- deriving instance Generic (GRUWithInitSpec inputSize hiddenSize numLayers directionality initialization dtype device)</span><span>
</span><span id="line-376"></span><span>
</span><span id="line-377"></span><span class="hs-comment">-- | A long, short-term memory layer with either fixed initial</span><span>
</span><span id="line-378"></span><span class="hs-comment">-- states for the memory cells and hidden state or learnable</span><span>
</span><span id="line-379"></span><span class="hs-comment">-- inital states for the memory cells and hidden state.</span><span>
</span><span id="line-380"></span><span class="hs-comment">--</span><span>
</span><span id="line-381"></span><span class="hs-keyword">data</span><span> </span><span id="GRUWithInit"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-var">GRUWithInit</span></a></span></span><span>
</span><span id="line-382"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730113"><span class="annot"><a href="#local-6989586621679730113"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-383"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730112"><span class="annot"><a href="#local-6989586621679730112"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-384"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730111"><span class="annot"><a href="#local-6989586621679730111"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span>
</span><span id="line-385"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730110"><span class="annot"><a href="#local-6989586621679730110"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNDirectionality"><span class="hs-identifier hs-type">RNNDirectionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-386"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730109"><span class="annot"><a href="#local-6989586621679730109"><span class="hs-identifier hs-type">initialization</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#RNNInitialization"><span class="hs-identifier hs-type">RNNInitialization</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-387"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730108"><span class="annot"><a href="#local-6989586621679730108"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.DType.html#DType"><span class="hs-identifier hs-type">D.DType</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-388"></span><span>  </span><span class="hs-special">(</span><span id="local-6989586621679730107"><span class="annot"><a href="#local-6989586621679730107"><span class="hs-identifier hs-type">device</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Device.html#DeviceType"><span class="hs-identifier hs-type">D.DeviceType</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Nat</span></span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-389"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-390"></span><span>  </span><span id="GRUWithConstInit"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInit"><span class="hs-identifier hs-var">GRUWithConstInit</span></a></span></span><span>
</span><span id="line-391"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679730567"><span class="annot"><a href="#local-6989586621679730567"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span id="local-6989586621679730566"><span class="annot"><a href="#local-6989586621679730566"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span id="local-6989586621679730565"><span class="annot"><a href="#local-6989586621679730565"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span id="local-6989586621679730564"><span class="annot"><a href="#local-6989586621679730564"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span id="local-6989586621679730563"><span class="annot"><a href="#local-6989586621679730563"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679730562"><span class="annot"><a href="#local-6989586621679730562"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-392"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="gruWithConstInit_gru"><span class="annot"><span class="annottext">GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  'ConstantInitialization
  dtype
  device
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithConstInit_gru"><span class="hs-identifier hs-var hs-var">gruWithConstInit_gru</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730567"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730566"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730565"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730564"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730563"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730562"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-393"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="gruWithConstInit_h"><span class="annot"><span class="annottext">GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  'ConstantInitialization
  dtype
  device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithConstInit_h"><span class="hs-identifier hs-var hs-var">gruWithConstInit_h</span></a></span></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730562"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730563"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730565"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730564"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730566"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-394"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-395"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730567"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730566"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730565"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730564"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730563"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730562"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-396"></span><span>  </span><span id="GRUWithLearnedInit"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInit"><span class="hs-identifier hs-var">GRUWithLearnedInit</span></a></span></span><span>
</span><span id="line-397"></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span> </span><span id="local-6989586621679730549"><span class="annot"><a href="#local-6989586621679730549"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span> </span><span id="local-6989586621679730548"><span class="annot"><a href="#local-6989586621679730548"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span> </span><span id="local-6989586621679730547"><span class="annot"><a href="#local-6989586621679730547"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span> </span><span id="local-6989586621679730546"><span class="annot"><a href="#local-6989586621679730546"><span class="hs-identifier hs-type">directionality</span></a></span></span><span> </span><span id="local-6989586621679730545"><span class="annot"><a href="#local-6989586621679730545"><span class="hs-identifier hs-type">dtype</span></a></span></span><span> </span><span id="local-6989586621679730544"><span class="annot"><a href="#local-6989586621679730544"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-398"></span><span>     </span><span class="hs-operator">.</span><span> </span><span class="hs-special">{</span><span> </span><span id="gruWithLearnedInit_gru"><span class="annot"><span class="annottext">GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  'LearnedInitialization
  dtype
  device
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithLearnedInit_gru"><span class="hs-identifier hs-var hs-var">gruWithLearnedInit_gru</span></a></span></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730549"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730548"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730547"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730546"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730545"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730544"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-399"></span><span>       </span><span class="hs-special">,</span><span> </span><span id="gruWithLearnedInit_h"><span class="annot"><span class="annottext">GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  'LearnedInitialization
  dtype
  device
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithLearnedInit_h"><span class="hs-identifier hs-var hs-var">gruWithLearnedInit_h</span></a></span></span><span>    </span><span class="hs-glyph">::</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730544"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730545"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730547"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730546"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730548"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-400"></span><span>       </span><span class="hs-special">}</span><span>
</span><span id="line-401"></span><span>    </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730549"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730548"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730547"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730546"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-type">LearnedInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730545"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730544"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-402"></span><span>
</span><span id="line-403"></span><span id="local-6989586621679730088"><span id="local-6989586621679730090"><span id="local-6989586621679730092"><span id="local-6989586621679730094"><span id="local-6989586621679730095"><span id="local-6989586621679730096"><span id="local-6989586621679730097"><span id="local-6989586621679730098"><span id="local-6989586621679730099"><span id="local-6989586621679730100"><span class="hs-keyword">deriving</span><span> </span><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Show</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730100"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730099"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730098"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730097"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730096"><span class="hs-identifier hs-type">initialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730095"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730094"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span></span></span></span></span><span>
</span><span id="line-404"></span><span class="hs-comment">-- TODO: https://ryanglscott.github.io/2018/02/11/how-to-derive-generic-for-some-gadts/</span><span>
</span><span id="line-405"></span><span class="hs-comment">-- deriving instance Generic (GRUWithInit inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device)</span><span>
</span><span id="line-406"></span><span>
</span><span id="line-407"></span><span id="local-6989586621679730082"><span id="local-6989586621679730083"><span id="local-6989586621679730084"><span id="local-6989586621679730085"><span id="local-6989586621679730086"><span id="local-6989586621679730087"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Generic</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730087"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730086"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730085"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730084"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730083"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730082"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-408"></span><span>  </span><span class="hs-keyword">type</span><span> </span><span id="Rep"><span class="annot"><span class="hs-identifier hs-var">Rep</span></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730087"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730086"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730085"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730084"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730083"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730082"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-409"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Rec0</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730087"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730086"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730085"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730084"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730083"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730082"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-410"></span><span>      </span><span class="annot"><span class="hs-operator hs-type">:*:</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Rec0</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730082"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730083"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730085"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730084"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730086"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-411"></span><span>
</span><span id="line-412"></span><span>  </span><span id="local-6989586621679730079"><span class="annot"><span class="annottext">from :: GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  'ConstantInitialization
  dtype
  device
-&gt; Rep
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'ConstantInitialization
        dtype
        device)
     x
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">from</span></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730076"><span id="local-6989586621679730077"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInit"><span class="hs-identifier hs-type">GRUWithConstInit</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; K1
     R
     (GRU inputSize hiddenSize numLayers directionality dtype device)
     x
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730077"><span class="hs-identifier hs-var">gruWithConstInit_gru</span></a></span><span> </span><span class="annot"><span class="annottext">K1
  R
  (GRU inputSize hiddenSize numLayers directionality dtype device)
  x
-&gt; K1
     R
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
     x
-&gt; (:*:)
     (K1
        R (GRU inputSize hiddenSize numLayers directionality dtype device))
     (K1
        R
        (Tensor
           device
           dtype
           '[numLayers * NumberOfDirections directionality, hiddenSize]))
     x
forall k (f :: k -&gt; Type) (g :: k -&gt; Type) (p :: k).
f p -&gt; g p -&gt; (:*:) f g p
</span><span class="hs-operator hs-var">:*:</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; K1
     R
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
     x
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730076"><span class="hs-identifier hs-var">gruWithConstInit_h</span></a></span><span>
</span><span id="line-413"></span><span>  </span><span id="local-6989586621679730074"><span class="annot"><span class="annottext">to :: Rep
  (GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device)
  x
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
</span><span class="hs-identifier hs-var hs-var hs-var hs-var">to</span></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span id="local-6989586621679730072"><span class="annot"><a href="#local-6989586621679730072"><span class="hs-identifier hs-var">gru</span></a></span></span><span> </span><span class="annot"><span class="hs-operator hs-type">:*:</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span id="local-6989586621679730071"><span class="annot"><a href="#local-6989586621679730071"><span class="hs-identifier hs-var">h</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInit"><span class="hs-identifier hs-var">GRUWithConstInit</span></a></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730072"><span class="hs-identifier hs-var">gru</span></a></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730071"><span class="hs-identifier hs-var">h</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-414"></span><span>
</span><span id="line-415"></span><span id="local-6989586621679730065"><span id="local-6989586621679730066"><span id="local-6989586621679730067"><span id="local-6989586621679730068"><span id="local-6989586621679730069"><span id="local-6989586621679730070"><span class="hs-keyword">instance</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Generic</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730070"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730069"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730068"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730067"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-type">LearnedInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730066"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730065"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-416"></span><span>  </span><span class="hs-keyword">type</span><span> </span><span id="Rep"><span class="annot"><span class="hs-identifier hs-var">Rep</span></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730070"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730069"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730068"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730067"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-type">LearnedInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730066"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730065"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-417"></span><span>    </span><span class="annot"><span class="hs-identifier hs-type">Rec0</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730070"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730069"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730068"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730067"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730066"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730065"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-418"></span><span>      </span><span class="annot"><span class="hs-operator hs-type">:*:</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">Rec0</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameter"><span class="hs-identifier hs-type">Parameter</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730065"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730066"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730068"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730067"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730069"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span class="hs-special">)</span><span>
</span><span id="line-419"></span><span>
</span><span id="line-420"></span><span>  </span><span id="local-6989586621679730062"><span class="annot"><span class="annottext">from :: GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  'LearnedInitialization
  dtype
  device
-&gt; Rep
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'LearnedInitialization
        dtype
        device)
     x
</span><a href="#local-6989586621679730062"><span class="hs-identifier hs-var hs-var hs-var hs-var">from</span></a></span></span><span> </span><span class="hs-special">(</span><span id="local-6989586621679730060"><span id="local-6989586621679730061"><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInit"><span class="hs-identifier hs-type">GRUWithLearnedInit</span></a></span><span> </span><span class="hs-special">{</span><span class="hs-glyph">..</span><span class="hs-special">}</span></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; K1
     R
     (GRU inputSize hiddenSize numLayers directionality dtype device)
     x
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730061"><span class="hs-identifier hs-var">gruWithLearnedInit_gru</span></a></span><span> </span><span class="annot"><span class="annottext">K1
  R
  (GRU inputSize hiddenSize numLayers directionality dtype device)
  x
-&gt; K1
     R
     (Parameter
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
     x
-&gt; (:*:)
     (K1
        R (GRU inputSize hiddenSize numLayers directionality dtype device))
     (K1
        R
        (Parameter
           device
           dtype
           '[numLayers * NumberOfDirections directionality, hiddenSize]))
     x
forall k (f :: k -&gt; Type) (g :: k -&gt; Type) (p :: k).
f p -&gt; g p -&gt; (:*:) f g p
</span><span class="hs-operator hs-var">:*:</span></span><span> </span><span class="annot"><span class="annottext">Parameter
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; K1
     R
     (Parameter
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
     x
forall k i c (p :: k). c -&gt; K1 i c p
</span><span class="hs-identifier hs-var">K1</span></span><span> </span><span class="annot"><span class="annottext">Parameter
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730060"><span class="hs-identifier hs-var">gruWithLearnedInit_h</span></a></span><span>
</span><span id="line-421"></span><span>  </span><span id="local-6989586621679730059"><span class="annot"><span class="annottext">to :: Rep
  (GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'LearnedInitialization
     dtype
     device)
  x
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'LearnedInitialization
     dtype
     device
</span><a href="#local-6989586621679730059"><span class="hs-identifier hs-var hs-var hs-var hs-var">to</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span id="local-6989586621679730058"><span class="annot"><a href="#local-6989586621679730058"><span class="hs-identifier hs-var">gru</span></a></span></span><span> </span><span class="annot"><span class="hs-operator hs-type">:*:</span></span><span> </span><span class="annot"><span class="hs-identifier hs-type">K1</span></span><span> </span><span id="local-6989586621679730057"><span class="annot"><a href="#local-6989586621679730057"><span class="hs-identifier hs-var">h</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'LearnedInitialization
     dtype
     device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'LearnedInitialization
     dtype
     device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInit"><span class="hs-identifier hs-var">GRUWithLearnedInit</span></a></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730058"><span class="hs-identifier hs-var">gru</span></a></span><span> </span><span class="annot"><span class="annottext">Parameter
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730057"><span class="hs-identifier hs-var">h</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-422"></span><span>
</span><span id="line-423"></span><span id="local-6989586621679730051"><span id="local-6989586621679730052"><span id="local-6989586621679730053"><span id="local-6989586621679730054"><span id="local-6989586621679730055"><span id="local-6989586621679730056"><span class="hs-keyword">instance</span><span>
</span><span id="line-424"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730056"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-425"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730055"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-426"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730054"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-427"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730053"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-428"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730052"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-429"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730051"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730056"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730055"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730054"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730053"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730052"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-430"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730051"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730056"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730055"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730054"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730053"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730052"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-431"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-type">GRUWithInitSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730051"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730056"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730055"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730054"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730053"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730052"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-432"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730051"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730056"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730055"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730054"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#ConstantInitialization"><span class="hs-identifier hs-type">ConstantInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730053"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730052"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-433"></span><span>  </span><span id="local-6989586621679730049"><span class="annot"><span class="annottext">sample :: GRUWithInitSpec
  inputSize
  hiddenSize
  numLayers
  directionality
  'ConstantInitialization
  dtype
  device
-&gt; IO
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'ConstantInitialization
        dtype
        device)
</span><a href="#local-6989586621679730049"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithZerosInitSpec"><span class="hs-identifier hs-type">GRUWithZerosInitSpec</span></a></span><span> </span><span id="local-6989586621679730048"><span class="annot"><span class="annottext">gruSpec :: GRUSpec inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730048"><span class="hs-identifier hs-var">gruSpec</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-434"></span><span>    </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInit"><span class="hs-identifier hs-var">GRUWithConstInit</span></a></span><span>
</span><span id="line-435"></span><span>      </span><span class="annot"><span class="annottext">(GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; Tensor
      device
      dtype
      '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; GRUWithInit
      inputSize
      hiddenSize
      numLayers
      directionality
      'ConstantInitialization
      dtype
      device)
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize]
      -&gt; GRUWithInit
           inputSize
           hiddenSize
           numLayers
           directionality
           'ConstantInitialization
           dtype
           device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">GRUSpec inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730048"><span class="hs-identifier hs-var">gruSpec</span></a></span><span>
</span><span id="line-436"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
   -&gt; GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'ConstantInitialization
        dtype
        device)
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
-&gt; IO
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'ConstantInitialization
        dtype
        device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
TensorOptions shape dtype device =&gt;
Tensor device dtype shape
</span><a href="Torch.Typed.Factories.html#zeros"><span class="hs-identifier hs-var">zeros</span></a></span><span>
</span><span id="line-437"></span><span>  </span><span class="annot"><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">sample</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInitSpec"><span class="hs-identifier hs-type">GRUWithConstInitSpec</span></a></span><span> </span><span id="local-6989586621679730047"><span class="annot"><span class="annottext">gruSpec :: GRUSpec inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730047"><span class="hs-identifier hs-var">gruSpec</span></a></span></span><span> </span><span id="local-6989586621679730046"><span class="annot"><span class="annottext">h :: Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730046"><span class="hs-identifier hs-var">h</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-438"></span><span>    </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'ConstantInitialization
     dtype
     device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInit"><span class="hs-identifier hs-var">GRUWithConstInit</span></a></span><span>
</span><span id="line-439"></span><span>      </span><span class="annot"><span class="annottext">(GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; Tensor
      device
      dtype
      '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; GRUWithInit
      inputSize
      hiddenSize
      numLayers
      directionality
      'ConstantInitialization
      dtype
      device)
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize]
      -&gt; GRUWithInit
           inputSize
           hiddenSize
           numLayers
           directionality
           'ConstantInitialization
           dtype
           device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">GRUSpec inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730047"><span class="hs-identifier hs-var">gruSpec</span></a></span><span>
</span><span id="line-440"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
   -&gt; GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'ConstantInitialization
        dtype
        device)
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
-&gt; IO
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'ConstantInitialization
        dtype
        device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730046"><span class="hs-identifier hs-var">h</span></a></span></span></span></span></span></span></span><span>
</span><span id="line-441"></span><span>
</span><span id="line-442"></span><span id="local-6989586621679730040"><span id="local-6989586621679730041"><span id="local-6989586621679730042"><span id="local-6989586621679730043"><span id="local-6989586621679730044"><span id="local-6989586621679730045"><span class="hs-keyword">instance</span><span>
</span><span id="line-443"></span><span>  </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730045"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-444"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730044"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-445"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730043"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-446"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDType"><span class="hs-identifier hs-type">KnownDType</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730042"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-447"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#KnownDevice"><span class="hs-identifier hs-type">KnownDevice</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730041"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-448"></span><span>  </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUSpec"><span class="hs-identifier hs-type">GRUSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730040"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730045"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730044"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730043"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730042"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730041"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-449"></span><span>                   </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730040"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730045"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730044"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730043"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730042"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730041"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-450"></span><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.NN.html#Randomizable"><span class="hs-identifier hs-type">A.Randomizable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInitSpec"><span class="hs-identifier hs-type">GRUWithInitSpec</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730040"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730045"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730044"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730043"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-type">LearnedInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730042"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730041"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-451"></span><span>                      </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span>     </span><span class="annot"><a href="#local-6989586621679730040"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730045"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730044"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730043"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="hs-special">'</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#LearnedInitialization"><span class="hs-identifier hs-type">LearnedInitialization</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730042"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730041"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-452"></span><span>  </span><span id="local-6989586621679730038"><span class="annot"><span class="annottext">sample :: GRUWithInitSpec
  inputSize
  hiddenSize
  numLayers
  directionality
  'LearnedInitialization
  dtype
  device
-&gt; IO
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'LearnedInitialization
        dtype
        device)
</span><a href="#local-6989586621679730038"><span class="hs-identifier hs-var hs-var hs-var hs-var">sample</span></a></span></span><span> </span><span id="local-6989586621679730037"><span class="annot"><span class="annottext">s :: GRUWithInitSpec
  inputSize
  hiddenSize
  numLayers
  directionality
  'LearnedInitialization
  dtype
  device
</span><a href="#local-6989586621679730037"><span class="hs-identifier hs-var">s</span></a></span></span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInitSpec"><span class="hs-identifier hs-type">GRUWithLearnedInitSpec</span></a></span><span> </span><span id="local-6989586621679730036"><span class="annot"><span class="annottext">gruSpec :: GRUSpec inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730036"><span class="hs-identifier hs-var">gruSpec</span></a></span></span><span> </span><span id="local-6989586621679730035"><span class="annot"><span class="annottext">h :: Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730035"><span class="hs-identifier hs-var">h</span></a></span></span><span class="hs-special">)</span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-453"></span><span>    </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'LearnedInitialization
     dtype
     device
forall (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat)
       (directionality :: RNNDirectionality) (dtype :: DType)
       (device :: (DeviceType, Nat)).
GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     'LearnedInitialization
     dtype
     device
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInit"><span class="hs-identifier hs-var">GRUWithLearnedInit</span></a></span><span>
</span><span id="line-454"></span><span>      </span><span class="annot"><span class="annottext">(GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; Parameter
      device
      dtype
      '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; GRUWithInit
      inputSize
      hiddenSize
      numLayers
      directionality
      'LearnedInitialization
      dtype
      device)
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
-&gt; IO
     (Parameter
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize]
      -&gt; GRUWithInit
           inputSize
           hiddenSize
           numLayers
           directionality
           'LearnedInitialization
           dtype
           device)
forall (f :: Type -&gt; Type) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">GRUSpec inputSize hiddenSize numLayers directionality dtype device
-&gt; IO
     (GRU inputSize hiddenSize numLayers directionality dtype device)
forall spec f. Randomizable spec f =&gt; spec -&gt; IO f
</span><a href="Torch.NN.html#sample"><span class="hs-identifier hs-var">A.sample</span></a></span><span> </span><span class="annot"><span class="annottext">GRUSpec inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730036"><span class="hs-identifier hs-var">gruSpec</span></a></span><span>
</span><span id="line-455"></span><span>      </span><span class="annot"><span class="annottext">IO
  (Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
   -&gt; GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'LearnedInitialization
        dtype
        device)
-&gt; IO
     (Parameter
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
-&gt; IO
     (GRUWithInit
        inputSize
        hiddenSize
        numLayers
        directionality
        'LearnedInitialization
        dtype
        device)
forall (f :: Type -&gt; Type) a b.
Applicative f =&gt;
f (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;*&gt;</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; IO
     (Parameter
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Tensor device dtype shape -&gt; IO (Parameter device dtype shape)
</span><a href="Torch.Typed.Parameter.html#makeIndependent"><span class="hs-identifier hs-var">makeIndependent</span></a></span><span> </span><span class="annot"><span class="annottext">(Tensor
   device
   dtype
   '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; IO
      (Parameter
         device
         dtype
         '[numLayers * NumberOfDirections directionality, hiddenSize]))
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
-&gt; IO
     (Parameter
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
forall (m :: Type -&gt; Type) a b. Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b
</span><span class="hs-operator hs-var">=&lt;&lt;</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; IO
     (Tensor
        device
        dtype
        '[numLayers * NumberOfDirections directionality, hiddenSize])
forall (f :: Type -&gt; Type) a. Applicative f =&gt; a -&gt; f a
</span><span class="hs-identifier hs-var">pure</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730035"><span class="hs-identifier hs-var">h</span></a></span><span class="hs-special">)</span></span></span></span></span></span></span><span>
</span><span id="line-456"></span><span>
</span><span id="line-457"></span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#gru"><span class="hs-identifier hs-type">gru</span></a></span><span>
</span><span id="line-458"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span>
</span><span id="line-459"></span><span>       </span><span id="local-6989586621679730455"><span class="annot"><a href="#local-6989586621679730455"><span class="hs-identifier hs-type">shapeOrder</span></a></span></span><span>
</span><span id="line-460"></span><span>       </span><span id="local-6989586621679730457"><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span>
</span><span id="line-461"></span><span>       </span><span id="local-6989586621679730452"><span class="annot"><a href="#local-6989586621679730452"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span>
</span><span id="line-462"></span><span>       </span><span id="local-6989586621679730459"><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span></span><span>
</span><span id="line-463"></span><span>       </span><span id="local-6989586621679730444"><span class="annot"><a href="#local-6989586621679730444"><span class="hs-identifier hs-type">initialization</span></a></span></span><span>
</span><span id="line-464"></span><span>       </span><span id="local-6989586621679730458"><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span>
</span><span id="line-465"></span><span>       </span><span id="local-6989586621679730451"><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span>
</span><span id="line-466"></span><span>       </span><span id="local-6989586621679730454"><span class="annot"><a href="#local-6989586621679730454"><span class="hs-identifier hs-type">outputSize</span></a></span></span><span>
</span><span id="line-467"></span><span>       </span><span id="local-6989586621679730456"><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span>
</span><span id="line-468"></span><span>       </span><span id="local-6989586621679730453"><span class="annot"><a href="#local-6989586621679730453"><span class="hs-identifier hs-type">inputShape</span></a></span></span><span>
</span><span id="line-469"></span><span>       </span><span id="local-6989586621679730450"><span class="annot"><a href="#local-6989586621679730450"><span class="hs-identifier hs-type">outputShape</span></a></span></span><span>
</span><span id="line-470"></span><span>       </span><span id="local-6989586621679730449"><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span></span><span>
</span><span id="line-471"></span><span>       </span><span id="local-6989586621679730446"><span class="annot"><a href="#local-6989586621679730446"><span class="hs-identifier hs-type">parameters</span></a></span></span><span>
</span><span id="line-472"></span><span>       </span><span id="local-6989586621679730445"><span class="annot"><a href="#local-6989586621679730445"><span class="hs-identifier hs-type">tensorParameters</span></a></span></span><span>
</span><span id="line-473"></span><span>       </span><span id="local-6989586621679730448"><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span></span><span>
</span><span id="line-474"></span><span>       </span><span id="local-6989586621679730447"><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-475"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-476"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-477"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span>
</span><span id="line-478"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-479"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#KnownRNNShapeOrder"><span class="hs-identifier hs-type">KnownRNNShapeOrder</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730455"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span>
</span><span id="line-480"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#KnownRNNDirectionality"><span class="hs-identifier hs-type">KnownRNNDirectionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-481"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730454"><span class="hs-identifier hs-type">outputSize</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-482"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730453"><span class="hs-identifier hs-type">inputShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNShape"><span class="hs-identifier hs-type">RNNShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730455"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730452"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-483"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730450"><span class="hs-identifier hs-type">outputShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNShape"><span class="hs-identifier hs-type">RNNShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730455"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730452"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730454"><span class="hs-identifier hs-type">outputSize</span></a></span><span>
</span><span id="line-484"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-485"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameterized"><span class="hs-identifier hs-type">Parameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679730446"><span class="hs-identifier hs-type">parameters</span></a></span><span>
</span><span id="line-486"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730445"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#GRUR"><span class="hs-identifier hs-type">GRUR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-487"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730445"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.Tensor.html#ATenTensor"><span class="hs-identifier hs-type">D.ATenTensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-488"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.HList.html#HMap%27"><span class="hs-identifier hs-type">HMap'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#ToDependent"><span class="hs-identifier hs-type">ToDependent</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730446"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730445"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span>
</span><span id="line-489"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-490"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Bool</span></span><span>
</span><span id="line-491"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span>
</span><span id="line-492"></span><span>       </span><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-493"></span><span>       </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-494"></span><span>       </span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-495"></span><span>       </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-496"></span><span>       </span><span class="annot"><a href="#local-6989586621679730444"><span class="hs-identifier hs-type">initialization</span></a></span><span>
</span><span id="line-497"></span><span>       </span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-498"></span><span>       </span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-499"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730453"><span class="hs-identifier hs-type">inputShape</span></a></span><span>
</span><span id="line-500"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730450"><span class="hs-identifier hs-type">outputShape</span></a></span><span>
</span><span id="line-501"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-502"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-503"></span><span id="gru"><span class="annot"><span class="annottext">gru :: Bool
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     initialization
     dtype
     device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gru"><span class="hs-identifier hs-var hs-var">gru</span></a></span></span><span> </span><span id="local-6989586621679730033"><span class="annot"><span class="annottext">dropoutOn :: Bool
</span><a href="#local-6989586621679730033"><span class="hs-identifier hs-var">dropoutOn</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithConstInit"><span class="hs-identifier hs-type">GRUWithConstInit</span></a></span><span> </span><span id="local-6989586621679730032"><span class="annot"><span class="annottext">gru :: GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730032"><span class="hs-identifier hs-var">gru</span></a></span></span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span> </span><span id="local-6989586621679730030"><span class="annot"><span class="annottext">dropoutProb :: Double
</span><a href="#local-6989586621679730030"><span class="hs-identifier hs-var">dropoutProb</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679730029"><span class="annot"><span class="annottext">hc :: Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730029"><span class="hs-identifier hs-var">hc</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679730028"><span class="annot"><span class="annottext">input :: Tensor device dtype inputShape
</span><a href="#local-6989586621679730028"><span class="hs-identifier hs-var">input</span></a></span></span><span>
</span><span id="line-504"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList tensorParameters
-&gt; Double
-&gt; Bool
-&gt; Tensor device dtype hcShape
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
forall k (shapeOrder :: RNNShapeOrder)
       (directionality :: RNNDirectionality) (numLayers :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (inputSize :: Nat)
       (outputSize :: Nat) (hiddenSize :: Nat) (inputShape :: [Nat])
       (outputShape :: [Nat]) (hcShape :: [Nat]) (tensorParameters :: [k])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat numLayers, KnownRNNShapeOrder shapeOrder,
 KnownRNNDirectionality directionality,
 outputSize ~ (hiddenSize * NumberOfDirections directionality),
 inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize,
 outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize,
 hcShape
 ~ '[numLayers * NumberOfDirections directionality, batchSize,
     hiddenSize],
 tensorParameters
 ~ GRUR inputSize hiddenSize numLayers directionality dtype device,
 Castable (HList tensorParameters) [ATenTensor]) =&gt;
HList tensorParameters
-&gt; Double
-&gt; Bool
-&gt; Tensor device dtype hcShape
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.Functional.html#gru"><span class="hs-identifier hs-var">Torch.Typed.Functional.gru</span></a></span><span>
</span><span id="line-505"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730455"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span>
</span><span id="line-506"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-507"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-508"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730452"><span class="hs-identifier hs-type">seqLen</span></a></span><span>
</span><span id="line-509"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span>
</span><span id="line-510"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-511"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730454"><span class="hs-identifier hs-type">outputSize</span></a></span><span>
</span><span id="line-512"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-513"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730453"><span class="hs-identifier hs-type">inputShape</span></a></span><span>
</span><span id="line-514"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730450"><span class="hs-identifier hs-type">outputShape</span></a></span><span>
</span><span id="line-515"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-516"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730445"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span>
</span><span id="line-517"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-518"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-519"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ToDependent -&gt; HList parameters -&gt; HList tensorParameters
forall k f (xs :: [k]) (ys :: [k]).
HMap' f xs ys =&gt;
f -&gt; HList xs -&gt; HList ys
</span><a href="Torch.HList.html#hmap%27"><span class="hs-identifier hs-var">hmap'</span></a></span><span> </span><span class="annot"><span class="annottext">ToDependent
</span><a href="Torch.Typed.Parameter.html#ToDependent"><span class="hs-identifier hs-var">ToDependent</span></a></span><span> </span><span class="annot"><span class="annottext">(HList parameters -&gt; HList tensorParameters)
-&gt; (GRU inputSize hiddenSize numLayers directionality dtype device
    -&gt; HList parameters)
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; HList tensorParameters
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; HList parameters
forall f (as :: [Type]). Parameterized f as =&gt; f -&gt; HList as
</span><a href="Torch.Typed.Parameter.html#flattenParameters"><span class="hs-identifier hs-var">flattenParameters</span></a></span><span> </span><span class="annot"><span class="annottext">(GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; HList tensorParameters)
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; HList tensorParameters
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730032"><span class="hs-identifier hs-var">gru</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-520"></span><span>    </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679730030"><span class="hs-identifier hs-var">dropoutProb</span></a></span><span>
</span><span id="line-521"></span><span>    </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679730033"><span class="hs-identifier hs-var">dropoutOn</span></a></span><span>
</span><span id="line-522"></span><span>    </span><span class="annot"><span class="annottext">Tensor device dtype hcShape
</span><a href="#local-6989586621679730022"><span class="hs-identifier hs-var">hc'</span></a></span><span>
</span><span id="line-523"></span><span>    </span><span class="annot"><span class="annottext">Tensor device dtype inputShape
</span><a href="#local-6989586621679730028"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-524"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-525"></span><span>  </span><span id="local-6989586621679730022"><span class="annot"><span class="annottext">hc' :: Tensor device dtype hcShape
</span><a href="#local-6989586621679730022"><span class="hs-identifier hs-var hs-var">hc'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-526"></span><span>    </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape hcShape, Numel shape ~ Numel hcShape) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype hcShape
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-527"></span><span>      </span><span class="annot"><span class="annottext">(Tensor
   device
   dtype
   '[batchSize, numLayers * NumberOfDirections directionality,
     hiddenSize]
 -&gt; Tensor device dtype hcShape)
-&gt; (Tensor
      device
      dtype
      '[numLayers * NumberOfDirections directionality, hiddenSize]
    -&gt; Tensor
         device
         dtype
         '[batchSize, numLayers * NumberOfDirections directionality,
           hiddenSize])
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor device dtype hcShape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Bool
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor
     device
     dtype
     '[batchSize, numLayers * NumberOfDirections directionality,
       hiddenSize]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', shape' ~ Broadcast shape shape') =&gt;
Bool -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#expand"><span class="hs-identifier hs-var">expand</span></a></span><span>
</span><span id="line-528"></span><span>          </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-529"></span><span>          </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span> </span><span class="hs-comment">-- TODO: What does the bool do?</span><span>
</span><span id="line-530"></span><span>      </span><span class="annot"><span class="annottext">(Tensor
   device
   dtype
   '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; Tensor device dtype hcShape)
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor device dtype hcShape
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Tensor
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730029"><span class="hs-identifier hs-var">hc</span></a></span><span>
</span><span id="line-531"></span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#gru"><span class="hs-identifier hs-var">gru</span></a></span><span> </span><span id="local-6989586621679730019"><span class="annot"><span class="annottext">dropoutOn :: Bool
</span><a href="#local-6989586621679730019"><span class="hs-identifier hs-var">dropoutOn</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithLearnedInit"><span class="hs-identifier hs-type">GRUWithLearnedInit</span></a></span><span> </span><span id="local-6989586621679730018"><span class="annot"><span class="annottext">gru :: GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730018"><span class="hs-identifier hs-var">gru</span></a></span></span><span class="hs-glyph">@</span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="hs-identifier">_</span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.html#Dropout"><span class="hs-identifier hs-type">Dropout</span></a></span><span> </span><span id="local-6989586621679730017"><span class="annot"><span class="annottext">dropoutProb :: Double
</span><a href="#local-6989586621679730017"><span class="hs-identifier hs-var">dropoutProb</span></a></span></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span id="local-6989586621679730016"><span class="annot"><span class="annottext">hc :: Parameter
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730016"><span class="hs-identifier hs-var">hc</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621679730015"><span class="annot"><span class="annottext">input :: Tensor device dtype inputShape
</span><a href="#local-6989586621679730015"><span class="hs-identifier hs-var">input</span></a></span></span><span>
</span><span id="line-532"></span><span>  </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">HList tensorParameters
-&gt; Double
-&gt; Bool
-&gt; Tensor device dtype hcShape
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
forall k (shapeOrder :: RNNShapeOrder)
       (directionality :: RNNDirectionality) (numLayers :: Nat)
       (seqLen :: Nat) (batchSize :: Nat) (inputSize :: Nat)
       (outputSize :: Nat) (hiddenSize :: Nat) (inputShape :: [Nat])
       (outputShape :: [Nat]) (hcShape :: [Nat]) (tensorParameters :: [k])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat numLayers, KnownRNNShapeOrder shapeOrder,
 KnownRNNDirectionality directionality,
 outputSize ~ (hiddenSize * NumberOfDirections directionality),
 inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize,
 outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize,
 hcShape
 ~ '[numLayers * NumberOfDirections directionality, batchSize,
     hiddenSize],
 tensorParameters
 ~ GRUR inputSize hiddenSize numLayers directionality dtype device,
 Castable (HList tensorParameters) [ATenTensor]) =&gt;
HList tensorParameters
-&gt; Double
-&gt; Bool
-&gt; Tensor device dtype hcShape
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.Functional.html#gru"><span class="hs-identifier hs-var">Torch.Typed.Functional.gru</span></a></span><span>
</span><span id="line-533"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730455"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span>
</span><span id="line-534"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-535"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-536"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730452"><span class="hs-identifier hs-type">seqLen</span></a></span><span>
</span><span id="line-537"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span>
</span><span id="line-538"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730451"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-539"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730454"><span class="hs-identifier hs-type">outputSize</span></a></span><span>
</span><span id="line-540"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-541"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730453"><span class="hs-identifier hs-type">inputShape</span></a></span><span>
</span><span id="line-542"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730450"><span class="hs-identifier hs-type">outputShape</span></a></span><span>
</span><span id="line-543"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-544"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730445"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span>
</span><span id="line-545"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730448"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-546"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730447"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-547"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">ToDependent -&gt; HList parameters -&gt; HList tensorParameters
forall k f (xs :: [k]) (ys :: [k]).
HMap' f xs ys =&gt;
f -&gt; HList xs -&gt; HList ys
</span><a href="Torch.HList.html#hmap%27"><span class="hs-identifier hs-var">hmap'</span></a></span><span> </span><span class="annot"><span class="annottext">ToDependent
</span><a href="Torch.Typed.Parameter.html#ToDependent"><span class="hs-identifier hs-var">ToDependent</span></a></span><span> </span><span class="annot"><span class="annottext">(HList parameters -&gt; HList tensorParameters)
-&gt; (GRU inputSize hiddenSize numLayers directionality dtype device
    -&gt; HList parameters)
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; HList tensorParameters
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; HList parameters
forall f (as :: [Type]). Parameterized f as =&gt; f -&gt; HList as
</span><a href="Torch.Typed.Parameter.html#flattenParameters"><span class="hs-identifier hs-var">flattenParameters</span></a></span><span> </span><span class="annot"><span class="annottext">(GRU inputSize hiddenSize numLayers directionality dtype device
 -&gt; HList tensorParameters)
-&gt; GRU inputSize hiddenSize numLayers directionality dtype device
-&gt; HList tensorParameters
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">GRU inputSize hiddenSize numLayers directionality dtype device
</span><a href="#local-6989586621679730018"><span class="hs-identifier hs-var">gru</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-548"></span><span>    </span><span class="annot"><span class="annottext">Double
</span><a href="#local-6989586621679730017"><span class="hs-identifier hs-var">dropoutProb</span></a></span><span>
</span><span id="line-549"></span><span>    </span><span class="annot"><span class="annottext">Bool
</span><a href="#local-6989586621679730019"><span class="hs-identifier hs-var">dropoutOn</span></a></span><span>
</span><span id="line-550"></span><span>    </span><span class="annot"><span class="annottext">Tensor device dtype hcShape
</span><a href="#local-6989586621679730014"><span class="hs-identifier hs-var">hc'</span></a></span><span>
</span><span id="line-551"></span><span>    </span><span class="annot"><span class="annottext">Tensor device dtype inputShape
</span><a href="#local-6989586621679730015"><span class="hs-identifier hs-var">input</span></a></span><span>
</span><span id="line-552"></span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-553"></span><span>  </span><span id="local-6989586621679730014"><span class="annot"><span class="annottext">hc' :: Tensor device dtype hcShape
</span><a href="#local-6989586621679730014"><span class="hs-identifier hs-var hs-var">hc'</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-554"></span><span>    </span><span class="annot"><span class="annottext">forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', Numel shape ~ Numel shape') =&gt;
Tensor device dtype shape -&gt; Tensor device dtype shape'
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape hcShape, Numel shape ~ Numel hcShape) =&gt;
Tensor device dtype shape -&gt; Tensor device dtype hcShape
</span><a href="Torch.Typed.Tensor.html#reshape"><span class="hs-identifier hs-var">reshape</span></a></span><span> </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730449"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-555"></span><span>      </span><span class="annot"><span class="annottext">(Tensor
   device
   dtype
   '[batchSize, numLayers * NumberOfDirections directionality,
     hiddenSize]
 -&gt; Tensor device dtype hcShape)
-&gt; (Parameter
      device
      dtype
      '[numLayers * NumberOfDirections directionality, hiddenSize]
    -&gt; Tensor
         device
         dtype
         '[batchSize, numLayers * NumberOfDirections directionality,
           hiddenSize])
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor device dtype hcShape
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Bool
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor
     device
     dtype
     '[batchSize, numLayers * NumberOfDirections directionality,
       hiddenSize]
forall (shape' :: [Nat]) (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
(KnownShape shape', shape' ~ Broadcast shape shape') =&gt;
Bool -&gt; Tensor device dtype shape -&gt; Tensor device dtype shape'
</span><a href="Torch.Typed.Functional.html#expand"><span class="hs-identifier hs-var">expand</span></a></span><span>
</span><span id="line-556"></span><span>          </span><span class="hs-glyph">@</span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730457"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730458"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730459"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730456"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-557"></span><span>          </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span> </span><span class="hs-comment">-- TODO: What does the bool do?</span><span>
</span><span id="line-558"></span><span>      </span><span class="annot"><span class="annottext">(Tensor
   device
   dtype
   '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; Tensor
      device
      dtype
      '[batchSize, numLayers * NumberOfDirections directionality,
        hiddenSize])
-&gt; (Parameter
      device
      dtype
      '[numLayers * NumberOfDirections directionality, hiddenSize]
    -&gt; Tensor
         device
         dtype
         '[numLayers * NumberOfDirections directionality, hiddenSize])
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor
     device
     dtype
     '[batchSize, numLayers * NumberOfDirections directionality,
       hiddenSize]
forall b c a. (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
</span><span class="hs-operator hs-var">.</span></span><span> </span><span class="annot"><span class="annottext">Parameter
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
forall (shape :: [Nat]) (dtype :: DType)
       (device :: (DeviceType, Nat)).
Parameter device dtype shape -&gt; Tensor device dtype shape
</span><a href="Torch.Typed.Parameter.html#toDependent"><span class="hs-identifier hs-var">toDependent</span></a></span><span>
</span><span id="line-559"></span><span>      </span><span class="annot"><span class="annottext">(Parameter
   device
   dtype
   '[numLayers * NumberOfDirections directionality, hiddenSize]
 -&gt; Tensor device dtype hcShape)
-&gt; Parameter
     device
     dtype
     '[numLayers * NumberOfDirections directionality, hiddenSize]
-&gt; Tensor device dtype hcShape
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="annot"><span class="annottext">Parameter
  device
  dtype
  '[numLayers * NumberOfDirections directionality, hiddenSize]
</span><a href="#local-6989586621679730016"><span class="hs-identifier hs-var">hc</span></a></span><span>
</span><span id="line-560"></span><span>
</span><span id="line-561"></span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithDropout"><span class="hs-identifier hs-type">gruWithDropout</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithoutDropout"><span class="hs-identifier hs-type">gruWithoutDropout</span></a></span><span>
</span><span id="line-562"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="hs-keyword">forall</span><span>
</span><span id="line-563"></span><span>       </span><span id="local-6989586621679730010"><span class="annot"><a href="#local-6989586621679730010"><span class="hs-identifier hs-type">shapeOrder</span></a></span></span><span>
</span><span id="line-564"></span><span>       </span><span id="local-6989586621679730009"><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span></span><span>
</span><span id="line-565"></span><span>       </span><span id="local-6989586621679730008"><span class="annot"><a href="#local-6989586621679730008"><span class="hs-identifier hs-type">seqLen</span></a></span></span><span>
</span><span id="line-566"></span><span>       </span><span id="local-6989586621679730007"><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span></span><span>
</span><span id="line-567"></span><span>       </span><span id="local-6989586621679730006"><span class="annot"><a href="#local-6989586621679730006"><span class="hs-identifier hs-type">initialization</span></a></span></span><span>
</span><span id="line-568"></span><span>       </span><span id="local-6989586621679730005"><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span></span><span>
</span><span id="line-569"></span><span>       </span><span id="local-6989586621679730004"><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span></span><span>
</span><span id="line-570"></span><span>       </span><span id="local-6989586621679730003"><span class="annot"><a href="#local-6989586621679730003"><span class="hs-identifier hs-type">outputSize</span></a></span></span><span>
</span><span id="line-571"></span><span>       </span><span id="local-6989586621679730002"><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span></span><span>
</span><span id="line-572"></span><span>       </span><span id="local-6989586621679730001"><span class="annot"><a href="#local-6989586621679730001"><span class="hs-identifier hs-type">inputShape</span></a></span></span><span>
</span><span id="line-573"></span><span>       </span><span id="local-6989586621679730000"><span class="annot"><a href="#local-6989586621679730000"><span class="hs-identifier hs-type">outputShape</span></a></span></span><span>
</span><span id="line-574"></span><span>       </span><span id="local-6989586621679729999"><span class="annot"><a href="#local-6989586621679729999"><span class="hs-identifier hs-type">hcShape</span></a></span></span><span>
</span><span id="line-575"></span><span>       </span><span id="local-6989586621679729998"><span class="annot"><a href="#local-6989586621679729998"><span class="hs-identifier hs-type">parameters</span></a></span></span><span>
</span><span id="line-576"></span><span>       </span><span id="local-6989586621679729997"><span class="annot"><a href="#local-6989586621679729997"><span class="hs-identifier hs-type">tensorParameters</span></a></span></span><span>
</span><span id="line-577"></span><span>       </span><span id="local-6989586621679729996"><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span></span><span>
</span><span id="line-578"></span><span>       </span><span id="local-6989586621679729995"><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span></span><span>
</span><span id="line-579"></span><span>   </span><span class="hs-operator">.</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-580"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-581"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span><span>
</span><span id="line-582"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><span class="hs-identifier hs-type">KnownNat</span></span><span> </span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-583"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#KnownRNNShapeOrder"><span class="hs-identifier hs-type">KnownRNNShapeOrder</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730010"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span>
</span><span id="line-584"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#KnownRNNDirectionality"><span class="hs-identifier hs-type">KnownRNNDirectionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-585"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730003"><span class="hs-identifier hs-type">outputSize</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-586"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730001"><span class="hs-identifier hs-type">inputShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNShape"><span class="hs-identifier hs-type">RNNShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730010"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730008"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-587"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730000"><span class="hs-identifier hs-type">outputShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#RNNShape"><span class="hs-identifier hs-type">RNNShape</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730010"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730008"><span class="hs-identifier hs-type">seqLen</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730003"><span class="hs-identifier hs-type">outputSize</span></a></span><span>
</span><span id="line-588"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679729999"><span class="hs-identifier hs-type">hcShape</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="hs-special">'</span><span class="hs-special">[</span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><span class="hs-operator hs-type">*</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#NumberOfDirections"><span class="hs-identifier hs-type">NumberOfDirections</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-589"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#Parameterized"><span class="hs-identifier hs-type">Parameterized</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRU"><span class="hs-identifier hs-type">GRU</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span class="hs-special">)</span><span> </span><span class="annot"><a href="#local-6989586621679729998"><span class="hs-identifier hs-type">parameters</span></a></span><span>
</span><span id="line-590"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="#local-6989586621679729997"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span> </span><span class="annot"><span class="hs-glyph hs-type">~</span></span><span> </span><span class="annot"><a href="Torch.Typed.Functional.html#GRUR"><span class="hs-identifier hs-type">GRUR</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-591"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi/html/src"><span class="hs-identifier hs-type">ATen.Castable</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="Torch.HList.html#HList"><span class="hs-identifier hs-type">HList</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729997"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span class="hs-special">)</span><span> </span><span class="hs-special">[</span><span class="annot"><a href="Torch.Tensor.html#ATenTensor"><span class="hs-identifier hs-type">D.ATenTensor</span></a></span><span class="hs-special">]</span><span>
</span><span id="line-592"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.HList.html#HMap%27"><span class="hs-identifier hs-type">HMap'</span></a></span><span> </span><span class="annot"><a href="Torch.Typed.Parameter.html#ToDependent"><span class="hs-identifier hs-type">ToDependent</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729998"><span class="hs-identifier hs-type">parameters</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729997"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span>
</span><span id="line-593"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-594"></span><span>  </span><span class="hs-glyph">=&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.NN.Recurrent.GRU.html#GRUWithInit"><span class="hs-identifier hs-type">GRUWithInit</span></a></span><span>
</span><span id="line-595"></span><span>       </span><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-596"></span><span>       </span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-597"></span><span>       </span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-598"></span><span>       </span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-599"></span><span>       </span><span class="annot"><a href="#local-6989586621679730006"><span class="hs-identifier hs-type">initialization</span></a></span><span>
</span><span id="line-600"></span><span>       </span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-601"></span><span>       </span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-602"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730001"><span class="hs-identifier hs-type">inputShape</span></a></span><span>
</span><span id="line-603"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679730000"><span class="hs-identifier hs-type">outputShape</span></a></span><span>
</span><span id="line-604"></span><span>     </span><span class="hs-special">,</span><span> </span><span class="annot"><a href="Torch.Typed.Tensor.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span> </span><span class="annot"><a href="#local-6989586621679729999"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-605"></span><span>     </span><span class="hs-special">)</span><span>
</span><span id="line-606"></span><span class="hs-comment">-- ^ Forward propagate the `GRU` module and apply dropout on the outputs of each layer.</span><span>
</span><span id="line-607"></span><span class="hs-comment">--</span><span>
</span><span id="line-608"></span><span class="hs-comment">-- &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn</span><span>
</span><span id="line-609"></span><span class="hs-comment">-- &gt;&gt;&gt; spec = GRUWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (GRUSpec (DropoutSpec 0.5))</span><span>
</span><span id="line-610"></span><span class="hs-comment">-- &gt;&gt;&gt; model &lt;- A.sample spec</span><span>
</span><span id="line-611"></span><span class="hs-comment">-- &gt;&gt;&gt; :t gruWithDropout @'BatchFirst model input</span><span>
</span><span id="line-612"></span><span class="hs-comment">-- gruWithDropout @'BatchFirst model input</span><span>
</span><span id="line-613"></span><span class="hs-comment">--   :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],</span><span>
</span><span id="line-614"></span><span class="hs-comment">--       Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])</span><span>
</span><span id="line-615"></span><span class="hs-comment">-- &gt;&gt;&gt; gruWithDropout @'BatchFirst model input</span><span>
</span><span id="line-616"></span><span class="hs-comment">-- (Tensor Float [5,16,60] ,Tensor Float [6,5,30] )</span><span>
</span><span id="line-617"></span><span id="gruWithDropout"><span class="annot"><span class="annottext">gruWithDropout :: GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  initialization
  dtype
  device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithDropout"><span class="hs-identifier hs-var hs-var">gruWithDropout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-618"></span><span>  </span><span class="annot"><span class="annottext">Bool
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     initialization
     dtype
     device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
forall (shapeOrder :: RNNShapeOrder) (batchSize :: Nat)
       (seqLen :: Nat) (directionality :: RNNDirectionality)
       (initialization :: RNNInitialization) (numLayers :: Nat)
       (inputSize :: Nat) (outputSize :: Nat) (hiddenSize :: Nat)
       (inputShape :: [Nat]) (outputShape :: [Nat]) (hcShape :: [Nat])
       (parameters :: [Type]) (tensorParameters :: [Type])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat (NumberOfDirections directionality), KnownNat numLayers,
 KnownNat batchSize, KnownNat hiddenSize,
 KnownRNNShapeOrder shapeOrder,
 KnownRNNDirectionality directionality,
 outputSize ~ (hiddenSize * NumberOfDirections directionality),
 inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize,
 outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize,
 hcShape
 ~ '[numLayers * NumberOfDirections directionality, batchSize,
     hiddenSize],
 Parameterized
   (GRU inputSize hiddenSize numLayers directionality dtype device)
   parameters,
 tensorParameters
 ~ GRUR inputSize hiddenSize numLayers directionality dtype device,
 Castable (HList tensorParameters) [ATenTensor],
 HMap' ToDependent parameters tensorParameters) =&gt;
Bool
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     initialization
     dtype
     device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gru"><span class="hs-identifier hs-var">Torch.Typed.NN.Recurrent.GRU.gru</span></a></span><span>
</span><span id="line-619"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730010"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span>
</span><span id="line-620"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span><span>
</span><span id="line-621"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730008"><span class="hs-identifier hs-type">seqLen</span></a></span><span>
</span><span id="line-622"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-623"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730006"><span class="hs-identifier hs-type">initialization</span></a></span><span>
</span><span id="line-624"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-625"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-626"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730003"><span class="hs-identifier hs-type">outputSize</span></a></span><span>
</span><span id="line-627"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-628"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730001"><span class="hs-identifier hs-type">inputShape</span></a></span><span>
</span><span id="line-629"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730000"><span class="hs-identifier hs-type">outputShape</span></a></span><span>
</span><span id="line-630"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729999"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-631"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729998"><span class="hs-identifier hs-type">parameters</span></a></span><span>
</span><span id="line-632"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729997"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span>
</span><span id="line-633"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-634"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-635"></span><span>    </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">True</span></span><span>
</span><span id="line-636"></span><span class="hs-comment">-- ^ Forward propagate the `GRU` module (without applying dropout on the outputs of each layer).</span><span>
</span><span id="line-637"></span><span class="hs-comment">--</span><span>
</span><span id="line-638"></span><span class="hs-comment">-- &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn</span><span>
</span><span id="line-639"></span><span class="hs-comment">-- &gt;&gt;&gt; spec = GRUWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (GRUSpec (DropoutSpec 0.5))</span><span>
</span><span id="line-640"></span><span class="hs-comment">-- &gt;&gt;&gt; model &lt;- A.sample spec</span><span>
</span><span id="line-641"></span><span class="hs-comment">-- &gt;&gt;&gt; :t gruWithoutDropout @'BatchFirst model input</span><span>
</span><span id="line-642"></span><span class="hs-comment">-- gruWithoutDropout @'BatchFirst model input</span><span>
</span><span id="line-643"></span><span class="hs-comment">--   :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],</span><span>
</span><span id="line-644"></span><span class="hs-comment">--       Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])</span><span>
</span><span id="line-645"></span><span class="hs-comment">-- &gt;&gt;&gt; gruWithoutDropout @'BatchFirst model input</span><span>
</span><span id="line-646"></span><span class="hs-comment">-- (Tensor Float [5,16,60] ,Tensor Float [6,5,30] )</span><span>
</span><span id="line-647"></span><span id="gruWithoutDropout"><span class="annot"><span class="annottext">gruWithoutDropout :: GRUWithInit
  inputSize
  hiddenSize
  numLayers
  directionality
  initialization
  dtype
  device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gruWithoutDropout"><span class="hs-identifier hs-var hs-var">gruWithoutDropout</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-648"></span><span>  </span><span class="annot"><span class="annottext">Bool
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     initialization
     dtype
     device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
forall (shapeOrder :: RNNShapeOrder) (batchSize :: Nat)
       (seqLen :: Nat) (directionality :: RNNDirectionality)
       (initialization :: RNNInitialization) (numLayers :: Nat)
       (inputSize :: Nat) (outputSize :: Nat) (hiddenSize :: Nat)
       (inputShape :: [Nat]) (outputShape :: [Nat]) (hcShape :: [Nat])
       (parameters :: [Type]) (tensorParameters :: [Type])
       (dtype :: DType) (device :: (DeviceType, Nat)).
(KnownNat (NumberOfDirections directionality), KnownNat numLayers,
 KnownNat batchSize, KnownNat hiddenSize,
 KnownRNNShapeOrder shapeOrder,
 KnownRNNDirectionality directionality,
 outputSize ~ (hiddenSize * NumberOfDirections directionality),
 inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize,
 outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize,
 hcShape
 ~ '[numLayers * NumberOfDirections directionality, batchSize,
     hiddenSize],
 Parameterized
   (GRU inputSize hiddenSize numLayers directionality dtype device)
   parameters,
 tensorParameters
 ~ GRUR inputSize hiddenSize numLayers directionality dtype device,
 Castable (HList tensorParameters) [ATenTensor],
 HMap' ToDependent parameters tensorParameters) =&gt;
Bool
-&gt; GRUWithInit
     inputSize
     hiddenSize
     numLayers
     directionality
     initialization
     dtype
     device
-&gt; Tensor device dtype inputShape
-&gt; (Tensor device dtype outputShape, Tensor device dtype hcShape)
</span><a href="Torch.Typed.NN.Recurrent.GRU.html#gru"><span class="hs-identifier hs-var">Torch.Typed.NN.Recurrent.GRU.gru</span></a></span><span>
</span><span id="line-649"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730010"><span class="hs-identifier hs-type">shapeOrder</span></a></span><span>
</span><span id="line-650"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730009"><span class="hs-identifier hs-type">batchSize</span></a></span><span>
</span><span id="line-651"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730008"><span class="hs-identifier hs-type">seqLen</span></a></span><span>
</span><span id="line-652"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730007"><span class="hs-identifier hs-type">directionality</span></a></span><span>
</span><span id="line-653"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730006"><span class="hs-identifier hs-type">initialization</span></a></span><span>
</span><span id="line-654"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730005"><span class="hs-identifier hs-type">numLayers</span></a></span><span>
</span><span id="line-655"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730004"><span class="hs-identifier hs-type">inputSize</span></a></span><span>
</span><span id="line-656"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730003"><span class="hs-identifier hs-type">outputSize</span></a></span><span>
</span><span id="line-657"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730002"><span class="hs-identifier hs-type">hiddenSize</span></a></span><span>
</span><span id="line-658"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730001"><span class="hs-identifier hs-type">inputShape</span></a></span><span>
</span><span id="line-659"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679730000"><span class="hs-identifier hs-type">outputShape</span></a></span><span>
</span><span id="line-660"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729999"><span class="hs-identifier hs-type">hcShape</span></a></span><span>
</span><span id="line-661"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729998"><span class="hs-identifier hs-type">parameters</span></a></span><span>
</span><span id="line-662"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729997"><span class="hs-identifier hs-type">tensorParameters</span></a></span><span>
</span><span id="line-663"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729996"><span class="hs-identifier hs-type">dtype</span></a></span><span>
</span><span id="line-664"></span><span>    </span><span class="hs-glyph">@</span><span class="annot"><a href="#local-6989586621679729995"><span class="hs-identifier hs-type">device</span></a></span><span>
</span><span id="line-665"></span><span>    </span><span class="annot"><span class="annottext">Bool
</span><span class="hs-identifier hs-var">False</span></span><span>
</span><span id="line-666"></span></pre></body></html>