-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Functional differentiable programming in Haskell
--   
--   Functional differentiable programming in Haskell
@package hasktorch
@version 0.2.0.0

module Torch.Backend
data Backend
CPU :: Backend
CUDA :: Backend
HIP :: Backend
SparseCPU :: Backend
SparseCUDA :: Backend
MSNPU :: Backend
XLA :: Backend
instance GHC.Show.Show Torch.Backend.Backend
instance GHC.Classes.Eq Torch.Backend.Backend
instance Torch.Internal.Class.Castable Torch.Backend.Backend Torch.Internal.Type.Backend

module Torch.Cast
instance Torch.Internal.Class.CppTuple2 (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)
instance Torch.Internal.Class.CppTuple3 (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)
instance Torch.Internal.Class.CppTuple4 (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)
instance Torch.Internal.Class.CppTuple5 (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)
instance Torch.Internal.Class.CppTuple6 (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.IntArray)

module Torch.DType
data DType

-- | Bool
Bool :: DType

-- | Byte
UInt8 :: DType

-- | Char
Int8 :: DType

-- | Short
Int16 :: DType

-- | Int
Int32 :: DType

-- | Long
Int64 :: DType

-- | Half
Half :: DType

-- | Float
Float :: DType

-- | Double
Double :: DType

-- | ComplexHalf
ComplexHalf :: DType

-- | ComplexFloat
ComplexFloat :: DType

-- | ComplexDouble
ComplexDouble :: DType

-- | QInt8
QInt8 :: DType

-- | QUInt8
QUInt8 :: DType

-- | QInt32
QInt32 :: DType

-- | BFloat16
BFloat16 :: DType
isIntegral :: DType -> Bool
byteLength :: DType -> Int
instance GHC.Read.Read Torch.DType.DType
instance GHC.Show.Show Torch.DType.DType
instance GHC.Classes.Eq Torch.DType.DType
instance Data.Reflection.Reifies GHC.Types.Bool Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Bool Torch.DType.DType
instance Data.Reflection.Reifies GHC.Word.Word8 Torch.DType.DType
instance Data.Reflection.Reifies GHC.Int.Int8 Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Int8 Torch.DType.DType
instance Data.Reflection.Reifies GHC.Int.Int16 Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Int16 Torch.DType.DType
instance Data.Reflection.Reifies GHC.Int.Int32 Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Int32 Torch.DType.DType
instance Data.Reflection.Reifies GHC.Types.Int Torch.DType.DType
instance Data.Reflection.Reifies GHC.Int.Int64 Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Int64 Torch.DType.DType
instance Data.Reflection.Reifies GHC.Types.Float Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Float Torch.DType.DType
instance Data.Reflection.Reifies GHC.Types.Double Torch.DType.DType
instance Data.Reflection.Reifies 'Torch.DType.Double Torch.DType.DType
instance Torch.Internal.Class.Castable Torch.DType.DType Torch.Internal.Type.ScalarType

module Torch.Data.Internal
runWithBuffer :: forall a m b. MonadBaseControl IO m => Int -> (Output a -> m ()) -> ContT b m (ListT m a)
liftedBracket :: MonadBaseControl IO m => m a -> (a -> m b) -> (a -> m c) -> m c
withBufferLifted :: MonadBaseControl IO m => Buffer a -> (Output a -> m l) -> (Input a -> m r) -> m (l, r)
fromInput' :: MonadBase IO m => Input a -> Producer' a m ()
toOutput' :: MonadBase IO m => Output a -> Consumer' a m ()
liftedFinally :: MonadBaseControl IO m => m a -> m b -> m a
atomically :: MonadIO m => STM a -> m a
instance Control.Monad.Base.MonadBase GHC.Types.IO m => Control.Monad.Base.MonadBase GHC.Types.IO (Pipes.Internal.Proxy a' a b' b m)

module Torch.Data.Pipeline

-- | The base dataset class. A dataset is capable of returning a sample for
--   a given key, and every <a>Dataset</a> has a known set of keys.
class (Ord k) => Dataset m dataset k sample | dataset -> m, dataset -> sample, dataset -> k
getItem :: Dataset m dataset k sample => dataset -> k -> m sample
keys :: Dataset m dataset k sample => dataset -> Set k

-- | Dataset options used when loading datasets. Specify shuffling
--   behavior, the number of threads to use, and the buffer size used to
--   store retrieved samples in each thread.
data DatasetOptions
DatasetOptions :: Int -> Int -> Sample -> DatasetOptions

-- | Max number of samples stored in each buffer at a given time.
[dataBufferSize] :: DatasetOptions -> Int

-- | Number of threads retrieving samples.
[numWorkers] :: DatasetOptions -> Int

-- | The ordering of samples streamed.
[shuffle] :: DatasetOptions -> Sample

-- | Default <a>DatasetOptions</a>. The <a>Int</a> parameter specifies the
--   number of workers, and sets the buffer size equal to the number of
--   workers. Sampling is sequential.
datasetOpts :: Int -> DatasetOptions

-- | A <a>Sample</a> determines the ordering of samples streamed out of a
--   dataset. You can either order sequentially, or supply a random
--   generator to shuffle samples.
data Sample
[Sequential] :: Sample
[Shuffle] :: RandomGen g => g -> Sample

-- | Return a stream of samples from the given dataset, along with a new
--   <a>Sample</a> value. The returned stream contains every sample
--   returned by <tt><a>getItem</a></tt> for every key in the set of keys
--   associated with the given dataset. The returned <a>Sample</a> value
--   returns an updated <a>Sample</a> value, this will be identical to the
--   original <a>Sample</a> value if sampling is <a>Sequential</a> but will
--   return a new random number generator if sampling is <a>Shuffle</a>.
streamFromMap :: forall m dataset k sample r. (Dataset m dataset k sample, MonadIO m, MonadBaseControl IO m) => DatasetOptions -> dataset -> ContT r m (ListT m sample, Sample)

module Torch.Data.StreamedPipeline

-- | The base datastream class. A dataset returns a stream of samples based
--   on a seed value.
class Monad m => Datastream m seed dataset sample | dataset -> sample
streamSamples :: Datastream m seed dataset sample => dataset -> seed -> ListT m sample

-- | Datastream options used when looding datastreams. Currently only
--   buffer size is configurable, since thread count is controlled by the
--   number of seeds (see <tt><a>streamFrom</a></tt> functions).
newtype DatastreamOptions
DatastreamOptions :: Int -> DatastreamOptions

-- | Max number of samples stored in each buffer at a given time.
[bufferSize] :: DatastreamOptions -> Int

-- | Default dataloader options, you should override the fields in this
--   record.
datastreamOpts :: DatastreamOptions

-- | Return a stream of samples from the given dataset as a continuation. A
--   stream of samples is generated for every seed in the given stream of
--   seeds, and all of these streams are merged into the output stream in a
--   non-deterministic order (if you need determinism see 'streamFrom'').
--   Every stream created for each seed value is made in its own thread.
streamFrom :: forall sample m dataset seed b. (Datastream m seed dataset sample, MonadBaseControl IO m, MonadBase IO m) => DatastreamOptions -> dataset -> ListT m seed -> ContT b m (ListT m sample)

-- | This function is the same as <a>streamFrom</a> except the seeds are
--   specified as a <a>Foldable</a>, and the stream returned has a
--   deterministic ordering. The results from each given seed are
--   interspersed in the order defined by the <tt><a>Foldable</a></tt> of
--   seeds.
streamFrom' :: forall sample m f dataset seed b. (Show sample, Datastream m seed dataset sample, MonadBaseControl IO m, MonadBase IO m, MonadIO m, Foldable f) => DatastreamOptions -> dataset -> f seed -> ContT b m (ListT m sample)
class (Applicative b, Applicative m, Monad b, Monad m) => MonadBase (b :: Type -> Type) (m :: Type -> Type) | m -> b

-- | Lift a computation from the base monad
liftBase :: MonadBase b m => b α -> m α

-- | <h2>Writing instances</h2>
--   
--   The usual way to write a <tt><a>MonadBaseControl</a></tt> instance for
--   a transformer stack over a base monad <tt>B</tt> is to write an
--   instance <tt>MonadBaseControl B B</tt> for the base monad, and
--   <tt>MonadTransControl T</tt> instances for every transformer
--   <tt>T</tt>. Instances for <tt><a>MonadBaseControl</a></tt> are then
--   simply implemented using <tt><a>ComposeSt</a></tt>,
--   <tt><a>defaultLiftBaseWith</a></tt>, <tt><a>defaultRestoreM</a></tt>.
class MonadBase b m => MonadBaseControl (b :: Type -> Type) (m :: Type -> Type) | m -> b where {
    
    -- | Monadic state that <tt>m</tt> adds to the base monad <tt>b</tt>.
    --   
    --   For all base (non-transformed) monads, <tt>StM m a ~ a</tt>:
    --   
    --   <pre>
    --   StM <a>IO</a>         a ~ a
    --   StM <a>Maybe</a>      a ~ a
    --   StM (<a>Either</a> e) a ~ a
    --   StM []         a ~ a
    --   StM ((-&gt;) r)   a ~ a
    --   StM <a>Identity</a>   a ~ a
    --   StM <a>STM</a>        a ~ a
    --   StM (<a>ST</a> s)     a ~ a
    --   </pre>
    --   
    --   If <tt>m</tt> is a transformed monad, <tt>m ~ t b</tt>,
    --   <tt><a>StM</a></tt> is the monadic state of the transformer <tt>t</tt>
    --   (given by its <a>StT</a> from <a>MonadTransControl</a>). For a
    --   transformer stack, <tt><a>StM</a></tt> is defined recursively:
    --   
    --   <pre>
    --   StM (<a>IdentityT</a>  m) a ~ <a>ComposeSt</a> <a>IdentityT</a> m a ~ StM m a
    --   StM (<a>MaybeT</a>     m) a ~ <a>ComposeSt</a> <a>MaybeT</a>    m a ~ StM m (<a>Maybe</a> a)
    --   StM (<a>ErrorT</a> e   m) a ~ <a>ComposeSt</a> <a>ErrorT</a>    m a ~ <a>Error</a> e =&gt; StM m (<a>Either</a> e a)
    --   StM (<a>ExceptT</a> e  m) a ~ <a>ComposeSt</a> <a>ExceptT</a>   m a ~ StM m (<a>Either</a> e a)
    --   StM (<a>ListT</a>      m) a ~ <a>ComposeSt</a> <a>ListT</a>     m a ~ StM m [a]
    --   StM (<a>ReaderT</a> r  m) a ~ <a>ComposeSt</a> <a>ReaderT</a>   m a ~ StM m a
    --   StM (<a>StateT</a> s   m) a ~ <a>ComposeSt</a> <a>StateT</a>    m a ~ StM m (a, s)
    --   StM (<a>WriterT</a> w  m) a ~ <a>ComposeSt</a> <a>WriterT</a>   m a ~ <a>Monoid</a> w =&gt; StM m (a, w)
    --   StM (<a>RWST</a> r w s m) a ~ <a>ComposeSt</a> <a>RWST</a>      m a ~ <a>Monoid</a> w =&gt; StM m (a, s, w)
    --   </pre>
    type family StM (m :: Type -> Type) a;
}

-- | <tt>liftBaseWith</tt> is similar to <tt>liftIO</tt> and
--   <tt>liftBase</tt> in that it lifts a base computation to the
--   constructed monad.
--   
--   Instances should satisfy similar laws as the <tt>MonadIO</tt> and
--   <a>MonadBase</a> laws:
--   
--   <pre>
--   liftBaseWith . const . return = return
--   </pre>
--   
--   <pre>
--   liftBaseWith (const (m &gt;&gt;= f)) = liftBaseWith (const m) &gt;&gt;= liftBaseWith . const . f
--   </pre>
--   
--   The difference with <tt>liftBase</tt> is that before lifting the base
--   computation <tt>liftBaseWith</tt> captures the state of <tt>m</tt>. It
--   then provides the base computation with a <a>RunInBase</a> function
--   that allows running <tt>m</tt> computations in the base monad on the
--   captured state:
--   
--   <pre>
--   withFileLifted :: MonadBaseControl IO m =&gt; FilePath -&gt; IOMode -&gt; (Handle -&gt; m a) -&gt; m a
--   withFileLifted file mode action = liftBaseWith (\runInBase -&gt; withFile file mode (runInBase . action)) &gt;&gt;= restoreM
--                                -- = control $ \runInBase -&gt; withFile file mode (runInBase . action)
--                                -- = liftBaseOp (withFile file mode) action
--   </pre>
--   
--   <tt><a>liftBaseWith</a></tt> is usually not implemented directly, but
--   using <tt><a>defaultLiftBaseWith</a></tt>.
liftBaseWith :: MonadBaseControl b m => (RunInBase m b -> b a) -> m a

-- | Construct a <tt>m</tt> computation from the monadic state of
--   <tt>m</tt> that is returned from a <a>RunInBase</a> function.
--   
--   Instances should satisfy:
--   
--   <pre>
--   liftBaseWith (\runInBase -&gt; runInBase m) &gt;&gt;= restoreM = m
--   </pre>
--   
--   <tt><a>restoreM</a></tt> is usually not implemented directly, but
--   using <tt><a>defaultRestoreM</a></tt>.
restoreM :: MonadBaseControl b m => StM m a -> m a

module Torch.Data.Dataset

-- | This type is actually not very useful. | It would actually be better
--   to define a transform | on top of another dataset, since then we can
--   do this in parallel
data CollatedDataset m dataset batch collatedBatch
CollatedDataset :: dataset -> Int -> Pipe [batch] collatedBatch m () -> CollatedDataset m dataset batch collatedBatch
[set] :: CollatedDataset m dataset batch collatedBatch -> dataset
[chunkSize] :: CollatedDataset m dataset batch collatedBatch -> Int
[collateFn] :: CollatedDataset m dataset batch collatedBatch -> Pipe [batch] collatedBatch m ()
instance Torch.Data.StreamedPipeline.Datastream m seed dataset batch => Torch.Data.StreamedPipeline.Datastream m seed (Torch.Data.Dataset.CollatedDataset m dataset batch collatedBatch) collatedBatch

module Torch.Data.CsvDatastream
type BufferSize = Int
data NamedColumns
Unnamed :: NamedColumns
Named :: NamedColumns

-- | A CSV datastream. The datastream instance of this type streams samples
--   of <tt>batches</tt> from a CSV file at the specified file path.
--   Batches are yielded in constant memory, but if shuffling is enabled,
--   then there will be at most <tt><a>BufferSize</a></tt> records stored
--   in memory.
data CsvDatastream' batches (named :: NamedColumns)
CsvDatastream' :: FilePath -> !Word8 -> HasHeader -> Int -> Maybe BufferSize -> Bool -> CsvDatastream' batches (named :: NamedColumns)

-- | CSV file path.
[filePath] :: CsvDatastream' batches (named :: NamedColumns) -> FilePath

-- | Column delimiter.
[delimiter] :: CsvDatastream' batches (named :: NamedColumns) -> !Word8

-- | Does the file have a header?
[hasHeader] :: CsvDatastream' batches (named :: NamedColumns) -> HasHeader

-- | Batch size. , filter :: Maybe (batches -&gt; Bool)
[batchSize] :: CsvDatastream' batches (named :: NamedColumns) -> Int

-- | Buffered shuffle with specified buffer size.
[bufferedShuffle] :: CsvDatastream' batches (named :: NamedColumns) -> Maybe BufferSize

-- | Drop the last batch if it is less than batch size.
[dropLast] :: CsvDatastream' batches (named :: NamedColumns) -> Bool

-- | A specialized version of CsvDatastream'. Use this type if you want to
--   decode a CSV file with records defined by the order of the columns.
type CsvDatastream batches = CsvDatastream' batches Unnamed

-- | A specialized version of CsvDatastream'. Use this type if you want to
--   decode a CSV file with records that have
--   <tt><a>FromNamedRecord</a></tt> instance. This decodes each field of
--   the record by the corresponding column with the given header name.
type CsvDatastreamNamed batches = CsvDatastream' batches Named

-- | Produce a CsvDatastream' from the given file with default options, and
--   comma separated columns.
csvDatastream :: forall (isNamed :: NamedColumns) batches. FilePath -> CsvDatastream' batches isNamed

-- | Produce a CsvDatastream' from the given file with default options, and
--   tab separated columns.
tsvDatastream :: forall (isNamed :: NamedColumns) batches. FilePath -> CsvDatastream' batches isNamed

-- | A type that can be converted from a single CSV field, with the
--   possibility of failure.
--   
--   When writing an instance, use <a>empty</a>, <a>mzero</a>, or
--   <a>fail</a> to make a conversion fail, e.g. if a <a>Field</a> can't be
--   converted to the given type.
--   
--   Example type and instance:
--   
--   <pre>
--   {-# LANGUAGE OverloadedStrings #-}
--   
--   data Color = Red | Green | Blue
--   
--   instance FromField Color where
--       parseField s
--           | s == "R"  = pure Red
--           | s == "G"  = pure Green
--           | s == "B"  = pure Blue
--           | otherwise = mzero
--   </pre>
class FromField a
parseField :: FromField a => Field -> Parser a

-- | A type that can be converted from a single CSV record, with the
--   possibility of failure.
--   
--   When writing an instance, use <a>empty</a>, <a>mzero</a>, or
--   <a>fail</a> to make a conversion fail, e.g. if a <a>Record</a> has the
--   wrong number of columns.
--   
--   Given this example data:
--   
--   <pre>
--   John,56
--   Jane,55
--   </pre>
--   
--   here's an example type and instance:
--   
--   <pre>
--   data Person = Person { name :: !Text, age :: !Int }
--   
--   instance FromRecord Person where
--       parseRecord v
--           | length v == 2 = Person &lt;$&gt;
--                             v .! 0 &lt;*&gt;
--                             v .! 1
--           | otherwise     = mzero
--   </pre>
class FromRecord a
parseRecord :: FromRecord a => Record -> Parser a

-- | A type that can be converted from a single CSV record, with the
--   possibility of failure.
--   
--   When writing an instance, use <a>empty</a>, <a>mzero</a>, or
--   <a>fail</a> to make a conversion fail, e.g. if a <a>Record</a> has the
--   wrong number of columns.
--   
--   Given this example data:
--   
--   <pre>
--   name,age
--   John,56
--   Jane,55
--   </pre>
--   
--   here's an example type and instance:
--   
--   <pre>
--   {-# LANGUAGE OverloadedStrings #-}
--   
--   data Person = Person { name :: !Text, age :: !Int }
--   
--   instance FromNamedRecord Person where
--       parseNamedRecord m = Person &lt;$&gt;
--                            m .: "name" &lt;*&gt;
--                            m .: "age"
--   </pre>
--   
--   Note the use of the <tt>OverloadedStrings</tt> language extension
--   which enables <a>ByteString</a> values to be written as string
--   literals.
class FromNamedRecord a
parseNamedRecord :: FromNamedRecord a => NamedRecord -> Parser a
instance (Control.Monad.Trans.Control.MonadBaseControl GHC.Types.IO m, Pipes.Safe.MonadSafe m, Data.Csv.Conversion.FromNamedRecord batch) => Torch.Data.StreamedPipeline.Datastream m () (Torch.Data.CsvDatastream.CsvDatastreamNamed batch) (Data.Vector.Vector batch)
instance (Control.Monad.Trans.Control.MonadBaseControl GHC.Types.IO m, Pipes.Safe.MonadSafe m, Data.Csv.Conversion.FromRecord batch) => Torch.Data.StreamedPipeline.Datastream m () (Torch.Data.CsvDatastream.CsvDatastream batch) (Data.Vector.Vector batch)

module Torch.Data.Utils

-- | Run a map function in parallel over the given stream.
pmap :: (MonadIO m, MonadBaseControl IO m) => Buffer b -> (a -> b) -> ListT m a -> ContT r m (ListT m b)

-- | Run a pipe in parallel over the given stream.
pmap' :: (MonadIO m, MonadBaseControl IO m) => Buffer b -> Pipe a b m () -> ListT m a -> ContT r m (ListT m b)

-- | Map a ListT transform over the given the stream in parallel. This
--   should be useful for using functions which groups elements of a stream
--   and yields them downstream.
pmapGroup :: (MonadIO m, MonadBaseControl IO m) => Buffer b -> (ListT m a -> ListT m b) -> ListT m a -> ContT r m (ListT m b)

-- | Run a given batching function in parallel. See <a>collate</a> for how
--   the given samples are batched.
bufferedCollate :: (MonadIO m, MonadBaseControl IO m) => Buffer batch -> Int -> ([sample] -> Maybe batch) -> ListT m sample -> ContT r m (ListT m batch)

-- | Run a batching function with integer batch size over the given stream.
--   The elements of the stream are split into lists of the given batch
--   size and are collated with the given function. Only Just values are
--   yielded downstream. If the last chunk of samples is less than the
--   given batch size then the batching function will be passed a list of
--   length less than batch size.
collate :: Monad m => Int -> ([sample] -> Maybe batch) -> ListT m sample -> ListT m batch

-- | Enumerate the given stream, zipping each element with an index.
enumerateData :: Monad m => ListT m a -> Producer (a, Int) m ()

-- | An In-Memory cached dataset. See the <a>cache</a> function for how to
--   create a cached dataset.
data CachedDataset (m :: * -> *) sample

-- | Enumerate a given stream and store it as a <a>CachedDataset</a>. This
--   function should be used after a time consuming preprocessing pipeline
--   and used in subsequent epochs to avoid repeating the preprocessing
--   pipeline.
cache :: Monad m => ListT m sample -> m (CachedDataset m sample)
instance GHC.Base.Applicative m => Torch.Data.Pipeline.Dataset m (Torch.Data.Utils.CachedDataset m sample) GHC.Types.Int sample


-- | Modules for defining datasets and how to efficiently iterate over
--   them. If you have an indexable (fixed-size) dataset, see
--   <a>Torch.Data.Pipeline</a>. If you want to stream in your data then
--   see <a>Torch.Data.StreamedPipeline</a>. The <a>Torch.Data.Utils</a>
--   module provides some convienient functions for both indexable and
--   streamed datasets.
--   
--   The mnist examples show how to run data for a predefined dataset.
module Torch.Data

module Torch.Device
data DeviceType
CPU :: DeviceType
CUDA :: DeviceType
data Device
Device :: DeviceType -> Int16 -> Device
[deviceType] :: Device -> DeviceType
[deviceIndex] :: Device -> Int16
instance GHC.Show.Show Torch.Device.DeviceType
instance GHC.Classes.Ord Torch.Device.DeviceType
instance GHC.Classes.Eq Torch.Device.DeviceType
instance GHC.Show.Show Torch.Device.Device
instance GHC.Classes.Ord Torch.Device.Device
instance GHC.Classes.Eq Torch.Device.Device
instance Torch.Internal.Class.Castable Torch.Device.DeviceType Torch.Internal.Type.DeviceType

module Torch.Dimname
newtype Dimname
Dimname :: ForeignPtr Dimname -> Dimname
instance Data.String.IsString Torch.Dimname.Dimname
instance Torch.Internal.Class.Castable Torch.Dimname.Dimname (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Dimname)
instance Torch.Internal.Class.Castable [Torch.Dimname.Dimname] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.DimnameList)

module Torch.HList
type family ListLength (xs :: [k]) :: Nat
data family HList (xs :: [k])
pattern (:.) :: forall x (xs :: [Type]). x -> HList xs -> HList (x : xs)
infixr 2 :.
class Apply f a b
apply :: Apply f a b => f -> a -> b

-- | Stronger version of <a>Apply</a> that allows for better inference of
--   the return type
class Apply' f a b | f a -> b
apply' :: Apply' f a b => f -> a -> b
data AFst
AFst :: AFst
data ASnd
ASnd :: ASnd
class HMap f (xs :: [k]) (ys :: [k])
hmap :: HMap f xs ys => f -> HList xs -> HList ys

-- | Alternative version of <a>HMap</a> with better type inference based on
--   <a>Apply</a>`
class HMap' f (xs :: [k]) (ys :: [k]) | f xs -> ys
hmap' :: HMap' f xs ys => f -> HList xs -> HList ys
class HMapM m f (xs :: [k]) (ys :: [k])
hmapM :: HMapM m f xs ys => f -> HList xs -> m (HList ys)
class HMapM' m f (xs :: [k]) (ys :: [k]) | f xs -> ys
hmapM' :: HMapM' m f xs ys => f -> HList xs -> m (HList ys)
class Applicative f => HSequence f (xs :: [k]) (ys :: [k]) | xs -> ys, ys f -> xs
hsequence :: HSequence f xs ys => HList xs -> f (HList ys)
class HFoldr f acc xs res | f acc xs -> res
hfoldr :: HFoldr f acc xs res => f -> acc -> HList xs -> res
class HFoldrM m f acc xs res | m f acc xs -> res
hfoldrM :: HFoldrM m f acc xs res => f -> acc -> HList xs -> m res
data HNothing
HNothing :: HNothing
newtype HJust x
HJust :: x -> HJust x
class HUnfold f res xs
hunfoldr' :: HUnfold f res xs => f -> res -> HList xs
type family HUnfoldRes s xs
hunfoldr :: forall f res (xs :: [Type]) a. (Apply f a res, HUnfold f res xs, res ~ HUnfoldRes a xs) => f -> a -> HList xs
class HUnfoldM m f res xs
hunfoldrM' :: HUnfoldM m f res xs => f -> res -> m (HList xs)
type family HUnfoldMRes m s xs
hunfoldrM :: forall (m :: Type -> Type) f res (xs :: [Type]) a. (HUnfoldM m f res xs, Apply f a res, res ~ HUnfoldMRes m a xs) => f -> a -> m (HList xs)
type HReplicate n e = HReplicateFD n e (HReplicateR n e)
hreplicate :: forall n e. HReplicate n e => e -> HList (HReplicateR n e)
class HReplicateFD (n :: Nat) (e :: Type) (es :: [Type]) | n e -> es
hreplicateFD :: HReplicateFD n e es => e -> HList es
type family HReplicateR (n :: Nat) (e :: a) :: [a]
type HConcat xs = HConcatFD xs (HConcatR xs)
hconcat :: HConcat xs => HList xs -> HList (HConcatR xs)
type family HConcatR (a :: [Type]) :: [Type]
type family UnHList a :: [Type]
class HConcatFD (xxs :: [k]) (xs :: [k]) | xxs -> xs
hconcatFD :: HConcatFD xxs xs => HList xxs -> HList xs
type HAppend as bs = HAppendFD as bs (as ++ bs)
happend :: HAppend as bs => HList as -> HList bs -> HList (as ++ bs)
hunappend :: (cs ~ (as ++ bs), HAppend as bs) => HList cs -> (HList as, HList bs)
class HAppendFD (a :: [k]) (b :: [k]) (ab :: [k]) | a b -> ab, a ab -> b
happendFD :: HAppendFD a b ab => HList a -> HList b -> HList ab
hunappendFD :: HAppendFD a b ab => HList ab -> (HList a, HList b)
type family (as :: [k]) ++ (bs :: [k]) :: [k]
class HZip (xs :: [k]) (ys :: [k]) (zs :: [k]) | xs ys -> zs, zs -> xs ys
hzip :: HZip xs ys zs => HList xs -> HList ys -> HList zs
hunzip :: HZip xs ys zs => HList zs -> (HList xs, HList ys)
class HZip' (xs :: [k]) (ys :: [k]) (zs :: [k]) | xs ys -> zs
hzip' :: HZip' xs ys zs => HList xs -> HList ys -> HList zs
data HZipF
HZipF :: HZipF
htranspose :: forall (acc :: [Type]) (xs :: [Type]) (xxs :: [Type]) (res :: Type). (HReplicateFD (ListLength xs) (HList ('[] :: [Type])) acc, HFoldr HZipF (HList acc) (HList xs : xxs) res) => HList (HList xs : xxs) -> res
class HZipWith f (xs :: [k]) (ys :: [k]) (zs :: [k]) | f xs ys -> zs
hzipWith :: HZipWith f xs ys zs => f -> HList xs -> HList ys -> HList zs
class HZipWithM m f (xs :: [k]) (ys :: [k]) (zs :: [k]) | f xs ys -> zs
hzipWithM :: HZipWithM m f xs ys zs => f -> HList xs -> HList ys -> m (HList zs)
class HZip3 (as :: [k]) (bs :: [k]) (cs :: [k]) (ds :: [k]) | as bs cs -> ds, ds -> as bs cs
hzip3 :: HZip3 as bs cs ds => HList as -> HList bs -> HList cs -> HList ds
hunzip3 :: HZip3 as bs cs ds => HList ds -> (HList as, HList bs, HList cs)
class HZipWith3 f (as :: [k]) (bs :: [k]) (cs :: [k]) (ds :: [k]) | f as bs cs -> ds
hzipWith3 :: HZipWith3 f as bs cs ds => f -> HList as -> HList bs -> HList cs -> HList ds
class HCartesianProduct (xs :: [k]) (ys :: [k]) (zs :: [k]) | xs ys -> zs
hproduct :: HCartesianProduct xs ys zs => HList xs -> HList ys -> HList zs
class HAttach x (ys :: [k]) (zs :: [k]) | x ys -> zs
hattach :: HAttach x ys zs => x -> HList ys -> HList zs
instance Torch.HList.HAttach x '[] '[]
instance Torch.HList.HAttach x ys xys => Torch.HList.HAttach x (y : ys) ((x, y) : xys)
instance (Torch.HList.HCartesianProduct xs ys zs, Torch.HList.HAttach x ys xys, Torch.HList.HAppendFD xys zs zs') => Torch.HList.HCartesianProduct (x : xs) ys zs'
instance forall k (ys :: [k]). Torch.HList.HCartesianProduct '[] ys '[]
instance Torch.HList.HZipWith3 f '[] '[] '[] '[]
instance (Torch.HList.Apply' f (a, b, c) d, Torch.HList.HZipWith3 f as bs cs ds) => Torch.HList.HZipWith3 f (a : as) (b : bs) (c : cs) (d : ds)
instance Torch.HList.HZip3 '[] '[] '[] '[]
instance ((a, b, c) GHC.Types.~ d, Torch.HList.HZip3 as bs cs ds) => Torch.HList.HZip3 (a : as) (b : bs) (c : cs) (d : ds)
instance GHC.Base.Applicative m => Torch.HList.HZipWithM m f '[] '[] '[]
instance (GHC.Base.Applicative m, Torch.HList.Apply' f (x, y) (m z), Torch.HList.HZipWithM m f xs ys zs) => Torch.HList.HZipWithM m f (x : xs) (y : ys) (z : zs)
instance Torch.HList.HZipWith f '[] '[] '[]
instance (Torch.HList.Apply' f (x, y) z, Torch.HList.HZipWith f xs ys zs) => Torch.HList.HZipWith f (x : xs) (y : ys) (z : zs)
instance forall k (a :: [k]) (b :: [k]) (c :: [k]) x y. (Torch.HList.HZip' a b c, x GHC.Types.~ (Torch.HList.HList a, Torch.HList.HList b), y GHC.Types.~ Torch.HList.HList c) => Torch.HList.Apply' Torch.HList.HZipF x y
instance Torch.HList.HZip' '[] '[] '[]
instance (Torch.HList.HList (x : y) GHC.Types.~ z, Torch.HList.HZip' xs ys zs) => Torch.HList.HZip' (x : xs) (Torch.HList.HList y : ys) (z : zs)
instance Torch.HList.HZip '[] '[] '[]
instance ((x, y) GHC.Types.~ z, Torch.HList.HZip xs ys zs) => Torch.HList.HZip (x : xs) (y : ys) (z : zs)
instance (Torch.HList.HConcatFD as bs, Torch.HList.HAppendFD a bs cs) => Torch.HList.HConcatFD (Torch.HList.HList a : as) cs
instance forall k (b :: [k]). Torch.HList.HAppendFD '[] b b
instance Torch.HList.HAppendFD as bs cs => Torch.HList.HAppendFD (a : as) bs (a : cs)
instance Torch.HList.HConcatFD '[] '[]
instance Torch.HList.HReplicateFD 0 e '[]
instance (Torch.HList.HReplicateFD (n GHC.TypeNats.- 1) e es, es' GHC.Types.~ (e : es), 1 GHC.TypeNats.<= n) => Torch.HList.HReplicateFD n e es'
instance (GHC.Base.Monad m, Torch.HList.HUnfoldM m f res xs, Torch.HList.Apply f s res, res GHC.Types.~ Torch.HList.HUnfoldMRes m s xs) => Torch.HList.HUnfoldM m f (m (Torch.HList.HJust (x, s))) (x : xs)
instance GHC.Base.Monad m => Torch.HList.HUnfoldM m f (m Torch.HList.HNothing) '[]
instance (Torch.HList.Apply f s res, Torch.HList.HUnfold f res xs, res GHC.Types.~ Torch.HList.HUnfoldRes s xs) => Torch.HList.HUnfold f (Torch.HList.HJust (x, s)) (x : xs)
instance Torch.HList.HUnfold f Torch.HList.HNothing '[]
instance (GHC.Base.Monad m, acc GHC.Types.~ res) => Torch.HList.HFoldrM m f acc '[] res
instance (GHC.Base.Monad m, Torch.HList.Apply' f (x, m res) (m res'), Torch.HList.HFoldrM m f acc xs res) => Torch.HList.HFoldrM m f acc (x : xs) res'
instance (acc GHC.Types.~ res) => Torch.HList.HFoldr f acc '[] res
instance (Torch.HList.Apply' f (x, res) res', Torch.HList.HFoldr f acc xs res) => Torch.HList.HFoldr f acc (x : xs) res'
instance GHC.Base.Applicative f => Torch.HList.HSequence f '[] '[]
instance (GHC.Base.Applicative g, Torch.HList.HSequence f xs ys, y GHC.Types.~ x, f GHC.Types.~ g) => Torch.HList.HSequence g (f x : xs) (y : ys)
instance GHC.Base.Applicative m => Torch.HList.HMapM' m f '[] '[]
instance (GHC.Base.Applicative m, Torch.HList.Apply' f x (m y), Torch.HList.HMapM' m f xs ys) => Torch.HList.HMapM' m f (x : xs) (y : ys)
instance GHC.Base.Monad m => Torch.HList.HMapM m f '[] '[]
instance (GHC.Base.Monad m, Torch.HList.Apply f x (m y), Torch.HList.HMapM m f xs ys) => Torch.HList.HMapM m f (x : xs) (y : ys)
instance Torch.HList.HMap' f '[] '[]
instance (Torch.HList.Apply' f x y, Torch.HList.HMap' f xs ys) => Torch.HList.HMap' f (x : xs) (y : ys)
instance Torch.HList.HMap f '[] '[]
instance (Torch.HList.Apply f x y, Torch.HList.HMap f xs ys) => Torch.HList.HMap f (x : xs) (y : ys)
instance Torch.HList.Apply' Torch.HList.ASnd (a, b) b
instance Torch.HList.Apply' Torch.HList.AFst (a, b) a
instance GHC.Show.Show (Torch.HList.HList '[])
instance (GHC.Show.Show e, GHC.Show.Show (Torch.HList.HList l)) => GHC.Show.Show (Torch.HList.HList (e : l))
instance GHC.Classes.Eq (Torch.HList.HList '[])
instance (GHC.Classes.Eq x, GHC.Classes.Eq (Torch.HList.HList xs)) => GHC.Classes.Eq (Torch.HList.HList (x : xs))
instance GHC.Base.Semigroup (Torch.HList.HList '[])
instance (GHC.Base.Semigroup a, GHC.Base.Semigroup (Torch.HList.HList as)) => GHC.Base.Semigroup (Torch.HList.HList (a : as))
instance GHC.Base.Monoid (Torch.HList.HList '[])
instance (GHC.Base.Monoid a, GHC.Base.Monoid (Torch.HList.HList as)) => GHC.Base.Monoid (Torch.HList.HList (a : as))
instance GHC.Exts.IsList (GHC.Maybe.Maybe (Torch.HList.HList '[a]))
instance (GHC.Exts.IsList (GHC.Maybe.Maybe (Torch.HList.HList (a : as))), a GHC.Types.~ GHC.Exts.Item (GHC.Maybe.Maybe (Torch.HList.HList (a : as)))) => GHC.Exts.IsList (GHC.Maybe.Maybe (Torch.HList.HList (a : a : as)))

module Torch.Layout
data Layout
Strided :: Layout
Sparse :: Layout
Mkldnn :: Layout
instance GHC.Show.Show Torch.Layout.Layout
instance GHC.Classes.Eq Torch.Layout.Layout
instance Torch.Internal.Class.Castable Torch.Layout.Layout Torch.Internal.Type.Layout

module Torch.Lens

-- | Type synonym for lens
type Lens s t a b = forall f. Functor f => (a -> f b) -> s -> f t
type Lens' s a = Lens s s a a
type Traversal s t a b = forall f. Applicative f => (a -> f b) -> s -> f t
type Traversal' s a = Traversal s s a a
class HasTypes s a
types_ :: HasTypes s a => Traversal' s a
types_ :: (HasTypes s a, Generic s, GHasTypes (Rep s) a) => Traversal' s a
over :: Traversal' s a -> (a -> a) -> s -> s
flattenValues :: forall a s. Traversal' s a -> s -> [a]
replaceValues :: forall a s. Traversal' s a -> s -> [a] -> s
types :: forall a s. HasTypes s a => Traversal' s a
class GHasTypes s a
gtypes :: forall b. GHasTypes s a => Traversal' (s b) a
instance (GHC.Generics.Generic s, Torch.Lens.GHasTypes (GHC.Generics.Rep s) a) => Torch.Lens.HasTypes s a
instance Torch.Lens.HasTypes s a => Torch.Lens.GHasTypes (GHC.Generics.K1 i s) a
instance Torch.Lens.HasTypes s a => Torch.Lens.HasTypes [s] a
instance (Torch.Lens.HasTypes s0 a, Torch.Lens.HasTypes s1 a) => Torch.Lens.HasTypes (s0, s1) a
instance (Torch.Lens.HasTypes s0 a, Torch.Lens.HasTypes s1 a, Torch.Lens.HasTypes s2 a) => Torch.Lens.HasTypes (s0, s1, s2) a
instance Torch.Lens.GHasTypes GHC.Generics.U1 a
instance (Torch.Lens.GHasTypes f a, Torch.Lens.GHasTypes g a) => Torch.Lens.GHasTypes (f GHC.Generics.:+: g) a
instance (Torch.Lens.GHasTypes f a, Torch.Lens.GHasTypes g a) => Torch.Lens.GHasTypes (f GHC.Generics.:*: g) a
instance Torch.Lens.GHasTypes s a => Torch.Lens.GHasTypes (GHC.Generics.M1 i t s) a

module Torch.Scalar
class (Castable a (ForeignPtr Scalar)) => Scalar a
instance Torch.Scalar.Scalar GHC.Types.Float
instance Torch.Scalar.Scalar GHC.Types.Double
instance Torch.Scalar.Scalar GHC.Types.Int
instance Torch.Scalar.Scalar GHC.Types.Bool
instance Torch.Internal.Class.Castable GHC.Types.Float (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Double (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Int (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)
instance Torch.Internal.Class.Castable GHC.Types.Bool (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Scalar)

module Torch.TensorOptions
type ATenTensorOptions = ForeignPtr TensorOptions
newtype TensorOptions
TensorOptions :: ATenTensorOptions -> TensorOptions
defaultOpts :: TensorOptions
withDType :: DType -> TensorOptions -> TensorOptions
withDevice :: Device -> TensorOptions -> TensorOptions
withLayout :: Layout -> TensorOptions -> TensorOptions
instance GHC.Show.Show Torch.TensorOptions.TensorOptions
instance Torch.Internal.Class.Castable Torch.TensorOptions.TensorOptions Torch.TensorOptions.ATenTensorOptions

module Torch.Tensor
type ATenTensor = ForeignPtr Tensor
newtype Tensor
Unsafe :: ATenTensor -> Tensor

-- | Returns the total number of elements in the input tensor.
numel :: Tensor -> Int

-- | Returns the size of a given dimension of the input tensor.
size :: Int -> Tensor -> Int

-- | Returns the shape of the tensor
shape :: Tensor -> [Int]

-- | Returns the dimensions of the input tensor
dim :: Tensor -> Int

-- | Returns the device on which the tensor is currently allocated
device :: Tensor -> Device

-- | Returns the data type of the input tensor
dtype :: Tensor -> DType
toDouble :: Tensor -> Double
toInt :: Tensor -> Int

-- | Casts the input tensor to the given data type
_toType :: DType -> Tensor -> Tensor
toType :: forall a. HasTypes a Tensor => DType -> a -> a
toDevice :: forall a. HasTypes a Tensor => Device -> a -> a

-- | Casts the input tensor to given device
_toDevice :: Device -> Tensor -> Tensor

-- | Slices the input tensor along the selected dimension at the given
--   index.
select :: Int -> Int -> Tensor -> Tensor

-- | Returns a new tensor which indexes the input tensor along dimension
--   dim using the entries in index which is a LongTensor.
indexSelect :: Int -> Tensor -> Tensor -> Tensor
indexSelect' :: Int -> [Int] -> Tensor -> Tensor

-- | Slices the input tensor along the selected dimension at the given
--   range.
sliceDim :: Int -> Int -> Int -> Int -> Tensor -> Tensor
isContiguous :: Tensor -> Bool
contiguous :: Tensor -> Tensor

-- | Returns a tensor with the same data and number of elements as input,
--   but with the specified shape.
reshape :: [Int] -> Tensor -> Tensor
toSparse :: Tensor -> Tensor
toDense :: Tensor -> Tensor
toMKLDNN :: Tensor -> Tensor
toCPU :: Tensor -> Tensor
toCUDA :: Tensor -> Tensor
withTensorOptions :: Tensor -> TensorOptions -> Tensor
newtype RawTensorIndexList
RawTensorIndexList :: ForeignPtr (StdVector TensorIndex) -> RawTensorIndexList
newtype RawTensorIndex
RawTensorIndex :: ForeignPtr TensorIndex -> RawTensorIndex
(!) :: TensorIndex a => Tensor -> a -> Tensor
maskedFill :: (TensorIndex a, TensorLike t) => Tensor -> a -> t -> Tensor
data None
None :: None
data Ellipsis
Ellipsis :: Ellipsis
newtype Slice a
Slice :: a -> Slice a
class TensorIndex a
pushIndex :: TensorIndex a => [RawTensorIndex] -> a -> [RawTensorIndex]
toLens :: (TensorIndex a, TensorLike b) => a -> Lens' Tensor b
toLens :: (TensorIndex a, TensorLike b) => a -> Lens' Tensor b
asValue :: TensorLike a => Tensor -> a
class TensorLike a
asTensor' :: TensorLike a => a -> TensorOptions -> Tensor
asTensor :: TensorLike a => a -> Tensor
_asValue :: TensorLike a => Tensor -> a
_dtype :: TensorLike a => DType
_dims :: TensorLike a => a -> [Int]
_deepDims :: TensorLike a => a -> Maybe [Int]
_peekElemOff :: TensorLike a => Ptr () -> Int -> [Int] -> IO a
_pokeElemOff :: TensorLike a => Ptr () -> Int -> a -> IO ()
bool_opts :: TensorOptions
uint8_opts :: TensorOptions
int64_opts :: TensorOptions
float_opts :: TensorOptions
double_opts :: TensorOptions
withTensor :: Tensor -> (Ptr () -> IO a) -> IO a
class AsTensors as
toTensors :: AsTensors as => as -> Vector Tensor
toTensors :: (AsTensors as, Generic as, GAsTensors (Rep as)) => as -> Vector Tensor
class GAsTensors record
gToTensors :: GAsTensors record => record as -> Vector Tensor
instance GHC.Classes.Eq Torch.Tensor.None
instance GHC.Show.Show Torch.Tensor.None
instance GHC.Classes.Eq Torch.Tensor.Ellipsis
instance GHC.Show.Show Torch.Tensor.Ellipsis
instance GHC.Classes.Eq a => GHC.Classes.Eq (Torch.Tensor.Slice a)
instance GHC.Show.Show a => GHC.Show.Show (Torch.Tensor.Slice a)
instance Torch.Tensor.TensorLike a => Torch.Tensor.AsTensors a
instance (Torch.Tensor.GAsTensors ls, Torch.Tensor.GAsTensors rs) => Torch.Tensor.GAsTensors (ls GHC.Generics.:*: rs)
instance (Torch.Tensor.GAsTensors ls, Torch.Tensor.GAsTensors rs) => Torch.Tensor.GAsTensors (ls GHC.Generics.:+: rs)
instance Torch.Tensor.GAsTensors ls => Torch.Tensor.GAsTensors (GHC.Generics.M1 i c ls)
instance Torch.Tensor.TensorLike ls => Torch.Tensor.GAsTensors (GHC.Generics.K1 i ls)
instance Torch.Tensor.TensorIndex Torch.Tensor.None
instance Torch.Tensor.TensorIndex Torch.Tensor.Ellipsis
instance Torch.Tensor.TensorIndex GHC.Types.Bool
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (a, a))
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (a, a, a))
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (Torch.Tensor.None, Torch.Tensor.None, a))
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice a)
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (a, Torch.Tensor.None))
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (a, Torch.Tensor.None, a))
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (Torch.Tensor.None, a, a))
instance GHC.Real.Integral a => Torch.Tensor.TensorIndex (Torch.Tensor.Slice (Torch.Tensor.None, a))
instance Torch.Tensor.TensorIndex (Torch.Tensor.Slice ())
instance Torch.Tensor.TensorIndex GHC.Types.Int
instance Torch.Tensor.TensorIndex GHC.Integer.Type.Integer
instance Torch.Tensor.TensorIndex Torch.Tensor.Tensor
instance Torch.Tensor.TensorIndex ()
instance (Torch.Tensor.TensorIndex a, Torch.Tensor.TensorIndex b) => Torch.Tensor.TensorIndex (a, b)
instance (Torch.Tensor.TensorIndex a, Torch.Tensor.TensorIndex b, Torch.Tensor.TensorIndex c) => Torch.Tensor.TensorIndex (a, b, c)
instance (Torch.Tensor.TensorIndex a, Torch.Tensor.TensorIndex b, Torch.Tensor.TensorIndex c, Torch.Tensor.TensorIndex d) => Torch.Tensor.TensorIndex (a, b, c, d)
instance (Torch.Tensor.TensorIndex a, Torch.Tensor.TensorIndex b, Torch.Tensor.TensorIndex c, Torch.Tensor.TensorIndex d, Torch.Tensor.TensorIndex e) => Torch.Tensor.TensorIndex (a, b, c, d, e)
instance (Data.Reflection.Reifies a Torch.DType.DType, Foreign.Storable.Storable a) => Torch.Tensor.TensorLike a
instance Torch.Tensor.TensorLike GHC.Types.Bool
instance Torch.Tensor.TensorLike Torch.Tensor.Tensor
instance Torch.Tensor.TensorLike a => Torch.Tensor.TensorLike (a, a)
instance Torch.Tensor.TensorLike a => Torch.Tensor.TensorLike [a]
instance Torch.Internal.Class.Castable Torch.Tensor.RawTensorIndex (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorIndex)
instance Torch.Internal.Class.Castable Torch.Tensor.Tensor Torch.Tensor.ATenTensor
instance Torch.Lens.HasTypes Torch.Tensor.Tensor Torch.Tensor.Tensor
instance Torch.Lens.HasTypes (a -> a) Torch.Tensor.Tensor
instance Torch.Lens.HasTypes GHC.Types.Int Torch.Tensor.Tensor
instance Torch.Lens.HasTypes GHC.Types.Double Torch.Tensor.Tensor
instance Torch.Lens.HasTypes GHC.Types.Float Torch.Tensor.Tensor
instance Torch.Lens.HasTypes GHC.Types.Bool Torch.Tensor.Tensor
instance GHC.Show.Show Torch.Tensor.Tensor
instance Torch.Internal.Class.Castable [Torch.Tensor.Tensor] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)
instance Torch.Internal.Class.Castable [Torch.Tensor.Tensor] (GHC.ForeignPtr.ForeignPtr (Torch.Internal.Type.C10List Torch.Internal.Type.Tensor))
instance Torch.Internal.Class.Castable [Torch.Tensor.Tensor] (GHC.ForeignPtr.ForeignPtr (Torch.Internal.Type.C10List (Torch.Internal.Type.C10Optional Torch.Internal.Type.Tensor)))
instance Torch.Lens.HasTypes GHC.Types.Int GHC.Types.Int
instance Torch.Lens.HasTypes GHC.Types.Float GHC.Types.Float
instance Torch.Lens.HasTypes GHC.Types.Double GHC.Types.Double
instance Torch.Lens.HasTypes GHC.Types.Bool GHC.Types.Bool

module Torch.TensorFactories
type FactoryType = ForeignPtr IntArray -> ForeignPtr TensorOptions -> IO (ForeignPtr Tensor)
type FactoryTypeWithDimnames = ForeignPtr IntArray -> ForeignPtr DimnameList -> ForeignPtr TensorOptions -> IO (ForeignPtr Tensor)
mkFactory :: FactoryType -> [Int] -> TensorOptions -> IO Tensor
mkFactoryUnsafe :: FactoryType -> [Int] -> TensorOptions -> Tensor
mkFactoryWithDimnames :: FactoryTypeWithDimnames -> [(Int, Dimname)] -> TensorOptions -> IO Tensor
mkFactoryUnsafeWithDimnames :: FactoryTypeWithDimnames -> [(Int, Dimname)] -> TensorOptions -> Tensor
mkDefaultFactory :: ([Int] -> TensorOptions -> a) -> [Int] -> a
mkDefaultFactoryWithDimnames :: ([(Int, Dimname)] -> TensorOptions -> a) -> [(Int, Dimname)] -> a

-- | Returns a tensor filled with the scalar value 1, with the shape
--   defined by the variable argument size.
ones :: [Int] -> TensorOptions -> Tensor

-- | Returns a tensor filled with the scalar value 1, with the same size as
--   input tensor
onesLike :: Tensor -> Tensor

-- | Returns a tensor filled with the scalar value 0, with the shape
--   defined by the variable argument size.
zeros :: [Int] -> TensorOptions -> Tensor

-- | Returns a tensor filled with the scalar value 0, with the same size as
--   input tensor
zerosLike :: Tensor -> Tensor

-- | Returns a tensor filled with random numbers from a uniform
--   distribution on the interval [0,1)
randIO :: [Int] -> TensorOptions -> IO Tensor

-- | Returns a tensor filled with random numbers from a standard normal
--   distribution.
randnIO :: [Int] -> TensorOptions -> IO Tensor

-- | Returns a tensor filled with random integers generated uniformly
--   between low (inclusive) and high (exclusive).
randintIO :: Int -> Int -> [Int] -> TensorOptions -> IO Tensor

-- | Returns a tensor with the same size as input that is filled with
--   random numbers from standard normal distribution.
randnLikeIO :: Tensor -> IO Tensor

-- | Returns a tensor with the same size as input that is filled with
--   random numbers from a uniform distribution on the interval [0,1).
randLikeIO :: Tensor -> TensorOptions -> IO Tensor
fullLike :: Tensor -> Float -> TensorOptions -> IO Tensor
onesWithDimnames :: [(Int, Dimname)] -> TensorOptions -> Tensor
zerosWithDimnames :: [(Int, Dimname)] -> TensorOptions -> Tensor
randWithDimnames :: [(Int, Dimname)] -> TensorOptions -> IO Tensor
randnWithDimnames :: [(Int, Dimname)] -> TensorOptions -> IO Tensor

-- | Returns a one-dimensional tensor of steps equally spaced points
--   between start and end.
linspace :: (Scalar a, Scalar b) => a -> b -> Int -> TensorOptions -> Tensor
logspace :: (Scalar a, Scalar b) => a -> b -> Int -> Double -> TensorOptions -> Tensor
eyeSquare :: Int -> TensorOptions -> Tensor

-- | Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
eye :: Int -> Int -> TensorOptions -> Tensor

-- | Returns a tensor of given size filled with fill_value.
full :: Scalar a => [Int] -> a -> TensorOptions -> Tensor

-- | Constructs a sparse tensors in COO(rdinate) format with non-zero
--   elements at the given indices with the given values.
sparseCooTensor :: Tensor -> Tensor -> [Int] -> TensorOptions -> Tensor
ones' :: [Int] -> Tensor
zeros' :: [Int] -> Tensor
randIO' :: [Int] -> IO Tensor
randnIO' :: [Int] -> IO Tensor
randintIO' :: Int -> Int -> [Int] -> IO Tensor
randLikeIO' :: Tensor -> IO Tensor
bernoulliIO' :: Tensor -> IO Tensor
bernoulliIO :: Tensor -> Double -> IO Tensor
poissonIO :: Tensor -> IO Tensor
multinomialIO' :: Tensor -> Int -> IO Tensor
multinomialIO :: Tensor -> Int -> Bool -> IO Tensor
normalIO' :: Tensor -> IO Tensor
normalIO :: Tensor -> Tensor -> IO Tensor
normalScalarIO :: Tensor -> Double -> IO Tensor
normalScalarIO' :: Double -> Tensor -> IO Tensor
normalWithSizeIO :: Double -> Double -> Int -> IO Tensor
rreluIO''' :: Tensor -> IO Tensor
rreluIO'' :: Scalar a => Tensor -> a -> IO Tensor
rreluIO' :: Scalar a => Tensor -> a -> a -> IO Tensor
rreluIO :: Scalar a => Tensor -> a -> a -> Bool -> IO Tensor
rreluWithNoiseIO''' :: Tensor -> Tensor -> IO Tensor
rreluWithNoiseIO'' :: Scalar a => Tensor -> Tensor -> a -> IO Tensor
rreluWithNoiseIO' :: Scalar a => Tensor -> Tensor -> a -> a -> IO Tensor
rreluWithNoiseIO :: Scalar a => Tensor -> Tensor -> a -> a -> Bool -> IO Tensor
onesWithDimnames' :: [(Int, Dimname)] -> Tensor
zerosWithDimnames' :: [(Int, Dimname)] -> Tensor
randWithDimnames' :: [(Int, Dimname)] -> IO Tensor
randnWithDimnames' :: [(Int, Dimname)] -> IO Tensor
linspace' :: (Scalar a, Scalar b) => a -> b -> Int -> Tensor
logspace' :: (Scalar a, Scalar b) => a -> b -> Int -> Double -> Tensor
eyeSquare' :: Int -> Tensor
eye' :: Int -> Int -> Tensor
full' :: Scalar a => [Int] -> a -> Tensor
sparseCooTensor' :: Tensor -> Tensor -> [Int] -> Tensor

-- | Returns a 1-D tensor with values from the interval [start, end) taken
--   with common difference step beginning from start.
arange :: Int -> Int -> Int -> TensorOptions -> Tensor

-- | Returns a 1-D tensor with values from the interval [start, end) taken
--   with common difference step beginning from start.
arange' :: Int -> Int -> Int -> Tensor

module Torch.Index

-- | Generate a slice from a <a>python compatible expression</a>. When you
--   take the odd-numbered element of tensor with `tensor[1::2]` in python,
--   you can write `tensor ! [slice|1::2|]` in hasktorch.
slice :: QuasiQuoter

-- | Generate a lens from a <a>python compatible expression</a>. When you
--   take the odd-numbered elements of tensor with `tensor[1::2]` in
--   python, you can write `tensor ^. [lslice|1::2|]` in hasktorch. When
--   you put 2 in the odd numbered elements of the tensor, you can write
--   `tensor &amp; [lslice|1::2|] ~. 2`.
lslice :: QuasiQuoter

module Torch.Functional.Internal
align_tensors :: [Tensor] -> [Tensor]
dropout :: Tensor -> Double -> Bool -> Tensor
feature_dropout :: Tensor -> Double -> Bool -> Tensor
alpha_dropout :: Tensor -> Double -> Bool -> Tensor
feature_alpha_dropout :: Tensor -> Double -> Bool -> Tensor
abs :: Tensor -> Tensor
absolute :: Tensor -> Tensor
angle :: Tensor -> Tensor
view_as_real :: Tensor -> Tensor
view_as_complex :: Tensor -> Tensor
sgn :: Tensor -> Tensor
real :: Tensor -> Tensor
imag :: Tensor -> Tensor
conj :: Tensor -> Tensor
acos :: Tensor -> Tensor
arccos :: Tensor -> Tensor
avg_pool1d :: Tensor -> Int -> Int -> Int -> Bool -> Bool -> Tensor
adaptive_avg_pool1d :: Tensor -> Int -> Tensor
adaptive_max_pool1d :: Tensor -> Int -> (Tensor, Tensor)
add :: Tensor -> Tensor -> Float -> Tensor
addScalar :: Tensor -> Float -> Float -> Tensor
addmv :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
addr :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
affine_grid_generator :: Tensor -> [Int] -> Bool -> Tensor
allDim :: Tensor -> Int -> Bool -> Tensor
allWithDimname :: Tensor -> Dimname -> Bool -> Tensor
allclose :: Tensor -> Tensor -> Double -> Double -> Bool -> Bool
anyDim :: Tensor -> Int -> Bool -> Tensor
anyWithDimname :: Tensor -> Dimname -> Bool -> Tensor
argmax :: Tensor -> Int -> Bool -> Tensor
argmin :: Tensor -> Int -> Bool -> Tensor
acosh :: Tensor -> Tensor
arccosh :: Tensor -> Tensor
asinh :: Tensor -> Tensor
arcsinh :: Tensor -> Tensor
atanh :: Tensor -> Tensor
arctanh :: Tensor -> Tensor
as_strided :: Tensor -> [Int] -> [Int] -> Int -> Tensor
asin :: Tensor -> Tensor
arcsin :: Tensor -> Tensor
atan :: Tensor -> Tensor
arctan :: Tensor -> Tensor
atleast_1d_t :: Tensor -> Tensor
atleast_1d_l :: [Tensor] -> [Tensor]
atleast_2d_t :: Tensor -> Tensor
atleast_2d_l :: [Tensor] -> [Tensor]
atleast_3d_t :: Tensor -> Tensor
atleast_3d_l :: [Tensor] -> [Tensor]
baddbmm :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
batch_norm :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> Bool -> Tensor
quantized_batch_norm :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Double -> Double -> Int -> Tensor
bilinear :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor
binary_cross_entropy :: Tensor -> Tensor -> Tensor -> Int -> Tensor
binary_cross_entropy_with_logits :: Tensor -> Tensor -> Tensor -> Tensor -> Int -> Tensor
bincount :: Tensor -> Tensor -> Int -> Tensor
bitwise_not :: Tensor -> Tensor
copysign_tt :: Tensor -> Tensor -> Tensor
copysign_ts :: Tensor -> Float -> Tensor
logical_not :: Tensor -> Tensor
logical_xor :: Tensor -> Tensor -> Tensor
logical_and :: Tensor -> Tensor -> Tensor
logical_or :: Tensor -> Tensor -> Tensor
bmm :: Tensor -> Tensor -> Tensor
broadcast_tensors :: [Tensor] -> [Tensor]
broadcast_to :: Tensor -> [Int] -> Tensor
cat :: [Tensor] -> Int -> Tensor
catWithDimname :: [Tensor] -> Dimname -> Tensor
block_diag :: [Tensor] -> Tensor
ceil :: Tensor -> Tensor
chain_matmul :: [Tensor] -> Tensor
unsafe_chunk :: Tensor -> Int -> Int -> [Tensor]
chunk :: Tensor -> Int -> Int -> [Tensor]
tensor_split_ttl :: Tensor -> Tensor -> Int -> [Tensor]
clamp_tss :: Tensor -> Float -> Float -> Tensor
clamp_ttt :: Tensor -> Tensor -> Tensor -> Tensor
clamp_max_ts :: Tensor -> Float -> Tensor
clamp_max_tt :: Tensor -> Tensor -> Tensor
clamp_min_ts :: Tensor -> Float -> Tensor
clamp_min_tt :: Tensor -> Tensor -> Tensor
clip_tss :: Tensor -> Float -> Float -> Tensor
clip_ttt :: Tensor -> Tensor -> Tensor -> Tensor
cudnn_is_acceptable :: Tensor -> Bool
complex :: Tensor -> Tensor -> Tensor
polar :: Tensor -> Tensor -> Tensor
constant_pad_nd :: Tensor -> [Int] -> Float -> Tensor
convolution :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Bool -> [Int] -> Int -> Tensor
convolution_overrideable :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Bool -> [Int] -> Int -> Tensor
convolution_backward_overrideable :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Bool -> [Int] -> Int -> (Bool, Bool, Bool) -> (Tensor, Tensor, Tensor)
conv1d_tttllll :: Tensor -> Tensor -> Tensor -> Int -> Int -> Int -> Int -> Tensor
conv2d :: Tensor -> Tensor -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Int -> Tensor
conv3d_tttllll :: Tensor -> Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Int -> Tensor
conv1d_tttlsll :: Tensor -> Tensor -> Tensor -> Int -> String -> Int -> Int -> Tensor
conv2d_tttlsll :: Tensor -> Tensor -> Tensor -> (Int, Int) -> String -> (Int, Int) -> Int -> Tensor
conv3d_tttlsll :: Tensor -> Tensor -> Tensor -> (Int, Int, Int) -> String -> (Int, Int, Int) -> Int -> Tensor
conv_tbc :: Tensor -> Tensor -> Tensor -> Int -> Tensor
conv_transpose1d :: Tensor -> Tensor -> Tensor -> Int -> Int -> Int -> Int -> Int -> Tensor
conv_transpose2d :: Tensor -> Tensor -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Int -> (Int, Int) -> Tensor
conv_transpose3d :: Tensor -> Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Int -> (Int, Int, Int) -> Tensor
cos :: Tensor -> Tensor
cosh :: Tensor -> Tensor
cosine_embedding_loss :: Tensor -> Tensor -> Tensor -> Double -> Int -> Tensor
cudnn_affine_grid_generator :: Tensor -> Int -> Int -> Int -> Int -> Tensor
cudnn_batch_norm :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> (Tensor, Tensor, Tensor, Tensor)
cudnn_convolution_tttllllbb :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
cudnn_convolution_ttllllbb :: Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
cudnn_convolution_ttllllbbb :: Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Bool -> Tensor
cudnn_convolution_backward_input :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Bool -> Tensor
cudnn_convolution_backward_weight :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Bool -> Tensor
cudnn_convolution_transpose_tttlllllbb :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
cudnn_convolution_transpose_ttlllllbb :: Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
cudnn_convolution_transpose_ttlllllbbb :: Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Bool -> Tensor
cudnn_convolution_transpose_backward_input :: Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Bool -> Tensor
cudnn_convolution_transpose_backward_weight :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Bool -> Tensor
cudnn_convolution_relu :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Tensor
cudnn_convolution_add_relu :: Tensor -> Tensor -> Tensor -> Float -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Tensor
cudnn_grid_sampler :: Tensor -> Tensor -> Tensor
cummax_tl :: Tensor -> Int -> (Tensor, Tensor)
cummax_tn :: Tensor -> Dimname -> (Tensor, Tensor)
cummin_tl :: Tensor -> Int -> (Tensor, Tensor)
cummin_tn :: Tensor -> Dimname -> (Tensor, Tensor)
cumprod :: Tensor -> Int -> DType -> Tensor
cumprodWithDimname :: Tensor -> Dimname -> DType -> Tensor
cumsum :: Tensor -> Int -> DType -> Tensor
cumsumWithDimname :: Tensor -> Dimname -> DType -> Tensor
ctcLoss' :: Tensor -> Tensor -> [Int] -> [Int] -> Int -> Int -> Bool -> Tensor
ctcLoss :: Tensor -> Tensor -> Tensor -> Tensor -> Int -> Int -> Bool -> Tensor
diag_embed :: Tensor -> Int -> Int -> Int -> Tensor
diagflat :: Tensor -> Int -> Tensor
diagonal_tlll :: Tensor -> Int -> Int -> Int -> Tensor
diagonal_tnnnl :: Tensor -> Dimname -> Dimname -> Dimname -> Int -> Tensor
diff :: Tensor -> Int -> Int -> Tensor -> Tensor -> Tensor
gradient_tsll :: Tensor -> Float -> Int -> Int -> [Tensor]
div :: Tensor -> Tensor -> Tensor
div_tts :: Tensor -> Tensor -> String -> Tensor
divScalar :: Tensor -> Float -> Tensor
div_tss :: Tensor -> Float -> String -> Tensor
divide_tt :: Tensor -> Tensor -> Tensor
divide_ts :: Tensor -> Float -> Tensor
divide_tts :: Tensor -> Tensor -> String -> Tensor
divide_tss :: Tensor -> Float -> String -> Tensor
true_divide_tt :: Tensor -> Tensor -> Tensor
true_divide_ts :: Tensor -> Float -> Tensor
dot :: Tensor -> Tensor -> Tensor
vdot :: Tensor -> Tensor -> Tensor
einsum :: String -> [Tensor] -> Tensor
embedding :: Tensor -> Tensor -> Int -> Bool -> Bool -> Tensor
row_stack :: [Tensor] -> Tensor
embedding_bag_tttblbtb :: Tensor -> Tensor -> Tensor -> Bool -> Int -> Bool -> Tensor -> Bool -> (Tensor, Tensor, Tensor, Tensor)
embedding_bag_tttblbtbl :: Tensor -> Tensor -> Tensor -> Bool -> Int -> Bool -> Tensor -> Bool -> Int -> (Tensor, Tensor, Tensor, Tensor)
empty_quantized :: [Int] -> Tensor -> Tensor
erf :: Tensor -> Tensor
erfc :: Tensor -> Tensor
exp :: Tensor -> Tensor
exp2 :: Tensor -> Tensor
expm1 :: Tensor -> Tensor
flatten :: Tensor -> Int -> Int -> Tensor
flattenTo :: Tensor -> Int -> Int -> Dimname -> Tensor
flattenToWithDimname :: Tensor -> Dimname -> Dimname -> Dimname -> Tensor
flattenToWithDimnames :: Tensor -> [Dimname] -> Dimname -> Tensor
floor :: Tensor -> Tensor
floor_divide_tt :: Tensor -> Tensor -> Tensor
floor_divide_ts :: Tensor -> Float -> Tensor
frac :: Tensor -> Tensor
gcd :: Tensor -> Tensor -> Tensor
lcm :: Tensor -> Tensor -> Tensor
grid_sampler :: Tensor -> Tensor -> Int -> Int -> Bool -> Tensor
grid_sampler_2d :: Tensor -> Tensor -> Int -> Int -> Bool -> Tensor
grid_sampler_3d :: Tensor -> Tensor -> Int -> Int -> Bool -> Tensor
hinge_embedding_loss :: Tensor -> Tensor -> Double -> Int -> Tensor
group_norm :: Tensor -> Int -> Tensor -> Tensor -> Double -> Bool -> Tensor
native_group_norm :: Tensor -> Tensor -> Tensor -> Int -> Int -> Int -> Int -> Double -> (Tensor, Tensor, Tensor)
index :: Tensor -> [Tensor] -> Tensor
indexCopy :: Tensor -> Int -> Tensor -> Tensor -> Tensor
indexCopyWithDimname :: Tensor -> Dimname -> Tensor -> Tensor -> Tensor
index_put :: Tensor -> [Tensor] -> Tensor -> Bool -> Tensor
instance_norm :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> Bool -> Tensor
inverse :: Tensor -> Tensor
isclose :: Tensor -> Tensor -> Double -> Double -> Bool -> Tensor
isnan :: Tensor -> Tensor
is_distributed :: Tensor -> Bool
is_floating_point :: Tensor -> Bool
is_complex :: Tensor -> Bool
isreal :: Tensor -> Tensor
is_nonzero :: Tensor -> Bool
is_same_size :: Tensor -> Tensor -> Bool
is_signed :: Tensor -> Bool
kl_div :: Tensor -> Tensor -> Int -> Bool -> Tensor
kron :: Tensor -> Tensor -> Tensor
kthvalue :: Tensor -> Int -> Int -> Bool -> (Tensor, Tensor)
kthvalueWithDimname :: Tensor -> Int -> Dimname -> Bool -> (Tensor, Tensor)
layer_norm :: Tensor -> [Int] -> Tensor -> Tensor -> Double -> Bool -> Tensor
native_layer_norm :: Tensor -> [Int] -> Tensor -> Tensor -> Double -> (Tensor, Tensor, Tensor)
nan_to_num :: Tensor -> Double -> Double -> Double -> Tensor
linear :: Tensor -> Tensor -> Tensor -> Tensor
mkldnn_linear :: Tensor -> Tensor -> Tensor -> Tensor
mkldnn_linear_backward_input :: [Int] -> Tensor -> Tensor -> Tensor
mkldnn_linear_backward_weights :: Tensor -> Tensor -> Tensor -> Bool -> (Tensor, Tensor)
fbgemm_linear_int8_weight_fp32_activation :: Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor -> Tensor
fbgemm_linear_int8_weight :: Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor -> Tensor
fbgemm_linear_quantize_weight :: Tensor -> (Tensor, Tensor, Double, Int)
fbgemm_pack_gemm_matrix_fp16 :: Tensor -> Tensor
fbgemm_linear_fp16_weight_fp32_activation :: Tensor -> Tensor -> Tensor -> Tensor
fbgemm_linear_fp16_weight :: Tensor -> Tensor -> Tensor -> Tensor
quantizeFbgemm' :: Tensor -> Tensor
quantizeFbgemm :: Tensor -> Int -> Int -> Tensor
ldexp :: Tensor -> Tensor -> Tensor
log :: Tensor -> Tensor
log10 :: Tensor -> Tensor
log1p :: Tensor -> Tensor
log2 :: Tensor -> Tensor
logaddexp :: Tensor -> Tensor -> Tensor
logaddexp2 :: Tensor -> Tensor -> Tensor
xlogy_tt :: Tensor -> Tensor -> Tensor
xlogy_st :: Float -> Tensor -> Tensor
xlogy_ts :: Tensor -> Float -> Tensor
logdet :: Tensor -> Tensor
logSoftmax :: Tensor -> Int -> DType -> Tensor
logSoftmaxWithDimname :: Tensor -> Dimname -> DType -> Tensor
logcumsumexp_tl :: Tensor -> Int -> Tensor
logcumsumexp_tn :: Tensor -> Dimname -> Tensor
logsumexp :: Tensor -> Int -> Bool -> Tensor
logsumexpWithDimnameList :: Tensor -> [Dimname] -> Bool -> Tensor
margin_ranking_loss :: Tensor -> Tensor -> Tensor -> Double -> Int -> Tensor
matmul :: Tensor -> Tensor -> Tensor
matrixRank :: Tensor -> Double -> Bool -> Tensor
matrixRank' :: Tensor -> Bool -> Tensor
matrix_power :: Tensor -> Int -> Tensor
matrix_exp :: Tensor -> Tensor
maxDim :: Tensor -> Int -> Bool -> (Tensor, Tensor)
maxWithDimname :: Tensor -> Dimname -> Bool -> (Tensor, Tensor)
amax :: Tensor -> Int -> Bool -> Tensor
max_pool1d_with_indices :: Tensor -> Int -> Int -> Int -> Int -> Bool -> (Tensor, Tensor)
max_pool1d :: Tensor -> Int -> Int -> Int -> Int -> Bool -> Tensor
max_pool2d :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Bool -> Tensor
mkldnn_max_pool2d :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Bool -> Tensor
mkldnn_max_pool3d :: Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Bool -> Tensor
quantized_max_pool1d :: Tensor -> Int -> Int -> Int -> Int -> Bool -> Tensor
quantized_max_pool2d :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Bool -> Tensor
max_pool3d :: Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Bool -> Tensor
meanAll :: Tensor -> DType -> Tensor
meanDim :: Tensor -> Int -> Bool -> DType -> Tensor
meanWithDimnames :: Tensor -> [Dimname] -> Bool -> DType -> Tensor
medianAll :: Tensor -> Tensor
medianDim :: Tensor -> Int -> Bool -> (Tensor, Tensor)
medianWithDimname :: Tensor -> Dimname -> Bool -> (Tensor, Tensor)
nanmedian_t :: Tensor -> Tensor
nanmedian_tlb :: Tensor -> Int -> Bool -> (Tensor, Tensor)
nanmedian_tnb :: Tensor -> Dimname -> Bool -> (Tensor, Tensor)
minDim :: Tensor -> Int -> Bool -> (Tensor, Tensor)
minWithDimname :: Tensor -> Dimname -> Bool -> (Tensor, Tensor)
amin :: Tensor -> Int -> Bool -> Tensor
mkldnn_convolution :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Tensor
mkldnn_convolution_backward_input :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Tensor
mkldnn_convolution_backward_weights :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> (Tensor, Tensor)
miopen_batch_norm :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> (Tensor, Tensor, Tensor)
miopen_convolution :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_convolution_backward_input :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_convolution_backward_bias :: Tensor -> Tensor
miopen_convolution_backward_weight :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_convolution_transpose :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_convolution_transpose_backward_input :: Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_convolution_transpose_backward_weight :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_depthwise_convolution :: Tensor -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_depthwise_convolution_backward_input :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_depthwise_convolution_backward_weight :: [Int] -> Tensor -> Tensor -> [Int] -> [Int] -> [Int] -> Int -> Bool -> Bool -> Tensor
miopen_rnn :: Tensor -> [Tensor] -> Int -> Tensor -> Tensor -> Int -> Int -> Int -> Bool -> Double -> Bool -> Bool -> [Int] -> Tensor -> (Tensor, Tensor, Tensor, Tensor, Tensor)
mm :: Tensor -> Tensor -> Tensor
mode :: Tensor -> Int -> Bool -> (Tensor, Tensor)
modeWithDimname :: Tensor -> Dimname -> Bool -> (Tensor, Tensor)
mul :: Tensor -> Tensor -> Tensor
mulScalar :: Tensor -> Float -> Tensor
multiply_tt :: Tensor -> Tensor -> Tensor
multiply_ts :: Tensor -> Float -> Tensor
mv :: Tensor -> Tensor -> Tensor
mvlgamma :: Tensor -> Int -> Tensor
narrow_copy :: Tensor -> Int -> Int -> Int -> Tensor
narrow_tlll :: Tensor -> Int -> Int -> Int -> Tensor
narrow_tltl :: Tensor -> Int -> Tensor -> Int -> Tensor
native_batch_norm :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> (Tensor, Tensor, Tensor)
batch_norm_stats :: Tensor -> Double -> (Tensor, Tensor)
batch_norm_elemt :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Double -> Tensor
batch_norm_gather_stats :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Double -> Double -> Int -> (Tensor, Tensor)
batch_norm_gather_stats_with_counts :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Double -> Double -> Tensor -> (Tensor, Tensor)
batch_norm_backward_reduce :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Bool -> Bool -> (Tensor, Tensor, Tensor, Tensor)
batch_norm_backward_elemt :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor
batch_norm_update_stats :: Tensor -> Tensor -> Tensor -> Double -> (Tensor, Tensor)
is_vulkan_available :: Bool
pairwise_distance :: Tensor -> Tensor -> Double -> Double -> Bool -> Tensor
cdist :: Tensor -> Tensor -> Double -> Int -> Tensor
pdist :: Tensor -> Double -> Tensor
cosine_similarity :: Tensor -> Tensor -> Int -> Double -> Tensor
permute :: Tensor -> [Int] -> Tensor
pixel_shuffle :: Tensor -> Int -> Tensor
pixel_unshuffle :: Tensor -> Int -> Tensor
channel_shuffle :: Tensor -> Int -> Tensor
pinverse :: Tensor -> Double -> Tensor
poisson_nll_loss :: Tensor -> Tensor -> Bool -> Bool -> Double -> Int -> Tensor
rad2deg :: Tensor -> Tensor
deg2rad :: Tensor -> Tensor
ravel :: Tensor -> Tensor
reciprocal :: Tensor -> Tensor
neg :: Tensor -> Tensor
negative :: Tensor -> Tensor
repeatInterleaveRange :: Tensor -> Tensor
repeatInterleave :: Tensor -> Tensor -> Int -> Tensor
repeatInterleaveScalar :: Tensor -> Int -> Int -> Tensor
reshape :: Tensor -> [Int] -> Tensor
round :: Tensor -> Tensor
relu :: Tensor -> Tensor
relu6 :: Tensor -> Tensor
prelu :: Tensor -> Tensor -> Tensor
gelu :: Tensor -> Tensor
hardshrink :: Tensor -> Float -> Tensor
rsqrt :: Tensor -> Tensor
selectWithDimname :: Tensor -> Dimname -> Int -> Tensor
select :: Tensor -> Int -> Int -> Tensor
selu :: Tensor -> Tensor
celu :: Tensor -> Float -> Tensor
silu :: Tensor -> Tensor
mish :: Tensor -> Tensor
sigmoid :: Tensor -> Tensor
logit :: Tensor -> Double -> Tensor
sin :: Tensor -> Tensor
sinc :: Tensor -> Tensor
sinh :: Tensor -> Tensor
size :: Tensor -> Int -> Int
sizeWithDimname :: Tensor -> Dimname -> Int
slice :: Tensor -> Int -> Int -> Int -> Int -> Tensor
slogdet :: Tensor -> (Tensor, Tensor)
smm :: Tensor -> Tensor -> Tensor
softmax :: Tensor -> Int -> DType -> Tensor
softmaxWithDimname :: Tensor -> Dimname -> DType -> Tensor
unsafe_split :: Tensor -> Int -> Int -> [Tensor]
split :: Tensor -> Int -> Int -> [Tensor]
unsafe_split_with_sizes :: Tensor -> [Int] -> Int -> [Tensor]
split_with_sizes :: Tensor -> [Int] -> Int -> [Tensor]
squeezeAll :: Tensor -> Tensor
squeezeDim :: Tensor -> Int -> Tensor
squeezeWithDimname :: Tensor -> Dimname -> Tensor
sspaddmm :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
stack :: [Tensor] -> Int -> Tensor
hstack :: [Tensor] -> Tensor
vstack :: [Tensor] -> Tensor
dstack :: [Tensor] -> Tensor
stft :: Tensor -> Int -> Int -> Int -> Tensor -> Bool -> Bool -> Bool -> Tensor
istft :: Tensor -> Int -> Int -> Int -> Tensor -> Bool -> Bool -> Bool -> Int -> Bool -> Tensor
stride :: Tensor -> Int -> Int
strideWithDimname :: Tensor -> Dimname -> Int
sumAll :: Tensor -> DType -> Tensor
sumDim :: Tensor -> Int -> Bool -> DType -> Tensor
sumWithDimnames :: Tensor -> [Dimname] -> Bool -> DType -> Tensor
nansum_ts :: Tensor -> DType -> Tensor
nansum_tlbs :: Tensor -> Int -> Bool -> DType -> Tensor
sqrt :: Tensor -> Tensor
square :: Tensor -> Tensor
stdAll :: Tensor -> Bool -> Tensor
stdDim :: Tensor -> Int -> Bool -> Bool -> Tensor
std_tllb :: Tensor -> Int -> Int -> Bool -> Tensor
stdMeanAll :: Tensor -> Bool -> (Tensor, Tensor)
stdMeanDim :: Tensor -> Int -> Bool -> Bool -> (Tensor, Tensor)
std_mean_tllb :: Tensor -> Int -> Int -> Bool -> (Tensor, Tensor)
stdMeanWithDimnames :: Tensor -> [Dimname] -> Bool -> Bool -> (Tensor, Tensor)
std_mean_tNlb :: Tensor -> [Dimname] -> Int -> Bool -> (Tensor, Tensor)
stdWithDimnames :: Tensor -> [Dimname] -> Bool -> Bool -> Tensor
std_tNlb :: Tensor -> [Dimname] -> Int -> Bool -> Tensor
prodAll :: Tensor -> DType -> Tensor
prodDim :: Tensor -> Int -> Bool -> DType -> Tensor
prodWithDimnames :: Tensor -> Dimname -> Bool -> DType -> Tensor
t :: Tensor -> Tensor
tan :: Tensor -> Tensor
tanh :: Tensor -> Tensor
tensordot :: Tensor -> Tensor -> [Int] -> [Int] -> Tensor
threshold :: Tensor -> Float -> Float -> Tensor
tile :: Tensor -> [Int] -> Tensor
transpose :: Tensor -> Int -> Int -> Tensor
transposeWithDimname :: Tensor -> Dimname -> Dimname -> Tensor
one_hot :: Tensor -> Int -> Tensor
flip :: Tensor -> [Int] -> Tensor
fliplr :: Tensor -> Tensor
flipud :: Tensor -> Tensor
roll :: Tensor -> Int -> Int -> Tensor
rot90 :: Tensor -> Int -> [Int] -> Tensor
trapz :: Tensor -> Tensor -> Int -> Tensor
trapzScalar :: Tensor -> Double -> Int -> Tensor
triplet_margin_loss :: Tensor -> Tensor -> Tensor -> Double -> Double -> Double -> Bool -> Int -> Tensor
trunc :: Tensor -> Tensor
fix :: Tensor -> Tensor
unique_dim :: Tensor -> Int -> Bool -> Bool -> Bool -> (Tensor, Tensor, Tensor)
unique_consecutive :: Tensor -> Bool -> Bool -> Int -> (Tensor, Tensor, Tensor)
unique_dim_consecutive :: Tensor -> Int -> Bool -> Bool -> (Tensor, Tensor, Tensor)
unsqueeze :: Tensor -> Int -> Tensor
vander :: Tensor -> Int -> Bool -> Tensor
var :: Tensor -> Bool -> Tensor
varDim :: Tensor -> Int -> Bool -> Bool -> Tensor
var_tllb :: Tensor -> Int -> Int -> Bool -> Tensor
varWithDimnames :: Tensor -> [Dimname] -> Bool -> Bool -> Tensor
var_tNlb :: Tensor -> [Dimname] -> Int -> Bool -> Tensor
varMean :: Tensor -> Bool -> (Tensor, Tensor)
varMeanDim :: Tensor -> Int -> Bool -> Bool -> (Tensor, Tensor)
var_mean_tllb :: Tensor -> Int -> Int -> Bool -> (Tensor, Tensor)
varMeanWithDimnames :: Tensor -> [Dimname] -> Bool -> Bool -> (Tensor, Tensor)
var_mean_tNlb :: Tensor -> [Dimname] -> Int -> Bool -> (Tensor, Tensor)
where' :: Tensor -> Tensor -> Tensor -> Tensor
where_tst :: Tensor -> Float -> Tensor -> Tensor
where_tts :: Tensor -> Tensor -> Float -> Tensor
where_tss :: Tensor -> Float -> Float -> Tensor
isNonZero :: Tensor -> [Tensor]
norm_except_dim :: Tensor -> Int -> Int -> Tensor
native_norm_ts :: Tensor -> Float -> Tensor
native_norm_tslbs :: Tensor -> Float -> Int -> Bool -> DType -> Tensor
normCastAll :: Tensor -> Float -> DType -> Tensor
normAll :: Tensor -> Float -> Tensor
normCastDim :: Tensor -> Float -> Int -> Bool -> DType -> Tensor
normDim :: Tensor -> Float -> Int -> Bool -> Tensor
norm_tsNbs :: Tensor -> Float -> [Dimname] -> Bool -> DType -> Tensor
norm_tsNb :: Tensor -> Float -> [Dimname] -> Bool -> Tensor
frexp :: Tensor -> (Tensor, Tensor)
frobeniusNormAll :: Tensor -> Tensor
frobeniusNormDim :: Tensor -> Int -> Bool -> Tensor
nuclearNormAll :: Tensor -> Bool -> Tensor
nuclearNormDim :: Tensor -> (Int, Int) -> Bool -> Tensor
clone :: Tensor -> MemoryFormat -> Tensor
positive :: Tensor -> Tensor
sub :: Tensor -> Tensor -> Float -> Tensor
subScalar :: Tensor -> Float -> Float -> Tensor
subtract_tts :: Tensor -> Tensor -> Float -> Tensor
subtract_tss :: Tensor -> Float -> Float -> Tensor
rsub :: Tensor -> Tensor -> Float -> Tensor
heaviside :: Tensor -> Tensor -> Tensor
rsubScalar :: Tensor -> Float -> Float -> Tensor
addmm :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
hspmm :: Tensor -> Tensor -> Tensor
unbind :: Tensor -> Int -> [Tensor]
unbindWithDimname :: Tensor -> Dimname -> [Tensor]
mkldnn_reorder_conv2d_weight :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Int -> Tensor
mkldnn_reorder_conv3d_weight :: Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Int -> Tensor
quantize_per_tensor_tdls :: Tensor -> Double -> Int -> DType -> Tensor
quantize_per_tensor_ltts :: [Tensor] -> Tensor -> Tensor -> DType -> [Tensor]
quantize_per_channel :: Tensor -> Tensor -> Tensor -> Int -> DType -> Tensor
dequantize_t :: Tensor -> Tensor
dequantize_l :: [Tensor] -> [Tensor]
q_scale :: Tensor -> Double
q_zero_point :: Tensor -> Int
q_per_channel_scales :: Tensor -> Tensor
q_per_channel_zero_points :: Tensor -> Tensor
q_per_channel_axis :: Tensor -> Int
int_repr :: Tensor -> Tensor
fake_quantize_per_tensor_affine :: Tensor -> Double -> Int -> Int -> Int -> Tensor
fake_quantize_per_tensor_affine_cachemask :: Tensor -> Double -> Int -> Int -> Int -> (Tensor, Tensor)
fake_quantize_per_channel_affine :: Tensor -> Tensor -> Tensor -> Int -> Int -> Int -> Tensor
fake_quantize_per_channel_affine_cachemask :: Tensor -> Tensor -> Tensor -> Int -> Int -> Int -> (Tensor, Tensor)
choose_qparams_optimized :: Tensor -> Int -> Int -> Double -> Int -> (Tensor, Tensor)
meshgrid :: [Tensor] -> [Tensor]
cartesian_prod :: [Tensor] -> Tensor
combinations :: Tensor -> Int -> Bool -> Tensor
resultType :: Tensor -> Tensor -> DType
resultTypeScalar :: Tensor -> Float -> DType
resultTypeScalar' :: Float -> Tensor -> DType
resultTypeScalars :: Float -> Float -> DType
can_cast :: DType -> DType -> Bool
promote_types :: DType -> DType -> DType
lstm :: Tensor -> [Tensor] -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> (Tensor, Tensor, Tensor)
lstm' :: Tensor -> Tensor -> [Tensor] -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> (Tensor, Tensor, Tensor)
gru :: Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> (Tensor, Tensor)
gru' :: Tensor -> Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> (Tensor, Tensor)
rnnTanh :: Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> (Tensor, Tensor)
rnnTanh' :: Tensor -> Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> (Tensor, Tensor)
rnnRelu :: Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> (Tensor, Tensor)
rnnRelu' :: Tensor -> Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> (Tensor, Tensor)
lstm_cell :: Tensor -> [Tensor] -> Tensor -> Tensor -> Tensor -> Tensor -> (Tensor, Tensor)
gru_cell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor
rnn_tanh_cell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor
rnn_relu_cell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor
quantized_lstm_cell :: Tensor -> [Tensor] -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> (Tensor, Tensor)
quantized_gru_cell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> Tensor
quantized_rnn_relu_cell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> Tensor
quantized_rnn_tanh_cell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> Tensor
maskedFillScalar :: Tensor -> Tensor -> Float -> Tensor
maskedFill :: Tensor -> Tensor -> Tensor -> Tensor
masked_scatter :: Tensor -> Tensor -> Tensor -> Tensor
put :: Tensor -> Tensor -> Tensor -> Bool -> Tensor
indexAdd :: Tensor -> Int -> Tensor -> Tensor -> Tensor
index_add_tltts :: Tensor -> Int -> Tensor -> Tensor -> Float -> Tensor
index_add_tntts :: Tensor -> Dimname -> Tensor -> Tensor -> Float -> Tensor
indexFillScalar :: Tensor -> Int -> Tensor -> Float -> Tensor
indexFill :: Tensor -> Int -> Tensor -> Tensor -> Tensor
indexFillScalarWithDimname :: Tensor -> Dimname -> Tensor -> Float -> Tensor
indexFillWithDimname :: Tensor -> Dimname -> Tensor -> Tensor -> Tensor
scatter :: Tensor -> Int -> Tensor -> Tensor -> Tensor
scatterScalar :: Tensor -> Int -> Tensor -> Float -> Tensor
scatterWithDimname :: Tensor -> Dimname -> Tensor -> Tensor -> Tensor
scatterScalarWithDimname :: Tensor -> Dimname -> Tensor -> Float -> Tensor
scatterAdd :: Tensor -> Int -> Tensor -> Tensor -> Tensor
scatterAddWithDimname :: Tensor -> Dimname -> Tensor -> Tensor -> Tensor
bitwise_and_ts :: Tensor -> Float -> Tensor
bitwise_and_tt :: Tensor -> Tensor -> Tensor
bitwise_or_ts :: Tensor -> Float -> Tensor
bitwise_or_tt :: Tensor -> Tensor -> Tensor
bitwiseXorScalar :: Tensor -> Float -> Tensor
bitwiseXor :: Tensor -> Tensor -> Tensor
addbmm :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
diag :: Tensor -> Int -> Tensor
cross :: Tensor -> Tensor -> Int -> Tensor
triu :: Tensor -> Int -> Tensor
tril :: Tensor -> Int -> Tensor
trace :: Tensor -> Tensor
neScalar :: Tensor -> Float -> Tensor
ne :: Tensor -> Tensor -> Tensor
not_equal_ts :: Tensor -> Float -> Tensor
not_equal_tt :: Tensor -> Tensor -> Tensor
eqScalar :: Tensor -> Float -> Tensor
eq :: Tensor -> Tensor -> Tensor
geScalar :: Tensor -> Float -> Tensor
ge :: Tensor -> Tensor -> Tensor
greater_equal_ts :: Tensor -> Float -> Tensor
greater_equal_tt :: Tensor -> Tensor -> Tensor
leScalar :: Tensor -> Float -> Tensor
le :: Tensor -> Tensor -> Tensor
less_equal_ts :: Tensor -> Float -> Tensor
less_equal_tt :: Tensor -> Tensor -> Tensor
gtScalar :: Tensor -> Float -> Tensor
gt :: Tensor -> Tensor -> Tensor
greater_ts :: Tensor -> Float -> Tensor
greater_tt :: Tensor -> Tensor -> Tensor
ltScalar :: Tensor -> Float -> Tensor
lt :: Tensor -> Tensor -> Tensor
less_ts :: Tensor -> Float -> Tensor
less_tt :: Tensor -> Tensor -> Tensor
take :: Tensor -> Tensor -> Tensor
take_along_dim :: Tensor -> Tensor -> Int -> Tensor
indexSelect :: Tensor -> Int -> Tensor -> Tensor
indexSelectWithDimname :: Tensor -> Dimname -> Tensor -> Tensor
masked_select :: Tensor -> Tensor -> Tensor
nonzero :: Tensor -> Tensor
nonzero_numpy :: Tensor -> [Tensor]
gather :: Tensor -> Int -> Tensor -> Bool -> Tensor
gatherWithDimname :: Tensor -> Dimname -> Tensor -> Bool -> Tensor
addcmul :: Tensor -> Tensor -> Tensor -> Float -> Tensor
addcdiv :: Tensor -> Tensor -> Tensor -> Float -> Tensor
cross_entropy_loss :: Tensor -> Tensor -> Tensor -> Int -> Int -> Tensor
lstsq :: Tensor -> Tensor -> (Tensor, Tensor)
triangular_solve :: Tensor -> Tensor -> Bool -> Bool -> Bool -> (Tensor, Tensor)
symeig :: Tensor -> Bool -> Bool -> (Tensor, Tensor)
eig :: Tensor -> Bool -> (Tensor, Tensor)
svd :: Tensor -> Bool -> Bool -> (Tensor, Tensor, Tensor)
swapaxes :: Tensor -> Int -> Int -> Tensor
swapdims :: Tensor -> Int -> Int -> Tensor
cholesky :: Tensor -> Bool -> Tensor
cholesky_solve :: Tensor -> Tensor -> Bool -> Tensor
solve :: Tensor -> Tensor -> (Tensor, Tensor)
cholesky_inverse :: Tensor -> Bool -> Tensor
qr :: Tensor -> Bool -> (Tensor, Tensor)
geqrf :: Tensor -> (Tensor, Tensor)
orgqr :: Tensor -> Tensor -> Tensor
ormqr :: Tensor -> Tensor -> Tensor -> Bool -> Bool -> Tensor
lu_solve :: Tensor -> Tensor -> Tensor -> Tensor
lu_unpack :: Tensor -> Tensor -> Bool -> Bool -> (Tensor, Tensor, Tensor)
lgamma :: Tensor -> Tensor
digamma :: Tensor -> Tensor
polygamma :: Int -> Tensor -> Tensor
erfinv :: Tensor -> Tensor
i0 :: Tensor -> Tensor
sign :: Tensor -> Tensor
signbit :: Tensor -> Tensor
dist :: Tensor -> Tensor -> Float -> Tensor
atan2 :: Tensor -> Tensor -> Tensor
lerpScalar :: Tensor -> Tensor -> Float -> Tensor
lerp :: Tensor -> Tensor -> Tensor -> Tensor
histc :: Tensor -> Int -> Float -> Float -> Tensor
fmodScalar :: Tensor -> Float -> Tensor
fmod :: Tensor -> Tensor -> Tensor
hypot :: Tensor -> Tensor -> Tensor
igamma :: Tensor -> Tensor -> Tensor
igammac :: Tensor -> Tensor -> Tensor
nextafter :: Tensor -> Tensor -> Tensor
remainderScalar :: Tensor -> Float -> Tensor
remainder :: Tensor -> Tensor -> Tensor
minAll :: Tensor -> Tensor
fmin :: Tensor -> Tensor -> Tensor
maxAll :: Tensor -> Tensor
fmax :: Tensor -> Tensor -> Tensor
maximum :: Tensor -> Tensor -> Tensor
max :: Tensor -> Tensor -> Tensor
minimum :: Tensor -> Tensor -> Tensor
min :: Tensor -> Tensor -> Tensor
quantile_tdlb :: Tensor -> Double -> Int -> Bool -> Tensor
quantile_ttlb :: Tensor -> Tensor -> Int -> Bool -> Tensor
nanquantile_tdlb :: Tensor -> Double -> Int -> Bool -> Tensor
nanquantile_ttlb :: Tensor -> Tensor -> Int -> Bool -> Tensor
quantile_tdlbs :: Tensor -> Double -> Int -> Bool -> String -> Tensor
quantile_ttlbs :: Tensor -> Tensor -> Int -> Bool -> String -> Tensor
nanquantile_tdlbs :: Tensor -> Double -> Int -> Bool -> String -> Tensor
nanquantile_ttlbs :: Tensor -> Tensor -> Int -> Bool -> String -> Tensor
sort :: Tensor -> Int -> Bool -> (Tensor, Tensor)
sort_tblb :: Tensor -> Bool -> Int -> Bool -> (Tensor, Tensor)
sortWithDimname :: Tensor -> Dimname -> Bool -> (Tensor, Tensor)
sort_tbnb :: Tensor -> Bool -> Dimname -> Bool -> (Tensor, Tensor)
msort :: Tensor -> Tensor
argsort :: Tensor -> Int -> Bool -> Tensor
argsortWithDimname :: Tensor -> Dimname -> Bool -> Tensor
topk :: Tensor -> Int -> Int -> Bool -> Bool -> (Tensor, Tensor)
all :: Tensor -> Tensor
any :: Tensor -> Tensor
renorm :: Tensor -> Float -> Int -> Float -> Tensor
equal :: Tensor -> Tensor -> Bool
pow :: Tensor -> Tensor -> Tensor
powScalar' :: Float -> Tensor -> Tensor
powScalar :: Tensor -> Float -> Tensor
float_power_tt :: Tensor -> Tensor -> Tensor
float_power_st :: Float -> Tensor -> Tensor
float_power_ts :: Tensor -> Float -> Tensor
alias :: Tensor -> Tensor
bucketize_ttbb :: Tensor -> Tensor -> Bool -> Bool -> Tensor
bucketize_stbb :: Float -> Tensor -> Bool -> Bool -> Tensor
searchsorted_ttbb :: Tensor -> Tensor -> Bool -> Bool -> Tensor
searchsorted_tsbb :: Tensor -> Float -> Bool -> Bool -> Tensor
mse_loss :: Tensor -> Tensor -> Int -> Tensor
l1_loss :: Tensor -> Tensor -> Int -> Tensor
multi_margin_loss :: Tensor -> Tensor -> Float -> Float -> Tensor -> Int -> Tensor
multilabel_margin_loss :: Tensor -> Tensor -> Int -> Tensor
multilabel_margin_loss_forward :: Tensor -> Tensor -> Int -> (Tensor, Tensor)
nll_loss_nd :: Tensor -> Tensor -> Tensor -> Int -> Int -> Tensor
nll_loss :: Tensor -> Tensor -> Tensor -> Int -> Int -> Tensor
nll_loss_forward :: Tensor -> Tensor -> Tensor -> Int -> Int -> (Tensor, Tensor)
nll_loss2d :: Tensor -> Tensor -> Tensor -> Int -> Int -> Tensor
nll_loss2d_forward :: Tensor -> Tensor -> Tensor -> Int -> Int -> (Tensor, Tensor)
smooth_l1_loss :: Tensor -> Tensor -> Int -> Double -> Tensor
huber_loss :: Tensor -> Tensor -> Int -> Double -> Tensor
soft_margin_loss :: Tensor -> Tensor -> Int -> Tensor
elu :: Tensor -> Float -> Float -> Float -> Tensor
glu :: Tensor -> Int -> Tensor
hardsigmoid :: Tensor -> Tensor
hardtanh :: Tensor -> Float -> Float -> Tensor
hardswish :: Tensor -> Tensor
leaky_relu :: Tensor -> Float -> Tensor
log_sigmoid :: Tensor -> Tensor
log_sigmoid_forward :: Tensor -> (Tensor, Tensor)
softplus :: Tensor -> Float -> Float -> Tensor
softshrink :: Tensor -> Float -> Tensor
adaptive_avg_pool2d :: Tensor -> (Int, Int) -> Tensor
mkldnn_adaptive_avg_pool2d :: Tensor -> (Int, Int) -> Tensor
adaptive_avg_pool3d :: Tensor -> (Int, Int, Int) -> Tensor
adaptive_max_pool2d :: Tensor -> (Int, Int) -> (Tensor, Tensor)
adaptive_max_pool3d :: Tensor -> (Int, Int, Int) -> (Tensor, Tensor)
avg_pool2d :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Bool -> Bool -> Int -> Tensor
avg_pool3d :: Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Bool -> Bool -> Int -> Tensor
fractional_max_pool2d :: Tensor -> (Int, Int) -> (Int, Int) -> Tensor -> (Tensor, Tensor)
fractional_max_pool3d :: Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor -> (Tensor, Tensor)
max_pool2d_with_indices :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Bool -> (Tensor, Tensor)
max_pool3d_with_indices :: Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Bool -> (Tensor, Tensor)
max_unpool2d :: Tensor -> Tensor -> (Int, Int) -> Tensor
max_unpool3d :: Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor
reflection_pad1d :: Tensor -> (Int, Int) -> Tensor
reflection_pad2d :: Tensor -> (Int, Int, Int, Int) -> Tensor
replication_pad1d :: Tensor -> (Int, Int) -> Tensor
replication_pad2d :: Tensor -> (Int, Int, Int, Int) -> Tensor
replication_pad3d :: Tensor -> (Int, Int, Int, Int, Int, Int) -> Tensor
upsample_linear1d_tlba :: Tensor -> [Int] -> Bool -> [Double] -> Tensor
upsample_bilinear2d_tlba :: Tensor -> [Int] -> Bool -> [Double] -> Tensor
upsample_trilinear3d_tlba :: Tensor -> [Int] -> Bool -> [Double] -> Tensor
upsample_bicubic2d_tlba :: Tensor -> [Int] -> Bool -> [Double] -> Tensor
upsample_nearest1d_tla :: Tensor -> [Int] -> [Double] -> Tensor
upsample_nearest2d_tla :: Tensor -> [Int] -> [Double] -> Tensor
upsample_nearest3d_tla :: Tensor -> [Int] -> [Double] -> Tensor
upsample_bilinear2d_tlbdd :: Tensor -> (Int, Int) -> Bool -> Double -> Double -> Tensor
upsample_bicubic2d_tlbdd :: Tensor -> (Int, Int) -> Bool -> Double -> Double -> Tensor
upsample_trilinear3d_tlbddd :: Tensor -> (Int, Int, Int) -> Bool -> Double -> Double -> Double -> Tensor
upsample_nearest2d_tldd :: Tensor -> (Int, Int) -> Double -> Double -> Tensor
upsample_nearest3d_tlddd :: Tensor -> (Int, Int, Int) -> Double -> Double -> Double -> Tensor
slow_conv_transpose2d :: Tensor -> Tensor -> (Int, Int) -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Tensor
slow_conv_transpose3d :: Tensor -> Tensor -> (Int, Int, Int) -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor
thnn_conv2d :: Tensor -> Tensor -> (Int, Int) -> Tensor -> (Int, Int) -> (Int, Int) -> Tensor
thnn_conv2d_forward :: Tensor -> Tensor -> (Int, Int) -> Tensor -> (Int, Int) -> (Int, Int) -> (Tensor, Tensor, Tensor)
thnn_conv_depthwise2d :: Tensor -> Tensor -> (Int, Int) -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Tensor
thnn_conv_depthwise2d_forward :: Tensor -> Tensor -> (Int, Int) -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Tensor
conv_depthwise3d :: Tensor -> Tensor -> (Int, Int, Int) -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor
slow_conv3d :: Tensor -> Tensor -> (Int, Int, Int) -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor
slow_conv3d_forward :: Tensor -> Tensor -> (Int, Int, Int) -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Tensor, Tensor, Tensor)
slow_conv_dilated2d :: Tensor -> Tensor -> (Int, Int) -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Tensor
slow_conv_dilated3d :: Tensor -> Tensor -> (Int, Int, Int) -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor
col2im :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Tensor
column_stack :: [Tensor] -> Tensor
im2col :: Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Tensor
isfinite :: Tensor -> Tensor
isinf :: Tensor -> Tensor
isposinf :: Tensor -> Tensor
isneginf :: Tensor -> Tensor
special_entr :: Tensor -> Tensor
special_expm1 :: Tensor -> Tensor
special_exp2 :: Tensor -> Tensor
special_gammaln :: Tensor -> Tensor
special_erf :: Tensor -> Tensor
special_erfc :: Tensor -> Tensor
special_erfinv :: Tensor -> Tensor
special_xlog1py_tt :: Tensor -> Tensor -> Tensor
special_xlog1py_st :: Float -> Tensor -> Tensor
special_xlog1py_ts :: Tensor -> Float -> Tensor
special_i0e :: Tensor -> Tensor
special_logit :: Tensor -> Double -> Tensor
special_expit :: Tensor -> Tensor
fft_fft :: Tensor -> Int -> Int -> String -> Tensor
fft_ifft :: Tensor -> Int -> Int -> String -> Tensor
fft_rfft :: Tensor -> Int -> Int -> String -> Tensor
fft_irfft :: Tensor -> Int -> Int -> String -> Tensor
fft_hfft :: Tensor -> Int -> Int -> String -> Tensor
fft_ihfft :: Tensor -> Int -> Int -> String -> Tensor
fft_fft2 :: Tensor -> Int -> Int -> String -> Tensor
fft_ifft2 :: Tensor -> Int -> Int -> String -> Tensor
fft_rfft2 :: Tensor -> Int -> Int -> String -> Tensor
fft_irfft2 :: Tensor -> Int -> Int -> String -> Tensor
fft_fftn :: Tensor -> Int -> Int -> String -> Tensor
fft_ifftn :: Tensor -> Int -> Int -> String -> Tensor
fft_rfftn :: Tensor -> Int -> Int -> String -> Tensor
fft_irfftn :: Tensor -> Int -> Int -> String -> Tensor
fft_fftshift :: Tensor -> Int -> Tensor
fft_ifftshift :: Tensor -> Int -> Tensor
linalg_cholesky_ex :: Tensor -> Bool -> (Tensor, Tensor)
linalg_cholesky :: Tensor -> Tensor
linalg_det :: Tensor -> Tensor
det :: Tensor -> Tensor
linalg_lstsq :: Tensor -> Tensor -> Double -> String -> (Tensor, Tensor, Tensor, Tensor)
linalg_slogdet :: Tensor -> (Tensor, Tensor)
linalg_eig :: Tensor -> (Tensor, Tensor)
linalg_eigvals :: Tensor -> Tensor
linalg_eigh :: Tensor -> String -> (Tensor, Tensor)
linalg_eigvalsh :: Tensor -> String -> Tensor
linalg_householder_product :: Tensor -> Tensor -> Tensor
linalg_inv_ex :: Tensor -> Bool -> (Tensor, Tensor)
linalg_inv :: Tensor -> Tensor
inner :: Tensor -> Tensor -> Tensor
outer :: Tensor -> Tensor -> Tensor
ger :: Tensor -> Tensor -> Tensor
linalg_vector_norm :: Tensor -> Float -> Int -> Bool -> DType -> Tensor
linalg_svd :: Tensor -> Bool -> (Tensor, Tensor, Tensor)
linalg_svdvals :: Tensor -> Tensor
linalg_pinv_tdb :: Tensor -> Double -> Bool -> Tensor
linalg_pinv_ttb :: Tensor -> Tensor -> Bool -> Tensor
linalg_solve :: Tensor -> Tensor -> Tensor
linalg_tensorinv :: Tensor -> Int -> Tensor
linalg_tensorsolve :: Tensor -> Tensor -> [Int] -> Tensor
linalg_qr :: Tensor -> String -> (Tensor, Tensor)
linalg_matrix_power :: Tensor -> Int -> Tensor
linalg_matrix_rank_tdb :: Tensor -> Double -> Bool -> Tensor
linalg_matrix_rank_ttb :: Tensor -> Tensor -> Bool -> Tensor
linalg_multi_dot :: [Tensor] -> Tensor
segment_reduce :: Tensor -> String -> Tensor -> Tensor -> Int -> Bool -> Float -> Tensor
pad_sequence :: [Tensor] -> Bool -> Double -> Tensor
flatten_dense_tensors :: [Tensor] -> Tensor
unflatten_dense_tensors :: Tensor -> [Tensor] -> [Tensor]

module Torch.Functional
newtype Diag
Diag :: Int -> Diag
data CeilMode
Ceil :: CeilMode
Floor :: CeilMode
data KeepDim
KeepDim :: KeepDim
RemoveDim :: KeepDim
newtype Dim
Dim :: Int -> Dim
data Reduction
ReduceNone :: Reduction
ReduceMean :: Reduction
ReduceSum :: Reduction
data Tri
Upper :: Tri
Lower :: Tri
kOne :: ForeignPtr Scalar
isUpper :: Tri -> Bool

-- | Returns the mean value of all elements in the input tensor.
mean :: Tensor -> Tensor

-- | Returns the standard deviation of all elements in the input tensor.
std :: Tensor -> Tensor

-- | Returns the variance of all elements in the input tensor.
var :: Tensor -> Tensor

-- | Returns the sum of all elements in the input tensor.
sumAll :: Tensor -> Tensor

-- | Computes the element-wise absolute value of the given input tensor.
abs :: Tensor -> Tensor

-- | Computes the fractional portion of each element in input. out_i =
--   input_i - (floor . abs) input_i * (sign input_i)
frac :: Tensor -> Tensor
keepdim :: KeepDim -> Bool

-- | Returns the indices of the maximum value of all elements in the input
--   tensor.
argmax :: Dim -> KeepDim -> Tensor -> Tensor

-- | Each element of the tensor other added to each element of the tensor
--   input. The resulting tensor is returned.
add :: Tensor -> Tensor -> Tensor

-- | Multiplies each element of the tensor other to each element of the
--   input tensor and returns a new resulting tensor.
mul :: Tensor -> Tensor -> Tensor

-- | Element wise subtraction of other tensor from input tensor and returns
--   a new resulting tensor
sub :: Tensor -> Tensor -> Tensor

-- | Element wise division of input tensor by other tensor and returns a
--   new resulting tensor
div :: Tensor -> Tensor -> Tensor

-- | ceil
ceil :: Tensor -> Tensor

-- | floor
floor :: Tensor -> Tensor

-- | min
min :: Tensor -> Tensor

-- | max
max :: Tensor -> Tensor

-- | median
median :: Tensor -> Tensor

-- | Adds each element of the input input with the scalar and returns a new
--   resulting tensor.
addScalar :: Scalar a => a -> Tensor -> Tensor

-- | Subtracts each element of the input input with the scalar and returns
--   a new resulting tensor.
subScalar :: Scalar a => a -> Tensor -> Tensor

-- | Multiplies each element of the input input with the scalar and returns
--   a new resulting tensor.
mulScalar :: Scalar a => a -> Tensor -> Tensor

-- | Divides each element of the input input with the scalar and returns a
--   new resulting tensor.
divScalar :: Scalar a => a -> Tensor -> Tensor

-- | Matrix product of two tensors.
--   
--   The behavior depends on the dimensionality of the tensors as follows:
--   
--   If both tensors are 1-dimensional, the dot product (scalar) is
--   returned. If both arguments are 2-dimensional, the matrix-matrix
--   product is returned. If the first argument is 1-dimensional and the
--   second argument is 2-dimensional, a 1 is prepended to its dimension
--   for the purpose of the matrix multiply. After the matrix multiply, the
--   prepended dimension is removed. If the first argument is 2-dimensional
--   and the second argument is 1-dimensional, the matrix-vector product is
--   returned. If both arguments are at least 1-dimensional and at least
--   one argument is N-dimensional (where N &gt; 2), then a batched matrix
--   multiply is returned. If the first argument is 1-dimensional, a 1 is
--   prepended to its dimension for the purpose of the batched matrix
--   multiply and removed after. If the second argument is 1-dimensional, a
--   1 is appended to its dimension for the purpose of the batched matrix
--   multiple and removed after. The non-matrix (i.e. batch) dimensions are
--   broadcasted (and thus must be broadcastable). For example, if input is
--   a (j times 1 times n times m)(j×1×n×m) tensor and other is a (k times
--   m times p)(k×m×p) tensor, out will be an (j times k times n times
--   p)(j×k×n×p) tensor.
matmul :: Tensor -> Tensor -> Tensor

-- | A simple lookup table that looks up embeddings in a fixed dictionary
--   and size. This module is often used to retrieve word embeddings using
--   indices. The input to the module is a list of indices, and the
--   embedding matrix, and the output is the corresponding word embeddings.
embedding :: Bool -> Bool -> Tensor -> Int -> Tensor -> Tensor
embedding' :: Tensor -> Tensor -> Tensor

-- | A one hot encoding of the given input. The encoding is based on the
--   given number of classes.
oneHot :: Int -> Tensor -> Tensor

-- | Computes the error function of each element
erf :: Tensor -> Tensor

-- | Computes the complementary error function of each element of input
erfc :: Tensor -> Tensor

-- | Computes the inverse error function of each element of input. The
--   inverse error function is defined in the range (-1, 1)(−1,1) as:
--   erfinv(erf(x)) = x
erfinv :: Tensor -> Tensor

-- | Computes the logarithm of the gamma function on input.
lgamma :: Tensor -> Tensor

-- | Computes the logarithmic derivative of the gamma function on input.
digamma :: Tensor -> Tensor

-- | Computes the nth derivative of the digamma function on input. n geq
--   0n≥0 is called the order of the polygamma function.
polygamma :: Int -> Tensor -> Tensor

-- | Computes the multivariate log-gamma function with dimension pp
--   element-wise. All elements must be greater than (p-1)/2, otherwise an
--   error would be thrown.
mvlgamma :: Int -> Tensor -> Tensor

-- | Returns a new tensor with the exponential of the elements of the input
--   tensor input.
exp :: Tensor -> Tensor

-- | Returns a new tensor with the natural logarithm of (1 + input).
log1p :: Tensor -> Tensor

-- | Returns a new tensor with the logarithm to the base 2 of the elements
--   of input.
log2 :: Tensor -> Tensor

-- | Returns a new tensor with the natural logarithm of the elements of
--   input.
log :: Tensor -> Tensor

-- | Returns a new tensor with the logarithm to the base 10 of the elements
--   of input.
log10 :: Tensor -> Tensor

-- | Takes the power of each element in input with exponent and returns a
--   tensor with the result.
pow :: Scalar a => a -> Tensor -> Tensor

-- | Takes the power of each element in input with exponent and returns a
--   tensor with the result. Exponent is a tensor with the same number of
--   elements as input.
powt :: Tensor -> Tensor -> Tensor

-- | Applies the rectified linear unit function element-wise.
relu :: Tensor -> Tensor

-- | Applies Exponential linear unit function element-wise, with alpha
--   input, &lt;math&gt;
elu :: Scalar s => s -> Tensor -> Tensor

-- | Applies exponential linear unit function element wise with default
--   alpha value = 1
elu' :: Tensor -> Tensor

-- | Applies element-wise, &lt;math&gt; , with
--   α=1.6732632423543772848170429916717 and
--   scale=1.0507009873554804934193349852946.
selu :: Tensor -> Tensor

-- | Applies element-wise, &lt;math&gt;.
celu :: Float -> Tensor -> Tensor

-- | Applies the element-wise function sigmoid.
sigmoid :: Tensor -> Tensor

-- | Applies a softmax function. It is applied to all slices along dim, and
--   will re-scale them so that the elements lie in the range [0, 1] and
--   sum to 1.
softmax :: Dim -> Tensor -> Tensor

-- | Applies a softmax followed by a logarithm. While mathematically
--   equivalent to log(softmax(x)), doing these two operations separately
--   is slower, and numerically unstable. This function uses an alternative
--   formulation to compute the output and gradient correctly.
logSoftmax :: Dim -> Tensor -> Tensor

-- | Thresholds each element of the input Tensor.
threshold :: Float -> Float -> Tensor -> Tensor

-- | Returns a new tensor with the sine of the elements of input.
sin :: Tensor -> Tensor

-- | Returns a new tensor with the cos of the elements of input.
cos :: Tensor -> Tensor

-- | Returns a new tensor with the tangent of the elements of input.
tan :: Tensor -> Tensor

-- | Returns a new tensor with the hyperbolic sine of the elements of
--   input.
sinh :: Tensor -> Tensor

-- | Returns a new tensor with the hyperbolic cosine of the elements of
--   input.
cosh :: Tensor -> Tensor

-- | Returns a new tensor with the hyperbolic tangent of the elements of
--   input.
tanh :: Tensor -> Tensor

-- | Returns a new tensor with the square-root of the elements of input.
sqrt :: Tensor -> Tensor

-- | Computes input &gt; other element-wise. The second argument can be a
--   number or a tensor whose shape is broadcastable with the first
--   argument.
gt :: Tensor -> Tensor -> Tensor
(>.) :: Tensor -> Tensor -> Tensor

-- | Computes input &lt; other element-wise. The second argument can be a
--   number or a tensor whose shape is broadcastable with the first
--   argument.
lt :: Tensor -> Tensor -> Tensor
(<.) :: Tensor -> Tensor -> Tensor

-- | Computes input &gt;= other element-wise. The second argument can be a
--   number or a tensor whose shape is broadcastable with the first
--   argument.
ge :: Tensor -> Tensor -> Tensor
(>=.) :: Tensor -> Tensor -> Tensor

-- | Computes input &lt;= other element-wise. The second argument can be a
--   number or a tensor whose shape is broadcastable with the first
--   argument.
le :: Tensor -> Tensor -> Tensor
(<=.) :: Tensor -> Tensor -> Tensor

-- | Computes input == other element-wise. The second argument can be a
--   number or a tensor whose shape is broadcastable with the first
--   argument.
eq :: Tensor -> Tensor -> Tensor
(==.) :: Tensor -> Tensor -> Tensor

-- | Returns a new tensor with the elements of input at the given indices.
--   The input tensor is treated as if it were viewed as a 1-D tensor. The
--   result takes the same shape as the indices.
take :: Tensor -> Tensor -> Tensor

-- | Returns a new 1-D tensor which indexes the input tensor according to
--   the boolean mask mask which is a BoolTensor. The shapes of the mask
--   tensor and the input tensor don’t need to match, but they must be
--   broadcastable.
maskedSelect :: Tensor -> Tensor -> Tensor

-- | Returns a tuple of 1-D tensors, one for each dimension in input, each
--   containing the indices (in that dimension) of all non-zero elements of
--   input .
nonzero :: Tensor -> Tensor
isclose :: Double -> Double -> Bool -> Tensor -> Tensor -> Tensor
isnan :: Tensor -> Tensor
isNonzero :: Tensor -> Bool
isSameSize :: Tensor -> Tensor -> Bool
isSigned :: Tensor -> Bool

-- | Computes input /= other element-wise. The second argument can be a
--   number or a tensor whose shape is broadcastable with the first
--   argument.
ne :: Tensor -> Tensor -> Tensor
(/=.) :: Tensor -> Tensor -> Tensor

-- | Casting to given <tt>Dtype</tt>, where <tt>Dtype</tt> is an object
--   that represents the data type of a tensor in hasktorch.
toDType :: DType -> Tensor -> Tensor

-- | squeezeAll
squeezeAll :: Tensor -> Tensor

-- | squeezeDim
squeezeDim :: Int -> Tensor -> Tensor

-- | Returns a tuple (values, indices) where values is the cumulative
--   maximum of elements of input in the dimension dim. And indices is the
--   index location of each maximum value found in the dimension dim.
cummax :: Int -> Tensor -> (Tensor, Tensor)

-- | Returns a tuple (values, indices) where values is the cumulative
--   minimum of elements of input in the dimension dim. And indices is the
--   index location of each maximum value found in the dimension dim.
cummin :: Int -> Tensor -> (Tensor, Tensor)

-- | Returns the cumulative product of elements of input in the dimension
--   dim. For example, if input is a vector of size N, the result will also
--   be a vector of size N, with elements.
cumprod :: Int -> DType -> Tensor -> Tensor

-- | Returns the cumulative sum of elements of input in the dimension dim.
--   For example, if input is a vector of size N, the result will also be a
--   vector of size N, with elements.
cumsum :: Int -> DType -> Tensor -> Tensor

-- | Function that measures the Binary Cross Entropy between the target and
--   the output.
binaryCrossEntropyLoss :: Reduction -> Tensor -> Tensor -> Tensor -> Tensor

-- | Binary Cross Entropy with weights defaulted to 1.0 &amp; reduction
--   defaulted to ReduceMean
binaryCrossEntropyLoss' :: Tensor -> Tensor -> Tensor

-- | This loss combines a Sigmoid layer and the BCELoss in one single
--   class. This version is more numerically stable than using a plain
--   Sigmoid followed by a BCELoss as, by combining the operations into one
--   layer, we take advantage of the log-sum-exp trick for numerical
--   stability.
binaryCrossEntropyWithLogits :: Reduction -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor

-- | Creates a criterion that measures the mean squared error (squared L2
--   norm) between each element in the <tt>input</tt> and <tt>target</tt>.
mseLoss :: Tensor -> Tensor -> Tensor

-- | The negative log likelihood loss.
nllLoss' :: Tensor -> Tensor -> Tensor

-- | Returns cosine similarity between x1 and x2, computed along dim.
cosineSimilarity :: Dim -> Double -> Tensor -> Tensor -> Tensor

-- | Returns cosine similarity with defaulted options.
cosineSimilarity' :: Tensor -> Tensor -> Tensor

-- | The Connectionist Temporal Classification loss. Calculates loss
--   between a continuous (unsegmented) time series and a target sequence.
--   CTCLoss sums over the probability of possible alignments of input to
--   target, producing a loss value which is differentiable with respect to
--   each input node. The alignment of input to target is assumed to be
--   “many-to-one”, which limits the length of the target sequence such
--   that it must be leq≤ the input length.
ctcLoss :: Bool -> Int -> Reduction -> [Int] -> [Int] -> Tensor -> Tensor -> Tensor

-- | Returns CTC loss with defaulted options.
ctcLoss' :: Reduction -> [Int] -> [Int] -> Tensor -> Tensor -> Tensor

-- | Returns the p-norm of (input - other) The shapes of input and other
--   must be broadcastable.
dist :: Float -> Tensor -> Tensor -> Tensor

-- | Measures the loss given an input tensor xx and a labels tensor yy
--   (containing 1 or -1). This is usually used for measuring whether two
--   inputs are similar or dissimilar, e.g. using the L1 pairwise distance
--   as xx, and is typically used for learning nonlinear embeddings or
--   semi-supervised learning.
hingeEmbeddingLoss :: Double -> Reduction -> Tensor -> Tensor -> Tensor
marginRankingLoss :: Tensor -> Tensor -> Tensor -> Double -> Reduction -> Tensor

-- | The 2D negative log likelihood loss
nllLoss2D :: Reduction -> Int -> Tensor -> Tensor -> Tensor -> Tensor

-- | Creates a criterion that optimizes a multi-class classification hinge
--   loss (margin-based loss) between input &lt;math&gt; (a 2D mini-batch
--   Tensor) and output &lt;math&gt; (which is a 1D tensor of target class
--   indices)
multiMarginLoss :: Reduction -> Float -> Float -> Tensor -> Tensor -> Tensor -> Tensor

-- | Creates a criterion that optimizes a multi-label one-versus-all loss
--   based on max-entropy, between input &lt;math&gt; and target
--   &lt;math&gt; of size &lt;math&gt; .
multiLabelMarginLoss :: Reduction -> Tensor -> Tensor -> Tensor

-- | The Kullback-Leibler divergence Loss KL divergence is a useful
--   distance measure for continuous distributions and is often useful when
--   performing direct regression over the space of (discretely sampled)
--   continuous output distributions. As with NLLLoss, the input given is
--   expected to contain log-probabilities and is not restricted to a 2D
--   Tensor. The targets are interpreted as probabilities by default, but
--   could be considered as log-probabilities with log_target set to True.
--   This criterion expects a target Tensor of the same size as the input
--   Tensor.
klDiv :: Reduction -> Tensor -> Tensor -> Tensor

-- | Creates a criterion that uses a squared term if the absolute
--   element-wise error falls below 1 and an L1 term otherwise. It is less
--   sensitive to outliers than the MSELoss and in some cases prevents
--   exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick). Also
--   known as the Huber loss.
smoothL1Loss :: Reduction -> Tensor -> Tensor -> Tensor

-- | Creates a criterion that optimizes a two-class classification logistic
--   loss between input tensor &lt;math&gt; and target tensor &lt;math&gt;
--   (containing 1 or -1).
softMarginLoss :: Reduction -> Tensor -> Tensor -> Tensor

-- | Applies a 1D adaptive max pooling over an input signal composed of
--   several input planes.
adaptiveMaxPool1d :: Int -> Tensor -> (Tensor, Tensor)

-- | Applies a 2D adaptive max pooling over an input signal composed of
--   several input planes.
adaptiveMaxPool2d :: (Int, Int) -> Tensor -> (Tensor, Tensor)

-- | Applies a 3D adaptive max pooling over an input signal composed of
--   several input planes
adaptiveMaxPool3d :: (Int, Int) -> Tensor -> (Tensor, Tensor)

-- | maxPool1dWithIndices
maxPool1dWithIndices :: Int -> Int -> Int -> Int -> CeilMode -> Tensor -> (Tensor, Tensor)

-- | Applies a 1D max pooling over an input signal composed of several
--   input planes.
maxPool1d :: Int -> Int -> Int -> Int -> CeilMode -> Tensor -> Tensor

-- | Applies a 2D max pooling over an input signal composed of several
--   input planes.
maxPool2d :: (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> CeilMode -> Tensor -> Tensor

-- | Applies a 3D max pooling over an input signal composed of several
--   input planes.
maxPool3d :: (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> CeilMode -> Tensor -> Tensor

-- | Calculates resulting dimensions from a 2d maxpool operation see
--   <a>https://pytorch.org/docs/master/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d</a>
maxPool2dDim :: (Int, Int) -> (Int, Int) -> (Int, Int) -> (Int, Int) -> CeilMode -> (Int, Int) -> (Int, Int)

-- | Applies a 1D average pooling over an input signal composed of several
--   input planes.
avgPool1d :: Int -> Int -> Int -> CeilMode -> Bool -> Tensor -> Tensor
avgPool1d' :: Int -> Int -> Int -> Tensor -> Tensor

-- | Applies a 1D adaptive average pooling over an input signal composed of
--   several input planes.
adaptiveAvgPool1d :: Int -> Tensor -> Tensor

-- | Applies a 2D adaptive average pooling over an input signal composed of
--   several input planes.
adaptiveAvgPool2d :: (Int, Int) -> Tensor -> Tensor

-- | Applies a 3D adaptive average pooling over an input signal composed of
--   several input planes.
adaptiveAvgPool3d :: (Int, Int, Int) -> Tensor -> Tensor

-- | Takes the inverse of the square matrix input. <tt>input</tt> can be
--   batches of 2D square tensors, in which case this function would return
--   a tensor composed of individual inverses.
inverse :: Tensor -> Tensor

-- | Solves a system of equations with a triangular coefficient matrix AA
--   and multiple right-hand sides bb
triangularSolve :: Tensor -> Bool -> Bool -> Bool -> Tensor -> (Tensor, Tensor)

-- | This function returns eigenvalues and eigenvectors of a real symmetric
--   matrix input or a batch of real symmetric matrices, represented by a
--   namedtuple (eigenvalues, eigenvectors).
symeig :: Bool -> Tri -> Tensor -> (Tensor, Tensor)

-- | Computes the eigenvalues and eigenvectors of a real square matrix
eig :: Bool -> Tensor -> (Tensor, Tensor)

-- | This function returns a namedtuple (U, S, V) which is the singular
--   value decomposition of a input real matrix or batches of real matrices
--   input such that input = U * diag(S) * V^T
svd :: Bool -> Bool -> Tensor -> (Tensor, Tensor, Tensor)

-- | Computes the Cholesky decomposition of a symmetric positive-definite
--   matrix AA or for batches of symmetric positive-definite matrices.
cholesky :: Tri -> Tensor -> Tensor

-- | Solves a linear system of equations with a positive semidefinite
--   matrix to be inverted given its Cholesky factor matrix uu .
choleskySolve :: Tri -> Tensor -> Tensor -> Tensor

-- | This function returns the solution to the system of linear equations
--   represented by AX = BAX=B and the LU factorization of A, in order as a
--   namedtuple solution, LU. LU contains L and U factors for LU
--   factorization of A
solve :: Tensor -> Tensor -> (Tensor, Tensor)

-- | Solves a linear system of equations with a positive semidefinite
--   matrix to be inverted given its Cholesky factor matrix uu .
choleskyInverse :: Tri -> Tensor -> Tensor

-- | This is a low-level function for calling LAPACK directly. This
--   function returns a namedtuple (a, tau) as defined in LAPACK
--   documentation for geqrf.
geqrf :: Tensor -> (Tensor, Tensor)

-- | Computes the orthogonal matrix Q of a QR factorization, from the
--   <tt>(input, input2)</tt> tuple returned by <a>geqrf</a> function. This
--   directly calls the underlying LAPACK function <tt>?orgqr</tt>. See
--   LAPACK documentation for <tt>orgqr</tt> for further details.
orgqr :: Tensor -> Tensor -> Tensor

-- | Multiplies mat (given by input3) by the orthogonal Q matrix of the QR
--   factorization formed by torch.geqrf() that is represented by (a, tau)
--   (given by (input, input2)). This directly calls the underlying LAPACK
--   function ?ormqr. See LAPACK documentation for ormqr for further
--   details.
ormqr :: Tensor -> Tensor -> Bool -> Bool -> Tensor -> Tensor

-- | Returns the LU solve of the linear system Ax = bAx=b using the
--   partially pivoted LU factorization of A from torch.lu().
luSolve :: Tensor -> Tensor -> Tensor -> Tensor

-- | During training, randomly zeroes some of the elements of the input
--   tensor with probability p using samples from a Bernoulli distribution.
dropout :: Double -> Bool -> Tensor -> IO Tensor
featureDropout :: Double -> Bool -> Tensor -> IO Tensor

-- | Applies alpha dropout to the input.
alphaDropout :: Double -> Bool -> Tensor -> IO Tensor
featureAlphaDropout :: Double -> Bool -> Tensor -> IO Tensor

-- | Computes the bitwise NOT of the given input tensor. The input tensor
--   must be of integral or Boolean types. For bool tensors, it computes
--   the logical NOT.
bitwiseNot :: Tensor -> Tensor

-- | Computes the element-wise logical NOT of the given input tensor. If
--   not specified, the output tensor will have the bool dtype. If the
--   input tensor is not a bool tensor, zeros are treated as False and
--   non-zeros are treated as True.
logicalNot :: Tensor -> Tensor
logicalXor :: Tensor -> Tensor -> Tensor
logicalAnd :: Tensor -> Tensor -> Tensor
logicalOr :: Tensor -> Tensor -> Tensor

-- | Concatenates the given sequence of seq tensors in the given dimension.
--   All tensors must either have the same shape (except in the
--   concatenating dimension) or be empty.
cat :: Dim -> [Tensor] -> Tensor
index :: [Tensor] -> Tensor -> Tensor
indexCopy :: Int -> Tensor -> Tensor -> Tensor -> Tensor
indexCopyWithDimname :: Dimname -> Tensor -> Tensor -> Tensor -> Tensor

-- | Puts values from the tensor value into the input tensor (out-of-place)
--   using the indices specified in indices (which is a tuple of Tensors).
--   The expression tensor.index_put_(indices, value) is equivalent to
--   tensor[indices] = value. If accumulate is True, the elements in value
--   are added to self. If accumulate is False, the behavior is undefined
--   if indices contain duplicate elements.
indexPut :: Bool -> [Tensor] -> Tensor -> Tensor -> Tensor

-- | Splits a tensor into a specific number of chunks. Last chunk will be
--   smaller if the tensor size along the given dimension dim is not
--   divisible by chunks.
chunk :: Int -> Dim -> Tensor -> [Tensor]

-- | Clamp all elements in input into the range [ min, max ] and return a
--   resulting tensor.
clamp :: Float -> Float -> Tensor -> Tensor

-- | Clamps all elements in input to be smaller or equal max.
clampMax :: Float -> Tensor -> Tensor

-- | Clamps all elements in input to be larger or equal min.
clampMin :: Float -> Tensor -> Tensor
cudnnIsAcceptable :: Tensor -> Bool

-- | Pads the input tensor boundaries with a constant value.
constantPadNd1d :: [Int] -> Float -> Tensor -> Tensor

-- | Applies a 1D convolution over an input signal composed of several
--   input planes.
conv1d :: Tensor -> Tensor -> Int -> Int -> Int -> Int -> Tensor -> Tensor
conv1d' :: Tensor -> Tensor -> Int -> Int -> Tensor -> Tensor

-- | Applies a 2D convolution over an input signal composed of several
--   input planes.
conv2d :: Tensor -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Int -> Tensor -> Tensor
conv2d' :: Tensor -> Tensor -> (Int, Int) -> (Int, Int) -> Tensor -> Tensor

-- | Applies a 3D convolution over an input signal composed of several
--   input planes.
conv3d :: Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Int -> Tensor -> Tensor
conv3d' :: Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor -> Tensor

-- | Applies a 1D transposed convolution over an input signal composed of
--   several input planes
convTranspose1d :: Tensor -> Tensor -> Int -> Int -> Int -> Int -> Tensor -> Tensor
convTranspose1d' :: Tensor -> Tensor -> Int -> Int -> Tensor -> Tensor

-- | Applies a 2D transposed convolution over an input signal composed of
--   several input planes
convTranspose2d :: Tensor -> Tensor -> (Int, Int) -> (Int, Int) -> (Int, Int) -> Int -> Tensor -> Tensor
convTranspose2d' :: Tensor -> Tensor -> (Int, Int) -> (Int, Int) -> Tensor -> Tensor

-- | Applies a 3D transposed convolution over an input signal composed of
--   several input planes
convTranspose3d :: Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> (Int, Int, Int) -> Int -> Tensor -> Tensor
convTranspose3d' :: Tensor -> Tensor -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor -> Tensor

-- | Returns a new tensor with the signs of the elements of <tt>input</tt>
sign :: Tensor -> Tensor

-- | Returns a tensor that is a transposed version of <tt>input</tt>. The
--   given dimensions <tt>dim0</tt> and <tt>dim1</tt> are swapped.
transpose :: Dim -> Dim -> Tensor -> Tensor

-- | transpose special case for a 2D tensor
transpose2D :: Tensor -> Tensor

-- | Returns a tensor with the elements of input as the diagonal. The
--   second argument controls which diagonal to consider: If Int = 0, it is
--   the main diagonal. If Int &gt; 0, it is above the main diagonal. If
--   Int &lt; 0, it is below the main diagonal.
diag :: Diag -> Tensor -> Tensor
diagEmbed :: Diag -> Dim -> Dim -> Tensor -> Tensor

-- | If input is a vector (1-D tensor), then returns a 2-D square tensor
--   with the elements of input as the diagonal. If input is a tensor with
--   more than one dimension, then returns a 2-D tensor with diagonal
--   elements equal to a flattened input. The argument offset controls
--   which diagonal to consider: If offset = 0, it is the main diagonal. If
--   offset &gt; 0, it is above the main diagonal. If offset &lt; 0, it is
--   below the main diagonal.
diagflat :: Diag -> Tensor -> Tensor

-- | Returns a partial view of input with the its diagonal elements with
--   respect to dim1 and dim2 appended as a dimension at the end of the
--   shape. Applying diagEmbed to the output of this function with the same
--   arguments yields a diagonal matrix with the diagonal entries of the
--   input. However, diagEmbed has different default dimensions, so those
--   need to be explicitly specified.
diagonal :: Diag -> Dim -> Dim -> Tensor -> Tensor

-- | Returns True if all elements in the tensor are True, False otherwise.
all :: Tensor -> Bool

-- | Returns True if any elements in the tensor are True, False otherwise.
any :: Tensor -> Bool

-- | Returns True if all elements in each row of the tensor in the given
--   dimension dim are True, False otherwise. If keepdim is True, the
--   output tensor is of the same size as input except in the dimension dim
--   where it is of size 1. Otherwise, dim is squeezed, resulting in the
--   output tensor having 1 fewer dimension than input.
allDim :: Dim -> Bool -> Tensor -> Tensor

-- | Returns True if any elements in each row of the tensor in the given
--   dimension dim are True, False otherwise. If keepdim is True, the
--   output tensor is of the same size as input except in the dimension dim
--   where it is of size 1. Otherwise, dim is squeezed, resulting in the
--   output tensor having 1 fewer dimension than input.
anyDim :: Dim -> Bool -> Tensor -> Tensor

-- | Permute the dimensions of this tensor.
permute :: [Int] -> Tensor -> Tensor

-- | expand TODO: figure out what the <tt>implicit</tt> boolean value does
expand :: Tensor -> Bool -> [Int] -> Tensor

-- | flatten
flatten :: Dim -> Dim -> Tensor -> Tensor

-- | flattenAll
flattenAll :: Tensor -> Tensor
lstm :: Tensor -> [Tensor] -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> (Tensor, Tensor, Tensor)
lstm' :: Tensor -> [Tensor] -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Tensor -> (Tensor, Tensor, Tensor)
gru :: Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> Tensor -> (Tensor, Tensor)
gru' :: Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Tensor -> (Tensor, Tensor)
rnnTanh :: Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> Tensor -> (Tensor, Tensor)
rnnTanh' :: Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Tensor -> (Tensor, Tensor)
rnnRelu :: Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> Bool -> Tensor -> (Tensor, Tensor)
rnnRelu' :: Tensor -> Tensor -> Tensor -> [Tensor] -> Bool -> Int -> Double -> Bool -> Bool -> (Tensor, Tensor)

-- | A long short-term memory (LSTM) cell.
lstmCell :: Tensor -> Tensor -> Tensor -> Tensor -> (Tensor, Tensor) -> Tensor -> (Tensor, Tensor)

-- | A gated recurrent unit (GRU) cell
gruCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor

-- | An Elman RNN cell with tanh non-linearity
rnnTanhCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor

-- | An Elman RNN cell with ReLU non-linearity
rnnReluCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor

-- | A quantized long short-term memory (LSTM) cell.
quantizedLstmCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> (Tensor, Tensor) -> Tensor -> (Tensor, Tensor)

-- | A quantized long gated recurrent unit (GRU) cell.
quantizedGruCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> Tensor -> Tensor -> Tensor

-- | A quantized Elman RNN cell with relu non-linearity
quantizedRnnReluCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> Tensor -> Tensor -> Tensor

-- | A quantized Elman RNN cell with tanh non-linearity
quantizedRnnTanhCell :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Float -> Float -> Float -> Float -> Tensor -> Tensor -> Tensor

-- | Applies the soft shrinkage function elementwise
softShrink :: Float -> Tensor -> Tensor

-- | Concatenates sequence of tensors along a new dimension. All tensors
--   need to be of the same size.
stack :: Dim -> [Tensor] -> Tensor

-- | Returns the sum of each row of the input tensor in the given dimension
--   dim. If keepdim is True, the output tensor is of the same size as
--   input except in the dimension(s) dim where it is of size 1. Otherwise,
--   dim is squeezed, resulting in the output tensor having 1 (or len(dim))
--   fewer dimension(s).
sumDim :: Dim -> KeepDim -> DType -> Tensor -> Tensor

-- | Returns the k largest elements of the given input tensor along a given
--   dimension. If largest is False then the k smallest elements are
--   returned. The boolean option sorted if True, will make sure that the
--   returned k elements are themselves sorted A tuple of (values, indices)
--   is returned, where the indices are the indices of the elements in the
--   original input tensor.
topK :: Int -> Dim -> Bool -> Bool -> Tensor -> (Tensor, Tensor)

-- | Returns the log of summed exponentials of each row of the input tensor
--   in the given dimension dim. The computation is numerically stabilized.
logsumexp :: Bool -> Int -> Tensor -> Tensor

-- | Returns the upper triangular part of a matrix (2-D tensor) or batch of
--   matrices input, the other elements of the result tensor out are set to
--   0. The upper triangular part of the matrix is defined as the elements
--   on and above the diagonal. The argument diagonal controls which
--   diagonal to consider. If diagonal = 0, all elements on and above the
--   main diagonal are retained. A positive value excludes just as many
--   diagonals above the main diagonal, and similarly a negative value
--   includes just as many diagonals below the main diagonal. The main
--   diagonal are the set of indices &lt;math&gt; for &lt;math&gt;
--   &lt;math&gt; where &lt;math&gt; and &lt;math&gt; are the dimensions of
--   the matrix.
triu :: Diag -> Tensor -> Tensor

-- | Returns the lower triangular part of the matrix (2-D tensor) or batch
--   of matrices input, the other elements of the result tensor out are set
--   to 0. The lower triangular part of the matrix is defined as the
--   elements on and below the diagonal. The argument diagonal controls
--   which diagonal to consider. If diagonal = 0, all elements on and below
--   the main diagonal are retained. A positive value includes just as many
--   diagonals above the main diagonal, and similarly a negative value
--   excludes just as many diagonals below the main diagonal. The main
--   diagonals are the set of indices &lt;math&gt; for &lt;math&gt;
--   &lt;math&gt; where &lt;math&gt; and &lt;math&gt; are the dimensions of
--   the matrix.
tril :: Diag -> Tensor -> Tensor

-- | Returns a new tensor with the truncated integer values of the elements
--   of input.
trunc :: Tensor -> Tensor

-- | Returns the unique elements of the input tensor along a dimension.
uniqueDim :: Int -> Bool -> Bool -> Bool -> Tensor -> (Tensor, Tensor, Tensor)

-- | Eliminates all but the first element from every consecutive group of
--   equivalent elements. This function is different from uniqueDim in the
--   sense that this function only eliminates consecutive duplicate values.
uniqueConsecutive :: Bool -> Bool -> Int -> Tensor -> (Tensor, Tensor, Tensor)

-- | Eliminates all but the first element from every consecutive group of
--   equivalent elements along a dimension. This function is different from
--   uniqueDim in the sense that this function only eliminates consecutive
--   duplicate values.
uniqueDimConsecutive :: Int -> Bool -> Bool -> Tensor -> (Tensor, Tensor, Tensor)

-- | Returns a new tensor with a dimension of size one inserted at the
--   specified position. The returned tensor shares the same underlying
--   data with this tensor. A dim value within the range [(dim input) - 1,
--   (dim input) + 1)] can be used. Negative dim will correspond to
--   unsqueeze applied at dim = dim + (dim input) + 1
unsqueeze :: Dim -> Tensor -> Tensor

-- | Upsamples the input, using bilinear upsampling. Expected inputs are
--   spatial (4 dimensional).
upsampleBilinear2d :: (Int, Int) -> Bool -> Tensor -> Tensor

-- | Applies a 2D nearest neighbor upsampling to an input signal composed
--   of several input channels.
upsampleNearest2d :: (Int, Int) -> Double -> Double -> Tensor -> Tensor

-- | Splits the tensor into chunks of given size if possible.
split :: Int -> Dim -> Tensor -> [Tensor]

-- | Creates a criterion that measures the mean absolute error (MAE)
--   between each element in the input &lt;math&gt; and target &lt;math&gt;
--   .
l1Loss :: Reduction -> Tensor -> Tensor -> Tensor

-- | Applies the element-wise function: &lt;math&gt;
leakyRelu :: Float -> Tensor -> Tensor

-- | Applies the element-wise function: &lt;math&gt;
logSigmoid :: Tensor -> Tensor

-- | Returns a namedtuple (values, indices) where values is the maximum
--   value of each row of the input tensor in the given dimension dim. And
--   indices is the index location of each maximum value found (argmax). If
--   keepdim is True, the output tensors are of the same size as input
--   except in the dimension dim where they are of size 1. Otherwise, dim
--   is squeezed , resulting in the output tensors having 1 fewer dimension
--   than input.
maxDim :: Dim -> KeepDim -> Tensor -> (Tensor, Tensor)

-- | Returns a namedtuple (values, indices) where values is the minimum
--   value of each row of the input tensor in the given dimension dim. And
--   indices is the index location of each minimum value found (argmin). If
--   keepdim is True, the output tensors are of the same size as input
--   except in the dimension dim where they are of size 1. Otherwise, dim
--   is squeezed, resulting in the output tensors having 1 fewer dimension
--   than input.
minDim :: Dim -> KeepDim -> Tensor -> (Tensor, Tensor)

-- | Returns the mean value of each row of the input tensor in the given
--   dimension dim. If dim is a list of dimensions, reduce over all of
--   them. If keepdim is True, the output tensor is of the same size as
--   input except in the dimension(s) dim where it is of size 1. Otherwise,
--   dim is squeezed (see torch.squeeze()), resulting in the output tensor
--   having 1 (or len(dim)) fewer dimension(s).
meanDim :: Dim -> KeepDim -> DType -> Tensor -> Tensor

-- | Returns a namedtuple (values, indices) where values is the median
--   value of each row of the input tensor in the given dimension dim. And
--   indices is the index location of each median value found. By default,
--   dim is the last dimension of the input tensor. If keepdim is True, the
--   output tensors are of the same size as input except in the dimension
--   dim where they are of size 1. Otherwise, dim is squeezed (see
--   torch.squeeze()), resulting in the outputs tensor having 1 fewer
--   dimension than input.
medianDim :: Dim -> KeepDim -> Tensor -> (Tensor, Tensor)

-- | Returns the matrix product of the NN 2-D tensors. This product is
--   efficiently computed using the matrix chain order algorithm which
--   selects the order in which incurs the lowest cost in terms of
--   arithmetic operations. Note that since this is a function to compute
--   the product, NN needs to be greater than or equal to 2; if equal to 2
--   then a trivial matrix-matrix product is returned. If NN is 1, then
--   this is a no-op - the original matrix is returned as is.
chainMatmul :: [Tensor] -> Tensor

-- | Applies element-wise the function &lt;math&gt; where &lt;math&gt; is
--   the Cumulative Distribution Function for Gaussian Distribution.
gelu :: Tensor -> Tensor

-- | The gated linear unit. Computes: &lt;math&gt; where input is split in
--   half along dim to form a and b, &lt;math&gt; is the sigmoid function
--   and &lt;math&gt; is the element-wise product between matrices.
glu :: Dim -> Tensor -> Tensor

-- | Returns the standard-deviation and mean of all elements in the input
--   tensor. If unbiased is False, then the standard-deviation will be
--   calculated via the biased estimator. Otherwise, Bessel’s correction
--   will be used.
stdMean :: Bool -> Tensor -> (Tensor, Tensor)

-- | Returns the standard-deviation and mean of each row of the input
--   tensor in the dimension dim. If dim is a list of dimensions, reduce
--   over all of them. If keepdim is True, the output tensor is of the same
--   size as input except in the dimension(s) dim where it is of size 1.
--   Otherwise, dim is squeezed, resulting in the output tensor having 1
--   (or len(dim)) fewer dimension(s). If unbiased is False, then the
--   standard-deviation will be calculated via the biased estimator.
--   Otherwise, Bessel’s correction will be used.
stdMeanDim :: Dim -> Bool -> KeepDim -> Tensor -> (Tensor, Tensor)

-- | Returns a copy of input. Output tensor keeps a computational graph and
--   a requires_grad value of input tensor.
--   <a>https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861/41</a>
clone :: Tensor -> IO Tensor

-- | Returns a copy of input. Output tensor does not keep a computational
--   graph and a requires_grad value of input tensor.
detach :: Tensor -> IO Tensor

-- | Returns a new tensor with the same data as the input tensor but of a
--   different shape.
view :: [Int] -> Tensor -> Tensor

-- | Repeats this tensor along the specified dimensions.
repeat :: [Int] -> Tensor -> Tensor
batchNorm :: Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> Tensor -> Tensor
instanceNorm :: Tensor -> Tensor -> Tensor -> Tensor -> Bool -> Double -> Double -> Tensor -> Tensor
acos :: Tensor -> Tensor
addmv :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
addr :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
allclose :: Tensor -> Tensor -> Double -> Double -> Bool -> Bool
argmin :: Tensor -> Int -> Bool -> Tensor
asin :: Tensor -> Tensor
atan :: Tensor -> Tensor
baddbmm :: Tensor -> Tensor -> Tensor -> Float -> Float -> Tensor
bmm :: Tensor -> Tensor -> Tensor
conj :: Tensor -> Tensor
det :: Tensor -> Tensor
dot :: Tensor -> Tensor -> Tensor
einsum :: String -> [Tensor] -> Tensor
expm1 :: Tensor -> Tensor
ger :: Tensor -> Tensor -> Tensor
logdet :: Tensor -> Tensor
lstsq :: Tensor -> Tensor -> (Tensor, Tensor)
mv :: Tensor -> Tensor -> Tensor
sumWithDimnames :: Tensor -> [Dimname] -> Bool -> DType -> Tensor
instance GHC.Show.Show Torch.Functional.Tri
instance GHC.Classes.Eq Torch.Functional.Tri
instance GHC.Show.Show Torch.Functional.Reduction
instance GHC.Classes.Eq Torch.Functional.Reduction
instance GHC.Show.Show Torch.Functional.KeepDim
instance GHC.Classes.Eq Torch.Functional.KeepDim
instance GHC.Show.Show Torch.Functional.CeilMode
instance GHC.Classes.Eq Torch.Functional.CeilMode
instance Torch.Internal.Class.Castable Torch.Functional.CeilMode Foreign.C.Types.CBool
instance Torch.Internal.Class.Castable Torch.Functional.Reduction GHC.Int.Int64
instance GHC.Num.Num Torch.Tensor.Tensor
instance GHC.Classes.Eq Torch.Tensor.Tensor
instance GHC.Real.Fractional Torch.Tensor.Tensor

module Torch.Initializers
data NonLinearity
Identity :: NonLinearity
Sigmoid :: NonLinearity
Tanh :: NonLinearity
Relu :: NonLinearity
LeakyRelu :: Float -> NonLinearity
data FanMode
FanIn :: FanMode
FanOut :: FanMode
newtype Shape
Shape :: [Int] -> Shape

-- | Gain scaling value for He initialization
calculateGain :: NonLinearity -> Float

-- | Fan-in / Fan-out scaling calculation
calculateFan :: [Int] -> (Int, Int)

-- | Xavier Initialization - Uniform
xavierUniform :: Float -> [Int] -> IO Tensor

-- | Xavier Initialization - Normal
xavierNormal :: Float -> [Int] -> IO Tensor

-- | Get fan in or fan out value depending on selected fan mode, used by
--   Kaiming
getter :: FanMode -> (Int, Int) -> Int

-- | Kaiming Initialization - Uniform
kaimingUniform :: FanMode -> NonLinearity -> [Int] -> IO Tensor

-- | Kaiming Initialization - Normal
kaimingNormal :: FanMode -> NonLinearity -> [Int] -> IO Tensor

-- | Handle weights + bias based on
--   <a>https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L79</a>
kaimingFC :: [Int] -> IO (Tensor, Tensor)
kaimingUniform' :: [Int] -> IO Tensor
kaimingNormal' :: [Int] -> IO Tensor
xavierUniform' :: [Int] -> IO Tensor
xavierNormal' :: [Int] -> IO Tensor

module Torch.Distributions.Constraints
type Constraint = Tensor -> Tensor
dependent :: Constraint
boolean :: Constraint
integerInterval :: Int -> Int -> Constraint
integerLessThan :: Int -> Constraint
integerGreaterThan :: Int -> Constraint
integerLessThanEq :: Int -> Constraint
integerGreaterThanEq :: Int -> Constraint
real :: Constraint
greaterThan :: Float -> Constraint
greaterThanEq :: Float -> Constraint
lessThan :: Float -> Constraint
lessThanEq :: Float -> Constraint
interval :: Float -> Float -> Constraint
halfOpenInterval :: Float -> Float -> Constraint
simplex :: Constraint
nonNegativeInteger :: Constraint
positiveInteger :: Constraint
positive :: Constraint
unitInterval :: Constraint

module Torch.Distributions.Distribution
data Scale
class Distribution a
batchShape :: Distribution a => a -> [Int]
eventShape :: Distribution a => a -> [Int]
expand :: Distribution a => a -> [Int] -> a
support :: Distribution a => a -> Constraint
mean :: Distribution a => a -> Tensor
variance :: Distribution a => a -> Tensor
sample :: Distribution a => a -> [Int] -> IO Tensor
logProb :: Distribution a => a -> Tensor -> Tensor
entropy :: Distribution a => a -> Tensor
enumerateSupport :: Distribution a => a -> Bool -> Tensor
stddev :: Distribution a => a -> Tensor
perplexity :: Distribution a => a -> Tensor

-- | Converts a tensor of logits into probabilities. Note that for the |
--   binary case, each value denotes log odds, whereas for the |
--   multi-dimensional case, the values along the last dimension denote |
--   the log probabilities (possibly unnormalized) of the events.
logitsToProbs :: Bool -> Tensor -> Tensor
clampProbs :: Tensor -> Tensor

-- | Converts a tensor of probabilities into logits. For the binary case, |
--   this denotes the probability of occurrence of the event indexed by
--   `1`. | For the multi-dimensional case, the values along the last
--   dimension | denote the probabilities of occurrence of each of the
--   events.
probsToLogits :: Bool -> Tensor -> Tensor

-- | Returns the size of the sample returned by the distribution, given | a
--   <tt>sampleShape</tt>. Note, that the batch and event shapes of a
--   distribution | instance are fixed at the time of construction. If this
--   is empty, the | returned shape is upcast to (1,). | Args: |
--   sampleShape (torch.Size): the size of the sample to be drawn.
extendedShape :: Distribution a => a -> [Int] -> [Int]

module Torch.Distributions.Categorical

-- | Creates a categorical distribution parameterized by either
--   :attr:<a>probs</a> or | :attr:<a>logits</a> (but not both). | ..
--   note:: | It is equivalent to the distribution that
--   :func:`torch.multinomial` | samples from. | Samples are integers from
--   :math:`{0, ldots, K-1}` where <tt>K</tt> is ``probs.size(-1)``. | If
--   :attr:<a>probs</a> is 1D with length-<tt>K</tt>, each element is the
--   relative | probability of sampling the class at that index. | If
--   :attr:<a>probs</a> is 2D, it is treated as a batch of relative
--   probability | vectors. | .. note:: :attr:<a>probs</a> must be
--   non-negative, finite and have a non-zero sum, | and it will be
--   normalized to sum to 1. | See also: `torch.multinomial` | Example:: |
--   &gt;&gt;&gt; m = Categorical.fromProbs $ D.asTensor [ 0.25, 0.25,
--   0.25, 0.25 ] | &gt;&gt;&gt; Distribution.sample m -- equal probability
--   of 0, 1, 2, 3 | tensor(3)
data Categorical
Categorical :: Tensor -> Tensor -> Categorical
[probs] :: Categorical -> Tensor
[logits] :: Categorical -> Tensor
fromProbs :: Tensor -> Categorical
fromLogits :: Tensor -> Categorical
instance GHC.Show.Show Torch.Distributions.Categorical.Categorical
instance Torch.Distributions.Distribution.Distribution Torch.Distributions.Categorical.Categorical

module Torch.Autograd

-- | Note: to create an <a>IndependentTensor</a> use
--   <a>makeIndependent</a>; | otherwise, Torch will complain the parameter
--   does not require a gradient.
newtype IndependentTensor
IndependentTensor :: Tensor -> IndependentTensor
[toDependent] :: IndependentTensor -> Tensor
grad :: Tensor -> [IndependentTensor] -> [Tensor]
requiresGrad :: Tensor -> Bool
makeIndependent :: Tensor -> IO IndependentTensor
makeIndependentWithRequiresGrad :: Tensor -> Bool -> IO IndependentTensor
instance GHC.Generics.Generic Torch.Autograd.IndependentTensor
instance GHC.Show.Show Torch.Autograd.IndependentTensor

module Torch.NN
type Parameter = IndependentTensor
type ParamStream a = State [Parameter] a
nextParameter :: ParamStream Parameter
class HasForward f a b | f a -> b
forward :: HasForward f a b => f -> a -> b
forward :: (HasForward f a b, Generic f, Generic a, Generic b, GHasForward (Rep f) (Rep a) (Rep b)) => f -> a -> b
forwardStoch :: HasForward f a b => f -> a -> IO b
forwardStoch :: (HasForward f a b, Generic f, Generic a, Generic b, GHasForward (Rep f) (Rep a) (Rep b)) => f -> a -> IO b
class GHasForward (f :: Type -> Type) (a :: Type -> Type) (b :: Type -> Type) | f a -> b
gForward :: forall c c' c''. GHasForward f a b => f c -> a c' -> b c''
gForwardStoch :: forall c c' c''. GHasForward f a b => f c -> a c' -> IO (b c)
class Parameterized f
flattenParameters :: Parameterized f => f -> [Parameter]
flattenParameters :: (Parameterized f, Generic f, GParameterized (Rep f)) => f -> [Parameter]
_replaceParameters :: Parameterized f => f -> ParamStream f
_replaceParameters :: (Parameterized f, Generic f, GParameterized (Rep f)) => f -> ParamStream f
replaceParameters :: Parameterized f => f -> [Parameter] -> f
class GParameterized f
gFlattenParameters :: forall a. GParameterized f => f a -> [Parameter]
_gReplaceParameters :: forall a. GParameterized f => f a -> ParamStream (f a)
class Randomizable spec f | spec -> f
sample :: Randomizable spec f => spec -> IO f
data LinearSpec
LinearSpec :: Int -> Int -> LinearSpec
[in_features] :: LinearSpec -> Int
[out_features] :: LinearSpec -> Int
data Linear
Linear :: Parameter -> Parameter -> Linear
[weight] :: Linear -> Parameter
[bias] :: Linear -> Parameter
linear :: Linear -> Tensor -> Tensor
linearForward :: Linear -> Tensor -> Tensor
data Conv1dSpec
Conv1dSpec :: Int -> Int -> Int -> Conv1dSpec
[inputChannelSize1d] :: Conv1dSpec -> Int
[outputChannelSize1d] :: Conv1dSpec -> Int
[kernelSize] :: Conv1dSpec -> Int
data Conv1d
Conv1d :: Parameter -> Parameter -> Conv1d
[conv1dWeight] :: Conv1d -> Parameter
[conv1dBias] :: Conv1d -> Parameter
conv1dForward :: Conv1d -> Int -> Int -> Tensor -> Tensor
data Conv2dSpec
Conv2dSpec :: Int -> Int -> Int -> Int -> Conv2dSpec
[inputChannelSize2d] :: Conv2dSpec -> Int
[outputChannelSize2d] :: Conv2dSpec -> Int
[kernelHeight2d] :: Conv2dSpec -> Int
[kernelWidth2d] :: Conv2dSpec -> Int
data Conv2d
Conv2d :: Parameter -> Parameter -> Conv2d
[conv2dWeight] :: Conv2d -> Parameter
[conv2dBias] :: Conv2d -> Parameter
conv2dForward :: Conv2d -> (Int, Int) -> (Int, Int) -> Tensor -> Tensor
data Conv3dSpec
Conv3dSpec :: Int -> Int -> Int -> Int -> Int -> Conv3dSpec
[inputChannelSize3d] :: Conv3dSpec -> Int
[outputChannelSize3d] :: Conv3dSpec -> Int
[kernelHeight3d] :: Conv3dSpec -> Int
[kernelWidth3d] :: Conv3dSpec -> Int
[kernelDepth3d] :: Conv3dSpec -> Int
data Conv3d
Conv3d :: Parameter -> Parameter -> Conv3d
[conv3dWeight] :: Conv3d -> Parameter
[conv3dBias] :: Conv3d -> Parameter
conv3dForward :: Conv3d -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor -> Tensor
data ConvTranspose1dSpec
ConvTranspose1dSpec :: Int -> Int -> Int -> ConvTranspose1dSpec
[trInputChannelSize1d] :: ConvTranspose1dSpec -> Int
[trOutputChannelSize1d] :: ConvTranspose1dSpec -> Int
[trKernelSize] :: ConvTranspose1dSpec -> Int
data ConvTranspose1d
ConvTranspose1d :: Parameter -> Parameter -> ConvTranspose1d
[convTranspose1dWeight] :: ConvTranspose1d -> Parameter
[convTranspose1dBias] :: ConvTranspose1d -> Parameter
convTranspose1dForward :: ConvTranspose1d -> Int -> Int -> Tensor -> Tensor
data ConvTranspose2dSpec
ConvTranspose2dSpec :: Int -> Int -> Int -> Int -> ConvTranspose2dSpec
[trInputChannelSize2d] :: ConvTranspose2dSpec -> Int
[trOutputChannelSize2d] :: ConvTranspose2dSpec -> Int
[trKernelHeight2d] :: ConvTranspose2dSpec -> Int
[trKernelWidth2d] :: ConvTranspose2dSpec -> Int
data ConvTranspose2d
ConvTranspose2d :: Parameter -> Parameter -> ConvTranspose2d
[convTranspose2dWeight] :: ConvTranspose2d -> Parameter
[convTranspose2dBias] :: ConvTranspose2d -> Parameter
convTranspose2dForward :: ConvTranspose2d -> (Int, Int) -> (Int, Int) -> Tensor -> Tensor
data ConvTranspose3dSpec
ConvTranspose3dSpec :: Int -> Int -> Int -> Int -> Int -> ConvTranspose3dSpec
[trInputChannelSize3d] :: ConvTranspose3dSpec -> Int
[trOutputChannelSize3d] :: ConvTranspose3dSpec -> Int
[trKernelHeight3d] :: ConvTranspose3dSpec -> Int
[trKernelWidth3d] :: ConvTranspose3dSpec -> Int
[trKernelDepth3d] :: ConvTranspose3dSpec -> Int
data ConvTranspose3d
ConvTranspose3d :: Parameter -> Parameter -> ConvTranspose3d
[convTranspose3dWeight] :: ConvTranspose3d -> Parameter
[convTranspose3dBias] :: ConvTranspose3d -> Parameter
convTranspose3dForward :: ConvTranspose3d -> (Int, Int, Int) -> (Int, Int, Int) -> Tensor -> Tensor
data BatchNormSpec
BatchNormSpec :: Int -> BatchNormSpec
[numFeatures] :: BatchNormSpec -> Int
data BatchNorm
BatchNorm :: Parameter -> Parameter -> Tensor -> Tensor -> BatchNorm
[batchNormWeight] :: BatchNorm -> Parameter
[batchNormBias] :: BatchNorm -> Parameter
[runningMean] :: BatchNorm -> Tensor
[runningVar] :: BatchNorm -> Tensor
batchNormForward :: BatchNorm -> Bool -> Double -> Double -> Tensor -> Tensor
data InstanceNormSpec
InstanceNormSpec :: Int -> InstanceNormSpec
[iNumFeatures] :: InstanceNormSpec -> Int
data InstanceNorm
InstanceNorm :: Parameter -> Parameter -> Tensor -> Tensor -> InstanceNorm
[instanceNormWeight] :: InstanceNorm -> Parameter
[instanceNormBias] :: InstanceNorm -> Parameter
[iRunningMean] :: InstanceNorm -> Tensor
[iRunningVar] :: InstanceNorm -> Tensor
instanceNormForward :: InstanceNorm -> Bool -> Double -> Double -> Tensor -> Tensor
data UpSampleSpec
UpSampleSpec :: Int -> Int -> UpSampleSpec
[upsampleInputFilters] :: UpSampleSpec -> Int
[upsampleStride] :: UpSampleSpec -> Int
data UpSample
UpSample :: UpSampleSpec -> UpSample
[upsampleSpec] :: UpSample -> UpSampleSpec
instance GHC.Classes.Eq Torch.NN.LinearSpec
instance GHC.Show.Show Torch.NN.LinearSpec
instance Torch.NN.Parameterized Torch.NN.Linear
instance GHC.Generics.Generic Torch.NN.Linear
instance GHC.Show.Show Torch.NN.Linear
instance GHC.Classes.Eq Torch.NN.Conv1dSpec
instance GHC.Show.Show Torch.NN.Conv1dSpec
instance Torch.NN.Parameterized Torch.NN.Conv1d
instance GHC.Generics.Generic Torch.NN.Conv1d
instance GHC.Show.Show Torch.NN.Conv1d
instance GHC.Classes.Eq Torch.NN.Conv2dSpec
instance GHC.Show.Show Torch.NN.Conv2dSpec
instance Torch.NN.Parameterized Torch.NN.Conv2d
instance GHC.Generics.Generic Torch.NN.Conv2d
instance GHC.Show.Show Torch.NN.Conv2d
instance GHC.Classes.Eq Torch.NN.Conv3dSpec
instance GHC.Show.Show Torch.NN.Conv3dSpec
instance Torch.NN.Parameterized Torch.NN.Conv3d
instance GHC.Generics.Generic Torch.NN.Conv3d
instance GHC.Show.Show Torch.NN.Conv3d
instance GHC.Classes.Eq Torch.NN.ConvTranspose1dSpec
instance GHC.Show.Show Torch.NN.ConvTranspose1dSpec
instance Torch.NN.Parameterized Torch.NN.ConvTranspose1d
instance GHC.Generics.Generic Torch.NN.ConvTranspose1d
instance GHC.Show.Show Torch.NN.ConvTranspose1d
instance GHC.Classes.Eq Torch.NN.ConvTranspose2dSpec
instance GHC.Show.Show Torch.NN.ConvTranspose2dSpec
instance Torch.NN.Parameterized Torch.NN.ConvTranspose2d
instance GHC.Generics.Generic Torch.NN.ConvTranspose2d
instance GHC.Show.Show Torch.NN.ConvTranspose2d
instance GHC.Classes.Eq Torch.NN.ConvTranspose3dSpec
instance GHC.Show.Show Torch.NN.ConvTranspose3dSpec
instance Torch.NN.Parameterized Torch.NN.ConvTranspose3d
instance GHC.Generics.Generic Torch.NN.ConvTranspose3d
instance GHC.Show.Show Torch.NN.ConvTranspose3d
instance GHC.Classes.Eq Torch.NN.BatchNormSpec
instance GHC.Show.Show Torch.NN.BatchNormSpec
instance GHC.Generics.Generic Torch.NN.BatchNorm
instance GHC.Show.Show Torch.NN.BatchNorm
instance GHC.Classes.Eq Torch.NN.InstanceNormSpec
instance GHC.Show.Show Torch.NN.InstanceNormSpec
instance GHC.Generics.Generic Torch.NN.InstanceNorm
instance GHC.Show.Show Torch.NN.InstanceNorm
instance GHC.Classes.Eq Torch.NN.UpSampleSpec
instance GHC.Show.Show Torch.NN.UpSampleSpec
instance Torch.NN.Parameterized Torch.NN.UpSample
instance GHC.Generics.Generic Torch.NN.UpSample
instance GHC.Show.Show Torch.NN.UpSample
instance Torch.NN.Randomizable Torch.NN.UpSampleSpec Torch.NN.UpSample
instance Torch.NN.HasForward Torch.NN.UpSample Torch.Tensor.Tensor Torch.Tensor.Tensor
instance Torch.NN.Parameterized Torch.NN.UpSampleSpec
instance Torch.NN.Randomizable Torch.NN.InstanceNormSpec Torch.NN.InstanceNorm
instance Torch.NN.Randomizable Torch.NN.BatchNormSpec Torch.NN.BatchNorm
instance Torch.NN.Randomizable Torch.NN.ConvTranspose3dSpec Torch.NN.ConvTranspose3d
instance Torch.NN.Randomizable Torch.NN.ConvTranspose2dSpec Torch.NN.ConvTranspose2d
instance Torch.NN.Randomizable Torch.NN.ConvTranspose1dSpec Torch.NN.ConvTranspose1d
instance Torch.NN.Randomizable Torch.NN.Conv3dSpec Torch.NN.Conv3d
instance Torch.NN.Randomizable Torch.NN.Conv2dSpec Torch.NN.Conv2d
instance Torch.NN.Randomizable Torch.NN.Conv1dSpec Torch.NN.Conv1d
instance Torch.NN.HasForward Torch.NN.Linear Torch.Tensor.Tensor Torch.Tensor.Tensor
instance Torch.NN.Randomizable Torch.NN.LinearSpec Torch.NN.Linear
instance Torch.NN.Parameterized Torch.Tensor.Tensor
instance Torch.NN.Parameterized Torch.NN.Parameter
instance Torch.Scalar.Scalar a => Torch.NN.Parameterized a
instance (Torch.NN.Parameterized a, Torch.NN.Parameterized b) => Torch.NN.Parameterized (a, b)
instance (Torch.NN.Parameterized a, Torch.NN.Parameterized b, Torch.NN.Parameterized c) => Torch.NN.Parameterized (a, b, c)
instance (Data.Foldable.Foldable t, Data.Traversable.Traversable t, Torch.NN.Parameterized a) => Torch.NN.Parameterized (t a)
instance Torch.NN.Parameterized (a -> a)
instance Torch.NN.Parameterized c => Torch.NN.GParameterized (GHC.Generics.K1 i c)
instance Torch.NN.GParameterized GHC.Generics.U1
instance (Torch.NN.GParameterized f, Torch.NN.GParameterized g) => Torch.NN.GParameterized (f GHC.Generics.:+: g)
instance (Torch.NN.GParameterized f, Torch.NN.GParameterized g) => Torch.NN.GParameterized (f GHC.Generics.:*: g)
instance Torch.NN.GParameterized f => Torch.NN.GParameterized (GHC.Generics.M1 i t f)
instance Torch.NN.HasForward f a b => Torch.NN.GHasForward (GHC.Generics.K1 i f) (GHC.Generics.K1 i a) (GHC.Generics.K1 i b)
instance Torch.NN.GHasForward GHC.Generics.U1 GHC.Generics.U1 GHC.Generics.U1
instance (Torch.NN.GHasForward f a b, Torch.NN.GHasForward g a' b', b'' GHC.Types.~ (b GHC.Generics.:+: b')) => Torch.NN.GHasForward (f GHC.Generics.:+: g) (a GHC.Generics.:+: a') b''
instance (Torch.NN.GHasForward f a b, Torch.NN.GHasForward g a' b', b'' GHC.Types.~ (b GHC.Generics.:*: b')) => Torch.NN.GHasForward (f GHC.Generics.:*: g) (a GHC.Generics.:*: a') b''
instance Torch.NN.GHasForward f a b => Torch.NN.GHasForward (GHC.Generics.M1 i t f) (GHC.Generics.M1 i t' a) (GHC.Generics.M1 i t' b)

module Torch.Optim
type LearningRate = Tensor
type Loss = Tensor
newtype Gradients
Gradients :: [Tensor] -> Gradients
newtype OptimizerState option
OptimizerState :: option -> OptimizerState option
grad' :: Loss -> [Parameter] -> Gradients
class Optimizer optimizer
step :: Optimizer optimizer => LearningRate -> Gradients -> [Tensor] -> optimizer -> ([Tensor], optimizer)

-- | run a single iteration of an optimizer, returning new parameters and
--   updated optimizer state
runStep :: (Optimizer optimizer, Parameterized model) => model -> optimizer -> Loss -> LearningRate -> IO (model, optimizer)

-- | run a single iteration of an optimizer, returning new parameters and
--   updated optimizer state
runStep' :: (Optimizer optimizer, Parameterized model) => model -> optimizer -> Gradients -> LearningRate -> IO (model, optimizer)
data GD
GD :: GD

-- | Stateless gradient descent step
gd :: LearningRate -> Gradients -> [Tensor] -> [Tensor]

-- | Gradient descent step with a dummy state variable
gd' :: LearningRate -> Gradients -> [Tensor] -> GD -> ([Tensor], GD)
sgd :: LearningRate -> [Parameter] -> [Tensor] -> [Tensor]
data GDM
GDM :: Float -> [Tensor] -> GDM
[beta] :: GDM -> Float
[momentum] :: GDM -> [Tensor]
gdm :: LearningRate -> Gradients -> [Tensor] -> GDM -> ([Tensor], GDM)

-- | State representation for Adam Optimizer
data Adam
Adam :: Float -> Float -> [Tensor] -> [Tensor] -> Int -> Adam
[beta1] :: Adam -> Float
[beta2] :: Adam -> Float
[m1] :: Adam -> [Tensor]
[m2] :: Adam -> [Tensor]
[iter] :: Adam -> Int
mkAdam :: Int -> Float -> Float -> [Parameter] -> Adam

-- | Adam step
adam :: LearningRate -> Gradients -> [Tensor] -> Adam -> ([Tensor], Adam)

-- | State representation for Adagrad Optimizer
data Adagrad
Adagrad :: [Tensor] -> Adagrad
[gsum] :: Adagrad -> [Tensor]

-- | Adagrad step
adagrad :: LearningRate -> Gradients -> [Tensor] -> Adagrad -> ([Tensor], Adagrad)

-- | syntactic sugar for looping with foldM
foldLoop :: a -> Int -> (a -> Int -> IO a) -> IO a
instance GHC.Show.Show Torch.Optim.Gradients
instance GHC.Show.Show Torch.Optim.GD
instance GHC.Show.Show Torch.Optim.GDM
instance GHC.Show.Show Torch.Optim.Adam
instance GHC.Show.Show Torch.Optim.Adagrad
instance Torch.Optim.Optimizer Torch.Optim.Adagrad
instance Torch.Optim.Optimizer Torch.Optim.Adam
instance Torch.Optim.Optimizer Torch.Optim.GDM
instance Torch.Optim.Optimizer Torch.Optim.GD

module Torch.Optim.CppOptim
type CppOptimizerRef = ForeignPtr Optimizer
data CppOptimizerState option
CppOptimizerState :: option -> CppOptimizerRef -> CppOptimizerState option
class CppOptimizer option
initOptimizer :: (CppOptimizer option, Parameterized model) => option -> model -> IO (CppOptimizerState option)
unsafeStep :: (CppOptimizer option, Parameterized model) => model -> CppOptimizerState option -> Tensor -> IO (model, CppOptimizerState option)
data AdagradOptions
AdagradOptions :: Double -> Double -> Double -> Double -> Double -> AdagradOptions
[adagradLr] :: AdagradOptions -> Double
[adagradLrDecay] :: AdagradOptions -> Double
[adagradWeightDecay] :: AdagradOptions -> Double
[adagradInitialAccumulatorValue] :: AdagradOptions -> Double
[adagradEps] :: AdagradOptions -> Double
data AdamOptions
AdamOptions :: Double -> (Double, Double) -> Double -> Double -> Bool -> AdamOptions
[adamLr] :: AdamOptions -> Double
[adamBetas] :: AdamOptions -> (Double, Double)
[adamEps] :: AdamOptions -> Double
[adamWeightDecay] :: AdamOptions -> Double
[adamAmsgrad] :: AdamOptions -> Bool
data AdamwOptions
AdamwOptions :: Double -> (Double, Double) -> Double -> Double -> Bool -> AdamwOptions
[adamwLr] :: AdamwOptions -> Double
[adamwBetas] :: AdamwOptions -> (Double, Double)
[adamwEps] :: AdamwOptions -> Double
[adamwWeightDecay] :: AdamwOptions -> Double
[adamwAmsgrad] :: AdamwOptions -> Bool
data LbfgsOptions
LbfgsOptions :: Double -> Int -> Int -> Double -> Double -> Int -> Maybe String -> LbfgsOptions
[lbfgsLr] :: LbfgsOptions -> Double
[lbfgsMaxIter] :: LbfgsOptions -> Int
[lbfgsMaxEval] :: LbfgsOptions -> Int
[lbfgsToleranceGrad] :: LbfgsOptions -> Double
[lbfgsToleranceChange] :: LbfgsOptions -> Double
[lbfgsHistorySize] :: LbfgsOptions -> Int
[lbfgsLineSearchFn] :: LbfgsOptions -> Maybe String
data RmspropOptions
RmspropOptions :: Double -> Double -> Double -> Double -> Double -> Bool -> RmspropOptions
[rmspropLr] :: RmspropOptions -> Double
[rmspropAlpha] :: RmspropOptions -> Double
[rmspropEps] :: RmspropOptions -> Double
[rmspropWeightDecay] :: RmspropOptions -> Double
[rmspropMomentum] :: RmspropOptions -> Double
[rmspropCentered] :: RmspropOptions -> Bool
data SGDOptions
SGDOptions :: Double -> Double -> Double -> Double -> Bool -> SGDOptions
[sgdLr] :: SGDOptions -> Double
[sgdMomentum] :: SGDOptions -> Double
[sgdDampening] :: SGDOptions -> Double
[sgdWeightDecay] :: SGDOptions -> Double
[sgdNesterov] :: SGDOptions -> Bool
saveState :: CppOptimizerState option -> FilePath -> IO ()
loadState :: CppOptimizerState option -> FilePath -> IO ()
instance GHC.Classes.Eq Torch.Optim.CppOptim.AdagradOptions
instance GHC.Show.Show Torch.Optim.CppOptim.AdagradOptions
instance GHC.Classes.Eq Torch.Optim.CppOptim.AdamOptions
instance GHC.Show.Show Torch.Optim.CppOptim.AdamOptions
instance GHC.Classes.Eq Torch.Optim.CppOptim.AdamwOptions
instance GHC.Show.Show Torch.Optim.CppOptim.AdamwOptions
instance GHC.Classes.Eq Torch.Optim.CppOptim.LbfgsOptions
instance GHC.Show.Show Torch.Optim.CppOptim.LbfgsOptions
instance GHC.Classes.Eq Torch.Optim.CppOptim.RmspropOptions
instance GHC.Show.Show Torch.Optim.CppOptim.RmspropOptions
instance GHC.Classes.Eq Torch.Optim.CppOptim.SGDOptions
instance GHC.Show.Show Torch.Optim.CppOptim.SGDOptions
instance Data.Default.Class.Default Torch.Optim.CppOptim.SGDOptions
instance Torch.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.SGDOptions
instance Data.Default.Class.Default Torch.Optim.CppOptim.RmspropOptions
instance Torch.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.RmspropOptions
instance Data.Default.Class.Default Torch.Optim.CppOptim.LbfgsOptions
instance Torch.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.LbfgsOptions
instance Data.Default.Class.Default Torch.Optim.CppOptim.AdamwOptions
instance Torch.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.AdamwOptions
instance Data.Default.Class.Default Torch.Optim.CppOptim.AdamOptions
instance Torch.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.AdamOptions
instance Data.Default.Class.Default Torch.Optim.CppOptim.AdagradOptions
instance Torch.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.AdagradOptions
instance Torch.Optim.CppOptim.CppOptimizer option => Torch.Optim.Optimizer (Torch.Optim.CppOptim.CppOptimizerState option)

module Torch.Script
newtype ScriptModule
UnsafeScriptModule :: ForeignPtr Module -> ScriptModule
newtype RawModule
UnsafeRawModule :: ForeignPtr Module -> RawModule
type RawIValue = ForeignPtr IValue
newtype Blob
UnsafeBlob :: ForeignPtr (C10Ptr Blob) -> Blob
newtype Object
UnsafeObject :: ForeignPtr (C10Ptr IVObject) -> Object
newtype Future
UnsafeFuture :: ForeignPtr (C10Ptr IVFuture) -> Future
newtype Capsule
UnsafeCapsule :: ForeignPtr (C10Ptr Capsule) -> Capsule

-- | See <a>https://github.com/pytorch/pytorch/wiki/PyTorch-IR</a>
newtype Graph
UnsafeGraph :: ForeignPtr (SharedPtr JitGraph) -> Graph
data JitGraph
JitGraph :: [JitValue] -> [JitValue] -> [JitNode] -> JitGraph
[graphInputs] :: JitGraph -> [JitValue]
[graphOutputs] :: JitGraph -> [JitValue]
[graphNodes] :: JitGraph -> [JitNode]
data JitNode
JitNode :: [JitValue] -> [JitValue] -> String -> JitNode
[nodeInputs] :: JitNode -> [JitValue]
[nodeOutputs] :: JitNode -> [JitValue]
[nodeKind] :: JitNode -> String
data JitValue
JitValue :: Int -> String -> JitValue
[valueId] :: JitValue -> Int
[valueType] :: JitValue -> String
data IValue
IVNone :: IValue
IVTensor :: Tensor -> IValue
IVDouble :: Double -> IValue
IVInt :: Int64 -> IValue
IVBool :: Bool -> IValue
IVTuple :: [IValue] -> IValue
IVIntList :: [Int64] -> IValue
IVDoubleList :: [Double] -> IValue
IVBoolList :: [Bool] -> IValue
IVString :: String -> IValue
IVTensorList :: [Tensor] -> IValue
IVBlob :: IValue
IVGenericList :: [IValue] -> IValue
IVGenericDict :: [(IValue, IValue)] -> IValue
IVFuture :: IValue
IVDevice :: IValue
IVObject :: IValue
IVUninitialized :: IValue
IVCapsule :: IValue
newModule :: String -> IO RawModule
saveScript :: ScriptModule -> FilePath -> IO ()
saveScript' :: RawModule -> FilePath -> IO ()
data LoadMode
WithoutRequiredGrad :: LoadMode
WithRequiredGrad :: LoadMode

-- | Load a torchscript file
loadScript :: LoadMode -> FilePath -> IO ScriptModule
loadScript' :: FilePath -> IO RawModule
registerParameter :: RawModule -> String -> Tensor -> Bool -> IO ()
registerModule :: RawModule -> String -> RawModule -> IO ()
getParameters :: ScriptModule -> [Tensor]
getParametersIO :: RawModule -> IO [Tensor]
setParameters :: RawModule -> [Tensor] -> IO ()
updateParameters :: LoadMode -> ScriptModule -> [Tensor] -> ScriptModule
getNamedParameters :: ScriptModule -> [(String, Tensor)]
getNamedBuffers :: ScriptModule -> [(String, Tensor)]

-- | Load all attributes including training flags This function returns
--   IVObject type as Tensor type. To get Tensor type, use get
--   getNamedParameters and getNamedBuffers.
getNamedAttributes :: ScriptModule -> [(String, IValue)]
getNamedModules :: ScriptModule -> [(String, ScriptModule)]
getNamedChildren :: ScriptModule -> [(String, ScriptModule)]
toScriptModule :: RawModule -> IO ScriptModule
toRawModule :: ScriptModule -> IO RawModule
cloneRawModule :: RawModule -> IO RawModule
data RuntimeMode
Eval :: RuntimeMode
Train :: RuntimeMode
setRuntimeMode :: RawModule -> RuntimeMode -> IO ()
define :: RawModule -> String -> IO ()
dumpToStr :: ScriptModule -> Bool -> Bool -> Bool -> IO String
dumpToStr' :: ScriptModule -> IO String
runMethod :: ScriptModule -> String -> [IValue] -> IValue
runMethod1 :: ScriptModule -> String -> IValue -> IValue
trace :: String -> String -> ([Tensor] -> IO [Tensor]) -> [Tensor] -> IO RawModule

-- | This function generates torchscript-module from Parameterized-instance
--   of hasktorch. Usage is below. -- &gt;&gt; let example_inputs =
--   asTensor (4::Float) -- &gt;&gt; init_parameters &lt;- sample MonoSpec
--   -- &gt;&gt; mutableTorchscript &lt;- traceWithParameters
--   <a>MyModule</a> -- (parameters [example_inputs'] -&gt; return
--   [(traced_function parameters example_inputs')]) -- init_parameters --
--   [example_inputs] -- &gt;&gt; immutableTorchscript &lt;- toScriptModule
--   mutableTorchscript -- &gt;&gt; save immutableTorchscript
--   "<a>torchscript file</a>"
traceWithParameters :: Parameterized f => String -> (f -> [Tensor] -> IO [Tensor]) -> f -> [Tensor] -> IO RawModule
traceAsGraph :: ([Tensor] -> IO [Tensor]) -> [Tensor] -> IO Graph
printGraph :: Graph -> IO String

-- | Output onnx file from graph. (really experimental implementation)
--   printOnnx uses export_onnx function of libtorch. It outputs following
--   error, because prim::Constant symbol using torchscript does not exist.
--   -- Exception: ONNX export failed: Couldn't export operator
--   prim::Constant -- Defined at: -- Graph we tried to export: -- graph(%0
--   : Float(), -- %1 : Float()): -- %2 : int = prim::Constant[value=1]()
--   -- %3 : Float() = aten::add(%0, %1, %2) -- return (%3) -- ; type:
--   std::runtime_error On the other hand, torch.onnx.export of python
--   works. onnx's symbol map is in python code.
--   <a>https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py</a>
--   
--   If you need onnx-file, at first make torchscript by trace , then
--   convert torchscript into onnx by python-code.
printOnnx :: Graph -> IO String
graphToJitGraph :: Graph -> IO JitGraph
instance GHC.Classes.Eq Torch.Script.JitValue
instance GHC.Show.Show Torch.Script.JitValue
instance GHC.Classes.Eq Torch.Script.JitNode
instance GHC.Show.Show Torch.Script.JitNode
instance GHC.Classes.Eq Torch.Script.JitGraph
instance GHC.Show.Show Torch.Script.JitGraph
instance GHC.Show.Show Torch.Script.IValue
instance GHC.Classes.Eq Torch.Script.LoadMode
instance GHC.Show.Show Torch.Script.LoadMode
instance GHC.Classes.Eq Torch.Script.RuntimeMode
instance GHC.Show.Show Torch.Script.RuntimeMode
instance Torch.NN.Parameterized Torch.Script.ScriptModule
instance Torch.NN.HasForward Torch.Script.ScriptModule [Torch.Script.IValue] Torch.Script.IValue
instance Torch.Internal.Class.Castable [Torch.Script.IValue] [Torch.Script.RawIValue]
instance Torch.Internal.Class.Castable Torch.Script.IValue Torch.Script.RawIValue
instance Torch.Internal.Class.Castable Torch.Script.Graph (GHC.ForeignPtr.ForeignPtr (Torch.Internal.Type.SharedPtr Torch.Internal.Type.JitGraph))
instance GHC.Show.Show Torch.Script.Capsule
instance GHC.Show.Show Torch.Script.Future
instance GHC.Show.Show Torch.Script.Object
instance GHC.Show.Show Torch.Script.Blob
instance Torch.Internal.Class.Castable Torch.Script.RawModule (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Module)
instance GHC.Show.Show Torch.Script.ScriptModule
instance Torch.Internal.Class.Castable Torch.Script.ScriptModule (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Module)

module Torch.Serialize
save :: [Tensor] -> FilePath -> IO ()
load :: FilePath -> IO [Tensor]

-- | Save state_dict
pickleSave :: IValue -> FilePath -> IO ()

-- | Load a state_dict file You should use a dict function of pytorch to
--   save a state_dict file as follows.
--   
--   <pre>
--   torch.save(dict(model.state_dict()), "state_dict.pth")
--   </pre>
pickleLoad :: FilePath -> IO IValue
saveParams :: Parameterized f => f -> FilePath -> IO ()
loadParams :: Parameterized b => b -> FilePath -> IO b
class RawFile a
loadBinary :: RawFile a => Handle -> a -> IO a
saveBinary :: RawFile a => Handle -> a -> IO ()
instance Torch.Serialize.RawFile Torch.Tensor.Tensor

module Torch.Random
mkGenerator :: Device -> Word64 -> IO Generator
data Generator
randn :: [Int] -> TensorOptions -> Generator -> (Tensor, Generator)
randn' :: [Int] -> Generator -> (Tensor, Generator)
rand :: [Int] -> TensorOptions -> Generator -> (Tensor, Generator)
rand' :: [Int] -> Generator -> (Tensor, Generator)
randint :: Int -> Int -> [Int] -> TensorOptions -> Generator -> (Tensor, Generator)
randint' :: Int -> Int -> [Int] -> Generator -> (Tensor, Generator)
normal :: Double -> Double -> [Int] -> TensorOptions -> Generator -> (Tensor, Generator)
normal' :: Double -> Double -> [Int] -> Generator -> (Tensor, Generator)
instance GHC.Show.Show Torch.Random.Generator
instance GHC.Classes.Eq Torch.Random.Generator
instance GHC.Show.Show (GHC.Conc.Sync.TVar (Data.Either.Either (GHC.Word.Word64, Torch.Device.Device) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.Generator)))

module Torch

module Torch.NN.Recurrent.Cell.LSTM
data LSTMSpec
LSTMSpec :: Int -> Int -> LSTMSpec
[inputSize] :: LSTMSpec -> Int
[hiddenSize] :: LSTMSpec -> Int
data LSTMCell
LSTMCell :: Parameter -> Parameter -> Parameter -> Parameter -> LSTMCell
[weightsIH] :: LSTMCell -> Parameter
[weightsHH] :: LSTMCell -> Parameter
[biasIH] :: LSTMCell -> Parameter
[biasHH] :: LSTMCell -> Parameter
lstmCellForward :: LSTMCell -> (Tensor, Tensor) -> Tensor -> (Tensor, Tensor)
instance GHC.Show.Show Torch.NN.Recurrent.Cell.LSTM.LSTMSpec
instance GHC.Classes.Eq Torch.NN.Recurrent.Cell.LSTM.LSTMSpec
instance GHC.Show.Show Torch.NN.Recurrent.Cell.LSTM.LSTMCell
instance GHC.Generics.Generic Torch.NN.Recurrent.Cell.LSTM.LSTMCell
instance Torch.NN.Parameterized Torch.NN.Recurrent.Cell.LSTM.LSTMCell
instance Torch.NN.Randomizable Torch.NN.Recurrent.Cell.LSTM.LSTMSpec Torch.NN.Recurrent.Cell.LSTM.LSTMCell

module Torch.NN.Recurrent.Cell.GRU
data GRUSpec
GRUSpec :: Int -> Int -> GRUSpec
[inputSize] :: GRUSpec -> Int
[hiddenSize] :: GRUSpec -> Int
data GRUCell
GRUCell :: Parameter -> Parameter -> Parameter -> Parameter -> GRUCell
[weightsIH] :: GRUCell -> Parameter
[weightsHH] :: GRUCell -> Parameter
[biasIH] :: GRUCell -> Parameter
[biasHH] :: GRUCell -> Parameter
gruCellForward :: GRUCell -> Tensor -> Tensor -> Tensor
instance GHC.Show.Show Torch.NN.Recurrent.Cell.GRU.GRUSpec
instance GHC.Classes.Eq Torch.NN.Recurrent.Cell.GRU.GRUSpec
instance GHC.Show.Show Torch.NN.Recurrent.Cell.GRU.GRUCell
instance GHC.Generics.Generic Torch.NN.Recurrent.Cell.GRU.GRUCell
instance Torch.NN.Parameterized Torch.NN.Recurrent.Cell.GRU.GRUCell
instance Torch.NN.Randomizable Torch.NN.Recurrent.Cell.GRU.GRUSpec Torch.NN.Recurrent.Cell.GRU.GRUCell

module Torch.NN.Recurrent.Cell.Elman
data ElmanSpec
ElmanSpec :: Int -> Int -> ElmanSpec
[inputSize] :: ElmanSpec -> Int
[hiddenSize] :: ElmanSpec -> Int
data ElmanCell
ElmanCell :: Parameter -> Parameter -> Parameter -> Parameter -> ElmanCell
[weightsIH] :: ElmanCell -> Parameter
[weightsHH] :: ElmanCell -> Parameter
[biasIH] :: ElmanCell -> Parameter
[biasHH] :: ElmanCell -> Parameter
elmanCellForward :: ElmanCell -> Tensor -> Tensor -> Tensor
instance GHC.Show.Show Torch.NN.Recurrent.Cell.Elman.ElmanSpec
instance GHC.Classes.Eq Torch.NN.Recurrent.Cell.Elman.ElmanSpec
instance GHC.Show.Show Torch.NN.Recurrent.Cell.Elman.ElmanCell
instance GHC.Generics.Generic Torch.NN.Recurrent.Cell.Elman.ElmanCell
instance Torch.NN.Parameterized Torch.NN.Recurrent.Cell.Elman.ElmanCell
instance Torch.NN.Randomizable Torch.NN.Recurrent.Cell.Elman.ElmanSpec Torch.NN.Recurrent.Cell.Elman.ElmanCell


-- | Hasktorch is a library for scientific computing and differentiable
--   programming.
module Torch.Tutorial

module Torch.Typed.Aux
natValI :: forall n. KnownNat n => Int
natValInt16 :: forall n. KnownNat n => Int16
type family Fst (t :: (a, b)) :: a
type family Snd (t :: (a, b)) :: b
type family Fst3 (t :: (a, b, c)) :: a
type family Snd3 (t :: (a, b, c)) :: b
type family Trd3 (t :: (a, b, c)) :: c
type family DimOutOfBoundCheckImpl (shape :: [a]) (dim :: Nat) (xs :: [a]) (n :: Nat) :: Constraint
type DimOutOfBoundCheck shape dim = DimOutOfBoundCheckImpl shape dim shape dim
type family DimOutOfBound (shape :: [a]) (dim :: Nat)
type family IndexOutOfBound (shape :: [a]) (dim :: Nat) (idx :: Nat)
type family AppendToMaybe (h :: a) (mt :: Maybe [a]) :: Maybe [a]
type family AppendToMaybe' (h :: Maybe a) (mt :: Maybe [a]) :: Maybe [a]
type family MaybePrepend (mh :: Maybe a) (t :: [a]) :: [a]
type family LastDim (l :: [a]) :: Nat
type family Product (xs :: [Nat]) :: Nat
type family BackwardsImpl (last :: Nat) (n :: Nat) :: Nat
type Backwards l n = BackwardsImpl (LastDim l) n

-- | Evaluate a type-level constraint for whether or not the former shape
--   is a suffix of the latter shape
--   
--   <pre>
--   &gt;&gt;&gt; :kind! IsSuffixOf '[1] '[1]
--   IsSuffixOf '[1] '[1] :: Constraint
--   = () :: Constraint
--   
--   &gt;&gt;&gt; :kind! IsSuffixOf '[1] '[2, 1]
--   IsSuffixOf '[1] '[2, 1] :: Constraint
--   = () :: Constraint
--   
--   &gt;&gt;&gt; :kind! IsSuffixOf '[2] '[2, 1]
--   IsSuffixOf '[2] '[2, 1] :: Constraint
--   = (TypeError ...)
--   
--   &gt;&gt;&gt; :kind! IsSuffixOf '[1, 1] '[2, 1]
--   IsSuffixOf '[1, 1] '[2, 1] :: Constraint
--   = (TypeError ...)
--   
--   &gt;&gt;&gt; :kind! IsSuffixOf '[2, 1] '[2, 1]
--   IsSuffixOf '[2, 1] '[2, 1] :: Constraint
--   = () :: Constraint
--   </pre>
type IsSuffixOf xs ys = CheckIsSuffixOf xs ys (IsSuffixOfImpl xs ys (DropLengthMaybe xs ys))
type family CheckIsSuffixOf (xs :: [a]) (ys :: [a]) (result :: Bool) :: Constraint
type family IsSuffixOfImpl (xs :: [a]) (ys :: [a]) (mDelta :: Maybe [b]) :: Bool
type family DropLengthMaybe (xs :: [a]) (ys :: [b]) :: Maybe [b]
type family DropLength (xs :: [a]) (ys :: [b]) :: [b]
type family Init (xs :: [a]) :: [a]
type family Last (xs :: [a]) :: a
type family InsertImpl (n :: Nat) (x :: a) (l :: [a]) :: Maybe [a]
type family CheckInsert (n :: Nat) (x :: a) (l :: [a]) (result :: Maybe [a]) :: [a]
type family Insert (n :: Nat) (x :: a) (l :: [a]) :: [a]
type family RemoveImpl (l :: [a]) (n :: Nat) :: Maybe [a]
type family CheckRemove (l :: [a]) (n :: Nat) (result :: Maybe [a]) :: [a]
type Remove l n = CheckRemove l n (RemoveImpl l n)
type family IndexImpl (l :: [a]) (n :: Nat) :: Maybe a
type family CheckIndex (l :: [a]) (n :: Nat) (result :: Maybe a) :: a
type Index l n = CheckIndex l n (IndexImpl l n)
type family InRangeCheck (shape :: [Nat]) (dim :: Nat) (idx :: Nat) (ok :: Ordering) :: Constraint
type InRange shape dim idx = InRangeCheck shape dim idx (CmpNat idx (Index shape dim))
type family ReverseImpl (l :: [a]) (acc :: [a]) :: [a]
type Reverse l = ReverseImpl l '[]
type family ExtractDim (dim :: Nat) (shape :: [Nat]) :: Maybe Nat
type family ReplaceDim (dim :: Nat) (shape :: [Nat]) (n :: Nat) :: Maybe [Nat]
type family If c t e
type family AllDimsPositive (shape :: [Nat]) :: Constraint
type family IsAtLeast (n :: Nat) (m :: Nat) (cmp :: Ordering) :: Constraint
type (>=) (n :: Nat) (m :: Nat) = (IsAtLeast n m (CmpNat n m), KnownNat (n - m))
type family CmpDType (dtype :: DType) (dtype' :: DType) :: Ordering
type family DTypePromotionImpl (dtype :: DType) (dtype' :: DType) (ord :: Ordering) :: DType
type DTypePromotion dtype dtype' = DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')
type family DTypeIsFloatingPoint (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type family DTypeIsIntegral (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type family DTypeIsNotHalf (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type family DTypeIsNotBool (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type family UnsupportedDTypeForDevice (deviceType :: DeviceType) (dtype :: DType) :: Constraint
type family StandardFloatingPointDTypeValidation (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type family StandardDTypeValidation (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

module Torch.Typed.NN.Recurrent.Aux
data RNNInitialization
ConstantInitialization :: RNNInitialization
LearnedInitialization :: RNNInitialization

-- | Fan-in / Fan-out scaling calculation
calculateFan :: [Int] -> (Int, Int)

-- | Xavier Initialization - Uniform
xavierUniformFIXME :: Tensor -> Float -> [Int] -> IO Tensor
instance GHC.Generics.Generic Torch.Typed.NN.Recurrent.Aux.RNNInitialization
instance GHC.Show.Show Torch.Typed.NN.Recurrent.Aux.RNNInitialization

module Torch.Typed.Tensor
class KnownShape (shape :: [Nat])
shapeVal :: KnownShape shape => [Int]
getFiniteI :: Finite n -> Int
class KnownDType (dtype :: DType)
dtypeVal :: KnownDType dtype => DType
type family ComputeDType (dtype' :: dtype) :: DType
class KnownDevice (device :: (DeviceType, Nat))
deviceVal :: KnownDevice device => Device
type Size = Type -> Type
type Shape = [Type -> Type]
type family ToNat (shape :: Size) :: Nat
type family ToNats (shape :: Shape) :: [Nat]
type family FromNat (shape :: Nat) :: Size
type family FromNats (shape :: [Nat]) :: Shape
class Unnamed t where {
    type family UTShape t :: [Nat];
    type family UTDevice t :: (DeviceType, Nat);
    type family UTDType t :: DType;
}
toUnnamed :: forall device dtype shape. (Unnamed t, IsUnnamed t device dtype shape) => t -> Tensor device dtype shape
fromUnnamed :: forall device dtype shape. (Unnamed t, IsUnnamed t device dtype shape) => Tensor device dtype shape -> t
toDynamic :: Unnamed t => t -> Tensor
type family IsUnnamed t (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat]) :: Constraint
data Tensor (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat])
[UnsafeMkTensor] :: forall device dtype shape. Tensor -> Tensor device dtype shape
type CPUTensor = Tensor '( 'CPU, 0)
type CUDATensor deviceIndex = Tensor '( 'CUDA, deviceIndex)
data UnknownShapeTensor device dtype
UnknownShapeTensor :: Tensor device dtype shape -> UnknownShapeTensor device dtype
type family ComputeHaskellType (dtype :: DType) :: Type
type family ComputeItemType (ty :: Type) (shape :: [Nat]) :: Type
class TensorOptions (shape :: [Nat]) (dtype :: DType) (device :: (DeviceType, Nat))
optionsRuntimeShape :: TensorOptions shape dtype device => [Int]
optionsRuntimeDType :: TensorOptions shape dtype device => DType
optionsRuntimeDevice :: TensorOptions shape dtype device => Device
type family All (pred :: a -> Constraint) (l :: [a]) :: Constraint
data SomeShape
[SomeShape] :: forall (shape :: [Nat]). KnownShape shape => Proxy shape -> SomeShape
someShape :: [Int] -> SomeShape
data SomeDType
[SomeDType] :: forall (dtype :: DType). KnownDType dtype => Proxy dtype -> SomeDType
someDType :: DType -> SomeDType
data SomeDevice
[SomeDevice] :: forall (device :: (DeviceType, Nat)). KnownDevice device => Proxy device -> SomeDevice
someDevice :: Device -> SomeDevice
withTensor :: Tensor -> (forall shape dtype device. KnownShape shape => Tensor device dtype shape -> r) -> r
type family ComputeBroadcast (reversedShape :: [Nat]) (reversedShape' :: [Nat]) :: Maybe [Nat]
type family CheckBroadcast (shape :: [Nat]) (shape' :: [Nat]) (result :: Maybe [Nat]) :: [Nat]
type Broadcast shape shape' = CheckBroadcast shape shape' (ComputeBroadcast (Reverse shape) (Reverse shape'))
type family BasicArithmeticDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
add :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
sub :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
mul :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
div :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
type family ComparisonDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
gt :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
lt :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
ge :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
le :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
eq :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
ne :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(>.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(<.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(>=.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(<=.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(==.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(/=.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
type family ComputeMatMul (reversedShape :: [Nat]) (reversedShape' :: [Nat]) :: Maybe [Nat]
type family CheckMatMul (shape :: [Nat]) (shape' :: [Nat]) (result :: Maybe [Nat]) :: [Nat]
type MatMul shape shape' = CheckMatMul shape shape' (ComputeMatMul (Reverse shape) (Reverse shape'))
type family MatMulDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | matrix multiplication See
--   <a>https://pytorch.org/docs/stable/torch.html#torch.matmul</a>.
matmul :: forall shape'' shape shape' dtype device. (shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) => Tensor device dtype shape -> Tensor device dtype shape' -> Tensor device dtype shape''
select :: forall dim idx shape' shape dtype device. (KnownNat dim, KnownNat idx, InRange shape dim idx, shape' ~ Remove shape dim) => Tensor device dtype shape -> Tensor device dtype shape'
selectIdx :: forall dim n shape' shape dtype device. (KnownNat dim, n ~ Index shape dim, shape' ~ Remove shape dim) => Tensor device dtype shape -> Finite n -> Tensor device dtype shape'
type family Numel (shape :: [Nat]) :: Nat

-- | reshape &gt;&gt;&gt; t :: CPUTensor 'D.Int64 '[2,3,4] = fromJust
--   [[[111,112,113,114],[121,122,123,124],[131,132,133,134]],[[211,212,213,214],[221,222,223,224],[231,232,233,234]]]
--   &gt;&gt;&gt; t' = reshape <tt>'[24] t &gt;&gt;&gt; toList . Just $ t'
--   [111,112,113,114,121,122,123,124,131,132,133,134,211,212,213,214,221,222,223,224,231,232,233,234]
--   &gt;&gt;&gt; toList . Just $ reshape </tt>'[2,3,4] t'
--   [[[111,112,113,114],[121,122,123,124],[131,132,133,134]],[[211,212,213,214],[221,222,223,224],[231,232,233,234]]]
reshape :: forall shape' shape dtype device. (KnownShape shape', Numel shape ~ Numel shape') => Tensor device dtype shape -> Tensor device dtype shape'

-- | To avoid overlapped instance for (Unnamed t =&gt; Castable t
--   D.ATenTensor)
newtype Wrap a
Wrap :: a -> Wrap a
[unWrap] :: Wrap a -> a
data TensorListFold
TensorListFold :: TensorListFold
data TensorListUnfold
TensorListUnfold :: TensorListUnfold
toSparse :: Tensor device dtype shape -> Tensor device dtype shape
toDense :: Tensor device dtype shape -> Tensor device dtype shape

-- | move tensor to CPU TODO: can this fail?
toCPU :: forall device shape dtype. Tensor device dtype shape -> CPUTensor dtype shape

-- | move tensor to the first CUDA device TODO: what if this fails?
toCUDA :: forall device' device shape dtype. Tensor device dtype shape -> CUDATensor 0 dtype shape

-- | move tensor to device TODO: what if this fails?
toDevice :: forall device' device dtype shape t t'. (KnownDevice device', IsUnnamed t device dtype shape, Unnamed t', t' ~ ReplaceDevice'' t device') => t -> t'

-- | change tensor data type
toDType :: forall dtype' dtype device shape t t'. (KnownDType dtype', IsUnnamed t device dtype shape, Unnamed t', t' ~ ReplaceDType'' t dtype') => t -> t'

-- | returns tensor dimension uses compile-time information only
dim :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> Int

-- | returns tensor shape as list uses compile-time information only
shape :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> [Int]

-- | returns tensor data type uses compile-time information only
dtype :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> DType

-- | returns tensor device uses compile-time information only
device :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> Device
toInt :: Tensor device dtype shape -> Int
toFloat :: forall device. Tensor device 'Float '[] -> Float
toDouble :: forall device. Tensor device 'Double '[] -> Double
toBool :: forall device. Tensor device 'Bool '[] -> Bool
type family ToDType a :: DType
type family ToShape a :: Shape
type family FindDim (a :: Size) (shape :: Shape) :: Nat
data NamedTensor (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: Shape)
[FromTensor] :: forall device dtype shape' shape. shape ~ ToNats shape' => Tensor device dtype shape -> NamedTensor device dtype shape'
type family ReplaceDevice'' (tensor :: t) (device :: (DeviceType, Nat)) :: t
type family ReplaceDType'' (tensor :: t) (dtype :: DType) :: t
instance Torch.Internal.Class.Castable (Torch.Typed.Tensor.NamedTensor device dtype shape) Torch.Tensor.ATenTensor
instance Torch.Typed.Tensor.Unnamed (Torch.Typed.Tensor.NamedTensor device dtype shape)
instance Torch.Typed.Tensor.KnownDevice device => GHC.Num.Num (Torch.Typed.Tensor.NamedTensor device dtype shape)
instance Torch.Typed.Tensor.KnownDevice device => GHC.Real.Fractional (Torch.Typed.Tensor.NamedTensor device dtype shape)
instance GHC.Show.Show (Torch.Typed.Tensor.NamedTensor device dtype shape)
instance Torch.HList.Apply Torch.Typed.Tensor.TensorListUnfold [Torch.Tensor.ATenTensor] (GHC.Types.IO Torch.HList.HNothing)
instance Torch.Internal.Class.Castable x Torch.Tensor.ATenTensor => Torch.HList.Apply Torch.Typed.Tensor.TensorListUnfold [Torch.Tensor.ATenTensor] (GHC.Types.IO (Torch.HList.HJust (x, [Torch.Tensor.ATenTensor])))
instance (Torch.HList.HFoldrM GHC.Types.IO Torch.Typed.Tensor.TensorListFold [Torch.Tensor.ATenTensor] l [Torch.Tensor.ATenTensor], Torch.HList.Apply Torch.Typed.Tensor.TensorListUnfold [Torch.Tensor.ATenTensor] res, Torch.HList.HUnfoldM GHC.Types.IO Torch.Typed.Tensor.TensorListUnfold res l, res GHC.Types.~ Torch.HList.HUnfoldMRes GHC.Types.IO [Torch.Tensor.ATenTensor] l) => Torch.Internal.Class.Castable (Torch.HList.HList l) [Torch.Tensor.ATenTensor]
instance Torch.Internal.Class.Castable x Torch.Tensor.ATenTensor => Torch.HList.Apply' Torch.Typed.Tensor.TensorListFold (x, GHC.Types.IO [Torch.Tensor.ATenTensor]) (GHC.Types.IO [Torch.Tensor.ATenTensor])
instance Torch.Typed.Tensor.Unnamed t => Torch.Internal.Class.Castable (Torch.Typed.Tensor.Wrap t) Torch.Tensor.ATenTensor
instance (Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.Typed.Tensor.TensorOptions '[] dtype device
instance (GHC.TypeNats.KnownNat h, Torch.Typed.Tensor.TensorOptions t dtype device) => Torch.Typed.Tensor.TensorOptions (h : t) dtype device
instance (Torch.Tensor.TensorLike [Torch.Typed.Tensor.ComputeItemType (Torch.Typed.Tensor.ComputeHaskellType dtype) shape], Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.KnownShape shape) => GHC.Exts.IsList (GHC.Maybe.Maybe (Torch.Typed.Tensor.Tensor device dtype shape))
instance Torch.Typed.Tensor.Unnamed (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.Typed.Tensor.KnownDevice device => GHC.Num.Num (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.Typed.Tensor.KnownDevice device => GHC.Real.Fractional (Torch.Typed.Tensor.Tensor device dtype shape)
instance GHC.Show.Show (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.Internal.Class.Castable (Torch.Typed.Tensor.Tensor device dtype shape) Torch.Tensor.ATenTensor
instance Torch.Internal.Class.Castable [Torch.Typed.Tensor.Tensor device dtype shape] (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)
instance GHC.TypeNats.KnownNat n => Torch.Internal.Class.Castable (Data.Vector.Sized.Vector n (Torch.Typed.Tensor.Tensor device dtype shape)) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)
instance GHC.TypeNats.KnownNat n => Torch.Typed.Tensor.KnownDevice '( 'Torch.Device.CPU, n)
instance GHC.TypeNats.KnownNat n => Torch.Typed.Tensor.KnownDevice '( 'Torch.Device.CUDA, n)
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Bool
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.UInt8
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Int8
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Int16
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Int32
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Int64
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Half
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Float
instance Torch.Typed.Tensor.KnownDType 'Torch.DType.Double
instance Torch.Typed.Tensor.KnownShape '[]
instance (GHC.TypeNats.KnownNat h, Torch.Typed.Tensor.KnownShape t) => Torch.Typed.Tensor.KnownShape (h : t)
instance forall k (l :: [k]). Torch.Internal.Class.Castable (Torch.HList.HList l) [Torch.Tensor.ATenTensor] => Torch.Internal.Class.Castable (Torch.HList.HList l) (GHC.ForeignPtr.ForeignPtr Torch.Internal.Type.TensorList)

module Torch.Typed.Serialize

-- | save list of tensors to file
save :: forall tensors. Castable (HList tensors) [ATenTensor] => HList tensors -> FilePath -> IO ()

-- | load list of tensors from file
load :: forall tensors. Castable (HList tensors) [ATenTensor] => FilePath -> IO (HList tensors)

module Torch.Typed.Lens
class HasName (name :: Type -> Type) shape
name :: HasName name shape => Traversal' (NamedTensor device dtype shape) (NamedTensor device dtype (DropName name shape))
name :: (HasName name shape, KnownNat (NamedIdx name shape)) => Traversal' (NamedTensor device dtype shape) (NamedTensor device dtype (DropName name shape))
class HasField (field :: Symbol) shape
field :: HasField field shape => Lens' (NamedTensor device dtype shape) (NamedTensor device dtype (DropField field shape))
field :: (HasField field shape, FieldIdx field shape) => Lens' (NamedTensor device dtype shape) (NamedTensor device dtype (DropField field shape))
type family GHasField (field :: Symbol) f :: Bool
type family DropField (field :: Symbol) (a :: [Type -> Type]) :: [Type -> Type]
type family DropName (name :: Type -> Type) (a :: [Type -> Type]) :: [Type -> Type]
type family NamedIdx (name :: Type -> Type) (shape :: [Type -> Type]) :: Nat
class FieldIdx (field :: Symbol) (a :: [Type -> Type])

-- | Return field-id
fieldIdx :: FieldIdx field a => Proxy a -> [Maybe Int]
class FieldId (field :: Symbol) a

-- | Return field-id
fieldId :: FieldId field a => Proxy a -> Maybe Int

-- | Return field-id
fieldId :: (FieldId field a, Generic a, GFieldId field (Rep a)) => Proxy a -> Maybe Int
class GFieldId (field :: Symbol) (a :: Type -> Type)
gfieldId :: GFieldId field a => Proxy a -> Maybe Int
gfieldId' :: GFieldId field a => Proxy a -> (Maybe Int, Int)
instance (Torch.Typed.Lens.FieldId field (x ()), Torch.Typed.Lens.FieldIdx field xs) => Torch.Typed.Lens.FieldIdx field (x : xs)
instance Torch.Typed.Lens.FieldId field (Data.Vector.Sized.Vector n v)
instance (GHC.Generics.Generic s, Torch.Typed.Lens.GFieldId field (GHC.Generics.Rep s)) => Torch.Typed.Lens.FieldId field s
instance Torch.Typed.Lens.GFieldId field f => Torch.Typed.Lens.GFieldId field (GHC.Generics.M1 GHC.Generics.D t f)
instance Torch.Typed.Lens.GFieldId field f => Torch.Typed.Lens.GFieldId field (GHC.Generics.M1 GHC.Generics.C t f)
instance (GHC.TypeLits.KnownSymbol field, GHC.TypeLits.KnownSymbol field_) => Torch.Typed.Lens.GFieldId field (GHC.Generics.S1 ('GHC.Generics.MetaSel ('GHC.Maybe.Just field_) p f b) (GHC.Generics.Rec0 a))
instance Torch.Typed.Lens.GFieldId field (GHC.Generics.K1 c f)
instance Torch.Typed.Lens.GFieldId field GHC.Generics.U1
instance (Torch.Typed.Lens.GFieldId field f, Torch.Typed.Lens.GFieldId field g) => Torch.Typed.Lens.GFieldId field (f GHC.Generics.:*: g)
instance (Torch.Typed.Lens.GFieldId field f, Torch.Typed.Lens.GFieldId field g) => Torch.Typed.Lens.GFieldId field (f GHC.Generics.:+: g)
instance Torch.Typed.Lens.FieldIdx field shape => Torch.Typed.Lens.HasField field shape
instance Torch.Typed.Lens.FieldIdx field '[]
instance GHC.TypeNats.KnownNat (Torch.Typed.Lens.NamedIdx name shape) => Torch.Typed.Lens.HasName name shape
instance Torch.Tensor.TensorIndex [GHC.Maybe.Maybe GHC.Types.Int]

module Torch.Typed.Factories
zeros :: forall shape dtype device. TensorOptions shape dtype device => Tensor device dtype shape
full :: forall shape dtype device a. (TensorOptions shape dtype device, Scalar a) => a -> Tensor device dtype shape
ones :: forall shape dtype device. TensorOptions shape dtype device => Tensor device dtype shape
type family RandDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
rand :: forall shape dtype device. (TensorOptions shape dtype device, RandDTypeIsValid device dtype) => IO (Tensor device dtype shape)
randn :: forall shape dtype device. (TensorOptions shape dtype device, RandDTypeIsValid device dtype) => IO (Tensor device dtype shape)
randint :: forall shape dtype device. (TensorOptions shape dtype device, RandDTypeIsValid device dtype) => Int -> Int -> IO (Tensor device dtype shape)

-- | linspace &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t'
--   -&gt; D.asValue (toDynamic t') :: [Float]) $ linspace <tt>7 </tt>'(
--   'D.CPU, 0) 0 3 (Float,([7],[0.0,0.5,1.0,1.5,2.0,2.5,3.0]))
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t' -&gt;
--   D.asValue (toDynamic t') :: [Float]) $ linspace <tt>3 </tt>'( 'D.CPU,
--   0) 0 2 (Float,([3],[0.0,1.0,2.0]))
linspace :: forall steps device start end. (Scalar start, Scalar end, KnownNat steps, TensorOptions '[steps] 'Float device) => start -> end -> Tensor device 'Float '[steps]
eyeSquare :: forall n dtype device. (KnownNat n, TensorOptions '[n, n] dtype device) => Tensor device dtype '[n, n]
instance Torch.Typed.Tensor.TensorOptions shape dtype device => Data.Default.Class.Default (Torch.Typed.Tensor.Tensor device dtype shape)
instance (Torch.Typed.Tensor.TensorOptions shape' dtype device, shape' GHC.Types.~ Torch.Typed.Tensor.ToNats shape) => Data.Default.Class.Default (Torch.Typed.Tensor.NamedTensor device dtype shape)

module Torch.Typed.Functional

-- | Computes the bitwise NOT of the given input tensor. The input tensor
--   must be of integral or Boolean types. For bool tensors, it computes
--   the logical NOT.
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ bitwiseNot (ones :: CPUTensor 'D.Bool [3,3])
--   (Bool,[3,3])
--   </pre>
bitwiseNot :: forall device shape. Tensor device 'Bool shape -> Tensor device 'Bool shape

-- | Computes the element-wise logical NOT of the given input tensor. If
--   not specified, the output tensor will have the bool dtype. If the
--   input tensor is not a bool tensor, zeros are treated as False and
--   non-zeros are treated as True.
logicalNot :: forall device shape. Tensor device 'Bool shape -> Tensor device 'Bool shape
logicalXor :: forall device shape. Tensor device 'Bool shape -> Tensor device 'Bool shape -> Tensor device 'Bool shape
logicalAnd :: forall device shape. Tensor device 'Bool shape -> Tensor device 'Bool shape -> Tensor device 'Bool shape
logicalOr :: forall device shape. Tensor device 'Bool shape -> Tensor device 'Bool shape -> Tensor device 'Bool shape
type family SumDType (dtype :: DType) :: DType
type family SumDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | sumAll
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Int) $ sumAll (ones :: CPUTensor 'D.Bool '[2, 3])
--   (Int64,([],6))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Int) $ sumAll (ones :: CPUTensor 'D.UInt8 '[2, 3])
--   (Int64,([],6))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Int) $ sumAll (ones :: CPUTensor 'D.Int8 '[2, 3])
--   (Int64,([],6))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Int) $ sumAll (ones :: CPUTensor 'D.Int16 '[2, 3])
--   (Int64,([],6))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Int) $ sumAll (ones :: CPUTensor 'D.Int32 '[2, 3])
--   (Int64,([],6))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Int) $ sumAll (ones :: CPUTensor 'D.Int64 '[2, 3])
--   (Int64,([],6))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Float) $ sumAll (ones :: CPUTensor 'D.Float '[2, 3])
--   (Float,([],6.0))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: Double) $ sumAll (ones :: CPUTensor 'D.Double '[2, 3])
--   (Double,([],6.0))
--   </pre>
sumAll :: forall shape dtype' dtype device. (SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) => Tensor device dtype shape -> Tensor device dtype' '[]

-- | sumDim
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ sumDim @0 (ones :: CPUTensor 'D.Float '[3,4,5])
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; sumDim @1 (ones :: CPUTensor 'D.Float '[2,4])
--   Tensor Float [2] [ 4.0000   ,  4.0000   ]
--   </pre>
sumDim :: forall d shape shape' dtype dtype' device. (KnownNat d, shape' ~ DropValue shape d, SumDTypeIsValid device dtype, dtype' ~ SumDType dtype) => Tensor device dtype shape -> Tensor device dtype' shape'

-- | abs
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ abs (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
abs :: forall shape dtype device. StandardDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | ceil
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ ceil (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
ceil :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | floor
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ floor (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
floor :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape
type family MinMaxDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | min
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ min (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
min :: forall shape dtype device. (MinMaxDTypeIsValid device dtype, AllDimsPositive shape) => Tensor device dtype shape -> Tensor device dtype '[]

-- | max
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ max (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
max :: forall shape dtype device. (MinMaxDTypeIsValid device dtype, AllDimsPositive shape) => Tensor device dtype shape -> Tensor device dtype '[]
type family MeanDTypeValidation (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | Computes the mean while carrying out a full reduction of all tensor
--   dimensions.
--   
--   <pre>
--   &gt;&gt;&gt; meanAll (ones :: CPUTensor 'D.Float '[])
--   Tensor Float []  1.0000
--   
--   &gt;&gt;&gt; meanAll (zeros :: CPUTensor 'D.Float '[2,2])
--   Tensor Float []  0.0000
--   </pre>
meanAll :: forall shape dtype device. (MeanDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> Tensor device dtype '[]

-- | Computes the mean while carrying out a full reduction of all tensor
--   dimensions. This version is not restricted and can return NaN.
--   
--   <pre>
--   &gt;&gt;&gt; unsafeMeanAll (ones :: CPUTensor 'D.Float '[])
--   Tensor Float []  1.0000
--   
--   &gt;&gt;&gt; unsafeMeanAll (ones :: CPUTensor 'D.Float '[0])
--   Tensor Float [] NaN
--   
--   &gt;&gt;&gt; unsafeMeanAll (zeros :: CPUTensor 'D.Float '[2,2])
--   Tensor Float []  0.0000
--   </pre>
unsafeMeanAll :: forall shape dtype device. MeanDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype '[]

-- | Computes the mean and reduces the tensor over the specified dimension.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ meanDim @0 t
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ meanDim @1 t
--   (Float,[3,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ meanDim @2 t
--   (Float,[3,4])
--   </pre>
meanDim :: forall dim shape' shape dtype device. (KnownNat dim, shape' ~ DropValue shape dim, MeanDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> Tensor device dtype shape'

-- | Computes the mean and reduces the tensor over the specified dimension.
--   
--   <pre>
--   &gt;&gt;&gt; import Torch.Typed.Factories
--   
--   &gt;&gt;&gt; import Data.Default.Class
--   
--   &gt;&gt;&gt; t = def :: NamedTensor '( D.CPU, 0) 'D.Float '[Vector 3, Vector 4, Vector 5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ meanNamedDim @(Vector 4) t
--   (Float,[3,5])
--   </pre>
meanNamedDim :: forall dim shape' shape dtype device. (KnownNat (FindDim dim shape), shape' ~ DropNamedValue shape dim, MeanDTypeValidation device dtype) => NamedTensor device dtype shape -> NamedTensor device dtype shape'

-- | Computes the mean and optionally reduces the tensor over the specified
--   dimension.
--   
--   See <a>https://pytorch.org/docs/stable/torch.html#torch.mean</a> for
--   more information.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[5, 1], [3, 2], [4, 1], [2, 7]] :: CPUTensor 'D.Float '[4, 2]
--   
--   &gt;&gt;&gt; mean @0 @KeepDim t
--   Tensor Float [1,2] [[ 3.5000   ,  2.7500   ]]
--   </pre>
mean :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim, MeanDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> Tensor device dtype shape'

-- | Computes the median while carrying out a full reduction of all tensor
--   dimensions.
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ medianAll (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
medianAll :: forall shape dtype device. (StandardDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> Tensor device dtype '[]

-- | Computes the median and reduces the tensor over the specified
--   dimension.
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ medianDim @0 t
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ medianDim @1 t
--   (Float,[3,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ medianDim @2 t
--   (Float,[3,4])
--   </pre>
medianDim :: forall dim shape' shape dtype device. (KnownNat dim, shape' ~ DropValue shape dim, StandardDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> (Tensor device dtype shape', Tensor device 'Int64 shape')

-- | Computes the median and optionally reduces the tensor over the
--   specified dimension.
--   
--   See <a>https://pytorch.org/docs/stable/torch.html#torch.median</a> for
--   more information.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[5, 1], [3, 2], [4, 1], [2, 7]] :: CPUTensor 'D.Float '[4, 2]
--   </pre>
--   
--   <ul>
--   <li>- libtorch 1.7.0</li>
--   <li>- (Tensor Float [1,2] [[ 3.0000 , 1.0000 ]],Tensor Int64 [1,2] [[
--   1, 0]])</li>
--   <li>- libtorch 1.8.0 &gt;&gt;&gt; median <tt>0 </tt>KeepDim t (Tensor
--   Float [1,2] [[ 3.0000 , 1.0000 ]],Tensor Int64 [1,2] [[ 1, 2]])</li>
--   </ul>
median :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim, StandardDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> (Tensor device dtype shape', Tensor device 'Int64 shape')

-- | Returns a tuple '(modes, indices)' where <tt>modes</tt> is the mode
--   value of each row of the <tt>input</tt> tensor in the given dimension
--   <a>dim</a>, i.e. a value which appears most often in that row, and
--   <tt>indices</tt> is the index location of each mode value found.
--   
--   See <a>https://pytorch.org/docs/stable/torch.html#torch.mode</a> for
--   more information.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[0, 5], [0, 2], [3, 5]] :: CPUTensor 'D.Int64 '[3, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; (modes :: CPUTensor 'D.Int64 '[2], indices :: CPUTensor 'D.Int64 '[2]) = mode @0 @DropDim t
--   
--   &gt;&gt;&gt; (dtype modes, shape modes, D.asValue (toDynamic modes) :: [Int])
--   (Int64,[2],[0,5])
--   
--   &gt;&gt;&gt; (dtype indices, shape indices, D.asValue (toDynamic indices) :: [Int])
--   (Int64,[2],[1,2])
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[0, 0], [0, 1], [3, 3]] :: CPUTensor 'D.Float '[3, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; (modes :: CPUTensor 'D.Float '[3,1], indices :: CPUTensor 'D.Int64 '[3,1]) = mode @1 @KeepDim t
--   
--   &gt;&gt;&gt; (dtype modes, shape modes, D.asValue (toDynamic modes) :: [[Float]])
--   (Float,[3,1],[[0.0],[0.0],[3.0]])
--   
--   &gt;&gt;&gt; (dtype indices, shape indices, D.asValue (toDynamic indices) :: [[Int]])
--   (Int64,[3,1],[[1],[0],[1]])
--   </pre>
mode :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim, StandardDTypeValidation device dtype, AllDimsPositive shape) => Tensor device dtype shape -> (Tensor device dtype shape', Tensor device 'Int64 shape')

-- | addScalar TODO: what dtypes is this defined for? TODO: what scalar
--   types is this defined for?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ addScalar 1 (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
addScalar :: forall a shape dtype device. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | subScalar TODO: what dtypes is this defined for? TODO: what scalar
--   types is this defined for?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ subScalar 1 (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
subScalar :: forall a shape dtype device. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | mulScalar TODO: what dtypes is this defined for? TODO: what scalar
--   types is this defined for?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ mulScalar 2 (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
mulScalar :: forall a shape dtype device. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | divScalar TODO: what dtypes is this defined for? TODO: what scalar
--   types is this defined for?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ divScalar 2 (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   </pre>
divScalar :: forall a shape dtype device. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | powScalar TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ powScalar 2 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
powScalar :: forall a shape dtype device. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | erf
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ erf (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
erf :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | exp
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ exp (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
exp :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | log1p
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ log1p (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
log1p :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | log2 &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ log2 (ones ::
--   CPUTensor 'D.Float '[3,2]) (Float,[3,2])
log2 :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | log10
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ log10 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
log10 :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | pow this operation supports broadcasting TODO: probably only defined
--   for floating point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ pow (2 :: CPUTensor 'D.Float '[]) (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
pow :: forall shape'' shape shape' dtype device. (BasicArithmeticDTypeIsValid device dtype, shape'' ~ Broadcast shape shape') => Tensor device dtype shape -> Tensor device dtype shape' -> Tensor device dtype shape''

-- | relu activation function
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ relu (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
relu :: forall shape dtype device t. (StandardFloatingPointDTypeValidation device dtype, IsUnnamed t device dtype shape) => t -> t

-- | selu
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ selu (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
selu :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | mish <a>mish</a> is a smooth activation function, see
--   <a>https://arxiv.org/abs/1908.08681</a> for details.
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t -&gt; D.asValue (toDynamic t) :: [[Float]]) $ mish (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,([3,2],[[0.86509836,0.86509836],[0.86509836,0.86509836],[0.86509836,0.86509836]]))
--   </pre>
mish :: forall shape dtype device. (StandardFloatingPointDTypeValidation device dtype, BasicArithmeticDTypeIsValid device dtype, shape ~ Broadcast shape shape) => Tensor device dtype shape -> Tensor device dtype shape

-- | sigmoid
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ sigmoid (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
sigmoid :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | sin
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ sin (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
sin :: forall shape dtype device t. (StandardFloatingPointDTypeValidation device dtype, IsUnnamed t device dtype shape) => t -> t

-- | sinh
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ sinh (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
sinh :: forall shape dtype device t. (StandardFloatingPointDTypeValidation device dtype, IsUnnamed t device dtype shape) => t -> t

-- | cos
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ cos (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
cos :: forall shape dtype device t. (StandardFloatingPointDTypeValidation device dtype, IsUnnamed t device dtype shape) => t -> t

-- | sqrt
sqrt :: forall shape dtype device t. (StandardFloatingPointDTypeValidation device dtype, IsUnnamed t device dtype shape) => t -> t

-- | tanh
tanh :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | ConditionalReduction
--   
--   <pre>
--   &gt;&gt;&gt; :kind! ConditionalReduction '[3,2] ReduceNone
--   ConditionalReduction '[3,2] ReduceNone :: [Nat]
--   = '[3, 2]
--   
--   &gt;&gt;&gt; :kind! ConditionalReduction '[3,2] ReduceMean
--   ConditionalReduction '[3,2] ReduceMean :: [Nat]
--   = '[]
--   </pre>
type family ConditionalReduction (shape :: [Nat]) (reduction :: Reduction) :: [Nat]
class KnownReduction reduction
reductionVal :: KnownReduction reduction => Int

-- | binary cross entropy
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2,2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ binaryCrossEntropy @ReduceNone t t t
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ binaryCrossEntropy @ReduceMean t t t
--   (Float,[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ binaryCrossEntropy @ReduceSum t t t
--   (Float,[])
--   </pre>
binaryCrossEntropy :: forall (reduction :: Reduction) shape shape' dtype device. (KnownReduction reduction, shape' ~ ConditionalReduction shape reduction, StandardFloatingPointDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device dtype shape -> Tensor device dtype shape -> Tensor device dtype shape'

-- | mseLoss
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2,2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ mseLoss @ReduceNone t t
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ mseLoss @ReduceMean t t
--   (Float,[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ mseLoss @ReduceSum t t
--   (Float,[])
--   </pre>
mseLoss :: forall (reduction :: Reduction) shape shape' dtype device. (KnownReduction reduction, shape' ~ ConditionalReduction shape reduction, StandardFloatingPointDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device dtype shape -> Tensor device dtype shape'

-- | softmax
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2,2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ softmax @0 t
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ softmax @1 t
--   (Float,[2,2])
--   </pre>
softmax :: forall dim shape dtype device. (KnownNat dim, DimOutOfBoundCheck shape dim, KnownDType dtype, StandardFloatingPointDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device dtype shape

-- | logSoftmax
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2,2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ logSoftmax @0 t
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ logSoftmax @1 t
--   (Float,[2,2])
--   </pre>
logSoftmax :: forall dim shape dtype device. (KnownNat dim, DimOutOfBoundCheck shape dim, KnownDType dtype, StandardFloatingPointDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device dtype shape
type family Square (shape :: [Nat]) :: [Nat]
type family VectorOfSquare (shape :: [Nat]) :: [Nat]
type family FstSquareDim (shape :: [Nat]) :: Nat
type family InverseShapeIsValid (device :: (DeviceType, Nat)) (shape :: [Nat]) :: Constraint
type family InverseDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | inverse TODO: if rank &lt; n for any tensors in the batch, then this
--   will not work. we can't decide this statically, but we should prevent
--   runtime errors. therefore, return Maybe?
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- randn :: IO (CPUTensor 'D.Float '[3,2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ inverse t
--   (Float,[3,2,2])
--   
--   &gt;&gt;&gt; t &lt;- randn :: IO (CPUTensor 'D.Float '[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ inverse t
--   (Float,[2,2])
--   </pre>
inverse :: forall shape shape' dtype device. (shape' ~ Square shape, InverseShapeIsValid device shape, InverseDTypeIsValid device dtype) => Tensor device dtype shape -> Tensor device dtype shape'
type family SymeigDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | symeig Warning: torch.symeig is deprecated in favor of
--   torch.linalg.eigh and will be removed in a future PyTorch release. The
--   default behavior has changed from using the upper triangular portion
--   of the matrix by default to using the lower triangular portion. L, _ =
--   torch.symeig(A, upper=upper) should be replaced with L =
--   torch.linalg.eigvalsh(A, UPLO=<tt>U</tt> if upper else <tt>L</tt>) and
--   L, V = torch.symeig(A, eigenvectors=True) should be replaced with L, V
--   = torch.linalg.eigh(A, UPLO=<tt>U</tt> if upper else <tt>L</tt>)
--   (function operator())
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[3,2,2])
--   
--   &gt;&gt;&gt; (eigenVals,eigenVecs) = symeig Upper t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals -- Skip warning
--   ...
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; :t eigenVals
--   eigenVals :: Tensor '( 'D.CPU, 0) 'D.Float '[3, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVecs
--   (Float,[3,2,2])
--   
--   &gt;&gt;&gt; :t eigenVecs
--   eigenVecs :: Tensor '( 'D.CPU, 0) 'D.Float '[3, 2, 2]
--   
--   &gt;&gt;&gt; (eigenVals,eigenVecs) = symeig Lower t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVecs
--   (Float,[3,2,2])
--   </pre>
symeig :: forall shape shape' shape'' dtype device. (shape' ~ VectorOfSquare shape, shape'' ~ Square shape, SymeigDTypeIsValid device dtype) => Tri -> Tensor device dtype shape -> (Tensor device dtype shape', Tensor device dtype shape'')

-- | symeigvalues
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[3,2,2])
--   
--   &gt;&gt;&gt; eigenVals = symeigvalues Upper t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; :t eigenVals
--   eigenVals :: Tensor '( 'D.CPU, 0) 'D.Float '[3, 2]
--   </pre>
symeigvalues :: forall shape shape' dtype device. (shape' ~ VectorOfSquare shape, SymeigDTypeIsValid device dtype) => Tri -> Tensor device dtype shape -> Tensor device dtype shape'
data EigenVectors
EnableEigenVectors :: EigenVectors
DisableEigenVectors :: EigenVectors
class KnownEigenVectors a
enableEigenVectors :: KnownEigenVectors a => Bool
type family ConditionalEigenVectors (eigenvectors :: EigenVectors) (n :: Nat) :: [Nat]
type family EigDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | eig Warning: torch.eig is deprecated in favor of torch.linalg.eig and
--   will be removed in a future PyTorch release. torch.linalg.eig returns
--   complex tensors of dtype cfloat or cdouble rather than real tensors
--   mimicking complex tensors. L, _ = torch.eig(A) should be replaced with
--   L_complex = torch.linalg.eigvals(A) and L, V = torch.eig(A,
--   eigenvectors=True) should be replaced with L_complex, V_complex =
--   torch.linalg.eig(A) (function operator())
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[3,3])
--   
--   &gt;&gt;&gt; (eigenVals,eigenVecs) = eig @EnableEigenVectors t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals -- Skip warning
--   ...
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; :t eigenVals
--   eigenVals :: Tensor '( 'D.CPU, 0) 'D.Float '[3, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVecs
--   (Float,[3,3])
--   
--   &gt;&gt;&gt; :t eigenVecs
--   eigenVecs :: Tensor '( 'D.CPU, 0) 'D.Float '[3, 3]
--   
--   &gt;&gt;&gt; (eigenVals,eigenVecs) = eig @DisableEigenVectors t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVals
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ eigenVecs
--   (Float,[0])
--   
--   &gt;&gt;&gt; :t eigenVecs
--   eigenVecs :: Tensor '( 'D.CPU, 0) 'D.Float '[0]
--   </pre>
eig :: forall eigenvectors n shape dtype device. (KnownNat n, KnownEigenVectors eigenvectors, shape ~ ConditionalEigenVectors eigenvectors n, EigDTypeIsValid device dtype) => Tensor device dtype '[n, n] -> (Tensor device dtype '[n, 2], Tensor device dtype shape)
type family SVDShapes (shape :: [Nat]) (reduced :: ReducedSVD) :: ([Nat], [Nat], [Nat])
data ReducedSVD
ThinSVD :: ReducedSVD
FullSVD :: ReducedSVD
class KnownReducedSVD (reduced :: ReducedSVD)
reducedSVD :: KnownReducedSVD reduced => Bool
type family SVDDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | Singular Value Decomposition TODO: When <tt>compute_uv</tt> is
--   <a>False</a>, backward cannot be performed since <tt>u</tt> and
--   <tt>v</tt> from the forward pass are required for the backward
--   operation. There is no way to encode in the types at this point in
--   time. Thus, only <a>True</a> is supported currently.
--   
--   This function returns a tuple `(u, s, v)` which is the singular value
--   decomposition of a input real matrix or batches of real matrices input
--   such that `input = U×diag(S)×V^T`.
--   
--   <pre>
--   &gt;&gt;&gt; a &lt;- randn :: IO (CPUTensor 'D.Float '[3, 5])
--   
--   &gt;&gt;&gt; (u, s, v) = svd @'ThinSVD a
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ u
--   (Float,[3,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ s
--   (Float,[3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ v
--   (Float,[5,3])
--   
--   &gt;&gt;&gt; (u, s, v) = svd @'FullSVD a
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ u
--   (Float,[3,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ s
--   (Float,[3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ v
--   (Float,[5,5])
--   
--   &gt;&gt;&gt; a &lt;- randn :: IO (CPUTensor 'D.Float '[5, 3])
--   
--   &gt;&gt;&gt; (u, s, v) = svd @'ThinSVD a
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ u
--   (Float,[5,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ s
--   (Float,[3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ v
--   (Float,[3,3])
--   
--   &gt;&gt;&gt; (u, s, v) = svd @'FullSVD a
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ u
--   (Float,[5,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ s
--   (Float,[3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ v
--   (Float,[3,3])
--   </pre>
svd :: forall reduced shape shapeU shapeS shapeV dtype device. (KnownReducedSVD reduced, '(shapeU, shapeS, shapeV) ~ SVDShapes shape reduced, SVDDTypeIsValid device dtype) => Tensor device dtype shape -> (Tensor device dtype shapeU, Tensor device dtype shapeS, Tensor device dtype shapeV)
type family CholeskyDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | cholesky TODO: cholesky can throw if the input is not
--   positive-definite. Computes the Cholesky decomposition of a symmetric
--   positive-definite matrix. The operation supports batching.
--   
--   Warning: torch.cholesky is deprecated in favor of
--   torch.linalg.cholesky and will be removed in a future PyTorch release.
--   L = torch.cholesky(A) should be replaced with L =
--   torch.linalg.cholesky(A) and U = torch.cholesky(A, upper=True) should
--   be replaced with U = torch.linalg.cholesky(A.transpose(-2,
--   -1).conj()).transpose(-2, -1).conj() (function operator())
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[2,2])
--   
--   &gt;&gt;&gt; u = cholesky Upper (t `matmul` transpose2D t) -- Skip warning
--   ...
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ u
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; :t u
--   u :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 2]
--   </pre>
cholesky :: forall shape shape' dtype device. (shape' ~ Square shape, CholeskyDTypeIsValid device dtype) => Tri -> Tensor device dtype shape -> Tensor device dtype shape'

-- | choleskyInverse Computes the inverse of a symmetric positive-definite
--   matrix using its Cholesky factor, returned, e.g., by <a>cholesky</a>.
--   Unlike <a>cholesky</a>, this operation does not support batching. The
--   inverse is computed using the LAPACK routine `?potri`.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[2,2])
--   
--   &gt;&gt;&gt; tri = Upper
--   
--   &gt;&gt;&gt; u = cholesky tri (t `matmul` transpose2D t)
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ choleskyInverse tri u
--   (Float,[2,2])
--   </pre>
choleskyInverse :: forall n dtype device. (1 <= n, CholeskyDTypeIsValid device dtype) => Tri -> Tensor device dtype '[n, n] -> Tensor device dtype '[n, n]

-- | choleskySolve Solves the system of linear equations represented by `a
--   c = b` using the Cholesky factor matrix <tt>u</tt> of <tt>a</tt>
--   (returned, e.g., by <a>cholesky</a>), where <tt>a</tt> is a positive
--   semidefinite matrix. The operation supports batching.
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[3,3])
--   
--   &gt;&gt;&gt; a = t `matmul` transpose2D t
--   
--   &gt;&gt;&gt; b &lt;- rand :: IO (CPUTensor 'D.Float '[3,2])
--   
--   &gt;&gt;&gt; tri = Upper
--   
--   &gt;&gt;&gt; u = cholesky tri a
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ choleskySolve tri b u
--   (Float,[3,2])
--   </pre>
choleskySolve :: forall m_k m_m dtype device. (Square m_m ~ m_m, FstSquareDim m_m ~ FstSquareDim m_k, 1 <= FstSquareDim m_m, CholeskyDTypeIsValid device dtype) => Tri -> Tensor device dtype m_k -> Tensor device dtype m_m -> Tensor device dtype m_k
type family SolveDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | solve Solves the system of linear equations represented by `a c = b`
--   and also returns the LU decomposition of <tt>a</tt>. <tt>a</tt> has to
--   be a positive semidefinite matrix. The operation supports batching.
--   
--   Warning: torch.solve is deprecated in favor of torch.linalg.solveand
--   will be removed in a future PyTorch release. torch.linalg.solve has
--   its arguments reversed and does not return the LU factorization. To
--   get the LU factorization see torch.lu, which can be used with
--   torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution
--   should be replaced with X = torch.linalg.solve(A, B) (function
--   operator())
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- rand :: IO (CPUTensor 'D.Float '[10,10])
--   
--   &gt;&gt;&gt; a = t `matmul` transpose2D t
--   
--   &gt;&gt;&gt; b &lt;- rand :: IO (CPUTensor 'D.Float '[10,3])
--   
--   &gt;&gt;&gt; (c,lu) = solve b a
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ c -- Skip warning
--   ...
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ c
--   (Float,[10,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ lu
--   (Float,[10,10])
--   
--   &gt;&gt;&gt; :t c
--   c :: Tensor '( 'D.CPU, 0) 'D.Float '[10, 3]
--   
--   &gt;&gt;&gt; :t lu
--   lu :: Tensor '( 'D.CPU, 0) 'D.Float '[10, 10]
--   </pre>
solve :: forall m_k m_m dtype device. (Square m_m ~ m_m, FstSquareDim m_m ~ FstSquareDim m_k, 1 <= FstSquareDim m_m, SolveDTypeIsValid device dtype) => Tensor device dtype m_k -> Tensor device dtype m_m -> (Tensor device dtype m_k, Tensor device dtype m_m)

-- | geqrf TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted? <a>geqrf</a> computes a QR decomposition of
--   the given <tt>input</tt> matrix, but without constructing <tt>Q</tt>
--   and <tt>R</tt> as explicit separate matrices. Rather, this function
--   directly calls the underlying LAPACK function `?geqrf` which produces
--   a tuple `(a, tau)` of intermediate results as defined in the LAPACK
--   documentation for `?geqrf`.
--   
--   You can use <a>orgqr</a> on `(a, tau)` to compute the real orthogonal
--   matrix <tt>Q</tt>, but in general you may just want to use <tt>qr</tt>
--   instead.
--   
--   See the LAPACK documentation for `?geqrf` for further details,
--   <a>https://software.intel.com/en-us/node/521004</a>.
--   
--   <pre>
--   &gt;&gt;&gt; (a, tau) = geqrf (ones :: CPUTensor 'D.Float '[3,4])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ a
--   (Float,[3,4])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ tau
--   (Float,[3])
--   
--   &gt;&gt;&gt; (a, tau) = geqrf (ones :: CPUTensor 'D.Float '[4,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ a
--   (Float,[4,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ tau
--   (Float,[3])
--   </pre>
geqrf :: forall m n dtype device. Tensor device dtype '[m, n] -> (Tensor device dtype '[m, n], Tensor device dtype '[Min m n])

-- | orgqr TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted? Computes the orthogonal matrix <tt>Q</tt> of a
--   QR factorization from the `(a, tau)` tuple returned by <a>geqrf</a>.
--   
--   This directly calls the underlying LAPACK function `?orgqr`. See the
--   LAPACK documentation for `?orgqr` for further details,
--   <a>https://software.intel.com/en-us/mkl-developer-reference-c-orgqr</a>.
--   
--   When libtorch-1.7, this function behavior is changed. First dimention
--   should be greater than second dimention.
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ orgqr (ones :: CPUTensor 'D.Float '[4,3]) (ones :: CPUTensor 'D.Float '[3])
--   (Float,[4,3])
--   </pre>
orgqr :: forall m n dtype device. (KnownNat n, KnownNat m, n <= m) => Tensor device dtype '[m, n] -> Tensor device dtype '[n] -> Tensor device dtype '[m, n]

-- | sign works for all dtypes
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ sign (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
sign :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape
type family SetValue (shape :: [Nat]) (i :: Nat) (j :: Nat) :: [Nat]
type family GetValue (shape :: [Nat]) (i :: Nat) :: Nat

-- | Transpose
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Transpose '[3,2] 0 1
--   Transpose '[3,2] 0 1 :: [Nat]
--   = '[2, 3]
--   
--   &gt;&gt;&gt; :kind! Transpose '[3,2,1] 1 2
--   Transpose '[3,2,1] 1 2 :: [Nat]
--   = '[3, 1, 2]
--   </pre>
type family Transpose (shape :: [Nat]) (dim0 :: Nat) (dim1 :: Nat) :: [Nat]

-- | transpose See
--   "..<i>..</i>..<i>..</i>deps<i>pytorch</i>aten<i>src</i>ATen<i>native</i>TensorShape.cpp".
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ transpose @0 @1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[2,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ transpose @0 @1 (ones :: CPUTensor 'D.Float '[3,2,1])
--   (Float,[2,3,1])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ transpose @1 @2 (ones :: CPUTensor 'D.Float '[3,2,1])
--   (Float,[3,1,2])
--   </pre>
transpose :: forall n m shape shape' dtype device. (KnownNat n, KnownNat m, shape' ~ Transpose shape n m) => Tensor device dtype shape -> Tensor device dtype shape'

-- | transpose2d, special case for a 2D tensor
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ transpose2D (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[2,3])
--   </pre>
transpose2D :: forall (i :: Nat) (j :: Nat) dtype device. Tensor device dtype '[i, j] -> Tensor device dtype '[j, i]
class KnownTri (tri :: Tri)
triVal :: KnownTri tri => Tri
type family DiagSize (tri :: Tri) (index :: Nat) (m :: Nat) (n :: Nat) :: Nat
type family DiagShape (tri :: Tri) (index :: Nat) (shape :: [Nat]) :: [Nat]

-- | diag
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diag @'Upper @0 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diag @'Upper @1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[1])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diag @'Lower @1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[2])
--   </pre>
diag :: forall tri index shape shape' device dtype. (KnownTri tri, KnownNat index, StandardDTypeValidation device dtype, shape' ~ DiagShape tri index shape) => Tensor device dtype shape -> Tensor device dtype shape'

-- | all See
--   <a>https://pytorch.org/docs/stable/tensors.html#torch.BoolTensor.all</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = all (fromJust [False, False] :: CPUTensor 'D.Bool '[2])
--   
--   &gt;&gt;&gt; toInt t == 1
--   False
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; t = all (fromJust [False, True] :: CPUTensor 'D.Bool '[2])
--   
--   &gt;&gt;&gt; toInt t == 1
--   False
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; t = all (fromJust [True, True] :: CPUTensor 'D.Bool '[2])
--   
--   &gt;&gt;&gt; toInt t == 1
--   True
--   </pre>
all :: forall shape device. Tensor device 'Bool shape -> Tensor device 'Bool '[]

-- | any See
--   <a>https://pytorch.org/docs/stable/tensors.html#torch.BoolTensor.any</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = any (fromJust [False, False] :: CPUTensor 'D.Bool '[2])
--   
--   &gt;&gt;&gt; toInt t == 1
--   False
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; t = any (fromJust [False, True] :: CPUTensor 'D.Bool '[2])
--   
--   &gt;&gt;&gt; toInt t == 1
--   True
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; t = any (fromJust [True, True] :: CPUTensor 'D.Bool '[2])
--   
--   &gt;&gt;&gt; toInt t == 1
--   True
--   </pre>
any :: forall shape device. Tensor device 'Bool shape -> Tensor device 'Bool '[]
data KeepOrDropDim
KeepDim :: KeepOrDropDim
DropDim :: KeepOrDropDim
class KnownKeepOrDropDim keepOrDropDim
keepOrDropDimVal :: KnownKeepOrDropDim keepOrDropDim => Bool
type family ConditionalDropDimension (shape :: [Nat]) (dim :: Nat) (keepOrDropDim :: KeepOrDropDim) :: [Nat]

-- | allDim See
--   <a>https://pytorch.org/docs/stable/tensors.html#torch.BoolTensor.all</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[True, True], [True, False], [True, True], [True, True]] :: CPUTensor 'D.Bool '[4, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Bool]) $ allDim @1 @DropDim t
--   (Bool,([4],[True,False,True,True]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Bool]]) $ allDim @1 @KeepDim t
--   (Bool,([4,1],[[True],[False],[True],[True]]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Bool]) $ allDim @0 @DropDim t
--   (Bool,([2],[True,False]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Bool]]) $ allDim @0 @KeepDim t
--   (Bool,([1,2],[[True,False]]))
--   </pre>
allDim :: forall dim keepOrDropDim shape' shape device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim) => Tensor device 'Bool shape -> Tensor device 'Bool shape'

-- | anyDim See
--   <a>https://pytorch.org/docs/stable/tensors.html#torch.BoolTensor.any</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[True, True], [True, False], [True, True], [True, True]] :: CPUTensor 'D.Bool '[4, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Bool]) $ anyDim @1 @DropDim t
--   (Bool,([4],[True,True,True,True]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Bool]]) $ anyDim @1 @KeepDim t
--   (Bool,([4,1],[[True],[True],[True],[True]]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Bool]) $ anyDim @0 @DropDim t
--   (Bool,([2],[True,True]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Bool]]) $ anyDim @0 @KeepDim t
--   (Bool,([1,2],[[True,True]]))
--   </pre>
anyDim :: forall dim keepOrDropDim shape' shape device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim) => Tensor device 'Bool shape -> Tensor device 'Bool shape'

-- | dropout TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: get rid of IO by exposing the RNG
--   state TODO: can we use D.Scalar for the dropout probability?
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,2]
--   
--   &gt;&gt;&gt; t' &lt;- dropout 0.5 False t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t'
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; t'' &lt;- dropout 0.5 False t
--   
--   &gt;&gt;&gt; t ==. t''
--   Tensor Bool [3,2] [[ 1,  1],
--                      [ 1,  1],
--                      [ 1,  1]]
--   
--   &gt;&gt;&gt; t''' &lt;- dropout 0.0 True t
--   
--   &gt;&gt;&gt; t ==. t'''
--   Tensor Bool [3,2] [[ 1,  1],
--                      [ 1,  1],
--                      [ 1,  1]]
--   
--   &gt;&gt;&gt; t'''' &lt;- dropout 1.0 True t
--   
--   &gt;&gt;&gt; t''''
--   Tensor Float [3,2] [[ 0.0000,  0.0000],
--                       [ 0.0000,  0.0000],
--                       [ 0.0000,  0.0000]]
--   </pre>
dropout :: forall shape dtype device. Double -> Bool -> Tensor device dtype shape -> IO (Tensor device dtype shape)

-- | featureDropout TODO: probably only defined for floating point tensors,
--   or maybe numeric type is lifted? TODO: why not IO? TODO: can we use
--   D.Scalar for the dropout probability?
--   
--   <pre>
--   &gt;&gt;&gt; c = featureDropout 0.1 True (ones :: CPUTensor 'D.Float '[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ c
--   (Float,[2,2])
--   </pre>
featureDropout :: forall shape dtype device. Double -> Bool -> Tensor device dtype shape -> Tensor device dtype shape

-- | alphaDropout TODO: probably only defined for floating point tensors,
--   or maybe numeric type is lifted? TODO: why not IO? TODO: can we use
--   D.Scalar for the dropout probability?
--   
--   <pre>
--   &gt;&gt;&gt; c = alphaDropout 0.1 True (ones :: CPUTensor 'D.Float '[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ c
--   (Float,[2,2])
--   </pre>
alphaDropout :: forall shape dtype device. Double -> Bool -> Tensor device dtype shape -> Tensor device dtype shape

-- | featureAlphaDropout TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted? TODO: why not IO? TODO: can
--   we use D.Scalar for the dropout probability?
--   
--   <pre>
--   &gt;&gt;&gt; c = featureAlphaDropout 0.1 True (ones :: CPUTensor 'D.Float '[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ c
--   (Float,[2,2])
--   </pre>
featureAlphaDropout :: forall shape dtype device. Double -> Bool -> Tensor device dtype shape -> Tensor device dtype shape

-- | acos
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ acos (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
acos :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | avgPool1d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = avgPool1d @1 @1 @0 (ones :: CPUTensor 'D.Float '[1,3,4])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,4]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 4]
--   </pre>
avgPool1d :: forall kernelSize stride padding channelSize inputSize batchSize outputSize dtype device. (All KnownNat '[kernelSize, stride, padding, channelSize, inputSize, batchSize], ConvSideCheck inputSize kernelSize stride padding outputSize) => Tensor device dtype '[batchSize, channelSize, inputSize] -> Tensor device dtype '[batchSize, channelSize, outputSize]

-- | adaptiveAvgPool1d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = adaptiveAvgPool1d @8 (ones :: CPUTensor 'D.Float '[1,3,16])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,8]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8]
--   </pre>
adaptiveAvgPool1d :: forall outputSize channelSize inputSize batchSize dtype device. All KnownNat '[channelSize, inputSize, batchSize, outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize] -> Tensor device dtype '[batchSize, channelSize, outputSize]

-- | adaptiveMaxPool1d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; tt = adaptiveMaxPool1d @8 (ones :: CPUTensor 'D.Float '[1,3,16])
--   
--   &gt;&gt;&gt; shape . fst $ tt
--   [1,3,8]
--   
--   &gt;&gt;&gt; :t tt
--   tt
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8],
--         Tensor '( 'D.CPU, 0) 'D.Int64 '[1, 3, 8])
--   </pre>
adaptiveMaxPool1d :: forall outputSize channelSize inputSize batchSize dtype device. All KnownNat '[channelSize, inputSize, batchSize, outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize] -> (Tensor device dtype '[batchSize, channelSize, outputSize], Tensor device 'Int64 '[batchSize, channelSize, outputSize])

-- | addmv TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted? TODO: can we use D.Scalar for beta and alpha?
--   
--   <pre>
--   &gt;&gt;&gt; t = addmv 1 1 (ones :: CPUTensor 'D.Float '[3,2]) (zeros :: CPUTensor 'D.Float '[2]) (ones :: CPUTensor 'D.Float '[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[3])
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[3]
--   </pre>
addmv :: forall shape' shape n m dtype device. (KnownNat n, KnownNat m, shape' ~ Broadcast shape '[n]) => Float -> Float -> Tensor device dtype '[n, m] -> Tensor device dtype '[m] -> Tensor device dtype shape -> Tensor device dtype shape'

-- | allclose
--   
--   <pre>
--   &gt;&gt;&gt; allclose 0.1 0.1 True (ones :: CPUTensor 'D.Float '[3,3]) (ones :: CPUTensor 'D.Float '[3,3])
--   True
--   </pre>
allclose :: forall shape dtype device. Double -> Double -> Bool -> Tensor device dtype shape -> Tensor device dtype shape -> Bool

-- | argmax See
--   <a>https://pytorch.org/docs/stable/torch.html#torch.argmax</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[0, 1], [-1, 2], [0, 1], [0, -2]] :: CPUTensor 'D.Float '[4, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Int]) $ argmax @1 @DropDim t
--   (Int64,([4],[1,1,1,0]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Int]]) $ argmax @1 @KeepDim t
--   (Int64,([4,1],[[1],[1],[1],[0]]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Int]) $ argmax @0 @DropDim t
--   (Int64,([2],[0,1]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Int]]) $ argmax @0 @KeepDim t
--   (Int64,([1,2],[[0,1]]))
--   </pre>
argmax :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim, StandardDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device 'Int64 shape'

-- | argmin See
--   <a>https://pytorch.org/docs/stable/torch.html#torch.argmin</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[0, 1], [-1, 2], [0, 1], [0, -2]] :: CPUTensor 'D.Float '[4, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Int]) $ argmin @1 @DropDim t
--   (Int64,([4],[0,0,0,1]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Int]]) $ argmin @1 @KeepDim t
--   (Int64,([4,1],[[0],[0],[0],[1]]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Int]) $ argmin @0 @DropDim t
--   (Int64,([2],[1,3]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Int]]) $ argmin @0 @KeepDim t
--   (Int64,([1,2],[[1,3]]))
--   </pre>
argmin :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim, StandardDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device 'Int64 shape'

-- | asin
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ asin (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
asin :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | atan
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ atan (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
atan :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | baddbmm TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = baddbmm 1 1 (ones :: CPUTensor 'D.Float '[5,3,2]) (zeros :: CPUTensor 'D.Float '[5,2,4]) (ones :: CPUTensor 'D.Float '[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[5,3,4])
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 3, 4]
--   </pre>
baddbmm :: forall shape' shape batchSize n m k dtype device. (KnownNat n, KnownNat m, KnownNat k, shape' ~ Broadcast shape '[batchSize, n, m]) => Float -> Float -> Tensor device dtype '[batchSize, n, k] -> Tensor device dtype '[batchSize, k, m] -> Tensor device dtype shape -> Tensor device dtype shape'

-- | batched matrix multiplication TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ bmm (ones :: CPUTensor 'D.Float '[5,3,2]) (zeros :: CPUTensor 'D.Float '[5,2,4])
--   (Float,[5,3,4])
--   </pre>
bmm :: forall batchSize n m k dtype device. Tensor device dtype '[batchSize, n, k] -> Tensor device dtype '[batchSize, k, m] -> Tensor device dtype '[batchSize, n, m]

-- | BroadcastTensorsImpl
--   
--   <pre>
--   &gt;&gt;&gt; type Ty = BroadcastTensorsImpl '[] 'Nothing
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: Maybe ([Nat], D.DType, (D.DeviceType, Nat))
--   = 'Nothing
--   
--   &gt;&gt;&gt; type Ty = BroadcastTensorsImpl '[Tensor '( 'D.CPU, 0) 'D.Float '[1, 3], Tensor '( 'D.CPU, 0) 'D.Float '[2, 1]] 'Nothing
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: Maybe ([Nat], D.DType, (D.DeviceType, Nat))
--   = 'Just '( '[2, 3], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = BroadcastTensorsImpl '[Tensor '( 'D.CPU, 0) 'D.Float '[1, 3], Tensor '( 'D.CPU, 0) 'D.Float '[2, 1], Tensor '( 'D.CPU, 0) 'D.Float '[5, 1, 1]] 'Nothing
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: Maybe ([Nat], D.DType, (D.DeviceType, Nat))
--   = 'Just '( '[5, 2, 3], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = BroadcastTensorsImpl '[Tensor '( 'D.CPU, 0) 'D.Float '[1, 3], Tensor '( 'D.CPU, 0) 'D.Float '[2, 1], Tensor '( 'D.CPU, 0) 'D.Float '[1, 5, 1]] 'Nothing
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: Maybe ([Nat], D.DType, (D.DeviceType, Nat))
--   = 'Nothing
--   </pre>
type family BroadcastTensorsImpl (tensors :: [a]) (acc :: Maybe ([Nat], DType, (DeviceType, Nat))) :: Maybe ([Nat], DType, (DeviceType, Nat))
type family BroadcastTensorsCheck (tensors :: [a]) (result :: Maybe ([Nat], DType, (DeviceType, Nat))) :: [a]
type BroadcastTensors tensors = BroadcastTensorsCheck tensors (BroadcastTensorsImpl tensors 'Nothing)

-- | broadcast tensors TODO: broadcastTensors returns garbage data and is
--   hence broken See
--   <a>https://pytorch.org/docs/stable/_modules/torch/functional.html#broadcast_tensors</a>.
--   
--   <pre>
--   &gt;&gt;&gt; x = ones :: CPUTensor 'D.Float '[1, 3]
--   
--   &gt;&gt;&gt; y = ones :: CPUTensor 'D.Float '[2, 1]
--   
--   &gt;&gt;&gt; z = ones :: CPUTensor 'D.Float '[5, 1, 1]
--   </pre>
--   
--   <ul>
--   <li>- &gt;&gt;&gt; x' :. y' :. z' :. HNil = broadcastTensors (x :. y
--   :. z :. HNil)</li>
--   <li>- &gt;&gt;&gt; :type x'</li>
--   <li>- x' :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 2, 3]</li>
--   <li>- &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t
--   -&gt; D.asValue (toDynamic t) :: [[[Float]]]) $ x'</li>
--   <li>- &gt;&gt;&gt; :type y'</li>
--   <li>- y' :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 2, 3]</li>
--   <li>- &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t
--   -&gt; D.asValue (toDynamic t) :: [[[Float]]]) $ y'</li>
--   <li>- &gt;&gt;&gt; :type z'</li>
--   <li>- z' :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 2, 3]</li>
--   <li>- &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t
--   -&gt; D.asValue (toDynamic t) :: [[[Float]]]) $ z'</li>
--   </ul>
broadcastTensors :: forall tensors tensors'. (tensors' ~ BroadcastTensors tensors, Castable (HList tensors) [ATenTensor], Castable (HList tensors') [ATenTensor]) => HList tensors -> HList tensors'
type family CatImpl (dim :: Nat) (tensors :: [a]) (acc :: Maybe ([Nat], DType, (DeviceType, Nat))) :: Maybe ([Nat], DType, (DeviceType, Nat))
type family ComputeCatShape (dim :: Nat) (shape :: [Nat]) (acc :: Maybe ([Nat], DType, (DeviceType, Nat))) :: Maybe [Nat]
type family ComputeCatDType (dtype :: DType) (acc :: Maybe ([Nat], DType, (DeviceType, Nat))) :: Maybe DType
type family ComputeCatDevice (device :: (DeviceType, Nat)) (acc :: Maybe ([Nat], DType, (DeviceType, Nat))) :: Maybe (DeviceType, Nat)
type family CatCheck (res :: Maybe ([Nat], DType, (DeviceType, Nat))) :: ([Nat], DType, (DeviceType, Nat))

-- | Cat
--   
--   <pre>
--   &gt;&gt;&gt; type Ty = Cat 0 '[Tensor '( 'D.CPU, 0) 'D.Float '[1]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[1], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Cat 0 '[Tensor '( 'D.CPU, 0) 'D.Float '[1], Tensor '( 'D.CPU, 0) 'D.Float '[2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[3], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Cat 0 '[Tensor '( 'D.CPU, 0) 'D.Float '[1, 3], Tensor '( 'D.CPU, 0) 'D.Float '[2, 3]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[3, 3], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Cat 1 '[Tensor '( 'D.CPU, 0) 'D.Float '[3, 1], Tensor '( 'D.CPU, 0) 'D.Float '[3, 2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[3, 3], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Cat 1 '[Tensor '( 'D.CPU, 0) 'D.Float '[2, 5, 4, 2], Tensor '( 'D.CPU, 0) 'D.Float '[2, 1, 4, 2], Tensor '( 'D.CPU, 0) 'D.Float '[2, 3, 4, 2], Tensor '( 'D.CPU, 0) 'D.Float '[2, 1, 4, 2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[2, 10, 4, 2], 'D.Float, '( 'D.CPU, 0))
--   </pre>
type Cat dim tensors = CatCheck (CatImpl dim tensors Nothing)

-- | cat
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2,2]
--   
--   &gt;&gt;&gt; t' = cat @0 (t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'
--   (Float,([2,2],[[1.0,1.0],[1.0,1.0]]))
--   
--   &gt;&gt;&gt; t' = cat @1 (t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'
--   (Float,([2,2],[[1.0,1.0],[1.0,1.0]]))
--   
--   &gt;&gt;&gt; t' = cat @0 (t :. t :. t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[6, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'
--   (Float,([6,2],[[1.0,1.0],[1.0,1.0],[1.0,1.0],[1.0,1.0],[1.0,1.0],[1.0,1.0]]))
--   
--   &gt;&gt;&gt; t' = cat @1 (t :. t :. t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 6]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'
--   (Float,([2,6],[[1.0,1.0,1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0,1.0,1.0]]))
--   </pre>
cat :: forall dim shape dtype device tensors. (KnownNat dim, '(shape, dtype, device) ~ Cat dim tensors, Castable (HList tensors) [ATenTensor]) => HList tensors -> Tensor device dtype shape
type family ChunkImpl (chunkShapes :: Maybe [[Nat]]) (dtype :: DType) (device :: (DeviceType, Nat)) :: Maybe a
type family ChunkCheck (shape :: [Nat]) (dim :: Nat) (result :: Maybe a) :: a
type family ComputeChunksChunkGo (n' :: Nat) (r :: Nat) (cmp :: Ordering) (cmp' :: Ordering) :: [Nat]
type family ComputeChunksChunkGo0 (n' :: Nat) (chunks :: Nat) :: [Nat]
type family ComputeChunks (n :: Maybe Nat) (chunks :: Nat) :: Maybe [Nat]
type family ComputeChunks' (n :: Nat) (chunks :: Nat) (m :: Nat) :: [Nat]
type family ChunkShapesImpl (chunks :: Maybe [Nat]) (dim :: Nat) (shape :: [Nat]) :: Maybe [[Nat]]
type ChunkShapes chunks dim shape = ChunkShapesImpl (ComputeChunks (ExtractDim dim shape) chunks) dim shape
type Chunk chunks dim shape dtype device = ChunkCheck shape dim (ChunkImpl (ChunkShapes chunks dim shape) dtype device)

-- | chunk
--   
--   <ul>
--   <li>- &gt;&gt;&gt; :type chunk <tt>3 </tt>1 (ones :: CPUTensor
--   'D.Float '[2, 2])</li>
--   <li>- chunk <tt>3 </tt>1 (ones :: CPUTensor 'D.Float '[2, 2])</li>
--   <li>- :: HList</li>
--   <li>- '[Tensor '( 'D.CPU, 0) 'D.Float '[2, 1],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[2, 1]] &gt;&gt;&gt; t0 :. t1 :.
--   HNil = chunk <tt>3 </tt>1 (ones :: CPUTensor 'D.Float '[2, 2])
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t0 (Float,[2,1])
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t1 (Float,[2,1])</li>
--   <li>- &gt;&gt;&gt; :type chunk <tt>3 </tt>1 (ones :: CPUTensor
--   'D.Float '[1, 0, 3])</li>
--   <li>- chunk <tt>3 </tt>1 (ones :: CPUTensor 'D.Float '[1, 0, 3])</li>
--   <li>- :: HList</li>
--   <li>- '[Tensor '( 'D.CPU, 0) 'D.Float '[1, 0, 3],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[1, 0, 3],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[1, 0, 3]] &gt;&gt;&gt; t0 :. t1
--   :. t2 :. HNil = chunk <tt>3 </tt>1 (ones :: CPUTensor 'D.Float '[1, 0,
--   3]) &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t0 (Float,[1,0,3])
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t1 (Float,[1,0,3])
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t2 (Float,[1,0,3])</li>
--   <li>- &gt;&gt;&gt; :type chunk <tt>6 </tt>0 (ones :: CPUTensor
--   'D.Float '[19, 4])</li>
--   <li>- chunk <tt>6 </tt>0 (ones :: CPUTensor 'D.Float '[19, 4])</li>
--   <li>- :: HList</li>
--   <li>- '[Tensor '( 'D.CPU, 0) 'D.Float '[4, 4],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[4, 4],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[4, 4],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[4, 4],</li>
--   <li>- Tensor '( 'D.CPU, 0) 'D.Float '[3, 4]] &gt;&gt;&gt; t0 :. t1 :.
--   t2 :. t3 :. t4 :. HNil = chunk <tt>6 </tt>0 (ones :: CPUTensor
--   'D.Float '[19, 4]) &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t0
--   (Float,[4,4]) &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t1
--   (Float,[4,4]) &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t2
--   (Float,[4,4]) &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t3
--   (Float,[4,4]) &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t4
--   (Float,[3,4])</li>
--   </ul>
chunk :: forall chunks dim shape dtype device tensorChunks. (KnownNat chunks, KnownNat dim, tensorChunks ~ Chunk chunks dim shape dtype device, Castable (HList tensorChunks) [ATenTensor]) => Tensor device dtype shape -> HList tensorChunks

-- | clamp TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted? TODO: can we use D.Scalar for the minimum and
--   maximum values?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ clamp 0 1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
clamp :: forall shape dtype device a. Scalar a => a -> a -> Tensor device dtype shape -> Tensor device dtype shape

-- | clampMax TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: can we use D.Scalar for the
--   maximum value?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ clampMax 1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
clampMax :: forall shape dtype device a. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | clampMin TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: can we use D.Scalar for the
--   minimum value?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ clampMin 0 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
clampMin :: forall shape dtype device a. Scalar a => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | cudnnIsAcceptable TODO: calling this probably makes only sense when
--   the device is CUDA
cudnnIsAcceptable :: forall shape dtype device. Tensor device dtype shape -> Bool
constantPadNd1d :: forall (pad :: (Nat, Nat)) n dtype device. All KnownNat '[Fst pad, Snd pad, n] => Float -> Tensor device dtype '[n] -> Tensor device dtype '[(n + Fst pad) + Snd pad]
type ConvSideCheck (inputSize :: Nat) (kernelSize :: Nat) (stride :: Nat) (padding :: Nat) (outputSize :: Nat) = (1 <= kernelSize, 1 <= stride, (kernelSize - 1) <= (inputSize + (2 * padding)), 1 <= outputSize, outputSize ~ ConvOutputSize inputSize kernelSize stride padding)

-- | ConvOutputSize
--   
--   <pre>
--   &gt;&gt;&gt; :kind! ConvOutputSize 4 1 1 0
--   ConvOutputSize 4 1 1 0 :: Nat
--   = 4
--   </pre>
type family ConvOutputSize (inputSize :: Nat) (kernelSize :: Nat) (stride :: Nat) (padding :: Nat) :: Nat

-- | conv1d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = conv1d @1 @0 (ones :: CPUTensor 'D.Float '[10, 3, 1]) (ones :: CPUTensor 'D.Float '[10]) (ones :: CPUTensor 'D.Float '[1, 3, 4])
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 10, 4]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[Float]]]) $ t
--   (Float,([1,10,4],[[[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0]]]))
--   </pre>
conv1d :: forall (stride :: Nat) (padding :: Nat) inputChannelSize outputChannelSize kernelSize inputSize batchSize outputSize dtype device. (All KnownNat '[stride, padding, inputChannelSize, outputChannelSize, kernelSize, inputSize, batchSize, outputSize], ConvSideCheck inputSize kernelSize stride padding outputSize) => Tensor device dtype '[outputChannelSize, inputChannelSize, kernelSize] -> Tensor device dtype '[outputChannelSize] -> Tensor device dtype '[batchSize, inputChannelSize, inputSize] -> Tensor device dtype '[batchSize, outputChannelSize, outputSize]

-- | conv2d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = conv2d @'(1, 1) @'(0, 0) (ones :: CPUTensor 'D.Float '[10, 3, 1, 1]) (ones :: CPUTensor 'D.Float '[10]) (ones :: CPUTensor 'D.Float '[1, 3, 4, 5])
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 10, 4, 5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[[Float]]]]) $ t
--   (Float,([1,10,4,5],[[[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]]]]))
--   </pre>
conv2d :: forall (stride :: (Nat, Nat)) (padding :: (Nat, Nat)) inputChannelSize outputChannelSize kernelSize0 kernelSize1 inputSize0 inputSize1 batchSize outputSize0 outputSize1 dtype device. (All KnownNat '[Fst stride, Snd stride, Fst padding, Snd padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, inputSize0, inputSize1, batchSize, outputSize0, outputSize1], ConvSideCheck inputSize0 kernelSize0 (Fst stride) (Fst padding) outputSize0, ConvSideCheck inputSize1 kernelSize1 (Snd stride) (Snd padding) outputSize1) => Tensor device dtype '[outputChannelSize, inputChannelSize, kernelSize0, kernelSize1] -> Tensor device dtype '[outputChannelSize] -> Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1]

-- | conv3d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = conv3d @'(1, 1, 1) @'(0, 0, 0) (ones :: CPUTensor 'D.Float '[10, 3, 1, 1, 1]) (ones :: CPUTensor 'D.Float '[10]) (ones :: CPUTensor 'D.Float '[1, 3, 4, 5, 6])
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 10, 4, 5, 6]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[[[Float]]]]]) $ t
--   (Float,([1,10,4,5,6],[[[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]]]]))
--   </pre>
conv3d :: forall (stride :: (Nat, Nat, Nat)) (padding :: (Nat, Nat, Nat)) inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 inputSize0 inputSize1 inputSize2 batchSize outputSize0 outputSize1 outputSize2 dtype device. (All KnownNat '[Fst3 stride, Snd3 stride, Trd3 stride, Fst3 padding, Snd3 padding, Trd3 padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, kernelSize2, inputSize0, inputSize1, inputSize2, batchSize], ConvSideCheck inputSize0 kernelSize0 (Fst3 stride) (Fst3 padding) outputSize0, ConvSideCheck inputSize1 kernelSize1 (Snd3 stride) (Snd3 padding) outputSize1, ConvSideCheck inputSize2 kernelSize2 (Trd3 stride) (Trd3 padding) outputSize2) => Tensor device dtype '[outputChannelSize, inputChannelSize, kernelSize0, kernelSize1, kernelSize2] -> Tensor device dtype '[outputChannelSize] -> Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1, inputSize2] -> Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1, outputSize2]

-- | convTBC TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? 1D convolution over an input of shape
--   `[timeSize, batchSize, inputChannels]`.
convTBC :: forall padding timeSize batchSize kernelSize inputChannels outputChannels dtype device. KnownNat padding => Tensor device dtype '[kernelSize, inputChannels, outputChannels] -> Tensor device dtype '[outputChannels] -> Tensor device dtype '[timeSize, batchSize, inputChannels] -> Tensor device dtype '[((timeSize + (padding * 2)) + 1) - kernelSize, batchSize, outputChannels]

-- | convTranspose1d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = convTranspose1d @1 @0 (ones :: CPUTensor 'D.Float '[3, 10, 1]) (ones :: CPUTensor 'D.Float '[10]) (ones :: CPUTensor 'D.Float '[1, 3, 4])
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 10, 4]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[Float]]]) $ t
--   (Float,([1,10,4],[[[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0]]]))
--   </pre>
convTranspose1d :: forall (stride :: Nat) (padding :: Nat) inputChannelSize outputChannelSize kernelSize inputSize batchSize outputSize dtype device. (All KnownNat '[stride, padding, inputChannelSize, outputChannelSize, kernelSize, inputSize, batchSize, outputSize], ConvSideCheck inputSize kernelSize stride padding outputSize) => Tensor device dtype '[inputChannelSize, outputChannelSize, kernelSize] -> Tensor device dtype '[outputChannelSize] -> Tensor device dtype '[batchSize, inputChannelSize, inputSize] -> Tensor device dtype '[batchSize, outputChannelSize, outputSize]

-- | convTranspose2d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = convTranspose2d @'(1, 1) @'(0, 0) (ones :: CPUTensor 'D.Float '[3, 10, 1, 1]) (ones :: CPUTensor 'D.Float '[10]) (ones :: CPUTensor 'D.Float '[1, 3, 4, 5])
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 10, 4, 5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[[Float]]]]) $ t
--   (Float,([1,10,4,5],[[[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0]]]]))
--   </pre>
convTranspose2d :: forall (stride :: (Nat, Nat)) (padding :: (Nat, Nat)) inputChannelSize outputChannelSize kernelSize0 kernelSize1 inputSize0 inputSize1 batchSize outputSize0 outputSize1 dtype device. (All KnownNat '[Fst stride, Snd stride, Fst padding, Snd padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, inputSize0, inputSize1, batchSize, outputSize0, outputSize1], ConvSideCheck inputSize0 kernelSize0 (Fst stride) (Fst padding) outputSize0, ConvSideCheck inputSize1 kernelSize1 (Snd stride) (Snd padding) outputSize1) => Tensor device dtype '[inputChannelSize, outputChannelSize, kernelSize0, kernelSize1] -> Tensor device dtype '[outputChannelSize] -> Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1]

-- | convTranspose3d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = convTranspose3d @'(1, 1, 1) @'(0, 0, 0) (ones :: CPUTensor 'D.Float '[3, 10, 1, 1, 1]) (ones :: CPUTensor 'D.Float '[10]) (ones :: CPUTensor 'D.Float '[1, 3, 4, 5, 6])
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 10, 4, 5, 6]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[[[Float]]]]]) $ t
--   (Float,([1,10,4,5,6],[[[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]],[[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]],[[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0],[4.0,4.0,4.0,4.0,4.0,4.0]]]]]))
--   </pre>
convTranspose3d :: forall (stride :: (Nat, Nat, Nat)) (padding :: (Nat, Nat, Nat)) inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 inputSize0 inputSize1 inputSize2 batchSize outputSize0 outputSize1 outputSize2 dtype device. (All KnownNat '[Fst3 stride, Snd3 stride, Trd3 stride, Fst3 padding, Snd3 padding, Trd3 padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, kernelSize2, inputSize0, inputSize1, inputSize2, batchSize], ConvSideCheck inputSize0 kernelSize0 (Fst3 stride) (Fst3 padding) outputSize0, ConvSideCheck inputSize1 kernelSize1 (Snd3 stride) (Snd3 padding) outputSize1, ConvSideCheck inputSize2 kernelSize2 (Trd3 stride) (Trd3 padding) outputSize2) => Tensor device dtype '[inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, kernelSize2] -> Tensor device dtype '[outputChannelSize] -> Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1, inputSize2] -> Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1, outputSize2]

-- | cosh
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ cosh (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
cosh :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | Det
--   
--   <pre>
--   &gt;&gt;&gt; :kind! Det '[2,2]
--   Det '[2,2] :: [Nat]
--   = '[]
--   
--   &gt;&gt;&gt; :kind! Det '[3,2,2]
--   Det '[3,2,2] :: [Nat]
--   = '[3]
--   </pre>
type family Det (shape :: [Nat]) :: [Nat]

-- | det TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ det (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ det (ones :: CPUTensor 'D.Float '[3,2,2])
--   (Float,[3])
--   </pre>
det :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype (Det shape)
type family DimsDistinctAscendingCheck (dim1 :: Nat) (dim2 :: Nat) (cmp :: Ordering) :: Constraint
type family DimsDistinctAscending (dim1 :: Nat) (dim2 :: Nat) :: Constraint
type family DiagEmbedShapeImpl (dim1 :: Nat) (dim2 :: Nat) (shape :: [Nat]) (n :: Nat) :: [Nat]
type family DiagEmbedShape (index :: Nat) (dim1 :: Nat) (dim2 :: Nat) (shape :: [Nat]) :: [Nat]

-- | diagEmbed
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagEmbed @0 @1 @2 Upper (ones :: CPUTensor 'D.Float '[2,3])
--   (Float,[2,3,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagEmbed @1 @0 @2 Upper (ones :: CPUTensor 'D.Float '[2,3])
--   (Float,[4,2,4])
--   </pre>
diagEmbed :: forall index dim1 dim2 shape shape' device dtype. (KnownNat index, KnownNat dim1, KnownNat dim2, shape' ~ DiagEmbedShape index dim1 dim2 shape, DimsDistinctAscending dim1 dim2, StandardDTypeValidation device dtype) => Tri -> Tensor device dtype shape -> Tensor device dtype shape'
type family DiagflatShapeImpl (d :: Nat) :: [Nat]
type family DiagflatShape (index :: Nat) (shape :: [Nat]) :: [Nat]

-- | diagflat
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagflat @0 Upper (ones :: CPUTensor 'D.Float '[3])
--   (Float,[3,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagflat @1 Upper (ones :: CPUTensor 'D.Float '[3])
--   (Float,[4,4])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagflat @0 Upper (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[4,4])
--   </pre>
diagflat :: forall index shape shape' device dtype. (KnownNat index, shape' ~ DiagflatShape index shape, StandardDTypeValidation device dtype) => Tri -> Tensor device dtype shape -> Tensor device dtype shape'
type family NDimAtLeastCheck (ndim :: Nat) (shape :: [Nat]) (cmp :: Ordering) :: Constraint
type family NDimAtLeast (ndim :: Nat) (shape :: [Nat]) :: Constraint
type family DiagonalShape (tri :: Tri) (index :: Nat) (dim1 :: Nat) (dim2 :: Nat) (shape :: [Nat]) :: [Nat]

-- | diagonal
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagonal @'Upper @0 @0 @1 (ones :: CPUTensor 'D.Float '[3,3])
--   (Float,[3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagonal @'Upper @1 @0 @1 (ones :: CPUTensor 'D.Float '[3,3])
--   (Float,[2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ diagonal @'Lower @1 @1 @2 (ones :: CPUTensor 'D.Float '[2,5,4,2])
--   (Float,[2,2,4])
--   </pre>
diagonal :: forall tri index dim1 dim2 shape shape' device dtype. (KnownTri tri, KnownNat index, KnownNat dim1, KnownNat dim2, NDimAtLeast 2 shape, DimsDistinctAscending dim1 dim2, shape' ~ DiagonalShape tri index dim1 dim2 shape, StandardDTypeValidation device dtype) => Tensor device dtype shape -> Tensor device dtype shape'
type family DotDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | dot product Note that this function does not broadcast.
dot :: forall size dtype device. DotDTypeIsValid device dtype => Tensor device dtype '[size] -> Tensor device dtype '[size] -> Tensor device dtype '[]
class KnownMaybeNat (n :: Maybe Nat)
maybeNatVal :: KnownMaybeNat n => Maybe Integer
type family PaddingIdxCheck (idx :: Maybe Nat) (numEmbeds :: Nat) :: Constraint

-- | embedding TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: what about sparsity here? TODO:
--   what output dtypes are supported?
--   
--   <pre>
--   &gt;&gt;&gt; weights = fromJust [[1, 1], [2, 2], [3, 3], [4, 4]] :: CPUTensor 'D.Float '[4, 2]
--   
--   &gt;&gt;&gt; indices = fromJust [[0], [2], [0], [1]] :: CPUTensor 'D.Int64 '[4, 1]
--   
--   &gt;&gt;&gt; t = embedding @('Just 1) False False weights indices
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[4, 1, 2]
--   </pre>
--   
--   <ul>
--   <li>- libtorch 1.7</li>
--   <li>- &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t'
--   -&gt; D.asValue (toDynamic t') :: [[[Float]]]) $ t</li>
--   <li>-
--   (Float,([4,1,2],[[[1.0,1.0]],[[3.0,3.0]],[[1.0,1.0]],[[2.0,2.0]]]))</li>
--   <li>-</li>
--   <li>- libtorch 1.8</li>
--   <li>- The behavior of libtorch 1.8 changes. See
--   <a>https://github.com/pytorch/pytorch/issues/53368</a></li>
--   <li>-
--   (Float,([4,1,2],[[[1.0,1.0]],[[3.0,3.0]],[[1.0,1.0]],[[0.0,0.0]]]))</li>
--   <li>-</li>
--   <li>- libtorch 1.8.1</li>
--   <li>- The behavior of libtorch 1.8.1 is reverted.</li>
--   <li>-
--   (Float,([4,1,2],[[[1.0,1.0]],[[3.0,3.0]],[[1.0,1.0]],[[2.0,2.0]]]))
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t' -&gt;
--   D.asValue (toDynamic t') :: [[[Float]]]) $ t
--   (Float,([4,1,2],[[[1.0,1.0]],[[3.0,3.0]],[[1.0,1.0]],[[2.0,2.0]]]))
--   &gt;&gt;&gt; t = embedding @'Nothing False False weights indices
--   &gt;&gt;&gt; :type t t :: Tensor '( 'D.CPU, 0) 'D.Float '[4, 1, 2]
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t' -&gt;
--   D.asValue (toDynamic t') :: [[[Float]]]) $ t
--   (Float,([4,1,2],[[[1.0,1.0]],[[3.0,3.0]],[[1.0,1.0]],[[2.0,2.0]]]))</li>
--   </ul>
embedding :: forall (paddingIdx :: Maybe Nat) numEmbeds embedDim shape dtype device. (KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds) => Bool -> Bool -> Tensor device dtype '[numEmbeds, embedDim] -> Tensor device 'Int64 shape -> Tensor device dtype (Reverse (embedDim : Reverse shape))

-- | emptyLike TODO: this seems quite unsafe, the values of this tensor
--   will be random
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- emptyLike (ones :: CPUTensor 'D.Float '[3,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[3,4,5])
--   </pre>
emptyLike :: forall shape dtype device. Tensor device dtype shape -> IO (Tensor device dtype shape)

-- | erfc
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ erfc (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
erfc :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | expm1 TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ expm1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
expm1 :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | expand TODO: figure out what the <tt>implicit</tt> boolean value does
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2]
--   
--   &gt;&gt;&gt; t' = expand @'[3, 1, 2] False t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t'
--   (Float,[3,1,2])
--   
--   &gt;&gt;&gt; t'' = expand @'[3, 1, 2] True t
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t''
--   (Float,[3,1,2])
--   
--   &gt;&gt;&gt; toInt (all (t' ==. t'')) == 1
--   True
--   </pre>
expand :: forall shape' shape dtype device. (KnownShape shape', shape' ~ Broadcast shape shape') => Bool -> Tensor device dtype shape -> Tensor device dtype shape'

-- | flattenAll
--   
--   <pre>
--   &gt;&gt;&gt; t = flattenAll (ones :: CPUTensor 'D.Float '[3,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[6])
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[6]
--   </pre>
flattenAll :: forall shape dtype device. KnownShape shape => Tensor device dtype shape -> Tensor device dtype '[Product shape]

-- | frac
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ frac (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
frac :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | full like
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fullLike 3.0 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
fullLike :: forall shape dtype device. Float -> Tensor device dtype shape -> Tensor device dtype shape

-- | isclose TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ isclose 0.1 0.1 False (ones :: CPUTensor 'D.Float '[3,2]) (ones :: CPUTensor 'D.Float '[3,2])
--   (Bool,[3,2])
--   </pre>
isclose :: forall shape dtype device. Double -> Double -> Bool -> Tensor device dtype shape -> Tensor device dtype shape -> Tensor device 'Bool shape

-- | is NaN TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ isNaN (ones :: CPUTensor 'D.Float '[3,2])
--   (Bool,[3,2])
--   </pre>
isNaN :: forall shape dtype device. Tensor device dtype shape -> Tensor device 'Bool shape

-- | is distributed
isDistributed :: forall shape dtype device. Tensor device dtype shape -> Bool

-- | is floating point TODO: this can be decided statically
isFloatingPoint :: forall shape dtype device. Tensor device dtype shape -> Bool

-- | is complex
isComplex :: forall shape dtype device. Tensor device dtype shape -> Bool

-- | is non-zero this operation is only defined for tensors with shape '[]
--   or '[1]
isNonZero :: forall shape dtype device. Numel shape ~ 1 => Tensor device dtype shape -> Bool

-- | is same size TODO: this can be decided statically
isSameSize :: forall shape shape' dtype device. Tensor device dtype shape -> Tensor device dtype shape' -> Bool
isSigned :: forall shape dtype device. Tensor device dtype shape -> Bool

-- | layerNorm TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: figure out if and when CUDNN works
--   here, tie it also to the <a>device</a>
--   
--   <pre>
--   &gt;&gt;&gt; t = layerNorm @'[1, 2] @'[2, 1, 2] @'D.Float @'( 'D.CPU, 0) ones ones 0.01 ones
--   
--   &gt;&gt;&gt; :type t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 1, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[2,1,2])
--   </pre>
layerNorm :: forall normalizedShape shape dtype device. (KnownShape normalizedShape, IsSuffixOf normalizedShape shape) => Tensor device dtype normalizedShape -> Tensor device dtype normalizedShape -> Double -> Tensor device dtype shape -> Tensor device dtype shape

-- | linear TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   <a>https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#linear</a>
--   
--   <pre>
--   &gt;&gt;&gt; w = fromJust [[-0.5, -2,  0.5], [1.5, -0.5, 0.5]] :: CPUTensor 'D.Float '[2, 3]
--   
--   &gt;&gt;&gt; b = fromJust [0, 0.5] :: CPUTensor 'D.Float '[2]
--   
--   &gt;&gt;&gt; t = fromJust [[-2, 0.5, 1], [0.5, 0, 0], [0, 1, 0], [0, 0, 0], [1, -1, 0]] :: CPUTensor 'D.Float '[5, 3]
--   
--   &gt;&gt;&gt; t' = linear w b t
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'
--   (Float,([5,2],[[0.5,-2.25],[-0.25,1.25],[-2.0,0.0],[0.0,0.5],[1.5,2.5]]))
--   </pre>
linear :: forall batchSize inputFeatures outputFeatures dtype device. Tensor device dtype '[outputFeatures, inputFeatures] -> Tensor device dtype '[outputFeatures] -> Tensor device dtype '[batchSize, inputFeatures] -> Tensor device dtype '[batchSize, outputFeatures]

-- | linear' TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: can we use the ATen linear
--   function or not here?
--   <a>https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#linear</a>
--   
--   <pre>
--   &gt;&gt;&gt; w = fromJust [[-0.5, -2,  0.5], [1.5, -0.5, 0.5]] :: CPUTensor 'D.Float '[2, 3]
--   
--   &gt;&gt;&gt; b = fromJust [0, 0.5] :: CPUTensor 'D.Float '[2]
--   
--   &gt;&gt;&gt; t = fromJust [[-2, 0.5, 1], [0.5, 0, 0], [0, 1, 0], [0, 0, 0], [1, -1, 0]] :: CPUTensor 'D.Float '[5, 3]
--   
--   &gt;&gt;&gt; t' = linear' w b t
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'
--   (Float,([5,2],[[0.5,-2.25],[-0.25,1.25],[-2.0,0.0],[0.0,0.5],[1.5,2.5]]))
--   
--   &gt;&gt;&gt; t = fromJust [[[[-2, 0.5, 1], [0.5, 0, 0], [0, 1, 0], [0, 0, 0], [1, -1, 0]], [[-2, 0.5, 1], [0.5, 0, 0], [0, 1, 0], [0, 0, 0], [1, -1, 0]]]] :: CPUTensor 'D.Float '[1, 2, 5, 3]
--   
--   &gt;&gt;&gt; t' = linear' w b t
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 2, 5, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[[[Float]]]]) $ t'
--   (Float,([1,2,5,2],[[[[0.5,-2.25],[-0.25,1.25],[-2.0,0.0],[0.0,0.5],[1.5,2.5]],[[0.5,-2.25],[-0.25,1.25],[-2.0,0.0],[0.0,0.5],[1.5,2.5]]]]))
--   </pre>
linear' :: forall (inputFeatures :: Nat) (outputFeatures :: Nat) (shape :: [Nat]) (shape' :: [Nat]) dtype device (shape'' :: [Nat]). (shape'' ~ MatMul shape '[inputFeatures, outputFeatures], shape' ~ Broadcast shape'' shape'') => Tensor device dtype '[outputFeatures, inputFeatures] -> Tensor device dtype '[outputFeatures] -> Tensor device dtype shape -> Tensor device dtype shape'

-- | mkldnnLinear TODO: probably only defined for floating point tensors,
--   or maybe numeric type is lifted? TODO: mkldnnLinear does not return a
--   usuable tensor value and is hence broken TODO: figure out
--   <a>device</a> for this
--   
--   <pre>
--   &gt;&gt;&gt; w = fromJust [[-0.5, -2,  0.5], [1.5, -0.5, 0.5]] :: CPUTensor 'D.Float '[2, 3]
--   
--   &gt;&gt;&gt; b = fromJust [0, 0.5] :: CPUTensor 'D.Float '[2]
--   
--   &gt;&gt;&gt; t = fromJust [[-2, 0.5, 1], [0.5, 0, 0], [0, 1, 0], [0, 0, 0], [1, -1, 0]] :: CPUTensor 'D.Float '[5, 3]
--   </pre>
--   
--   <ul>
--   <li>- &gt;&gt;&gt; t' = mkldnnLinear (toMKLDNN w) (toMKLDNN b)
--   (toMKLDNN t)</li>
--   <li>- &gt;&gt;&gt; :type t'</li>
--   <li>- t' :: Tensor '( 'D.CPU, 0) 'D.Float '[5, 2]</li>
--   <li>- &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (t''
--   -&gt; D.asValue (toDynamic t'') :: [[Float]]) $ t'</li>
--   <li>-
--   (Float,([5,2],[[0.5,-2.25],[-0.25,1.25],[-2.0,0.0],[0.0,0.5],[1.5,2.5]]))</li>
--   </ul>
mkldnnLinear :: forall batchSize inputFeatures outputFeatures dtype device. Tensor device dtype '[outputFeatures, inputFeatures] -> Tensor device dtype '[outputFeatures] -> Tensor device dtype '[batchSize, inputFeatures] -> Tensor device dtype '[batchSize, outputFeatures]

-- | log TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted? TODO: will log throw for negative numbers or
--   just generate NaNs? should we return a Maybe?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ log (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
log :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | logDet TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: will logDet throw? and if so,
--   should we return a Maybe?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ logDet (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ logDet (ones :: CPUTensor 'D.Float '[3,2,2])
--   (Float,[3])
--   </pre>
logDet :: forall shape' shape dtype device. shape' ~ Det shape => Tensor device dtype shape -> Tensor device dtype shape'

-- | logarithm of the sum of the exponentials TODO: probably only defined
--   for floating point tensors, or maybe numeric type is lifted? See
--   <a>https://pytorch.org/docs/stable/torch.html#torch.logsumexp</a>.
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [[5, 1], [3, 2], [4, 1], [2, 7]] :: CPUTensor 'D.Float '[4, 2]
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Float]) $ logSumExp @1 @DropDim t
--   (Float,([4],[5.01815,3.3132617,4.0485873,7.0067153]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ logSumExp @1 @KeepDim t
--   (Float,([4,1],[[5.01815],[3.3132617],[4.0485873],[7.0067153]]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [Float]) $ logSumExp @0 @DropDim t
--   (Float,([2],[5.44019,7.0116277]))
--   </pre>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ logSumExp @0 @KeepDim t
--   (Float,([1,2],[[5.44019,7.0116277]]))
--   </pre>
logSumExp :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, Reifies dtype DType, DTypeIsFloatingPoint device dtype, shape' ~ ConditionalDropDimension shape dim keepOrDropDim) => Tensor device dtype shape -> Tensor device dtype shape'

-- | matrixPower TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? TODO: figure out input shape
--   restrictions, should be matrix or a batched matrix TODO: figure out
--   restrictions on the power, can it be zero or negative?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ matrixPower 2 (ones :: CPUTensor 'D.Float '[3,4,4])
--   (Float,[3,4,4])
--   </pre>
matrixPower :: forall shape' shape dtype device. shape' ~ Square shape => Int -> Tensor device dtype shape -> Tensor device dtype shape'

-- | maxValues TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ maxValues @0 @KeepDim t
--   (Float,[1,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ maxValues @0 @DropDim t
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ maxValues @1 @KeepDim t
--   (Float,[3,1,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ maxValues @1 @DropDim t
--   (Float,[3,5])
--   </pre>
maxValues :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim) => Tensor device dtype shape -> Tensor device dtype shape'

-- | minValues TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ minValues @0 @KeepDim t
--   (Float,[1,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ minValues @0 @DropDim t
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ minValues @1 @KeepDim t
--   (Float,[3,1,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ minValues @1 @DropDim t
--   (Float,[3,5])
--   </pre>
minValues :: forall dim keepOrDropDim shape' shape dtype device. (KnownNat dim, KnownKeepOrDropDim keepOrDropDim, shape' ~ ConditionalDropDimension shape dim keepOrDropDim) => Tensor device dtype shape -> Tensor device dtype shape'

-- | maxPool1d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = maxPool1d @1 @1 @0 (ones :: CPUTensor 'D.Float '[1,3,4])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,4]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 4]
--   </pre>
maxPool1d :: forall kernelSize stride padding channelSize inputSize batchSize outputSize dtype device. (All KnownNat '[kernelSize, stride, padding, channelSize, inputSize, batchSize], ConvSideCheck inputSize kernelSize stride padding outputSize) => Tensor device dtype '[batchSize, channelSize, inputSize] -> Tensor device dtype '[batchSize, channelSize, outputSize]

-- | maxPool2d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = maxPool2d @'(1,1) @'(1,1) @'(0,0) (ones :: CPUTensor 'D.Float '[1,3,4,5]) -- Skip warning
--   ...
--   
--   &gt;&gt;&gt; shape t
--   [1,3,4,5]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 4, 5]
--   </pre>
maxPool2d :: forall kernelSize stride padding channelSize inputSize0 inputSize1 batchSize outputSize0 outputSize1 dtype device. (All KnownNat '[Fst kernelSize, Snd kernelSize, Fst stride, Snd stride, Fst padding, Snd padding, channelSize, inputSize0, inputSize1, batchSize], ConvSideCheck inputSize0 (Fst kernelSize) (Fst stride) (Fst padding) outputSize0, ConvSideCheck inputSize1 (Snd kernelSize) (Snd stride) (Snd padding) outputSize1) => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, channelSize, outputSize0, outputSize1]

-- | mkldnnMaxPool2d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted? TODO: does this function
--   work, that is, does it return values without throwing? when does it
--   work? TODO: this should probably be only callable if the device is
--   MKLDNN? -- &gt;&gt;&gt; t = mkldnnMaxPool2d <tt>'(1,1) </tt>'(1,1)
--   @'(0,0) (toMKLDNN (ones :: CPUTensor 'D.Float '[1,3,4,5])) --
--   &gt;&gt;&gt; shape t -- [1,3,4,5] -- &gt;&gt;&gt; :t t -- t :: Tensor
--   '( 'D.CPU, 0) 'D.Float '[1, 3, 4, 5]
mkldnnMaxPool2d :: forall kernelSize stride padding channelSize inputSize0 inputSize1 batchSize outputSize0 outputSize1 dtype device. (All KnownNat '[Fst kernelSize, Snd kernelSize, Fst stride, Snd stride, Fst padding, Snd padding, channelSize, inputSize0, inputSize1, batchSize], ConvSideCheck inputSize0 (Fst kernelSize) (Fst stride) (Fst padding) outputSize0, ConvSideCheck inputSize1 (Snd kernelSize) (Snd stride) (Snd padding) outputSize1) => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, channelSize, outputSize0, outputSize1]

-- | quantizedMaxPool2d TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted? TODO: what are quantized
--   functions and when are they available? -- &gt;&gt;&gt; t =
--   quantizedMaxPool2d <tt>'(1,1) </tt>'(1,1) @'(0,0) (ones :: CPUTensor
--   'D.Float '[1,3,4,5]) -- &gt;&gt;&gt; shape t -- [1,3,4,5] --
--   &gt;&gt;&gt; :t t -- t :: Tensor 'D.Float '[1, 3, 4, 5]
quantizedMaxPool2d :: forall kernelSize stride padding channelSize inputSize0 inputSize1 batchSize outputSize0 outputSize1 dtype device. (All KnownNat '[Fst kernelSize, Snd kernelSize, Fst stride, Snd stride, Fst padding, Snd padding, channelSize, inputSize0, inputSize1, batchSize], ConvSideCheck inputSize0 (Fst kernelSize) (Fst stride) (Fst padding) outputSize0, ConvSideCheck inputSize1 (Snd kernelSize) (Snd stride) (Snd padding) outputSize1) => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, channelSize, outputSize0, outputSize1]

-- | maxPool3d TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = maxPool3d @'(1,1,1) @'(1,1,1) @'(0,0,0) (ones :: CPUTensor 'D.Float '[1,3,4,5,6])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,4,5,6]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 4, 5, 6]
--   </pre>
maxPool3d :: forall kernelSize stride padding channelSize inputSize0 inputSize1 inputSize2 batchSize outputSize0 outputSize1 outputSize2 dtype device. (All KnownNat '[Fst3 kernelSize, Snd3 kernelSize, Trd3 kernelSize, Fst3 stride, Snd3 stride, Trd3 stride, Fst3 padding, Snd3 padding, Trd3 padding, channelSize, inputSize0, inputSize1, inputSize2, batchSize], ConvSideCheck inputSize0 (Fst3 kernelSize) (Fst3 stride) (Fst3 padding) outputSize0, ConvSideCheck inputSize1 (Snd3 kernelSize) (Snd3 stride) (Snd3 padding) outputSize1, ConvSideCheck inputSize2 (Trd3 kernelSize) (Trd3 stride) (Trd3 padding) outputSize2) => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1, inputSize2] -> Tensor device dtype '[batchSize, channelSize, outputSize0, outputSize1, outputSize2]

-- | maskedFill TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2, 1, 3]
--   
--   &gt;&gt;&gt; m = fromJust [[False], [True], [False]] :: CPUTensor 'D.Bool '[3, 1]
--   
--   &gt;&gt;&gt; t' = maskedFill @Float m 0.5 t
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 3, 3]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\u -&gt; D.asValue (toDynamic u) :: [[[Float]]]) $ t'
--   (Float,([2,3,3],[[[1.0,1.0,1.0],[0.5,0.5,0.5],[1.0,1.0,1.0]],[[1.0,1.0,1.0],[0.5,0.5,0.5],[1.0,1.0,1.0]]]))
--   </pre>
maskedFill :: forall a shape shape' shape'' dtype device. (Scalar a, shape'' ~ Broadcast shape shape') => Tensor device 'Bool shape' -> a -> Tensor device dtype shape -> Tensor device dtype shape''

-- | matrix-matrix multiplication TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ mm (ones :: CPUTensor 'D.Float '[3,2]) (zeros :: CPUTensor 'D.Float '[2,4])
--   (Float,[3,4])
--   </pre>
mm :: forall n k m dtype device. Tensor device dtype '[n, k] -> Tensor device dtype '[k, m] -> Tensor device dtype '[n, m]

-- | matrix-vector multiplication TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ mv (ones :: CPUTensor 'D.Float '[3,2]) (zeros :: CPUTensor 'D.Float '[2])
--   (Float,[3])
--   </pre>
mv :: forall n m dtype device. Tensor device dtype '[n, m] -> Tensor device dtype '[m] -> Tensor device dtype '[n]
type family NarrowCheck (mbCurrent :: Maybe Nat) (mbUpdated :: Maybe [Nat]) (shape :: [Nat]) (dim :: Nat) (start :: Nat) (length :: Nat) :: [Nat]
type family Narrow' (dim :: Nat) (shape :: [Nat]) (current :: Maybe Nat) (start :: Nat) (length :: Nat) :: Maybe [Nat]
type family Narrow (shape :: [Nat]) (dim :: Nat) (start :: Nat) (length :: Nat) :: [Nat]

-- | <a>Narrow</a> a tensor by returning a tensor that is a slice from
--   <tt>start</tt> of length <a>length</a> along <a>dim</a>
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ narrow @0 @0 @2 (ones :: CPUTensor 'D.Float '[3,3,3])
--   (Float,[2,3,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ narrow @1 @1 @2 (ones :: CPUTensor 'D.Half '[3,3,3])
--   (Half,[3,2,3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ narrow @1 @1 @2 (ones :: CPUTensor 'D.Bool '[3,3,3])
--   (Bool,[3,2,3])
--   </pre>
narrow :: forall dim start length shape mbSize mbNewShape dtype device. (All KnownNat '[dim, start, length], All KnownNat shape) => Tensor device dtype shape -> Tensor device dtype (Narrow shape dim start length)

-- | onesLike
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ onesLike (ones :: CPUTensor 'D.Float '[3,4,5])
--   (Float,[3,4,5])
--   </pre>
onesLike :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | randLike TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- randLike (ones :: CPUTensor 'D.Float '[3,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[3,4,5])
--   </pre>
randLike :: forall shape dtype device. Tensor device dtype shape -> IO (Tensor device dtype shape)

-- | randnLike TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- randnLike (ones :: CPUTensor 'D.Float '[3,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[3,4,5])
--   </pre>
randnLike :: forall shape dtype device. Tensor device dtype shape -> IO (Tensor device dtype shape)

-- | reciprocal
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ reciprocal (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
reciprocal :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | negate TODO: probably not defined for <a>Bool</a> tensors
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ neg (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
neg :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | round TODO: probably only defined for floating point tensors
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ round (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
round :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | prelu activation function TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ prelu (ones :: CPUTensor 'D.Float '[]) (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
prelu :: forall shape dtype device. Tensor device dtype '[] -> Tensor device dtype shape -> Tensor device dtype shape
type family GeluDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint

-- | gelu activation function
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ round (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
gelu :: forall shape dtype device. GeluDTypeIsValid device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | rsqrt
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ rsqrt (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
rsqrt :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | celu activation function TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ celu 3.0 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
celu :: forall shape dtype device. Float -> Tensor device dtype shape -> Tensor device dtype shape
type family StackImpl (dim :: Nat) (tensors :: [a]) (count :: Nat) :: Maybe ([Nat], DType, (DeviceType, Nat))
type family MaybePair (a' :: Maybe a) (b' :: Maybe b) :: Maybe (a, b)
type family MaybeTriple (a' :: Maybe a) (b' :: Maybe b) (c' :: Maybe c) :: Maybe (a, b, c)
type family ComputeStackShape (shape :: [Nat]) (dim :: Nat) (count :: Nat) :: Maybe [Nat]
type family StackCheck (res :: Maybe ([Nat], DType, (DeviceType, Nat))) :: ([Nat], DType, (DeviceType, Nat))

-- | Stack
--   
--   <pre>
--   &gt;&gt;&gt; type Ty = Stack 0 '[Tensor '( 'D.CPU, 0) 'D.Float '[]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[1], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Stack 0 '[Tensor '( 'D.CPU, 0) 'D.Float '[2,2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[1, 2, 2], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Stack 1 '[Tensor '( 'D.CPU, 0) 'D.Float '[2,2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[2, 1, 2], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Stack 2 '[Tensor '( 'D.CPU, 0) 'D.Float '[2,2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[2, 2, 1], 'D.Float, '( 'D.CPU, 0))
--   
--   &gt;&gt;&gt; type Ty = Stack 2 '[Tensor '( 'D.CPU, 0) 'D.Float '[2,2], Tensor '( 'D.CPU, 0) 'D.Float '[2,2], Tensor '( 'D.CPU, 0) 'D.Float '[2,2]]
--   
--   &gt;&gt;&gt; :kind! Ty
--   Ty :: ([Nat], D.DType, (D.DeviceType, Nat))
--   = '( '[2, 2, 3], 'D.Float, '( 'D.CPU, 0))
--   </pre>
type Stack dim tensors = StackCheck (StackImpl dim tensors 1)

-- | stack
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[]
--   
--   &gt;&gt;&gt; t' = stack @0 (t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[1]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [Float]) $ t'
--   (Float,([1],[1.0]))
--   
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[2,2]
--   
--   &gt;&gt;&gt; t' = stack @0 (t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 2, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[[Float]]]) $ t'
--   (Float,([1,2,2],[[[1.0,1.0],[1.0,1.0]]]))
--   
--   &gt;&gt;&gt; t' = stack @1 (t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 1, 2]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[[Float]]]) $ t'
--   (Float,([2,1,2],[[[1.0,1.0]],[[1.0,1.0]]]))
--   
--   &gt;&gt;&gt; t' = stack @2 (t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 2, 1]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[[Float]]]) $ t'
--   (Float,([2,2,1],[[[1.0],[1.0]],[[1.0],[1.0]]]))
--   
--   &gt;&gt;&gt; t' = stack @2 (t :. t :. t :. HNil)
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Float '[2, 2, 3]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t'' -&gt; D.asValue (toDynamic t'') :: [[[Float]]]) $ t'
--   (Float,([2,2,3],[[[1.0,1.0,1.0],[1.0,1.0,1.0]],[[1.0,1.0,1.0],[1.0,1.0,1.0]]]))
--   </pre>
stack :: forall dim shape dtype device tensors. (KnownNat dim, '(shape, dtype, device) ~ Stack dim tensors, Castable (HList tensors) [ATenTensor]) => HList tensors -> Tensor device dtype shape
vecStack :: forall dim n shape dtype device. (KnownNat dim, KnownNat n) => Vector n (Tensor device dtype shape) -> Tensor device dtype (Insert dim n shape)

-- | t
--   
--   dtype &amp;&amp;&amp; shape $ t ones :: CPUTensor 'D.Float '<a>3,2</a>
t :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | tan
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ tan (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
tan :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | trunc TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ trunc (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
trunc :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | UnsqueezeImpl
--   
--   <pre>
--   &gt;&gt;&gt; :kind! UnsqueezeImpl '[4] 0
--   UnsqueezeImpl '[4] 0 :: Maybe [Nat]
--   = 'Just '[1, 4]
--   
--   &gt;&gt;&gt; :kind! UnsqueezeImpl '[4] 1
--   UnsqueezeImpl '[4] 1 :: Maybe [Nat]
--   = 'Just '[4, 1]
--   
--   &gt;&gt;&gt; :kind! UnsqueezeImpl '[4] 2
--   UnsqueezeImpl '[4] 2 :: Maybe [Nat]
--   = 'Nothing
--   </pre>
type family UnsqueezeImpl (shape :: [a]) (dim :: Nat) :: Maybe [a]
type family UnsqueezeCheck (shape :: [a]) (dim :: Nat) (result :: Maybe [a]) :: [a]
type Unsqueeze shape dim = UnsqueezeCheck shape dim (UnsqueezeImpl shape dim)

-- | unsqueeze
--   
--   <pre>
--   &gt;&gt;&gt; t = fromJust [1, 2, 3, 4] :: CPUTensor 'D.Int64 '[4]
--   
--   &gt;&gt;&gt; t' = unsqueeze @0 t
--   
--   &gt;&gt;&gt; :type t'
--   t' :: Tensor '( 'D.CPU, 0) 'D.Int64 '[1, 4]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\u -&gt; D.asValue (toDynamic u) :: [[Int]]) $ t'
--   (Int64,([1,4],[[1,2,3,4]]))
--   
--   &gt;&gt;&gt; t'' = unsqueeze @1 t
--   
--   &gt;&gt;&gt; :type t''
--   t'' :: Tensor '( 'D.CPU, 0) 'D.Int64 '[4, 1]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\u -&gt; D.asValue (toDynamic u) :: [[Int]]) $ t''
--   (Int64,([4,1],[[1],[2],[3],[4]]))
--   </pre>
unsqueeze :: forall dim shape shape' dtype device. (KnownNat dim, shape' ~ Unsqueeze shape dim) => Tensor device dtype shape -> Tensor device dtype shape'
type family SqueezeAll (shape :: [Nat]) :: [Nat]

-- | squeeze all dimensions
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ squeezeAll (ones :: CPUTensor 'D.Float '[2,1,2,1,2])
--   (Float,[2,2,2])
--   
--   &gt;&gt;&gt; squeezeAll (ones :: CPUTensor 'D.Float '[2,1,2,1,2])
--   Tensor Float [2,2,2] [[[ 1.0000   ,  1.0000   ],
--                          [ 1.0000   ,  1.0000   ]],
--                         [[ 1.0000   ,  1.0000   ],
--                          [ 1.0000   ,  1.0000   ]]]
--   </pre>
squeezeAll :: forall shape shape' dtype device. shape' ~ SqueezeAll shape => Tensor device dtype shape -> Tensor device dtype shape'
type family SqueezeDimImpl (shape :: [a]) (dim :: Nat) :: Maybe [a]
type family SqueezeDimCheck (shape :: [a]) (dim :: Nat) (result :: Maybe [a]) :: [a]

-- | Calculate the output shape of a squeeze along a given dimension
--   
--   <pre>
--   &gt;&gt;&gt; :kind! SqueezeDim '[2,1,2] 1
--   SqueezeDim '[2,1,2] 1 :: [Nat]
--   = '[2, 2]
--   </pre>
type SqueezeDim shape dim = SqueezeDimCheck shape dim (SqueezeDimImpl shape dim)

-- | squeeze a particular dimension
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ squeezeDim @1 (ones :: CPUTensor 'D.Float '[2,1,2,1,2])
--   (Float,[2,2,1,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ squeezeDim @3 (ones :: CPUTensor 'D.Float '[2,1,2,1,2])
--   (Float,[2,1,2,2])
--   </pre>
squeezeDim :: forall dim shape shape' dtype device. (KnownNat dim, shape' ~ SqueezeDim shape dim) => Tensor device dtype shape -> Tensor device dtype shape'

-- | zerosLike
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ zerosLike (ones :: CPUTensor 'D.Float '[3,4,5])
--   (Float,[3,4,5])
--   </pre>
zerosLike :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | clone
--   
--   <pre>
--   &gt;&gt;&gt; t &lt;- clone (ones :: CPUTensor 'D.Float '[3,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[3,2])
--   </pre>
clone :: forall shape dtype device. Tensor device dtype shape -> IO (Tensor device dtype shape)

-- | addmm TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted? TODO: can we use D.Scalar here for beta and
--   alpha?
--   
--   <pre>
--   &gt;&gt;&gt; t = addmm 1 1 (ones :: CPUTensor 'D.Float '[3,2]) (zeros :: CPUTensor 'D.Float '[2,4]) (ones :: CPUTensor 'D.Float '[])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ t
--   (Float,[3,4])
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[3, 4]
--   </pre>
addmm :: forall shape' shape n k m dtype device. (All KnownNat '[n, k, m], shape' ~ Broadcast shape '[n, m]) => Float -> Float -> Tensor device dtype '[n, k] -> Tensor device dtype '[k, m] -> Tensor device dtype shape -> Tensor device dtype shape'

-- | numel TODO: since this is decidable at compile time, this should
--   probably be calculated from the tensor type instead
numel :: forall shape dtype device. Tensor device dtype shape -> Int

-- | qScale TODO: are there any restrictions on the dtype?
qScale :: forall shape dtype device. Tensor device dtype shape -> Double

-- | qZeroPoint TODO: are there any restrictions on the dtype?
qZeroPoint :: forall shape dtype device. Tensor device dtype shape -> Int

-- | The directional specification of a recurrent function
data RNNDirectionality

-- | Forward and backward along the sequential axis using independant
--   parameters for each.
Bidirectional :: RNNDirectionality

-- | Forward along the sequential axis.
Unidirectional :: RNNDirectionality
type family NumberOfDirections (directionality :: RNNDirectionality) :: Nat
class KnownRNNDirectionality (directionality :: RNNDirectionality)
rnnBidirectional :: KnownRNNDirectionality directionality => Bool

-- | Specification for the sequential axis of a recurrent function.
data RNNShapeOrder

-- | Input is of shape (Batch, Sequence, Features)
BatchFirst :: RNNShapeOrder

-- | Input is of shape (Sequence, Batch, Features)
SequenceFirst :: RNNShapeOrder
class KnownRNNShapeOrder (shapeOrder :: RNNShapeOrder)
rnnBatchFirst :: KnownRNNShapeOrder shapeOrder => Bool
type family RNNShape (shapeOrder :: RNNShapeOrder) (seqLen :: Nat) (batchSize :: Nat) (featureSize :: Nat) :: [Nat]
type LSTMWIShape hiddenSize inputSize = '[4 * hiddenSize, inputSize]
type LSTMWHShape hiddenSize inputSize = '[4 * hiddenSize, hiddenSize]
type LSTMBIShape hiddenSize inputSize = '[4 * hiddenSize]
type LSTMBHShape hiddenSize inputSize = '[4 * hiddenSize]
type family LSTMRImpl (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) :: [[Nat]]
type family LSTMR' (shapes :: [[Nat]]) (dtype :: DType) (device :: (DeviceType, Nat)) :: [a]
type LSTMR inputSize hiddenSize numLayers directionality dtype device = LSTMR' (LSTMRImpl inputSize hiddenSize numLayers directionality) dtype device

-- | lstm Parameters for this ATen function are non-trivially provided. See
--   the <a>LSTM</a> module for doctests.
lstm :: forall shapeOrder directionality numLayers seqLen batchSize inputSize outputSize hiddenSize inputShape outputShape hxShape tensorParameters dtype device. (KnownNat numLayers, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hxShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], tensorParameters ~ LSTMR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor]) => HList tensorParameters -> Double -> Bool -> (Tensor device dtype hxShape, Tensor device dtype hxShape) -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hxShape, Tensor device dtype hxShape)

-- | lstmCell
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ lstmCell (ones :: CPUTensor 'D.Float '[12,2]) (ones :: CPUTensor 'D.Float '[12,3]) (ones :: CPUTensor 'D.Float '[12]) (ones :: CPUTensor 'D.Float '[12]) ((ones :: CPUTensor 'D.Float '[2,3]), (ones :: CPUTensor 'D.Float '[2,3])) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,3])
--   </pre>
lstmCell :: forall inputSize hiddenSize batchSize dtype device. Tensor device dtype '[4 * hiddenSize, inputSize] -> Tensor device dtype '[4 * hiddenSize, hiddenSize] -> Tensor device dtype '[4 * hiddenSize] -> Tensor device dtype '[4 * hiddenSize] -> (Tensor device dtype '[batchSize, hiddenSize], Tensor device dtype '[batchSize, hiddenSize]) -> Tensor device dtype '[batchSize, inputSize] -> (Tensor device dtype '[batchSize, hiddenSize], Tensor device dtype '[batchSize, hiddenSize])
type GRUWIShape hiddenSize inputSize = '[3 * hiddenSize, inputSize]
type GRUWHShape hiddenSize inputSize = '[3 * hiddenSize, hiddenSize]
type GRUBIShape hiddenSize inputSize = '[3 * hiddenSize]
type GRUBHShape hiddenSize inputSize = '[3 * hiddenSize]
type family GRURImpl (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) :: [[Nat]]
type family GRUR' (shapes :: [[Nat]]) (dtype :: DType) (device :: (DeviceType, Nat)) :: [a]
type GRUR inputSize hiddenSize numLayers directionality dtype device = GRUR' (GRURImpl inputSize hiddenSize numLayers directionality) dtype device

-- | gru Parameters for this ATen function are non-trivially provided. See
--   the <a>GRU</a> module for doctests.
gru :: forall shapeOrder directionality numLayers seqLen batchSize inputSize outputSize hiddenSize inputShape outputShape hcShape tensorParameters dtype device. (KnownNat numLayers, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hcShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], tensorParameters ~ GRUR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor]) => HList tensorParameters -> Double -> Bool -> Tensor device dtype hcShape -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hcShape)

-- | gruCell
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ gruCell (ones :: CPUTensor 'D.Float '[9,2]) (ones :: CPUTensor 'D.Float '[9,3]) (ones :: CPUTensor 'D.Float '[9]) (ones :: CPUTensor 'D.Float '[9]) (ones :: CPUTensor 'D.Float '[2,3]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,3])
--   </pre>
gruCell :: forall inputSize hiddenSize batchSize dtype device. Tensor device dtype '[3 * hiddenSize, inputSize] -> Tensor device dtype '[3 * hiddenSize, hiddenSize] -> Tensor device dtype '[3 * hiddenSize] -> Tensor device dtype '[3 * hiddenSize] -> Tensor device dtype '[batchSize, hiddenSize] -> Tensor device dtype '[batchSize, inputSize] -> Tensor device dtype '[batchSize, hiddenSize]
type family MatrixOrMatrixBatch (shape :: [Nat]) :: [Nat]

-- | triu TODO: triu is not implemented for D.Bool, or maybe numeric type
--   is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3, 4]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ triu 0 t
--   (Float,([3,4],[[1.0,1.0,1.0,1.0],[0.0,1.0,1.0,1.0],[0.0,0.0,1.0,1.0]]))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ triu 1 t
--   (Float,([3,4],[[0.0,1.0,1.0,1.0],[0.0,0.0,1.0,1.0],[0.0,0.0,0.0,1.0]]))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ triu (-1) t
--   (Float,([3,4],[[1.0,1.0,1.0,1.0],[1.0,1.0,1.0,1.0],[0.0,1.0,1.0,1.0]]))
--   </pre>
triu :: forall shape dtype device. shape ~ MatrixOrMatrixBatch shape => Int -> Tensor device dtype shape -> Tensor device dtype shape

-- | tril TODO: tril is not implemented for D.Bool, or maybe numeric type
--   is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3, 4]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ tril 0 t
--   (Float,([3,4],[[1.0,0.0,0.0,0.0],[1.0,1.0,0.0,0.0],[1.0,1.0,1.0,0.0]]))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ tril 1 t
--   (Float,([3,4],[[1.0,1.0,0.0,0.0],[1.0,1.0,1.0,0.0],[1.0,1.0,1.0,1.0]]))
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t' -&gt; D.asValue (toDynamic t') :: [[Float]]) $ tril (-1) t
--   (Float,([3,4],[[0.0,0.0,0.0,0.0],[1.0,0.0,0.0,0.0],[1.0,1.0,0.0,0.0]]))
--   </pre>
tril :: forall shape dtype device. shape ~ MatrixOrMatrixBatch shape => Int -> Tensor device dtype shape -> Tensor device dtype shape

-- | trace
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ trace (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
trace :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape
maskedSelect :: forall shape shape' shape'' dtype device. shape'' ~ Broadcast shape shape' => Tensor device 'Bool shape -> Tensor device dtype shape' -> UnknownShapeTensor device dtype

-- | nonzero
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nonzero (zeros :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
nonzero :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | GatherDimImpl
--   
--   <pre>
--   &gt;&gt;&gt; :kind! GatherDimImpl '[2, 1, 1] '[2, 4, 1] 1
--   GatherDimImpl '[2, 1, 1] '[2, 4, 1] 1 :: Maybe [Nat]
--   = 'Just '[2, 4, 1]
--   
--   &gt;&gt;&gt; :kind! GatherDimImpl '[2, 1, 1] '[2, 4, 2] 1
--   GatherDimImpl '[2, 1, 1] '[2, 4, 2] 1 :: Maybe [Nat]
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImpl '[2, 1, 1] '[2, 0, 1] 1
--   GatherDimImpl '[2, 1, 1] '[2, 0, 1] 1 :: Maybe [Nat]
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImpl '[2, 1, 1] '[2, 1] 1
--   GatherDimImpl '[2, 1, 1] '[2, 1] 1 :: Maybe [Nat]
--   = 'Nothing
--   
--   &gt;&gt;&gt; :kind! GatherDimImpl '[2, 1, 1] '[2, 1, 3] 2
--   GatherDimImpl '[2, 1, 1] '[2, 1, 3] 2 :: Maybe [Nat]
--   = 'Just '[2, 1, 3]
--   </pre>
type family GatherDimImpl (shape :: [Nat]) (shape' :: [Nat]) (dim :: Nat) :: Maybe [Nat]
type family GatherDimCheck (shape :: [a]) (shape' :: [a]) (dim :: Nat) (result :: Maybe [a]) :: [a]

-- | Calculate the output shape of a gather operation for a given index
--   shape along a given axis
--   
--   <pre>
--   &gt;&gt;&gt; :kind! GatherDim '[2, 1, 1] '[2, 1, 3] 2
--   GatherDim '[2, 1, 1] '[2, 1, 3] 2 :: [Nat]
--   = '[2, 1, 3]
--   </pre>
type GatherDim shape shape' dim = GatherDimCheck shape shape' dim (GatherDimImpl shape shape' dim)

-- | gather values along an axis for a specified dimension.
gatherDim :: forall dim shape shape' dtype device. (KnownNat dim, shape' ~ GatherDim shape shape' dim) => Tensor device 'Int64 shape' -> Tensor device dtype shape -> Tensor device dtype shape'

-- | lgamma function
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ lgamma (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
lgamma :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | digamma function
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ digamma (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
digamma :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | polygamma function TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
polygamma :: forall shape dtype device. Int -> Tensor device dtype shape -> Tensor device dtype shape

-- | inverse of the error function
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ erfinv (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
erfinv :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | minAll
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ minAll (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
minAll :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype '[]
type family DropValue (shape :: [Nat]) (i :: Nat) :: [Nat]
type family DropNamedValue (shape :: Shape) (i :: Size) :: Shape

-- | minDim
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ minDim @0 t
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ minDim @1 t
--   (Float,[3,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ minDim @2 t
--   (Float,[3,4])
--   </pre>
minDim :: forall d shape dtype device. KnownNat d => Tensor device dtype shape -> (Tensor device dtype (DropValue shape d), Tensor device 'Int64 (DropValue shape d))

-- | maxAll
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ maxAll (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
maxAll :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype '[]

-- | maxDim
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ maxDim @0 t
--   (Float,[4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ maxDim @1 t
--   (Float,[3,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ maxDim @2 t
--   (Float,[3,4])
--   </pre>
maxDim :: forall d shape dtype device. KnownNat d => Tensor device dtype shape -> (Tensor device dtype (DropValue shape d), Tensor device 'Int64 (DropValue shape d))
type family HasDim (dim :: Nat) (shape :: [Nat]) :: Constraint

-- | sortDim
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ sortDim @0 True t
--   (Float,[3,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ snd $ sortDim @0 True t
--   (Int64,[3,4,5])
--   </pre>
sortDim :: forall dim shape dtype device. (KnownNat dim, HasDim dim shape) => Bool -> Tensor device dtype shape -> (Tensor device dtype shape, Tensor device Int64 shape)

-- | sortNamedDim
--   
--   <pre>
--   &gt;&gt;&gt; import Torch.Typed.Factories
--   
--   &gt;&gt;&gt; import Data.Default.Class
--   
--   &gt;&gt;&gt; t = def :: NamedTensor '( D.CPU, 0) 'D.Float '[Vector 3, Vector 4, Vector 5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ fst $ sortNamedDim @(Vector 3) True t
--   (Float,[3,4,5])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ snd $ sortNamedDim @(Vector 3) True t
--   (Int64,[3,4,5])
--   </pre>
sortNamedDim :: forall dim shape dtype device. KnownNat (FindDim dim shape) => Bool -> NamedTensor device dtype shape -> (NamedTensor device dtype shape, NamedTensor device Int64 shape)

-- | argSortDim
--   
--   <pre>
--   &gt;&gt;&gt; t = ones :: CPUTensor 'D.Float '[3,4,5]
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ argSortDim @0 True t
--   (Int64,[3,4,5])
--   </pre>
argSortDim :: forall dim shape dtype device. (KnownNat dim, HasDim dim shape) => Bool -> Tensor device dtype shape -> Tensor device Int64 shape
argSortNamedDim :: forall dim shape dtype device. KnownNat (FindDim dim shape) => Bool -> NamedTensor device dtype shape -> NamedTensor device Int64 shape
type family TopKCheck (k :: Nat) (shape :: [Nat]) (dim :: Nat) (satd :: Maybe Nat) (result :: Maybe a) :: a
type TopK k shape dim = TopKCheck k shape dim (ExtractDim dim shape) (ReplaceDim dim shape k)
type family TopKDeviceAndDTypeCheck dtype (device :: (DeviceType, Nat)) :: Constraint

-- | Returns the k largest (if largest is <a>True</a>) elements of the
--   given input tensor along a given dimension.
--   
--   <pre>
--   &gt;&gt;&gt; topk @3 @1 True True (ones :: CPUTensor 'D.Float '[2,3])
--   (Tensor Float [2,3] [[ 1.0000   ,  1.0000   ,  1.0000   ],
--                       [ 1.0000   ,  1.0000   ,  1.0000   ]],Tensor Int64 [2,3] [[ 0,  1,  2],
--                       [ 0,  1,  2]])
--   
--   &gt;&gt;&gt; topk @0 @1 True True (ones :: CPUTensor 'D.Float '[2,3])
--   (Tensor Float [2,0] [[],
--                       []],Tensor Int64 [2,0] [[],
--                       []])
--   </pre>
topk :: forall k dim shape' shape dtype device. (KnownNat k, KnownNat dim, All KnownNat shape, TopKDeviceAndDTypeCheck dtype device, shape' ~ TopK k shape dim) => Bool -> Bool -> Tensor device dtype shape -> (Tensor device dtype shape', Tensor device 'Int64 shape')

-- | alias
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ alias (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
alias :: forall shape dtype device. Tensor device dtype shape -> Tensor device dtype shape

-- | L1 loss TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ l1Loss @ReduceNone (ones :: CPUTensor 'D.Float '[2,2]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ l1Loss @ReduceSum (ones :: CPUTensor 'D.Float '[2,2]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
l1Loss :: forall reduction shape dtype device. KnownReduction reduction => Tensor device dtype shape -> Tensor device dtype shape -> Tensor device dtype (ConditionalReduction shape reduction)

-- | negative log likelihood loss TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted? See
--   <a>https://pytorch.org/docs/stable/nn.functional.html?highlight=nll_loss#torch.nn.functional.nll_loss</a>.
--   
--   <pre>
--   &gt;&gt;&gt; input &lt;- randn @'[3, 5] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; target = fromJust [1, 0, 4] :: CPUTensor 'D.Int64 '[3]
--   
--   &gt;&gt;&gt; weight = ones @'[5] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceNone @3 @5 @'[] weight (-100) (logSoftmax @1 input) target
--   (Float,[3])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceMean @3 @5 @'[] weight (-100) (logSoftmax @1 input) target
--   (Float,[])
--   
--   &gt;&gt;&gt; input &lt;- randn @'[3, 5, 2] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; target = fromJust [[1, 1], [0, 1], [4, 0]] :: CPUTensor 'D.Int64 '[3, 2]
--   
--   &gt;&gt;&gt; weight = ones @'[5] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceNone @3 @5 @'[2] weight (-100) (logSoftmax @1 input) target
--   (Float,[3,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceMean @3 @5 @'[2] weight (-100) (logSoftmax @1 input) target
--   (Float,[])
--   
--   &gt;&gt;&gt; input &lt;- randn @'[3, 5, 1, 2] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; target = fromJust [[[1, 1]], [[0, 1]], [[4, 0]]] :: CPUTensor 'D.Int64 '[3, 1, 2]
--   
--   &gt;&gt;&gt; weight = ones @'[5] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceNone @3 @5 @'[1, 2] weight (-100) (logSoftmax @1 input) target
--   (Float,[3,1,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceMean @3 @5 @'[1, 2] weight (-100) (logSoftmax @1 input) target
--   (Float,[])
--   
--   &gt;&gt;&gt; input &lt;- randn @'[3, 5, 2, 1, 2] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; target = fromJust [[[[1, 1]], [[0, 2]]], [[[0, 1]], [[1, 0]]], [[[4, 0]], [[1, 2]]]] :: CPUTensor 'D.Int64 '[3, 2, 1, 2]
--   
--   &gt;&gt;&gt; weight = ones @'[5] @'D.Float @'( 'D.CPU, 0)
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceNone @3 @5 @'[2, 1, 2] weight (-100) (logSoftmax @1 input) target
--   (Float,[3,2,1,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ nllLoss @ReduceMean @3 @5 @'[2, 1, 2] weight (-100) (logSoftmax @1 input) target
--   (Float,[])
--   </pre>
nllLoss :: forall reduction n c ds dtype device. (KnownReduction reduction, KnownNat n, KnownNat c, KnownShape ds) => Tensor device dtype '[c] -> Int -> Tensor device dtype (n : (c : ds)) -> Tensor device 'Int64 (n : ds) -> Tensor device dtype (ConditionalReduction (n : ds) reduction)

-- | smooth L1 loss TODO: probably only defined for floating point tensors,
--   or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ smoothL1Loss @ReduceNone (ones :: CPUTensor 'D.Float '[2,2]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ smoothL1Loss @ReduceSum (ones :: CPUTensor 'D.Float '[2,2]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
smoothL1Loss :: forall reduction shape dtype device. KnownReduction reduction => Tensor device dtype shape -> Tensor device dtype shape -> Tensor device dtype (ConditionalReduction shape reduction)

-- | soft margin loss TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ softMarginLoss @ReduceNone (ones :: CPUTensor 'D.Float '[2,2]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[2,2])
--   
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ softMarginLoss @ReduceSum (ones :: CPUTensor 'D.Float '[2,2]) (ones :: CPUTensor 'D.Float '[2,2])
--   (Float,[])
--   </pre>
softMarginLoss :: forall reduction shape dtype device. KnownReduction reduction => Tensor device dtype shape -> Tensor device dtype shape -> Tensor device dtype (ConditionalReduction shape reduction)

-- | elu TODO: probably only defined for floating point tensors, or maybe
--   numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ elu 0.1 0.1 0.3 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
elu :: forall shape dtype a device. (Scalar a, StandardFloatingPointDTypeValidation device dtype) => a -> a -> a -> Tensor device dtype shape -> Tensor device dtype shape

-- | glu -- &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ glu (ones ::
--   CPUTensor 'D.Float '[3,2]) 1 -- (Float,[3,1]) -- &gt;&gt;&gt; dtype
--   &amp;&amp;&amp; shape $ glu (ones :: CPUTensor 'D.Float '[3,2]) 3 --
--   (Float,[3,2]) glu :: Tensor device dtype shape -&gt; Int -&gt; Tensor
--   device dtype shape glu _input _dim = unsafePerformIO $ (ATen.cast2
--   ATen.Managed.glu_tl) _input _dim
--   
--   hard tanh TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ hardTanh 0 1 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
hardTanh :: forall shape dtype device. Float -> Float -> Tensor device dtype shape -> Tensor device dtype shape

-- | leaky relu
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ leakyRelu 0.01 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
leakyRelu :: forall a shape dtype device. (Scalar a, StandardFloatingPointDTypeValidation device dtype) => a -> Tensor device dtype shape -> Tensor device dtype shape

-- | logarithm of the sigmoid
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ logSigmoid (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
logSigmoid :: forall shape dtype device. StandardFloatingPointDTypeValidation device dtype => Tensor device dtype shape -> Tensor device dtype shape

-- | softplus TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted? See
--   <a>https://pytorch.org/docs/stable/nn.functional.html?highlight=softplus#torch.nn.functional.softplus</a>.
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape &amp;&amp;&amp; (\t -&gt; D.asValue (toDynamic t) :: [[Float]]) $ softplus 1 20 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,([3,2],[[1.3132616,1.3132616],[1.3132616,1.3132616],[1.3132616,1.3132616]]))
--   </pre>
softplus :: forall a shape dtype device. Scalar a => a -> a -> Tensor device dtype shape -> Tensor device dtype shape

-- | soft shrink TODO: probably only defined for floating point tensors, or
--   maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; dtype &amp;&amp;&amp; shape $ softShrink 0.2 (ones :: CPUTensor 'D.Float '[3,2])
--   (Float,[3,2])
--   </pre>
softShrink :: forall shape dtype device. Float -> Tensor device dtype shape -> Tensor device dtype shape

-- | adaptive averaged 2-D pooling TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = adaptiveAvgPool2d @'(8,16) (ones :: CPUTensor 'D.Float '[1,3,16,32])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,8,16]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8, 16]
--   </pre>
adaptiveAvgPool2d :: forall outputSize channelSize inputSize0 inputSize1 batchSize dtype device. All KnownNat '[channelSize, inputSize0, inputSize1, batchSize, Fst outputSize, Snd outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, channelSize, Fst outputSize, Snd outputSize]

-- | MKLDNN adaptive averaged 2-D pooling TODO: probably only defined for
--   floating point tensors, or maybe numeric type is lifted? TODO: broken?
--   TODO: only defined for MKLDNN device? TODO: test for availability of
--   MKLDNN device? TODO: merge with adaptiveAvgPool2d and dispatch based
--   on (availability of MKLDNN) device in the function body?
--   
--   <ul>
--   <li>- &gt;&gt;&gt; t = mkldnnAdaptiveAvgPool2d @'(8,16) (toMKLDNN
--   (ones :: CPUTensor 'D.Float '[1,3,16,32]))</li>
--   <li>- &gt;&gt;&gt; shape t</li>
--   <li>- [1,3,8,16]</li>
--   <li>- &gt;&gt;&gt; :t t</li>
--   <li>- t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8, 16]</li>
--   </ul>
mkldnnAdaptiveAvgPool2d :: forall outputSize channelSize inputSize0 inputSize1 batchSize dtype device. All KnownNat '[channelSize, inputSize0, inputSize1, batchSize, Fst outputSize, Snd outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, channelSize, Fst outputSize, Snd outputSize]

-- | adaptive averaged 3-D pooling TODO: probably only defined for floating
--   point tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = adaptiveAvgPool3d @'(8,16,2) (ones :: CPUTensor 'D.Float '[1,3,16,32,4])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,8,16,2]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8, 16, 2]
--   </pre>
adaptiveAvgPool3d :: forall outputSize channelSize inputSize0 inputSize1 inputSize2 batchSize dtype device. All KnownNat '[channelSize, inputSize0, inputSize1, inputSize2, batchSize, Fst3 outputSize, Snd3 outputSize, Trd3 outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1, inputSize2] -> Tensor device dtype '[batchSize, channelSize, Fst3 outputSize, Snd3 outputSize, Trd3 outputSize]

-- | adaptive 2-D max-pool TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; (t, t') = adaptiveMaxPool2d @'(8,16) (ones :: CPUTensor 'D.Float '[1,3,16,32])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,8,16]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8, 16]
--   </pre>
adaptiveMaxPool2d :: forall outputSize channelSize inputSize0 inputSize1 batchSize dtype device. All KnownNat '[channelSize, inputSize0, inputSize1, batchSize, Fst outputSize, Snd outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> (Tensor device dtype '[batchSize, channelSize, Fst outputSize, Snd outputSize], Tensor device 'Int64 '[batchSize, channelSize, Fst outputSize, Snd outputSize])

-- | adaptive 3-D max-pool TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; (t, t') = adaptiveMaxPool3d @'(8,16,2) (ones :: CPUTensor 'D.Float '[1,3,16,32,4])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,8,16,2]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 8, 16, 2]
--   </pre>
adaptiveMaxPool3d :: forall outputSize channelSize inputSize0 inputSize1 inputSize2 batchSize dtype device. All KnownNat '[channelSize, inputSize0, inputSize1, inputSize2, batchSize, Fst3 outputSize, Snd3 outputSize, Trd3 outputSize] => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1, inputSize2] -> (Tensor device dtype '[batchSize, channelSize, Fst3 outputSize, Snd3 outputSize, Trd3 outputSize], Tensor device 'Int64 '[batchSize, channelSize, Fst3 outputSize, Snd3 outputSize, Trd3 outputSize])

-- | averaged 2-D pooling TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = avgPool2d @'(1,1) @'(1,1) @'(0,0) (ones :: CPUTensor 'D.Float '[1,3,4,5])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,4,5]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 4, 5]
--   </pre>
avgPool2d :: forall kernelSize stride padding channelSize inputSize0 inputSize1 batchSize outputSize0 outputSize1 dtype device. (All KnownNat '[Fst kernelSize, Snd kernelSize, Fst stride, Snd stride, Fst padding, Snd padding, channelSize, inputSize0, inputSize1, batchSize], ConvSideCheck inputSize0 (Fst kernelSize) (Fst stride) (Fst padding) outputSize0, ConvSideCheck inputSize1 (Snd kernelSize) (Snd stride) (Snd padding) outputSize1) => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1] -> Tensor device dtype '[batchSize, channelSize, outputSize0, outputSize1]

-- | averaged 3-D pooling TODO: probably only defined for floating point
--   tensors, or maybe numeric type is lifted?
--   
--   <pre>
--   &gt;&gt;&gt; t = avgPool3d @'(1,1,1) @'(1,1,1) @'(0,0,0) (ones :: CPUTensor 'D.Float '[1,3,4,5,6])
--   
--   &gt;&gt;&gt; shape t
--   [1,3,4,5,6]
--   
--   &gt;&gt;&gt; :t t
--   t :: Tensor '( 'D.CPU, 0) 'D.Float '[1, 3, 4, 5, 6]
--   </pre>
avgPool3d :: forall kernelSize stride padding channelSize inputSize0 inputSize1 inputSize2 batchSize outputSize0 outputSize1 outputSize2 dtype device. (All KnownNat '[Fst3 kernelSize, Snd3 kernelSize, Trd3 kernelSize, Fst3 stride, Snd3 stride, Trd3 stride, Fst3 padding, Snd3 padding, Trd3 padding, channelSize, inputSize0, inputSize1, inputSize2, batchSize], ConvSideCheck inputSize0 (Fst3 kernelSize) (Fst3 stride) (Fst3 padding) outputSize0, ConvSideCheck inputSize1 (Snd3 kernelSize) (Snd3 stride) (Snd3 padding) outputSize1, ConvSideCheck inputSize2 (Trd3 kernelSize) (Trd3 stride) (Trd3 padding) outputSize2) => Tensor device dtype '[batchSize, channelSize, inputSize0, inputSize1, inputSize2] -> Tensor device dtype '[batchSize, channelSize, outputSize0, outputSize1, outputSize2]
type family Upsample2dCheck shape h w
type Upsample2d shape h w = Upsample2dCheck shape h w

-- | Applies a 2D bilinear upsampling to an input signal composed of
--   several input channels.
--   
--   <pre>
--   &gt;&gt;&gt; (dtype &amp;&amp;&amp; shape) $ upsample_bilinear2d @3 @5 False (ones :: CPUTensor 'D.Float '[2,3,2,2])
--   (Float,[2,3,3,5])
--   </pre>
upsample_bilinear2d :: forall w h shape dtype device. (KnownNat h, KnownNat w, All KnownNat shape) => Bool -> Tensor device dtype shape -> Tensor device dtype (Upsample2d shape h w)

-- | Applies a 2D bicubic upsampling to an input signal composed of several
--   input channels.
--   
--   <pre>
--   &gt;&gt;&gt; (dtype &amp;&amp;&amp; shape) $ upsample_bicubic2d @3 @5 False (ones :: CPUTensor 'D.Float '[2,3,2,2])
--   (Float,[2,3,3,5])
--   </pre>
upsample_bicubic2d :: forall w h shape dtype device. (KnownNat h, KnownNat w, All KnownNat shape) => Bool -> Tensor device dtype shape -> Tensor device dtype (Upsample2d shape h w)

-- | Applies a 2D bicubic upsampling to an input signal composed of several
--   input channels.
--   
--   <pre>
--   &gt;&gt;&gt; (dtype &amp;&amp;&amp; shape) $ upsample_nearest2d @3 @5 (ones :: CPUTensor 'D.Float '[2,3,2,2])
--   (Float,[2,3,3,5])
--   </pre>
upsample_nearest2d :: forall w h shape dtype device. (KnownNat h, KnownNat w, All KnownNat shape) => Tensor device dtype shape -> Tensor device dtype (Upsample2d shape h w)
instance GHC.Generics.Generic Torch.Typed.Functional.RNNDirectionality
instance GHC.Show.Show Torch.Typed.Functional.RNNDirectionality
instance GHC.Generics.Generic Torch.Typed.Functional.RNNShapeOrder
instance GHC.Show.Show Torch.Typed.Functional.RNNShapeOrder
instance Torch.Typed.Functional.KnownRNNShapeOrder 'Torch.Typed.Functional.BatchFirst
instance Torch.Typed.Functional.KnownRNNShapeOrder 'Torch.Typed.Functional.SequenceFirst
instance Torch.Typed.Functional.KnownRNNDirectionality 'Torch.Typed.Functional.Bidirectional
instance Torch.Typed.Functional.KnownRNNDirectionality 'Torch.Typed.Functional.Unidirectional
instance GHC.TypeNats.KnownNat n => Torch.Typed.Functional.KnownMaybeNat ('GHC.Maybe.Just n)
instance Torch.Typed.Functional.KnownMaybeNat 'GHC.Maybe.Nothing
instance Torch.Typed.Functional.KnownKeepOrDropDim 'Torch.Typed.Functional.KeepDim
instance Torch.Typed.Functional.KnownKeepOrDropDim 'Torch.Typed.Functional.DropDim
instance Torch.Typed.Functional.KnownTri 'Torch.Functional.Upper
instance Torch.Typed.Functional.KnownTri 'Torch.Functional.Lower
instance Torch.Typed.Functional.KnownReducedSVD 'Torch.Typed.Functional.ThinSVD
instance Torch.Typed.Functional.KnownReducedSVD 'Torch.Typed.Functional.FullSVD
instance Torch.Typed.Functional.KnownEigenVectors 'Torch.Typed.Functional.EnableEigenVectors
instance Torch.Typed.Functional.KnownEigenVectors 'Torch.Typed.Functional.DisableEigenVectors
instance Torch.Typed.Functional.KnownReduction 'Torch.Functional.ReduceNone
instance Torch.Typed.Functional.KnownReduction 'Torch.Functional.ReduceMean
instance Torch.Typed.Functional.KnownReduction 'Torch.Functional.ReduceSum

module Torch.Typed.Parameter
class GParameterized (f :: Type -> Type) where {
    type family GParameters f :: [Type];
}
gFlattenParameters :: forall a. GParameterized f => f a -> HList (GParameters f)
gReplaceParameters :: forall a. GParameterized f => f a -> HList (GParameters f) -> f a
class Parameterized (f :: Type) where {
    type family Parameters f :: [Type];
    type Parameters f = GParameters (Rep f);
}
flattenParameters :: Parameterized f => f -> HList (Parameters f)
flattenParameters :: (Parameterized f, Generic f, GParameterized (Rep f), Parameters f ~ GParameters (Rep f)) => f -> HList (Parameters f)
replaceParameters :: Parameterized f => f -> HList (Parameters f) -> f
replaceParameters :: (Parameterized f, Generic f, GParameterized (Rep f), Parameters f ~ GParameters (Rep f)) => f -> HList (Parameters f) -> f
data MakeIndependent
MakeIndependent :: MakeIndependent
data ToDependent
ToDependent :: ToDependent
newtype Parameter (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat])
UnsafeMkParameter :: IndependentTensor -> Parameter (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat])
untypeParam :: Parameter device dtype shape -> Parameter
toDependent :: forall shape dtype device. Parameter device dtype shape -> Tensor device dtype shape
makeIndependent :: forall shape dtype device. Tensor device dtype shape -> IO (Parameter device dtype shape)
parameterToDevice :: forall device' device dtype shape. KnownDevice device' => Parameter device dtype shape -> Parameter device' dtype shape
parameterToDType :: forall dtype' dtype device shape. KnownDType dtype' => Parameter device dtype shape -> Parameter device dtype' shape
class Randomizable spec f | spec -> f
sample :: Randomizable spec f => spec -> IO f
instance GHC.Show.Show (Torch.Typed.Parameter.Parameter device dtype shape)
instance Torch.Typed.Parameter.Parameterized f => Torch.Typed.Parameter.GParameterized (GHC.Generics.K1 i f)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.Parameter.Parameter device dtype shape)
instance Torch.Typed.Parameter.Parameterized GHC.Types.Int
instance Torch.Typed.Parameter.Parameterized GHC.Types.Float
instance Torch.Typed.Parameter.Parameterized GHC.Types.Double
instance Torch.Typed.Parameter.Parameterized (Torch.HList.HList '[])
instance (Torch.Typed.Parameter.Parameterized f, Torch.Typed.Parameter.Parameterized (Torch.HList.HList fs), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters f) (Torch.Typed.Parameter.Parameters (Torch.HList.HList fs)) (Torch.Typed.Parameter.Parameters f Torch.HList.++ Torch.Typed.Parameter.Parameters (Torch.HList.HList fs))) => Torch.Typed.Parameter.Parameterized (Torch.HList.HList (f : fs))
instance (Torch.Typed.Parameter.GParameterized l, Torch.Typed.Parameter.GParameterized r, Torch.HList.HAppendFD (Torch.Typed.Parameter.GParameters l) (Torch.Typed.Parameter.GParameters r) (Torch.Typed.Parameter.GParameters l Torch.HList.++ Torch.Typed.Parameter.GParameters r)) => Torch.Typed.Parameter.GParameterized (l GHC.Generics.:*: r)
instance Torch.Typed.Parameter.GParameterized f => Torch.Typed.Parameter.GParameterized (GHC.Generics.M1 i t f)
instance Torch.Typed.Parameter.GParameterized GHC.Generics.U1
instance Torch.HList.Apply' Torch.Typed.Parameter.MakeIndependent (Torch.Typed.Tensor.Tensor device dtype shape) (GHC.Types.IO (Torch.Typed.Parameter.Parameter device dtype shape))
instance Torch.HList.Apply' Torch.Typed.Parameter.ToDependent (Torch.Typed.Parameter.Parameter device dtype shape) (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.NN.Randomizable (Torch.HList.HList '[]) (Torch.HList.HList '[])
instance (Torch.NN.Randomizable xSpec x, Torch.NN.Randomizable (Torch.HList.HList xsSpec) (Torch.HList.HList xs)) => Torch.NN.Randomizable (Torch.HList.HList (xSpec : xsSpec)) (Torch.HList.HList (x : xs))

module Torch.Typed.NamedTensor
class NamedTensorLike a where {
    type family ToNestedList a :: Type;
}
toNestedList :: NamedTensorLike a => a -> ToNestedList a
asNamedTensor :: NamedTensorLike a => a -> NamedTensor '( 'CPU, 0) (ToDType a) (ToShape a)
fromNestedList :: NamedTensorLike a => ToNestedList a -> a
fromNamedTensor :: NamedTensorLike a => NamedTensor '( 'CPU, 0) (ToDType a) (ToShape a) -> a
instance Torch.Typed.NamedTensor.NamedTensorLike GHC.Types.Bool
instance Torch.Typed.NamedTensor.NamedTensorLike GHC.Types.Int
instance Torch.Typed.NamedTensor.NamedTensorLike GHC.Types.Float
instance Torch.Typed.NamedTensor.NamedTensorLike GHC.Types.Double
instance (GHC.TypeNats.KnownNat n, Torch.Tensor.TensorLike (Torch.Typed.NamedTensor.ToNestedList a), Torch.Typed.NamedTensor.NamedTensorLike a) => Torch.Typed.NamedTensor.NamedTensorLike (Data.Vector.Sized.Vector n a)
instance (GHC.Types.Coercible (vec n a) (Data.Vector.Sized.Vector n a), GHC.TypeNats.KnownNat n, Torch.Tensor.TensorLike (Torch.Typed.NamedTensor.ToNestedList a), Torch.Typed.NamedTensor.NamedTensorLike a) => Torch.Typed.NamedTensor.NamedTensorLike (vec n a)
instance (GHC.Generics.Generic (g a), Data.Default.Class.Default (g a), Torch.Lens.HasTypes (g a) a, GHC.TypeNats.KnownNat (Torch.Typed.Tensor.ToNat g), Torch.Tensor.TensorLike (Torch.Typed.NamedTensor.ToNestedList a), Torch.Typed.NamedTensor.NamedTensorLike a) => Torch.Typed.NamedTensor.NamedTensorLike (g a)

module Torch.Typed.NN.Sparse
data EmbeddingType
Constant :: EmbeddingType
Learned :: EmbeddingType
data EmbeddingSpec (paddingIdx :: Maybe Nat) (numEmbeds :: Nat) (embedSize :: Nat) (embeddingType :: EmbeddingType) (dtype :: DType) (device :: (DeviceType, Nat))
[ConstEmbeddingSpec] :: forall paddingIdx numEmbeds embedSize dtype device. Tensor device dtype '[numEmbeds, embedSize] -> EmbeddingSpec paddingIdx numEmbeds embedSize 'Constant dtype device
[LearnedEmbeddingWithRandomInitSpec] :: forall paddingIdx numEmbeds embedSize dtype device. EmbeddingSpec paddingIdx numEmbeds embedSize 'Learned dtype device
[LearnedEmbeddingWithCustomInitSpec] :: forall paddingIdx numEmbeds embedSize dtype device. Tensor device dtype '[numEmbeds, embedSize] -> EmbeddingSpec paddingIdx numEmbeds embedSize 'Learned dtype device
data Embedding (paddingIdx :: Maybe Nat) (numEmbeds :: Nat) (embedSize :: Nat) (embeddingType :: EmbeddingType) (dtype :: DType) (device :: (DeviceType, Nat))
[ConstEmbedding] :: forall paddingIdx numEmbeds embedSize dtype device. {constEmbedWeights :: Tensor device dtype '[numEmbeds, embedSize]} -> Embedding paddingIdx numEmbeds embedSize 'Constant dtype device
[LearnedEmbedding] :: forall paddingIdx numEmbeds embedSize dtype device. {learnedEmbedWeights :: Parameter device dtype '[numEmbeds, embedSize]} -> Embedding paddingIdx numEmbeds embedSize 'Learned dtype device
embed :: forall paddingIdx shape numEmbeds embedSize embeddingType dtype device shape'. (KnownMaybeNat paddingIdx, PaddingIdxCheck paddingIdx numEmbeds, shape' ~ Reverse (embedSize : Reverse shape)) => Embedding paddingIdx numEmbeds embedSize embeddingType dtype device -> Tensor device 'Int64 shape -> Tensor device dtype shape'
instance GHC.Generics.Generic Torch.Typed.NN.Sparse.EmbeddingType
instance GHC.Show.Show Torch.Typed.NN.Sparse.EmbeddingType
instance GHC.Show.Show (Torch.Typed.NN.Sparse.EmbeddingSpec paddingIdx numEmbeds embedSize embeddingType dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize embeddingType dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize 'Torch.Typed.NN.Sparse.Constant dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize 'Torch.Typed.NN.Sparse.Learned dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize 'Torch.Typed.NN.Sparse.Constant dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize 'Torch.Typed.NN.Sparse.Learned dtype device)
instance (Torch.Typed.Functional.KnownMaybeNat paddingIdx, Torch.Typed.Functional.PaddingIdxCheck paddingIdx numEmbeds, shape' GHC.Types.~ Torch.Typed.Aux.Reverse (embedSize : Torch.Typed.Aux.Reverse shape)) => Torch.NN.HasForward (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize embeddingType dtype device) (Torch.Typed.Tensor.Tensor device 'Torch.DType.Int64 shape) (Torch.Typed.Tensor.Tensor device dtype shape')
instance Torch.NN.Randomizable (Torch.Typed.NN.Sparse.EmbeddingSpec paddingIdx numEmbeds embedSize 'Torch.Typed.NN.Sparse.Constant dtype device) (Torch.Typed.NN.Sparse.Embedding paddingIdx numEmbeds embedSize 'Torch.Typed.NN.Sparse.Constant dtype device)
instance (GHC.TypeNats.KnownNat numEmbeds, GHC.TypeNats.KnownNat embedSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Sparse.EmbeddingSpec 'GHC.Maybe.Nothing numEmbeds embedSize 'Torch.Typed.NN.Sparse.Learned dtype device) (Torch.Typed.NN.Sparse.Embedding 'GHC.Maybe.Nothing numEmbeds embedSize 'Torch.Typed.NN.Sparse.Learned dtype device)
instance (paddingIdx GHC.TypeNats.<= numEmbeds, 1 GHC.TypeNats.<= (numEmbeds GHC.TypeNats.- paddingIdx), (((numEmbeds GHC.TypeNats.- paddingIdx) GHC.TypeNats.- 1) GHC.TypeNats.+ (1 GHC.TypeNats.+ paddingIdx)) GHC.Types.~ numEmbeds, GHC.TypeNats.KnownNat paddingIdx, GHC.TypeNats.KnownNat numEmbeds, GHC.TypeNats.KnownNat embedSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Sparse.EmbeddingSpec ('GHC.Maybe.Just paddingIdx) numEmbeds embedSize 'Torch.Typed.NN.Sparse.Learned dtype device) (Torch.Typed.NN.Sparse.Embedding ('GHC.Maybe.Just paddingIdx) numEmbeds embedSize 'Torch.Typed.NN.Sparse.Learned dtype device)

module Torch.Typed.NN.Normalization
data LayerNormSpec (normalizedShape :: [Nat]) (dtype :: DType) (device :: (DeviceType, Nat))
[LayerNormSpec] :: forall normalizedShape dtype device. {layerNormEpsSpec :: Double} -> LayerNormSpec normalizedShape dtype device
data LayerNorm (normalizedShape :: [Nat]) (dtype :: DType) (device :: (DeviceType, Nat))
[LayerNorm] :: {layerNormWeight :: Parameter device dtype normalizedShape, layerNormBias :: Parameter device dtype normalizedShape, layerNormEps :: Double} -> LayerNorm normalizedShape dtype device
layerNormForward :: forall normalizedShape shape dtype device. (IsSuffixOf normalizedShape shape, KnownShape normalizedShape) => LayerNorm normalizedShape dtype device -> Tensor device dtype shape -> Tensor device dtype shape
instance GHC.Classes.Eq (Torch.Typed.NN.Normalization.LayerNormSpec normalizedShape dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Normalization.LayerNormSpec normalizedShape dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Normalization.LayerNorm normalizedShape dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Normalization.LayerNorm normalizedShape dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Normalization.LayerNorm normalizedShape dtype device)
instance (Torch.Typed.Aux.IsSuffixOf normalizedShape shape, Torch.Typed.Tensor.KnownShape normalizedShape) => Torch.NN.HasForward (Torch.Typed.NN.Normalization.LayerNorm normalizedShape dtype device) (Torch.Typed.Tensor.Tensor device dtype shape) (Torch.Typed.Tensor.Tensor device dtype shape)
instance (Torch.Typed.Tensor.TensorOptions normalizedShape dtype device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Normalization.LayerNormSpec normalizedShape dtype device) (Torch.Typed.NN.Normalization.LayerNorm normalizedShape dtype device)

module Torch.Typed.NN.Linear
data LinearSpec (inputFeatures :: Nat) (outputFeatures :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
LinearSpec :: LinearSpec (inputFeatures :: Nat) (outputFeatures :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data Linear (inputFeatures :: Nat) (outputFeatures :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[Linear] :: forall inputFeatures outputFeatures dtype device. {linearWeight :: Parameter device dtype '[outputFeatures, inputFeatures], linearBias :: Parameter device dtype '[outputFeatures]} -> Linear inputFeatures outputFeatures dtype device

-- | linear The constraints on this one are _very_ involved, so the partial
--   signatures make the code significantly cleaner.
linearForward :: _ => Linear _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
instance GHC.Classes.Eq (Torch.Typed.NN.Linear.LinearSpec inputFeatures outputFeatures dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Linear.LinearSpec inputFeatures outputFeatures dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Linear.Linear inputFeatures outputFeatures dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Linear.Linear inputFeatures outputFeatures dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Linear.Linear inputFeatures outputFeatures dtype device)
instance (shape'' GHC.Types.~ Torch.Typed.Tensor.MatMul shape '[inputFeatures, outputFeatures], shape' GHC.Types.~ Torch.Typed.Tensor.Broadcast shape'' shape'') => Torch.NN.HasForward (Torch.Typed.NN.Linear.Linear inputFeatures outputFeatures dtype device) (Torch.Typed.Tensor.Tensor device dtype shape) (Torch.Typed.Tensor.Tensor device dtype shape')
instance (GHC.TypeNats.KnownNat inputFeatures, GHC.TypeNats.KnownNat outputFeatures, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Linear.LinearSpec inputFeatures outputFeatures dtype device) (Torch.Typed.NN.Linear.Linear inputFeatures outputFeatures dtype device)

module Torch.Typed.NN.Dropout
data DropoutSpec
[DropoutSpec] :: {dropoutProbSpec :: Double} -> DropoutSpec
data Dropout
[Dropout] :: {dropoutProb :: Double} -> Dropout
dropoutForward :: forall shape dtype device. Dropout -> Bool -> Tensor device dtype shape -> IO (Tensor device dtype shape)
instance GHC.Classes.Eq Torch.Typed.NN.Dropout.DropoutSpec
instance GHC.Show.Show Torch.Typed.NN.Dropout.DropoutSpec
instance Torch.Typed.Parameter.Parameterized Torch.Typed.NN.Dropout.Dropout
instance GHC.Generics.Generic Torch.Typed.NN.Dropout.Dropout
instance GHC.Show.Show Torch.Typed.NN.Dropout.Dropout
instance Torch.NN.HasForward Torch.Typed.NN.Dropout.Dropout (Torch.Typed.Tensor.Tensor device dtype shape) (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.NN.Randomizable Torch.Typed.NN.Dropout.DropoutSpec Torch.Typed.NN.Dropout.Dropout

module Torch.Typed.NN.Transformer
residual :: forall (device :: (DeviceType, Nat)) (dtype :: DType) (dtype' :: DType) m (shape :: [Nat]) (shape' :: [Nat]) b. (BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')), Monad m) => (Tensor device dtype shape -> m (Tensor device dtype' shape')) -> (Tensor device (DTypePromotionImpl dtype dtype' (CmpDType dtype dtype')) (CheckBroadcast shape shape' (ComputeBroadcast (ReverseImpl shape ('[] :: [Nat])) (ReverseImpl shape' ('[] :: [Nat])))) -> m b) -> Tensor device dtype shape -> m b
data MultiheadAttentionSpec (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat) (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[MultiheadAttentionSpec] :: DropoutSpec -> MultiheadAttentionSpec embedDim kEmbedDim vEmbedDim numHeads dtype device
data MultiheadAttention (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat) (numHeads :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[MultiheadAttention] :: {mhaQInProj :: Linear embedDim embedDim dtype device " in-projection for query", mhaKInProj :: Linear kEmbedDim embedDim dtype device " in-projection for key", mhaVInProj :: Linear vEmbedDim embedDim dtype device " in-projection for value", mhaOutProj :: Linear embedDim embedDim dtype device " out-projection", mhaDropout :: Dropout " dropout"} -> MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device
multiheadAttention :: forall embedDim kEmbedDim vEmbedDim numHeads seqLen seqLen' batchSize headDim dtype device. (1 <= numHeads, embedDim ~ (headDim * numHeads), All KnownNat '[embedDim, kEmbedDim, vEmbedDim, numHeads, seqLen, seqLen', batchSize, headDim], KnownDType dtype, StandardFloatingPointDTypeValidation device dtype, MatMulDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype, dtype ~ SumDType dtype, SumDTypeIsValid device dtype, KnownDevice device) => MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device -> Bool -> Maybe (Tensor device dtype '[batchSize, seqLen', seqLen]) -> Maybe (Tensor device 'Bool '[batchSize, seqLen]) -> Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim]) -> Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim]) -> Tensor device dtype '[batchSize, seqLen', embedDim] -> Tensor device dtype '[batchSize, seqLen, kEmbedDim] -> Tensor device dtype '[batchSize, seqLen, vEmbedDim] -> IO (Tensor device dtype '[batchSize, seqLen', embedDim], Tensor device dtype '[batchSize, seqLen', seqLen])
data TransformerMLPSpec (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[TransformerMLPSpec] :: forall embedDim ffnDim dtype device. {dropout0Spec :: DropoutSpec " spec for relu dropout", dropout1Spec :: DropoutSpec " spec for other dropout", epsSpec :: Double " epsilon for layer norm"} -> TransformerMLPSpec embedDim ffnDim dtype device
data TransformerMLP (embedDim :: Nat) (ffnDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[TransformerMLP] :: forall embedDim ffnDim dtype device. {linear0 :: Linear embedDim ffnDim dtype device " first fully connected layer", linear1 :: Linear ffnDim embedDim dtype device " second fully connected layer", dropout0 :: Dropout " relu dropout", dropout1 :: Dropout " other dropout", ln :: LayerNorm '[embedDim] dtype device " layer norm"} -> TransformerMLP embedDim ffnDim dtype device
transformerMLP :: forall embedDim ffnDim seqLen batchSize dtype device. (BasicArithmeticDTypeIsValid device dtype, StandardFloatingPointDTypeValidation device dtype, KnownNat embedDim, IsSuffixOf '[embedDim] '[seqLen, batchSize, embedDim]) => TransformerMLP embedDim ffnDim dtype device -> Bool -> Tensor device dtype '[seqLen, batchSize, embedDim] -> IO (Tensor device dtype '[seqLen, batchSize, embedDim])
data TransformerLayerSpec (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[TransformerLayerSpec] :: forall embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device. {mhaSpec :: MultiheadAttentionSpec embedDim kEmbedDim vEmbedDim numHeads dtype device, attnDropoutSpec :: DropoutSpec, epsSpec' :: Double, mlpSpec :: TransformerMLPSpec embedDim ffnDim dtype device} -> TransformerLayerSpec embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
data TransformerLayer (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat) (numHeads :: Nat) (ffnDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[TransformerLayer] :: forall embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device. {transformerLayer_mha :: MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device " multi-head attention", transformerLayer_attnDropout :: Dropout " dropout", transformerLayer_ln :: LayerNorm '[embedDim] dtype device " layer norm", transformerLayer_mlp :: TransformerMLP embedDim ffnDim dtype device " MLP"} -> TransformerLayer embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device
transformerLayer :: forall (numHeads :: Nat) (ffnDim :: Nat) (embedDim :: Nat) (kEmbedDim :: Nat) (vEmbedDim :: Nat) (headDim :: Nat) (seqLen :: Nat) (seqLen' :: Nat) (batchSize :: Nat) dtype device. (1 <= numHeads, embedDim ~ (headDim * numHeads), All KnownNat '[embedDim, kEmbedDim, vEmbedDim, numHeads, seqLen, seqLen', batchSize, headDim], IsSuffixOf '[embedDim] '[batchSize, seqLen', embedDim], KnownDType dtype, dtype ~ SumDType dtype, StandardFloatingPointDTypeValidation device dtype, MatMulDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype, SumDTypeIsValid device dtype, KnownDevice device) => TransformerLayer embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device -> Bool -> Maybe (Tensor device dtype '[batchSize, seqLen', seqLen]) -> Maybe (Tensor device 'Bool '[batchSize, seqLen]) -> Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim]) -> Maybe (Tensor device dtype '[batchSize, seqLen', seqLen, headDim]) -> Tensor device dtype '[batchSize, seqLen', embedDim] -> Tensor device dtype '[batchSize, seqLen, kEmbedDim] -> Tensor device dtype '[batchSize, seqLen, vEmbedDim] -> IO (Tensor device dtype '[batchSize, seqLen', embedDim])
data TransformerLMSpec (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat) (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[TransformerLMSpec] :: forall numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device. {lmDropoutSpec :: DropoutSpec " dropout spec", lmLayerSpec :: TransformerLayerSpec embedDim embedDim embedDim numHeads ffnDim dtype device " spec for each and every transformer layer"} -> TransformerLMSpec numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device
data TransformerLM (numAttnLayers :: Nat) (numHeads :: Nat) (ffnDim :: Nat) (paddingIdx :: Nat) (numEmbeds :: Nat) (embedDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[TransformerLM] :: forall numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device. {tEmbedding :: Embedding ('Just paddingIdx) numEmbeds embedDim 'Learned dtype device " token embedding", tPosEmbedding :: Embedding 'Nothing 2048 embedDim 'Constant dtype device " positional embedding", tDropout :: Dropout " transformer dropout", tLayers :: HList (HReplicateR numAttnLayers (TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device)) " transformer layers", tProj :: Linear embedDim numEmbeds dtype device " final output projection"} -> TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device
data FoldLayers (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
FoldLayers :: Bool -> Maybe (Tensor device dtype '[batchSize, seqLen, seqLen]) -> Maybe (Tensor device 'Bool '[batchSize, seqLen]) -> FoldLayers (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | switch between training mode and evaluation mode (turns random dropout
--   on and off)
[flTrain] :: FoldLayers (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Bool

-- | optional attention mask
[flAttentionMask] :: FoldLayers (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Maybe (Tensor device dtype '[batchSize, seqLen, seqLen])

-- | optional key padding mask
[flKeyPaddingMask] :: FoldLayers (batchSize :: Nat) (seqLen :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Maybe (Tensor device 'Bool '[batchSize, seqLen])
transformerLM :: forall numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim seqLen batchSize dtype device. (All KnownNat '[paddingIdx, embedDim, seqLen, batchSize], (paddingIdx + 1) <= numEmbeds, 1 <= seqLen, HFoldrM IO (FoldLayers batchSize seqLen dtype device) (Tensor device dtype '[batchSize, seqLen, embedDim]) (HReplicateR numAttnLayers (TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device)) (Tensor device dtype '[batchSize, seqLen, embedDim]), BasicArithmeticDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device 'Int64, KnownDType dtype, KnownDevice device) => TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device -> Bool -> Tensor device 'Int64 '[batchSize, seqLen] -> IO (Tensor device dtype '[batchSize, seqLen, numEmbeds])
sinusoidal :: forall numEmbeds embedDim device. (All KnownNat '[numEmbeds, embedDim], 1 <= numEmbeds, 1 <= Div embedDim 2, (Div embedDim 2 * 2) ~ embedDim, StandardFloatingPointDTypeValidation device 'Float, BasicArithmeticDTypeIsValid device 'Float, KnownDevice device) => Tensor device 'Float '[numEmbeds, embedDim]
instance GHC.Classes.Eq (Torch.Typed.NN.Transformer.MultiheadAttentionSpec embedDim kEmbedDim vEmbedDim numHeads dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.MultiheadAttentionSpec embedDim kEmbedDim vEmbedDim numHeads dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Transformer.MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Transformer.MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Transformer.TransformerMLPSpec embedDim ffnDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.TransformerMLPSpec embedDim ffnDim dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Transformer.TransformerMLP embedDim ffnDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Transformer.TransformerMLP embedDim ffnDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.TransformerMLP embedDim ffnDim dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Transformer.TransformerLayerSpec embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.TransformerLayerSpec embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Transformer.TransformerLayer embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Transformer.TransformerLayer embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.TransformerLayer embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Transformer.TransformerLMSpec numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Transformer.TransformerLMSpec numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Transformer.TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device)
instance GHC.Show.Show (Torch.HList.HList (Torch.HList.HReplicateR numAttnLayers (Torch.Typed.NN.Transformer.TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device))) => GHC.Show.Show (Torch.Typed.NN.Transformer.TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device)
instance (1 GHC.TypeNats.<= numHeads, embedDim GHC.Types.~ (headDim GHC.TypeNats.* numHeads), Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[embedDim, numHeads, seqLen, batchSize, headDim], Torch.Typed.Aux.IsSuffixOf '[embedDim] '[batchSize, seqLen, embedDim], Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Aux.StandardFloatingPointDTypeValidation device dtype, Torch.Typed.Tensor.MatMulDTypeIsValid device dtype, Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype, dtype GHC.Types.~ Torch.Typed.Functional.SumDType dtype, Torch.Typed.Functional.SumDTypeIsValid device dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.HList.Apply' (Torch.Typed.NN.Transformer.FoldLayers batchSize seqLen dtype device) (Torch.Typed.NN.Transformer.TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device, GHC.Types.IO (Torch.Typed.Tensor.Tensor device dtype '[batchSize, seqLen, embedDim])) (GHC.Types.IO (Torch.Typed.Tensor.Tensor device dtype '[batchSize, seqLen, embedDim]))
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[paddingIdx, embedDim, seqLen, batchSize], (paddingIdx GHC.TypeNats.+ 1) GHC.TypeNats.<= numEmbeds, 1 GHC.TypeNats.<= seqLen, Torch.HList.HFoldrM GHC.Types.IO (Torch.Typed.NN.Transformer.FoldLayers batchSize seqLen dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, seqLen, embedDim]) (Torch.HList.HReplicateR numAttnLayers (Torch.Typed.NN.Transformer.TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device)) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, seqLen, embedDim]), Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype, Torch.Typed.Tensor.ComparisonDTypeIsValid device dtype, Torch.Typed.Tensor.ComparisonDTypeIsValid device 'Torch.DType.Int64, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.NN.HasForward (Torch.Typed.NN.Transformer.TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device) (Torch.Typed.Tensor.Tensor device 'Torch.DType.Int64 '[batchSize, seqLen]) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, seqLen, numEmbeds])
instance (layers GHC.Types.~ Torch.HList.HReplicateR numAttnLayers (Torch.Typed.NN.Transformer.TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device), Torch.Typed.Parameter.Parameterized (Torch.HList.HList layers), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.HList.HList layers)) '[Torch.Typed.Parameter.Parameter device dtype '[numEmbeds, embedDim], Torch.Typed.Parameter.Parameter device dtype '[numEmbeds]] (Torch.Typed.Parameter.Parameters (Torch.HList.HList layers) Torch.HList.++ '[Torch.Typed.Parameter.Parameter device dtype '[numEmbeds, embedDim], Torch.Typed.Parameter.Parameter device dtype '[numEmbeds]])) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Transformer.TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device)
instance (paddingIdx GHC.TypeNats.<= numEmbeds, 1 GHC.TypeNats.<= (numEmbeds GHC.TypeNats.- paddingIdx), (((numEmbeds GHC.TypeNats.- paddingIdx) GHC.TypeNats.- 1) GHC.TypeNats.+ (1 GHC.TypeNats.+ paddingIdx)) GHC.Types.~ numEmbeds, (GHC.TypeNats.Div embedDim 2 GHC.TypeNats.* 2) GHC.Types.~ embedDim, Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[ffnDim, paddingIdx, numEmbeds, embedDim], Torch.HList.HReplicate numAttnLayers (Torch.Typed.NN.Transformer.TransformerLayerSpec embedDim embedDim embedDim numHeads ffnDim dtype device), Torch.NN.Randomizable (Torch.HList.HList (Torch.HList.HReplicateR numAttnLayers (Torch.Typed.NN.Transformer.TransformerLayerSpec embedDim embedDim embedDim numHeads ffnDim dtype device))) (Torch.HList.HList (Torch.HList.HReplicateR numAttnLayers (Torch.Typed.NN.Transformer.TransformerLayer embedDim embedDim embedDim numHeads ffnDim dtype device))), Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Factories.RandDTypeIsValid device dtype, Torch.Typed.Aux.StandardFloatingPointDTypeValidation device 'Torch.DType.Float, Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device 'Torch.DType.Float, Torch.Typed.Tensor.KnownDevice device) => Torch.NN.Randomizable (Torch.Typed.NN.Transformer.TransformerLMSpec numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device) (Torch.Typed.NN.Transformer.TransformerLM numAttnLayers numHeads ffnDim paddingIdx numEmbeds embedDim dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[embedDim, kEmbedDim, vEmbedDim, numHeads, ffnDim], Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Transformer.TransformerLayerSpec embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device) (Torch.Typed.NN.Transformer.TransformerLayer embedDim kEmbedDim vEmbedDim numHeads ffnDim dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[embedDim, ffnDim], Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Transformer.TransformerMLPSpec embedDim ffnDim dtype device) (Torch.Typed.NN.Transformer.TransformerMLP embedDim ffnDim dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[embedDim, kEmbedDim, vEmbedDim, numHeads], Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Transformer.MultiheadAttentionSpec embedDim kEmbedDim vEmbedDim numHeads dtype device) (Torch.Typed.NN.Transformer.MultiheadAttention embedDim kEmbedDim vEmbedDim numHeads dtype device)

module Torch.Typed.NN.Recurrent.LSTM
data LSTMLayerSpec (inputSize :: Nat) (hiddenSize :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
LSTMLayerSpec :: LSTMLayerSpec (inputSize :: Nat) (hiddenSize :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
data LSTMLayer (inputSize :: Nat) (hiddenSize :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
[LSTMUnidirectionalLayer] :: Parameter device dtype (LSTMWIShape hiddenSize inputSize) -> Parameter device dtype (LSTMWHShape hiddenSize inputSize) -> Parameter device dtype (LSTMBIShape hiddenSize inputSize) -> Parameter device dtype (LSTMBHShape hiddenSize inputSize) -> LSTMLayer inputSize hiddenSize 'Unidirectional dtype device
[LSTMBidirectionalLayer] :: Parameter device dtype (LSTMWIShape hiddenSize inputSize) -> Parameter device dtype (LSTMWHShape hiddenSize inputSize) -> Parameter device dtype (LSTMBIShape hiddenSize inputSize) -> Parameter device dtype (LSTMBHShape hiddenSize inputSize) -> Parameter device dtype (LSTMWIShape hiddenSize inputSize) -> Parameter device dtype (LSTMWHShape hiddenSize inputSize) -> Parameter device dtype (LSTMBIShape hiddenSize inputSize) -> Parameter device dtype (LSTMBHShape hiddenSize inputSize) -> LSTMLayer inputSize hiddenSize 'Bidirectional dtype device
data LSTMLayerStackSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
LSTMLayerStackSpec :: LSTMLayerStackSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
data LSTMLayerStack (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
[LSTMLayer1] :: LSTMLayer inputSize hiddenSize directionality dtype device -> LSTMLayerStack inputSize hiddenSize 1 directionality dtype device
[LSTMLayerK] :: LSTMLayer (hiddenSize * NumberOfDirections directionality) hiddenSize directionality dtype device -> LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device -> LSTMLayerStack inputSize hiddenSize (numLayers + 1) directionality dtype device
class LSTMLayerStackParameterized (flag :: Bool) inputSize hiddenSize numLayers directionality dtype device where {
    type family LSTMLayerStackParameters flag inputSize hiddenSize numLayers directionality dtype device :: [Type];
}
lstmLayerStackFlattenParameters :: LSTMLayerStackParameterized flag inputSize hiddenSize numLayers directionality dtype device => Proxy flag -> LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device -> HList (LSTMLayerStackParameters flag inputSize hiddenSize numLayers directionality dtype device)
lstmLayerStackReplaceParameters :: LSTMLayerStackParameterized flag inputSize hiddenSize numLayers directionality dtype device => Proxy flag -> LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device -> HList (LSTMLayerStackParameters flag inputSize hiddenSize numLayers directionality dtype device) -> LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device
class LSTMLayerStackRandomizable (flag :: Bool) inputSize hiddenSize numLayers directionality dtype device
lstmLayerStackSample :: LSTMLayerStackRandomizable flag inputSize hiddenSize numLayers directionality dtype device => Proxy flag -> LSTMLayerStackSpec inputSize hiddenSize numLayers directionality dtype device -> IO (LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device)
newtype LSTMSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
LSTMSpec :: DropoutSpec -> LSTMSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
data LSTM (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
[LSTM] :: 1 <= numLayers => {lstm_layer_stack :: LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device, lstm_dropout :: Dropout} -> LSTM inputSize hiddenSize numLayers directionality dtype device

-- | Helper to do xavier uniform initializations on weight matrices and
--   orthagonal initializations for the gates. (When implemented.)
xavierUniformLSTM :: forall device dtype hiddenSize featureSize. (KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize, KnownDevice device, RandDTypeIsValid device dtype) => IO (Tensor device dtype '[4 * hiddenSize, featureSize])

-- | A specification for a long, short-term memory layer.
data LSTMWithInitSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (initialization :: RNNInitialization) (dtype :: DType) (device :: (DeviceType, Nat))

-- | Weights drawn from Xavier-Uniform with zeros-value initialized biases
--   and cell states.
[LSTMWithZerosInitSpec] :: forall inputSize hiddenSize numLayers directionality dtype device. LSTMSpec inputSize hiddenSize numLayers directionality dtype device -> LSTMWithInitSpec inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device

-- | Weights drawn from Xavier-Uniform with zeros-value initialized biases
--   and user-provided cell states.
[LSTMWithConstInitSpec] :: forall inputSize hiddenSize numLayers directionality dtype device. LSTMSpec inputSize hiddenSize numLayers directionality dtype device -> Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize] -> Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize] -> LSTMWithInitSpec inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device

-- | Weights drawn from Xavier-Uniform with zeros-value initialized biases
--   and learned cell states.
[LSTMWithLearnedInitSpec] :: forall inputSize hiddenSize numLayers directionality dtype device. LSTMSpec inputSize hiddenSize numLayers directionality dtype device -> Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize] -> Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize] -> LSTMWithInitSpec inputSize hiddenSize numLayers directionality 'LearnedInitialization dtype device

-- | A long, short-term memory layer with either fixed initial states for
--   the memory cells and hidden state or learnable inital states for the
--   memory cells and hidden state.
data LSTMWithInit (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (initialization :: RNNInitialization) (dtype :: DType) (device :: (DeviceType, Nat))
[LSTMWithConstInit] :: forall inputSize hiddenSize numLayers directionality dtype device. {lstmWithConstInit_lstm :: LSTM inputSize hiddenSize numLayers directionality dtype device, lstmWithConstInit_c :: Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize], lstmWithConstInit_h :: Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize]} -> LSTMWithInit inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device
[LSTMWithLearnedInit] :: forall inputSize hiddenSize numLayers directionality dtype device. {lstmWithLearnedInit_lstm :: LSTM inputSize hiddenSize numLayers directionality dtype device, lstmWithLearnedInit_c :: Parameter device dtype '[numLayers * NumberOfDirections directionality, hiddenSize], lstmWithLearnedInit_h :: Parameter device dtype '[numLayers * NumberOfDirections directionality, hiddenSize]} -> LSTMWithInit inputSize hiddenSize numLayers directionality 'LearnedInitialization dtype device
lstmForward :: forall shapeOrder batchSize seqLen directionality initialization numLayers inputSize outputSize hiddenSize inputShape outputShape hxShape parameters tensorParameters dtype device. (KnownNat (NumberOfDirections directionality), KnownNat numLayers, KnownNat batchSize, KnownNat hiddenSize, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hxShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], parameters ~ Parameters (LSTM inputSize hiddenSize numLayers directionality dtype device), Parameterized (LSTM inputSize hiddenSize numLayers directionality dtype device), tensorParameters ~ LSTMR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor], HMap' ToDependent parameters tensorParameters) => Bool -> LSTMWithInit inputSize hiddenSize numLayers directionality initialization dtype device -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hxShape, Tensor device dtype hxShape)

-- | Forward propagate the <a>LSTM</a> module (without applying dropout on
--   the outputs of each layer).
--   
--   <pre>
--   &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn
--   
--   &gt;&gt;&gt; spec = LSTMWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (LSTMSpec (DropoutSpec 0.5))
--   
--   &gt;&gt;&gt; model &lt;- A.sample spec
--   
--   &gt;&gt;&gt; :t lstmForwardWithoutDropout @'BatchFirst model input
--   lstmForwardWithoutDropout @'BatchFirst model input
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])
--   
--   &gt;&gt;&gt; (a,b,c) = lstmForwardWithoutDropout @'BatchFirst model input
--   
--   &gt;&gt;&gt; ((dtype a, shape a), (dtype b, shape b), (dtype c, shape c))
--   ((Float,[5,16,60]),(Float,[6,5,30]),(Float,[6,5,30]))
--   </pre>
--   
--   Forward propagate the <a>LSTM</a> module and apply dropout on the
--   outputs of each layer.
--   
--   <pre>
--   &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn
--   
--   &gt;&gt;&gt; spec = LSTMWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (LSTMSpec (DropoutSpec 0.5))
--   
--   &gt;&gt;&gt; model &lt;- A.sample spec
--   
--   &gt;&gt;&gt; :t lstmForwardWithDropout @'BatchFirst model input
--   lstmForwardWithDropout @'BatchFirst model input
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])
--   
--   &gt;&gt;&gt; (a,b,c) = lstmForwardWithDropout @'BatchFirst model input
--   
--   &gt;&gt;&gt; ((dtype a, shape a), (dtype b, shape b), (dtype c, shape c))
--   ((Float,[5,16,60]),(Float,[6,5,30]),(Float,[6,5,30]))
--   </pre>
lstmForwardWithDropout :: forall shapeOrder batchSize seqLen directionality initialization numLayers inputSize outputSize hiddenSize inputShape outputShape hxShape parameters tensorParameters dtype device. (KnownNat (NumberOfDirections directionality), KnownNat numLayers, KnownNat batchSize, KnownNat hiddenSize, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hxShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], parameters ~ Parameters (LSTM inputSize hiddenSize numLayers directionality dtype device), Parameterized (LSTM inputSize hiddenSize numLayers directionality dtype device), tensorParameters ~ LSTMR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor], HMap' ToDependent parameters tensorParameters) => LSTMWithInit inputSize hiddenSize numLayers directionality initialization dtype device -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hxShape, Tensor device dtype hxShape)

-- | Forward propagate the <a>LSTM</a> module and apply dropout on the
--   outputs of each layer.
--   
--   <pre>
--   &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn
--   
--   &gt;&gt;&gt; spec = LSTMWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (LSTMSpec (DropoutSpec 0.5))
--   
--   &gt;&gt;&gt; model &lt;- A.sample spec
--   
--   &gt;&gt;&gt; :t lstmForwardWithDropout @'BatchFirst model input
--   lstmForwardWithDropout @'BatchFirst model input
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])
--   
--   &gt;&gt;&gt; (a,b,c) = lstmForwardWithDropout @'BatchFirst model input
--   
--   &gt;&gt;&gt; ((dtype a, shape a), (dtype b, shape b), (dtype c, shape c))
--   ((Float,[5,16,60]),(Float,[6,5,30]),(Float,[6,5,30]))
--   </pre>
lstmForwardWithoutDropout :: forall shapeOrder batchSize seqLen directionality initialization numLayers inputSize outputSize hiddenSize inputShape outputShape hxShape parameters tensorParameters dtype device. (KnownNat (NumberOfDirections directionality), KnownNat numLayers, KnownNat batchSize, KnownNat hiddenSize, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hxShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], parameters ~ Parameters (LSTM inputSize hiddenSize numLayers directionality dtype device), Parameterized (LSTM inputSize hiddenSize numLayers directionality dtype device), tensorParameters ~ LSTMR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor], HMap' ToDependent parameters tensorParameters) => LSTMWithInit inputSize hiddenSize numLayers directionality initialization dtype device -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hxShape, Tensor device dtype hxShape)
instance GHC.Classes.Eq (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerSpec inputSize hiddenSize directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerSpec inputSize hiddenSize directionality dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.LSTM.LSTMSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInitSpec inputSize hiddenSize numLayers directionality initialization dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality initialization dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device)
instance (Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)) '[] (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device) Torch.HList.++ '[])) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device)
instance (Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)) '[Torch.Typed.Parameter.Parameter device dtype '[numLayers GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality, hiddenSize], Torch.Typed.Parameter.Parameter device dtype '[numLayers GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality, hiddenSize]] (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device) Torch.HList.++ '[Torch.Typed.Parameter.Parameter device dtype '[numLayers GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality, hiddenSize], Torch.Typed.Parameter.Parameter device dtype '[numLayers GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality, hiddenSize]])) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device)
instance (GHC.TypeNats.KnownNat hiddenSize, GHC.TypeNats.KnownNat numLayers, GHC.TypeNats.KnownNat (Torch.Typed.Functional.NumberOfDirections directionality), Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInitSpec inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device)
instance (GHC.TypeNats.KnownNat hiddenSize, GHC.TypeNats.KnownNat numLayers, GHC.TypeNats.KnownNat (Torch.Typed.Functional.NumberOfDirections directionality), Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInitSpec inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device)
instance Torch.NN.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMWithInit inputSize hiddenSize numLayers directionality initialization dtype device)
instance (1 GHC.TypeNats.<= numLayers) => GHC.Generics.Generic (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)
instance (1 GHC.TypeNats.<= numLayers, Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device)) (Torch.Typed.Parameter.Parameters Torch.Typed.NN.Dropout.Dropout) (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device) Torch.HList.++ Torch.Typed.Parameter.Parameters Torch.Typed.NN.Dropout.Dropout)) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)
instance Torch.NN.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)
instance (Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, GHC.TypeNats.KnownNat inputSize, GHC.TypeNats.KnownNat hiddenSize, GHC.TypeNats.KnownNat (Torch.Typed.Functional.NumberOfDirections directionality), Torch.Typed.Factories.RandDTypeIsValid device dtype, Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device), 1 GHC.TypeNats.<= numLayers) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTM inputSize hiddenSize numLayers directionality dtype device)
instance Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerSpec inputSize hiddenSize directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize directionality dtype device) => Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackRandomizable 'GHC.Types.False inputSize hiddenSize 1 directionality dtype device
instance (Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerSpec (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device), Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackSpec inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device)) => Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackRandomizable 'GHC.Types.True inputSize hiddenSize numLayers directionality dtype device
instance (1 GHC.TypeNats.<= numLayers, (2 GHC.TypeNats.<=? numLayers) GHC.Types.~ flag, Torch.Typed.Factories.RandDTypeIsValid device dtype, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackRandomizable flag inputSize hiddenSize numLayers directionality dtype device) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize directionality dtype device) => Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackParameterized 'GHC.Types.False inputSize hiddenSize 1 directionality dtype device
instance (Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device), Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device)) (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device)) (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device) Torch.HList.++ Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device))) => Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackParameterized 'GHC.Types.True inputSize hiddenSize numLayers directionality dtype device
instance (1 GHC.TypeNats.<= numLayers, (2 GHC.TypeNats.<=? numLayers) GHC.Types.~ flag, Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStackParameterized flag inputSize hiddenSize numLayers directionality dtype device) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device)
instance Torch.NN.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerStack inputSize hiddenSize numLayers directionality dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize 'Torch.Typed.Functional.Unidirectional dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize 'Torch.Typed.Functional.Bidirectional dtype device)
instance (Torch.Typed.Factories.RandDTypeIsValid device dtype, GHC.TypeNats.KnownNat inputSize, GHC.TypeNats.KnownNat hiddenSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerSpec inputSize hiddenSize 'Torch.Typed.Functional.Unidirectional dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize 'Torch.Typed.Functional.Unidirectional dtype device)
instance (Torch.Typed.Factories.RandDTypeIsValid device dtype, GHC.TypeNats.KnownNat inputSize, GHC.TypeNats.KnownNat hiddenSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.LSTM.LSTMLayerSpec inputSize hiddenSize 'Torch.Typed.Functional.Bidirectional dtype device) (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize 'Torch.Typed.Functional.Bidirectional dtype device)
instance Torch.NN.Parameterized (Torch.Typed.NN.Recurrent.LSTM.LSTMLayer inputSize hiddenSize directionality dtype device)

module Torch.Typed.NN.Recurrent.GRU
data GRULayerSpec (inputSize :: Nat) (hiddenSize :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
GRULayerSpec :: GRULayerSpec (inputSize :: Nat) (hiddenSize :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
data GRULayer (inputSize :: Nat) (hiddenSize :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
[GRUUnidirectionalLayer] :: Parameter device dtype (GRUWIShape hiddenSize inputSize) -> Parameter device dtype (GRUWHShape hiddenSize inputSize) -> Parameter device dtype (GRUBIShape hiddenSize inputSize) -> Parameter device dtype (GRUBHShape hiddenSize inputSize) -> GRULayer inputSize hiddenSize 'Unidirectional dtype device
[GRUBidirectionalLayer] :: Parameter device dtype (GRUWIShape hiddenSize inputSize) -> Parameter device dtype (GRUWHShape hiddenSize inputSize) -> Parameter device dtype (GRUBIShape hiddenSize inputSize) -> Parameter device dtype (GRUBHShape hiddenSize inputSize) -> Parameter device dtype (GRUWIShape hiddenSize inputSize) -> Parameter device dtype (GRUWHShape hiddenSize inputSize) -> Parameter device dtype (GRUBIShape hiddenSize inputSize) -> Parameter device dtype (GRUBHShape hiddenSize inputSize) -> GRULayer inputSize hiddenSize 'Bidirectional dtype device
data GRULayerStackSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
GRULayerStackSpec :: GRULayerStackSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
data GRULayerStack (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
[GRULayer1] :: GRULayer inputSize hiddenSize directionality dtype device -> GRULayerStack inputSize hiddenSize 1 directionality dtype device
[GRULayerK] :: GRULayer (hiddenSize * NumberOfDirections directionality) hiddenSize directionality dtype device -> GRULayerStack inputSize hiddenSize numLayers directionality dtype device -> GRULayerStack inputSize hiddenSize (numLayers + 1) directionality dtype device
class GRULayerStackParameterized (flag :: Bool) inputSize hiddenSize numLayers directionality dtype device where {
    type family GRULayerStackParameters flag inputSize hiddenSize numLayers directionality dtype device :: [Type];
}
gruLayerStackFlattenParameters :: GRULayerStackParameterized flag inputSize hiddenSize numLayers directionality dtype device => Proxy flag -> GRULayerStack inputSize hiddenSize numLayers directionality dtype device -> HList (GRULayerStackParameters flag inputSize hiddenSize numLayers directionality dtype device)
gruLayerStackReplaceParameters :: GRULayerStackParameterized flag inputSize hiddenSize numLayers directionality dtype device => Proxy flag -> GRULayerStack inputSize hiddenSize numLayers directionality dtype device -> HList (GRULayerStackParameters flag inputSize hiddenSize numLayers directionality dtype device) -> GRULayerStack inputSize hiddenSize numLayers directionality dtype device
class GRULayerStackRandomizable (flag :: Bool) inputSize hiddenSize numLayers directionality dtype device
gruLayerStackSample :: GRULayerStackRandomizable flag inputSize hiddenSize numLayers directionality dtype device => Proxy flag -> GRULayerStackSpec inputSize hiddenSize numLayers directionality dtype device -> IO (GRULayerStack inputSize hiddenSize numLayers directionality dtype device)
newtype GRUSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
GRUSpec :: DropoutSpec -> GRUSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
data GRU (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (dtype :: DType) (device :: (DeviceType, Nat))
[GRU] :: 1 <= numLayers => {gru_layer_stack :: GRULayerStack inputSize hiddenSize numLayers directionality dtype device, gru_dropout :: Dropout} -> GRU inputSize hiddenSize numLayers directionality dtype device

-- | Helper to do xavier uniform initializations on weight matrices and
--   orthagonal initializations for the gates. (When implemented.)
xavierUniformGRU :: forall device dtype hiddenSize featureSize. (KnownDType dtype, KnownNat hiddenSize, KnownNat featureSize, KnownDevice device, RandDTypeIsValid device dtype) => IO (Tensor device dtype '[3 * hiddenSize, featureSize])

-- | A specification for a long, short-term memory layer.
data GRUWithInitSpec (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (initialization :: RNNInitialization) (dtype :: DType) (device :: (DeviceType, Nat))

-- | Weights drawn from Xavier-Uniform with zeros-value initialized biases
--   and cell states.
[GRUWithZerosInitSpec] :: forall inputSize hiddenSize numLayers directionality dtype device. GRUSpec inputSize hiddenSize numLayers directionality dtype device -> GRUWithInitSpec inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device

-- | Weights drawn from Xavier-Uniform with zeros-value initialized biases
--   and user-provided cell states.
[GRUWithConstInitSpec] :: forall inputSize hiddenSize numLayers directionality dtype device. GRUSpec inputSize hiddenSize numLayers directionality dtype device -> Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize] -> GRUWithInitSpec inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device

-- | Weights drawn from Xavier-Uniform with zeros-value initialized biases
--   and learned cell states.
[GRUWithLearnedInitSpec] :: forall inputSize hiddenSize numLayers directionality dtype device. GRUSpec inputSize hiddenSize numLayers directionality dtype device -> Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize] -> GRUWithInitSpec inputSize hiddenSize numLayers directionality 'LearnedInitialization dtype device

-- | A long, short-term memory layer with either fixed initial states for
--   the memory cells and hidden state or learnable inital states for the
--   memory cells and hidden state.
data GRUWithInit (inputSize :: Nat) (hiddenSize :: Nat) (numLayers :: Nat) (directionality :: RNNDirectionality) (initialization :: RNNInitialization) (dtype :: DType) (device :: (DeviceType, Nat))
[GRUWithConstInit] :: forall inputSize hiddenSize numLayers directionality dtype device. {gruWithConstInit_gru :: GRU inputSize hiddenSize numLayers directionality dtype device, gruWithConstInit_h :: Tensor device dtype '[numLayers * NumberOfDirections directionality, hiddenSize]} -> GRUWithInit inputSize hiddenSize numLayers directionality 'ConstantInitialization dtype device
[GRUWithLearnedInit] :: forall inputSize hiddenSize numLayers directionality dtype device. {gruWithLearnedInit_gru :: GRU inputSize hiddenSize numLayers directionality dtype device, gruWithLearnedInit_h :: Parameter device dtype '[numLayers * NumberOfDirections directionality, hiddenSize]} -> GRUWithInit inputSize hiddenSize numLayers directionality 'LearnedInitialization dtype device
gruForward :: forall shapeOrder batchSize seqLen directionality initialization numLayers inputSize outputSize hiddenSize inputShape outputShape hcShape parameters tensorParameters dtype device. (KnownNat (NumberOfDirections directionality), KnownNat numLayers, KnownNat batchSize, KnownNat hiddenSize, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hcShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], parameters ~ Parameters (GRU inputSize hiddenSize numLayers directionality dtype device), Parameterized (GRU inputSize hiddenSize numLayers directionality dtype device), tensorParameters ~ GRUR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor], HMap' ToDependent parameters tensorParameters) => Bool -> GRUWithInit inputSize hiddenSize numLayers directionality initialization dtype device -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hcShape)

-- | Forward propagate the <a>GRU</a> module (without applying dropout on
--   the outputs of each layer).
--   
--   <pre>
--   &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn
--   
--   &gt;&gt;&gt; spec = GRUWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (GRUSpec (DropoutSpec 0.5))
--   
--   &gt;&gt;&gt; model &lt;- A.sample spec
--   
--   &gt;&gt;&gt; :t gruForwardWithoutDropout @'BatchFirst model input
--   gruForwardWithoutDropout @'BatchFirst model input
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])
--   
--   &gt;&gt;&gt; (a,b) = gruForwardWithoutDropout @'BatchFirst model input
--   
--   &gt;&gt;&gt; ((dtype a, shape a), (dtype b, shape b))
--   ((Float,[5,16,60]),(Float,[6,5,30]))
--   </pre>
--   
--   Forward propagate the <a>GRU</a> module and apply dropout on the
--   outputs of each layer.
--   
--   <pre>
--   &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn
--   
--   &gt;&gt;&gt; spec = GRUWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (GRUSpec (DropoutSpec 0.5))
--   
--   &gt;&gt;&gt; model &lt;- A.sample spec
--   
--   &gt;&gt;&gt; :t gruForwardWithDropout @'BatchFirst model input
--   gruForwardWithDropout @'BatchFirst model input
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])
--   
--   &gt;&gt;&gt; (a,b) = gruForwardWithDropout @'BatchFirst model input
--   
--   &gt;&gt;&gt; ((dtype a, shape a), (dtype b, shape b))
--   ((Float,[5,16,60]),(Float,[6,5,30]))
--   </pre>
gruForwardWithDropout :: forall shapeOrder batchSize seqLen directionality initialization numLayers inputSize outputSize hiddenSize inputShape outputShape hcShape parameters tensorParameters dtype device. (KnownNat (NumberOfDirections directionality), KnownNat numLayers, KnownNat batchSize, KnownNat hiddenSize, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hcShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], parameters ~ Parameters (GRU inputSize hiddenSize numLayers directionality dtype device), Parameterized (GRU inputSize hiddenSize numLayers directionality dtype device), tensorParameters ~ GRUR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor], HMap' ToDependent parameters tensorParameters) => GRUWithInit inputSize hiddenSize numLayers directionality initialization dtype device -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hcShape)

-- | Forward propagate the <a>GRU</a> module and apply dropout on the
--   outputs of each layer.
--   
--   <pre>
--   &gt;&gt;&gt; input :: CPUTensor 'D.Float '[5,16,10] &lt;- randn
--   
--   &gt;&gt;&gt; spec = GRUWithZerosInitSpec @10 @30 @3 @'Bidirectional @'D.Float @'( 'D.CPU, 0) (GRUSpec (DropoutSpec 0.5))
--   
--   &gt;&gt;&gt; model &lt;- A.sample spec
--   
--   &gt;&gt;&gt; :t gruForwardWithDropout @'BatchFirst model input
--   gruForwardWithDropout @'BatchFirst model input
--     :: (Tensor '( 'D.CPU, 0) 'D.Float '[5, 16, 60],
--         Tensor '( 'D.CPU, 0) 'D.Float '[6, 5, 30])
--   
--   &gt;&gt;&gt; (a,b) = gruForwardWithDropout @'BatchFirst model input
--   
--   &gt;&gt;&gt; ((dtype a, shape a), (dtype b, shape b))
--   ((Float,[5,16,60]),(Float,[6,5,30]))
--   </pre>
gruForwardWithoutDropout :: forall shapeOrder batchSize seqLen directionality initialization numLayers inputSize outputSize hiddenSize inputShape outputShape hcShape parameters tensorParameters dtype device. (KnownNat (NumberOfDirections directionality), KnownNat numLayers, KnownNat batchSize, KnownNat hiddenSize, KnownRNNShapeOrder shapeOrder, KnownRNNDirectionality directionality, outputSize ~ (hiddenSize * NumberOfDirections directionality), inputShape ~ RNNShape shapeOrder seqLen batchSize inputSize, outputShape ~ RNNShape shapeOrder seqLen batchSize outputSize, hcShape ~ '[numLayers * NumberOfDirections directionality, batchSize, hiddenSize], parameters ~ Parameters (GRU inputSize hiddenSize numLayers directionality dtype device), Parameterized (GRU inputSize hiddenSize numLayers directionality dtype device), tensorParameters ~ GRUR inputSize hiddenSize numLayers directionality dtype device, Castable (HList tensorParameters) [ATenTensor], HMap' ToDependent parameters tensorParameters) => GRUWithInit inputSize hiddenSize numLayers directionality initialization dtype device -> Tensor device dtype inputShape -> (Tensor device dtype outputShape, Tensor device dtype hcShape)
instance GHC.Classes.Eq (Torch.Typed.NN.Recurrent.GRU.GRULayerSpec inputSize hiddenSize directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRULayerSpec inputSize hiddenSize directionality dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Recurrent.GRU.GRULayerStackSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRULayerStackSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.GRU.GRUSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRUSpec inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRUWithInitSpec inputSize hiddenSize numLayers directionality initialization dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality initialization dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device)
instance (Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)) '[] (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device) Torch.HList.++ '[])) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device)
instance (Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)) '[Torch.Typed.Parameter.Parameter device dtype '[numLayers GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality, hiddenSize]] (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device) Torch.HList.++ '[Torch.Typed.Parameter.Parameter device dtype '[numLayers GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality, hiddenSize]])) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device)
instance (GHC.TypeNats.KnownNat hiddenSize, GHC.TypeNats.KnownNat numLayers, GHC.TypeNats.KnownNat (Torch.Typed.Functional.NumberOfDirections directionality), Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRUSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRUWithInitSpec inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device) (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.ConstantInitialization dtype device)
instance (GHC.TypeNats.KnownNat hiddenSize, GHC.TypeNats.KnownNat numLayers, GHC.TypeNats.KnownNat (Torch.Typed.Functional.NumberOfDirections directionality), Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRUSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRUWithInitSpec inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device) (Torch.Typed.NN.Recurrent.GRU.GRUWithInit inputSize hiddenSize numLayers directionality 'Torch.Typed.NN.Recurrent.Aux.LearnedInitialization dtype device)
instance (1 GHC.TypeNats.<= numLayers) => GHC.Generics.Generic (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)
instance (1 GHC.TypeNats.<= numLayers, Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device)) (Torch.Typed.Parameter.Parameters Torch.Typed.NN.Dropout.Dropout) (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device) Torch.HList.++ Torch.Typed.Parameter.Parameters Torch.Typed.NN.Dropout.Dropout)) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)
instance (Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, GHC.TypeNats.KnownNat inputSize, GHC.TypeNats.KnownNat hiddenSize, GHC.TypeNats.KnownNat (Torch.Typed.Functional.NumberOfDirections directionality), Torch.Typed.Factories.RandDTypeIsValid device dtype, Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerStackSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device), 1 GHC.TypeNats.<= numLayers) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRUSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRU inputSize hiddenSize numLayers directionality dtype device)
instance Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerSpec inputSize hiddenSize directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize directionality dtype device) => Torch.Typed.NN.Recurrent.GRU.GRULayerStackRandomizable 'GHC.Types.False inputSize hiddenSize 1 directionality dtype device
instance (Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerSpec (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device), Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerStackSpec inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device)) => Torch.Typed.NN.Recurrent.GRU.GRULayerStackRandomizable 'GHC.Types.True inputSize hiddenSize numLayers directionality dtype device
instance (1 GHC.TypeNats.<= numLayers, (2 GHC.TypeNats.<=? numLayers) GHC.Types.~ flag, Torch.Typed.Factories.RandDTypeIsValid device dtype, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.NN.Recurrent.GRU.GRULayerStackRandomizable flag inputSize hiddenSize numLayers directionality dtype device) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerStackSpec inputSize hiddenSize numLayers directionality dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize directionality dtype device) => Torch.Typed.NN.Recurrent.GRU.GRULayerStackParameterized 'GHC.Types.False inputSize hiddenSize 1 directionality dtype device
instance (Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device), Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device), Torch.HList.HAppendFD (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device)) (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRULayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device)) (Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize (numLayers GHC.TypeNats.- 1) directionality dtype device) Torch.HList.++ Torch.Typed.Parameter.Parameters (Torch.Typed.NN.Recurrent.GRU.GRULayer (hiddenSize GHC.TypeNats.* Torch.Typed.Functional.NumberOfDirections directionality) hiddenSize directionality dtype device))) => Torch.Typed.NN.Recurrent.GRU.GRULayerStackParameterized 'GHC.Types.True inputSize hiddenSize numLayers directionality dtype device
instance (1 GHC.TypeNats.<= numLayers, (2 GHC.TypeNats.<=? numLayers) GHC.Types.~ flag, Torch.Typed.NN.Recurrent.GRU.GRULayerStackParameterized flag inputSize hiddenSize numLayers directionality dtype device) => Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayerStack inputSize hiddenSize numLayers directionality dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize 'Torch.Typed.Functional.Unidirectional dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize 'Torch.Typed.Functional.Bidirectional dtype device)
instance (Torch.Typed.Factories.RandDTypeIsValid device dtype, GHC.TypeNats.KnownNat inputSize, GHC.TypeNats.KnownNat hiddenSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerSpec inputSize hiddenSize 'Torch.Typed.Functional.Unidirectional dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize 'Torch.Typed.Functional.Unidirectional dtype device)
instance (Torch.Typed.Factories.RandDTypeIsValid device dtype, GHC.TypeNats.KnownNat inputSize, GHC.TypeNats.KnownNat hiddenSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.GRU.GRULayerSpec inputSize hiddenSize 'Torch.Typed.Functional.Bidirectional dtype device) (Torch.Typed.NN.Recurrent.GRU.GRULayer inputSize hiddenSize 'Torch.Typed.Functional.Bidirectional dtype device)

module Torch.Typed.NN.Recurrent.Cell.LSTM

-- | A specification for a long, short-term memory (LSTM) cell.
data LSTMCellSpec (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | Weights and biases are drawn from the standard normal distibution
--   (having mean 0 and variance 1)
LSTMCellSpec :: LSTMCellSpec (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | A long, short-term memory cell.
data LSTMCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
LSTMCell :: Parameter device dtype '[4 * hiddenDim, inputDim] -> Parameter device dtype '[4 * hiddenDim, hiddenDim] -> Parameter device dtype '[4 * hiddenDim] -> Parameter device dtype '[4 * hiddenDim] -> LSTMCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | input-to-hidden weights
[lstmCell_w_ih] :: LSTMCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[4 * hiddenDim, inputDim]

-- | hidden-to-hidden weights
[lstmCell_w_hh] :: LSTMCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[4 * hiddenDim, hiddenDim]

-- | input-to-hidden bias
[lstmCell_b_ih] :: LSTMCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[4 * hiddenDim]

-- | hidden-to-hidden bias
[lstmCell_b_hh] :: LSTMCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[4 * hiddenDim]

-- | A single recurrent step of an <a>LSTMCell</a>
lstmCellForward :: forall inputDim hiddenDim batchSize dtype device. (KnownDType dtype, KnownNat inputDim, KnownNat hiddenDim, KnownNat batchSize) => LSTMCell inputDim hiddenDim dtype device -> (Tensor device dtype '[batchSize, hiddenDim], Tensor device dtype '[batchSize, hiddenDim]) -> Tensor device dtype '[batchSize, inputDim] -> (Tensor device dtype '[batchSize, hiddenDim], Tensor device dtype '[batchSize, hiddenDim])

-- | foldl' for lists of tensors unsing an <a>LSTMCell</a>
lstmCellFold :: forall inputDim hiddenDim batchSize dtype device. (KnownDType dtype, KnownNat inputDim, KnownNat hiddenDim, KnownNat batchSize) => LSTMCell inputDim hiddenDim dtype device -> (Tensor device dtype '[batchSize, hiddenDim], Tensor device dtype '[batchSize, hiddenDim]) -> [Tensor device dtype '[batchSize, inputDim]] -> (Tensor device dtype '[batchSize, hiddenDim], Tensor device dtype '[batchSize, hiddenDim])

-- | scanl' for lists of tensors unsing an <a>LSTMCell</a>
lstmCellScan :: forall inputDim hiddenDim batchSize dtype device. (KnownDType dtype, KnownNat inputDim, KnownNat hiddenDim, KnownNat batchSize) => LSTMCell inputDim hiddenDim dtype device -> (Tensor device dtype '[batchSize, hiddenDim], Tensor device dtype '[batchSize, hiddenDim]) -> [Tensor device dtype '[batchSize, inputDim]] -> [(Tensor device dtype '[batchSize, hiddenDim], Tensor device dtype '[batchSize, hiddenDim])]
instance GHC.Enum.Bounded (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device)
instance GHC.Enum.Enum (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device)
instance GHC.Classes.Ord (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCell inputDim hiddenDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCell inputDim hiddenDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCell inputDim hiddenDim dtype device)
instance (Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.KnownDType dtype, GHC.TypeNats.KnownNat inputDim, GHC.TypeNats.KnownNat hiddenDim, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCellSpec inputDim hiddenDim dtype device) (Torch.Typed.NN.Recurrent.Cell.LSTM.LSTMCell inputDim hiddenDim dtype device)

module Torch.Typed.NN.Recurrent.Cell.GRU

-- | A specification for a gated recurrent unit (GRU) cell.
data GRUCellSpec (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | Weights and biases are drawn from the standard normal distibution
--   (having mean 0 and variance 1)
GRUCellSpec :: GRUCellSpec (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | A gated recurrent unit (GRU) cell.
data GRUCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
GRUCell :: Parameter device dtype '[3 * hiddenDim, inputDim] -> Parameter device dtype '[3 * hiddenDim, hiddenDim] -> Parameter device dtype '[3 * hiddenDim] -> Parameter device dtype '[3 * hiddenDim] -> GRUCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))

-- | input-to-hidden weights
[gruCell_w_ih] :: GRUCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[3 * hiddenDim, inputDim]

-- | hidden-to-hidden weights
[gruCell_w_hh] :: GRUCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[3 * hiddenDim, hiddenDim]

-- | input-to-hidden bias
[gruCell_b_ih] :: GRUCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[3 * hiddenDim]

-- | hidden-to-hidden bias
[gruCell_b_hh] :: GRUCell (inputDim :: Nat) (hiddenDim :: Nat) (dtype :: DType) (device :: (DeviceType, Nat)) -> Parameter device dtype '[3 * hiddenDim]

-- | A single recurrent step of a <a>GRUCell</a>
gruCellForward :: forall inputDim hiddenDim batchSize dtype device. (KnownDType dtype, KnownNat inputDim, KnownNat hiddenDim, KnownNat batchSize) => GRUCell inputDim hiddenDim dtype device -> Tensor device dtype '[batchSize, hiddenDim] -> Tensor device dtype '[batchSize, inputDim] -> Tensor device dtype '[batchSize, hiddenDim]

-- | foldl' for lists of tensors unsing a <a>GRUCell</a>
gruFold :: forall inputDim hiddenDim batchSize dtype device. (KnownDType dtype, KnownNat inputDim, KnownNat hiddenDim, KnownNat batchSize) => GRUCell inputDim hiddenDim dtype device -> Tensor device dtype '[batchSize, hiddenDim] -> [Tensor device dtype '[batchSize, inputDim]] -> Tensor device dtype '[batchSize, hiddenDim]

-- | scanl' for lists of tensors unsing a <a>GRUCell</a>
gruCellScan :: forall inputDim hiddenDim batchSize dtype device. (KnownDType dtype, KnownNat inputDim, KnownNat hiddenDim, KnownNat batchSize) => GRUCell inputDim hiddenDim dtype device -> Tensor device dtype '[batchSize, hiddenDim] -> [Tensor device dtype '[batchSize, inputDim]] -> [Tensor device dtype '[batchSize, hiddenDim]]
instance GHC.Enum.Bounded (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device)
instance GHC.Enum.Enum (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device)
instance GHC.Classes.Ord (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCell inputDim hiddenDim dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCell inputDim hiddenDim dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCell inputDim hiddenDim dtype device)
instance (Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.KnownDType dtype, GHC.TypeNats.KnownNat inputDim, GHC.TypeNats.KnownNat hiddenDim, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCellSpec inputDim hiddenDim dtype device) (Torch.Typed.NN.Recurrent.Cell.GRU.GRUCell inputDim hiddenDim dtype device)

module Torch.Typed.NN.Recurrent

module Torch.Typed.NN.Convolution
data Conv1dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
Conv1dSpec :: Conv1dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data Conv1d (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[Conv1d] :: forall inputChannelSize outputChannelSize kernelSize dtype device. {conv1dWeight :: Parameter device dtype '[outputChannelSize, inputChannelSize, kernelSize], conv1dBias :: Parameter device dtype '[outputChannelSize]} -> Conv1d inputChannelSize outputChannelSize kernelSize dtype device

-- | conv1d The constraints on this one are _very_ involved, so the partial
--   signatures make the code significantly cleaner.
conv1dForward :: forall stride padding. _ => Conv1d _ _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
data Conv2dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
Conv2dSpec :: Conv2dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data Conv2d (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[Conv2d] :: forall inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device. {conv2dWeight :: Parameter device dtype '[outputChannelSize, inputChannelSize, kernelSize0, kernelSize1], conv2dBias :: Parameter device dtype '[outputChannelSize]} -> Conv2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device

-- | conv2d The constraints on this one are _very_ involved, so the partial
--   signatures make the code significantly cleaner.
conv2dForward :: forall stride padding. _ => Conv2d _ _ _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
data Conv3dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (kernelSize2 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
Conv3dSpec :: Conv3dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (kernelSize2 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data Conv3d (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (kernelSize2 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[Conv3d] :: forall inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device. {conv3dWeight :: Parameter device dtype '[outputChannelSize, inputChannelSize, kernelSize0, kernelSize1, kernelSize2], conv3dBias :: Parameter device dtype '[outputChannelSize]} -> Conv3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device

-- | conv3d The constraints on this one are _very_ involved, so the partial
--   signatures make the code significantly cleaner.
conv3dForward :: forall stride padding. _ => Conv3d _ _ _ _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
data ConvTranspose1dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
ConvTranspose1dSpec :: ConvTranspose1dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data ConvTranspose1d (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[ConvTranspose1d] :: forall inputChannelSize outputChannelSize kernelSize dtype device. {convTranspose1dWeight :: Parameter device dtype '[inputChannelSize, outputChannelSize, kernelSize], convTranspose1dBias :: Parameter device dtype '[outputChannelSize]} -> ConvTranspose1d inputChannelSize outputChannelSize kernelSize dtype device

-- | convTranspose1d The constraints on this one are _very_ involved, so
--   the partial signatures make the code significantly cleaner.
convTranspose1dForward :: forall stride padding. _ => ConvTranspose1d _ _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
data ConvTranspose2dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
ConvTranspose2dSpec :: ConvTranspose2dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data ConvTranspose2d (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[ConvTranspose2d] :: forall inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device. {convTranspose2dWeight :: Parameter device dtype '[inputChannelSize, outputChannelSize, kernelSize0, kernelSize1], convTranspose2dBias :: Parameter device dtype '[outputChannelSize]} -> ConvTranspose2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device

-- | convTranspose2d The constraints on this one are _very_ involved, so
--   the partial signatures make the code significantly cleaner.
convTranspose2dForward :: forall stride padding. _ => ConvTranspose2d _ _ _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
data ConvTranspose3dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (kernelSize2 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
ConvTranspose3dSpec :: ConvTranspose3dSpec (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (kernelSize2 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
data ConvTranspose3d (inputChannelSize :: Nat) (outputChannelSize :: Nat) (kernelSize0 :: Nat) (kernelSize1 :: Nat) (kernelSize2 :: Nat) (dtype :: DType) (device :: (DeviceType, Nat))
[ConvTranspose3d] :: forall inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device. {convTranspose3dWeight :: Parameter device dtype '[inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, kernelSize2], convTranspose3dBias :: Parameter device dtype '[outputChannelSize]} -> ConvTranspose3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device

-- | convTranspose3d The constraints on this one are _very_ involved, so
--   the partial signatures make the code significantly cleaner.
convTranspose3dForward :: forall stride padding. _ => ConvTranspose3d _ _ _ _ _ _ _ -> Tensor _ _ _ -> Tensor _ _ _
instance GHC.Classes.Eq (Torch.Typed.NN.Convolution.Conv1dSpec inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.Conv1dSpec inputChannelSize outputChannelSize kernelSize dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Convolution.Conv1d inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Convolution.Conv1d inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.Conv1d inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Convolution.Conv2dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.Conv2dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Convolution.Conv2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Convolution.Conv2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.Conv2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Convolution.Conv3dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.Conv3dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Convolution.Conv3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Convolution.Conv3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.Conv3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Convolution.ConvTranspose1dSpec inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.ConvTranspose1dSpec inputChannelSize outputChannelSize kernelSize dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Convolution.ConvTranspose1d inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Convolution.ConvTranspose1d inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.ConvTranspose1d inputChannelSize outputChannelSize kernelSize dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Convolution.ConvTranspose2dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.ConvTranspose2dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Convolution.ConvTranspose2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Convolution.ConvTranspose2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.ConvTranspose2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance GHC.Classes.Eq (Torch.Typed.NN.Convolution.ConvTranspose3dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.ConvTranspose3dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.NN.Convolution.ConvTranspose3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Generics.Generic (Torch.Typed.NN.Convolution.ConvTranspose3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance GHC.Show.Show (Torch.Typed.NN.Convolution.ConvTranspose3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[Torch.Typed.Aux.Fst3 stride, Torch.Typed.Aux.Snd3 stride, Torch.Typed.Aux.Trd3 stride, Torch.Typed.Aux.Fst3 padding, Torch.Typed.Aux.Snd3 padding, Torch.Typed.Aux.Trd3 padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, kernelSize2, inputSize0, inputSize1, inputSize2, batchSize], Torch.Typed.Functional.ConvSideCheck inputSize0 kernelSize0 (Torch.Typed.Aux.Fst3 stride) (Torch.Typed.Aux.Fst3 padding) outputSize0, Torch.Typed.Functional.ConvSideCheck inputSize1 kernelSize1 (Torch.Typed.Aux.Snd3 stride) (Torch.Typed.Aux.Snd3 padding) outputSize1, Torch.Typed.Functional.ConvSideCheck inputSize2 kernelSize2 (Torch.Typed.Aux.Trd3 stride) (Torch.Typed.Aux.Trd3 padding) outputSize2) => Torch.NN.HasForward (Torch.Typed.NN.Convolution.ConvTranspose3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1, inputSize2], Data.Proxy.Proxy stride, Data.Proxy.Proxy padding) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1, outputSize2])
instance (GHC.TypeNats.KnownNat inputChannelSize, GHC.TypeNats.KnownNat outputChannelSize, GHC.TypeNats.KnownNat kernelSize0, GHC.TypeNats.KnownNat kernelSize1, GHC.TypeNats.KnownNat kernelSize2, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Convolution.ConvTranspose3dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device) (Torch.Typed.NN.Convolution.ConvTranspose3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[Torch.Typed.Aux.Fst stride, Torch.Typed.Aux.Snd stride, Torch.Typed.Aux.Fst padding, Torch.Typed.Aux.Snd padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, inputSize0, inputSize1, batchSize, outputSize0, outputSize1], Torch.Typed.Functional.ConvSideCheck inputSize0 kernelSize0 (Torch.Typed.Aux.Fst stride) (Torch.Typed.Aux.Fst padding) outputSize0, Torch.Typed.Functional.ConvSideCheck inputSize1 kernelSize1 (Torch.Typed.Aux.Snd stride) (Torch.Typed.Aux.Snd padding) outputSize1) => Torch.NN.HasForward (Torch.Typed.NN.Convolution.ConvTranspose2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1], Data.Proxy.Proxy stride, Data.Proxy.Proxy padding) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1])
instance (GHC.TypeNats.KnownNat inputChannelSize, GHC.TypeNats.KnownNat outputChannelSize, GHC.TypeNats.KnownNat kernelSize0, GHC.TypeNats.KnownNat kernelSize1, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Convolution.ConvTranspose2dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device) (Torch.Typed.NN.Convolution.ConvTranspose2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[stride, padding, inputChannelSize, outputChannelSize, kernelSize, inputSize, batchSize, outputSize], Torch.Typed.Functional.ConvSideCheck inputSize kernelSize stride padding outputSize) => Torch.NN.HasForward (Torch.Typed.NN.Convolution.ConvTranspose1d inputChannelSize outputChannelSize kernelSize dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, inputChannelSize, inputSize], Data.Proxy.Proxy stride, Data.Proxy.Proxy padding) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, outputChannelSize, outputSize])
instance (GHC.TypeNats.KnownNat inputChannelSize, GHC.TypeNats.KnownNat outputChannelSize, GHC.TypeNats.KnownNat kernelSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Convolution.ConvTranspose1dSpec inputChannelSize outputChannelSize kernelSize dtype device) (Torch.Typed.NN.Convolution.ConvTranspose1d inputChannelSize outputChannelSize kernelSize dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[Torch.Typed.Aux.Fst3 stride, Torch.Typed.Aux.Snd3 stride, Torch.Typed.Aux.Trd3 stride, Torch.Typed.Aux.Fst3 padding, Torch.Typed.Aux.Snd3 padding, Torch.Typed.Aux.Trd3 padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, kernelSize2, inputSize0, inputSize1, inputSize2, batchSize], Torch.Typed.Functional.ConvSideCheck inputSize0 kernelSize0 (Torch.Typed.Aux.Fst3 stride) (Torch.Typed.Aux.Fst3 padding) outputSize0, Torch.Typed.Functional.ConvSideCheck inputSize1 kernelSize1 (Torch.Typed.Aux.Snd3 stride) (Torch.Typed.Aux.Snd3 padding) outputSize1, Torch.Typed.Functional.ConvSideCheck inputSize2 kernelSize2 (Torch.Typed.Aux.Trd3 stride) (Torch.Typed.Aux.Trd3 padding) outputSize2) => Torch.NN.HasForward (Torch.Typed.NN.Convolution.Conv3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1, inputSize2], Data.Proxy.Proxy stride, Data.Proxy.Proxy padding) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1, outputSize2])
instance (GHC.TypeNats.KnownNat inputChannelSize, GHC.TypeNats.KnownNat outputChannelSize, GHC.TypeNats.KnownNat kernelSize0, GHC.TypeNats.KnownNat kernelSize1, GHC.TypeNats.KnownNat kernelSize2, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Convolution.Conv3dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device) (Torch.Typed.NN.Convolution.Conv3d inputChannelSize outputChannelSize kernelSize0 kernelSize1 kernelSize2 dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[Torch.Typed.Aux.Fst stride, Torch.Typed.Aux.Snd stride, Torch.Typed.Aux.Fst padding, Torch.Typed.Aux.Snd padding, inputChannelSize, outputChannelSize, kernelSize0, kernelSize1, inputSize0, inputSize1, batchSize, outputSize0, outputSize1], Torch.Typed.Functional.ConvSideCheck inputSize0 kernelSize0 (Torch.Typed.Aux.Fst stride) (Torch.Typed.Aux.Fst padding) outputSize0, Torch.Typed.Functional.ConvSideCheck inputSize1 kernelSize1 (Torch.Typed.Aux.Snd stride) (Torch.Typed.Aux.Snd padding) outputSize1) => Torch.NN.HasForward (Torch.Typed.NN.Convolution.Conv2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, inputChannelSize, inputSize0, inputSize1], Data.Proxy.Proxy stride, Data.Proxy.Proxy padding) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, outputChannelSize, outputSize0, outputSize1])
instance (GHC.TypeNats.KnownNat inputChannelSize, GHC.TypeNats.KnownNat outputChannelSize, GHC.TypeNats.KnownNat kernelSize0, GHC.TypeNats.KnownNat kernelSize1, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Convolution.Conv2dSpec inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device) (Torch.Typed.NN.Convolution.Conv2d inputChannelSize outputChannelSize kernelSize0 kernelSize1 dtype device)
instance (Torch.Typed.Tensor.All GHC.TypeNats.KnownNat '[stride, padding, inputChannelSize, outputChannelSize, kernelSize, inputSize, batchSize, outputSize], Torch.Typed.Functional.ConvSideCheck inputSize kernelSize stride padding outputSize) => Torch.NN.HasForward (Torch.Typed.NN.Convolution.Conv1d inputChannelSize outputChannelSize kernelSize dtype device) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, inputChannelSize, inputSize], Data.Proxy.Proxy stride, Data.Proxy.Proxy padding) (Torch.Typed.Tensor.Tensor device dtype '[batchSize, outputChannelSize, outputSize])
instance (GHC.TypeNats.KnownNat inputChannelSize, GHC.TypeNats.KnownNat outputChannelSize, GHC.TypeNats.KnownNat kernelSize, Torch.Typed.Tensor.KnownDType dtype, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Factories.RandDTypeIsValid device dtype) => Torch.NN.Randomizable (Torch.Typed.NN.Convolution.Conv1dSpec inputChannelSize outputChannelSize kernelSize dtype device) (Torch.Typed.NN.Convolution.Conv1d inputChannelSize outputChannelSize kernelSize dtype device)

module Torch.Distributions.Bernoulli
data Bernoulli
Bernoulli :: Tensor -> Tensor -> Bernoulli
[probs] :: Bernoulli -> Tensor
[logits] :: Bernoulli -> Tensor
fromProbs :: Tensor -> Bernoulli
fromLogits :: Tensor -> Bernoulli
instance GHC.Show.Show Torch.Distributions.Bernoulli.Bernoulli
instance Torch.Distributions.Distribution.Distribution Torch.Distributions.Bernoulli.Bernoulli

module Torch.Typed.Device
class HasToDevice (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat)) (f :: Type) (g :: Type) | device' device f -> g, device' device g -> f
toDevice :: HasToDevice device' device f g => f -> g
type family ReplaceDevice (f :: k) (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat)) :: k
type family ReplaceDevice' (f :: k) (device' :: (DeviceType, Nat)) :: k
class GHasToDevice (device' :: (DeviceType, Nat)) (device :: (DeviceType, Nat)) (f :: Type -> Type) (g :: Type -> Type)
gToDevice :: forall a. GHasToDevice device' device f g => f a -> g a
class HasReplicate (devices' :: [(DeviceType, Nat)]) (device :: (DeviceType, Nat)) (f :: Type) (gs :: [Type]) | devices' device f -> gs
replicate :: HasReplicate devices' device f gs => f -> HList gs
class HasToDevices (devices' :: [(DeviceType, Nat)]) (devices :: [(DeviceType, Nat)]) (fs :: [Type]) (gs :: [Type]) | devices' devices fs -> gs, devices' devices gs -> fs
toDevices :: HasToDevices devices' devices fs gs => HList fs -> HList gs
type family GetDevice (f :: k) :: Maybe (DeviceType, Nat)
type family GetDevices (fs :: [k]) :: [(DeviceType, Nat)]
class HasScatter devices' device f gs | devices' device f -> gs
scatter :: HasScatter devices' device f gs => f -> HList gs
class HasGather device' devices fs g | device' devices fs -> g
gather :: HasGather device' devices fs g => HList fs -> g
instance (chunks GHC.Types.~ Torch.HList.ListLength fs, devices GHC.Types.~ Torch.Typed.Device.GetDevices fs, devices' GHC.Types.~ Torch.HList.HReplicateR chunks device', Torch.Typed.Device.HasToDevices devices' devices fs tensorChunks, '(shape, dtype, device') GHC.Types.~ Torch.Typed.Functional.Cat 0 tensorChunks, Torch.Internal.Class.Castable (Torch.HList.HList tensorChunks) [Torch.Tensor.ATenTensor]) => Torch.Typed.Device.HasGather device' devices fs (Torch.Typed.Tensor.Tensor device' dtype shape)
instance (chunks GHC.Types.~ Torch.HList.ListLength devices', tensorChunks GHC.Types.~ Torch.Typed.Functional.Chunk chunks 0 shape dtype device, Torch.Internal.Class.Castable (Torch.HList.HList tensorChunks) [Torch.Tensor.ATenTensor], devices GHC.Types.~ Torch.HList.HReplicateR chunks device, Torch.Typed.Device.HasToDevices devices' devices tensorChunks gs, GHC.TypeNats.KnownNat chunks) => Torch.Typed.Device.HasScatter devices' device (Torch.Typed.Tensor.Tensor device dtype shape) gs
instance Torch.Typed.Device.HasToDevices '[] '[] '[] '[]
instance (Torch.Typed.Device.HasToDevices devices' devices fs gs, Torch.Typed.Device.HasToDevice device' device f g) => Torch.Typed.Device.HasToDevices (device' : devices') (device : devices) (f : fs) (g : gs)
instance Torch.Typed.Device.HasReplicate '[] device f '[]
instance (Torch.Typed.Device.HasReplicate devices' device f gs, Torch.Typed.Device.HasToDevice device' device f g) => Torch.Typed.Device.HasReplicate (device' : devices') device f (g : gs)
instance (g GHC.Types.~ Torch.Typed.Device.ReplaceDevice f device' device, f GHC.Types.~ Torch.Typed.Device.ReplaceDevice g device device', GHC.Generics.Generic f, GHC.Generics.Generic g, Torch.Typed.Device.GHasToDevice device' device (GHC.Generics.Rep f) (GHC.Generics.Rep g)) => Torch.Typed.Device.HasToDevice device' device f g
instance (Torch.Typed.Device.GHasToDevice device' device l l', Torch.Typed.Device.GHasToDevice device' device r r') => Torch.Typed.Device.GHasToDevice device' device (l GHC.Generics.:*: r) (l' GHC.Generics.:*: r')
instance Torch.Typed.Device.HasToDevice device' device f g => Torch.Typed.Device.GHasToDevice device' device (GHC.Generics.K1 i f) (GHC.Generics.K1 i g)
instance Torch.Typed.Device.GHasToDevice device' device f g => Torch.Typed.Device.GHasToDevice device' device (GHC.Generics.M1 i t f) (GHC.Generics.M1 i t g)
instance Torch.Typed.Device.GHasToDevice device' device GHC.Generics.U1 GHC.Generics.U1
instance Torch.Typed.Device.HasToDevice device' device GHC.Types.Double GHC.Types.Double
instance Torch.Typed.Tensor.KnownDevice device' => Torch.Typed.Device.HasToDevice device' device (Torch.Typed.Tensor.Tensor device dtype shape) (Torch.Typed.Tensor.Tensor device' dtype shape)
instance Torch.Typed.Tensor.KnownDevice device' => Torch.Typed.Device.HasToDevice device' device (Torch.Typed.Parameter.Parameter device dtype shape) (Torch.Typed.Parameter.Parameter device' dtype shape)
instance Torch.Typed.Device.HasToDevice device' device (Torch.HList.HList '[]) (Torch.HList.HList '[])
instance (Torch.Typed.Device.HasToDevice device' device x x', Torch.Typed.Device.HasToDevice device' device (Torch.HList.HList xs) (Torch.HList.HList xs')) => Torch.Typed.Device.HasToDevice device' device (Torch.HList.HList (x : xs)) (Torch.HList.HList (x' : xs'))

module Torch.Typed.DType
class HasToDType dtype' dtype f g | dtype' dtype f -> g, dtype' dtype g -> f
toDType :: HasToDType dtype' dtype f g => f -> g
type family ReplaceDType (f :: k) (dtype' :: DType) (dtype :: DType) :: k
type family ReplaceDType' (f :: k) (dtype' :: DType) :: k
class GHasToDType (dtype' :: DType) (dtype :: DType) (f :: Type -> Type) (g :: Type -> Type)
gToDType :: forall a. GHasToDType dtype' dtype f g => f a -> g a
type family GetDType (f :: k) :: Maybe DType
instance (g GHC.Types.~ Torch.Typed.DType.ReplaceDType f dtype' dtype, f GHC.Types.~ Torch.Typed.DType.ReplaceDType g dtype dtype', GHC.Generics.Generic f, GHC.Generics.Generic g, Torch.Typed.DType.GHasToDType dtype' dtype (GHC.Generics.Rep f) (GHC.Generics.Rep g)) => Torch.Typed.DType.HasToDType dtype' dtype f g
instance (Torch.Typed.DType.GHasToDType dtype' dtype l l', Torch.Typed.DType.GHasToDType dtype' dtype r r') => Torch.Typed.DType.GHasToDType dtype' dtype (l GHC.Generics.:*: r) (l' GHC.Generics.:*: r')
instance Torch.Typed.DType.HasToDType dtype' dtype f g => Torch.Typed.DType.GHasToDType dtype' dtype (GHC.Generics.K1 i f) (GHC.Generics.K1 i g)
instance Torch.Typed.DType.GHasToDType dtype' dtype f g => Torch.Typed.DType.GHasToDType dtype' dtype (GHC.Generics.M1 i t f) (GHC.Generics.M1 i t g)
instance Torch.Typed.DType.GHasToDType dtype' dtype GHC.Generics.U1 GHC.Generics.U1
instance Torch.Typed.Tensor.KnownDType dtype' => Torch.Typed.DType.HasToDType dtype' dtype (Torch.Typed.Tensor.Tensor device dtype shape) (Torch.Typed.Tensor.Tensor device dtype' shape)
instance Torch.Typed.Tensor.KnownDType dtype' => Torch.Typed.DType.HasToDType dtype' dtype (Torch.Typed.Parameter.Parameter device dtype shape) (Torch.Typed.Parameter.Parameter device dtype' shape)

module Torch.Typed.Autograd
class HasGrad a b | a -> b

-- | calculate gradients of a zero-dimensional tensor with respect to a
--   list of parameters
grad :: forall dtype device. HasGrad a b => Tensor device dtype '[] -> a -> b
instance Torch.Typed.Autograd.HasGrad (Torch.Typed.Parameter.Parameter device dtype shape) (Torch.Typed.Tensor.Tensor device dtype shape)
instance Torch.Typed.Autograd.HasGrad (Torch.HList.HList '[]) (Torch.HList.HList '[])
instance (Torch.Typed.Autograd.HasGrad a b, Torch.Typed.Autograd.HasGrad (Torch.HList.HList as) (Torch.HList.HList bs), Torch.Internal.Class.Castable (Torch.HList.HList (b : bs)) [Torch.Tensor.ATenTensor]) => Torch.Typed.Autograd.HasGrad (Torch.HList.HList (a : as)) (Torch.HList.HList (b : bs))

module Torch.Typed.Optim
type LearningRate device dtype = Tensor device dtype '[]
type Loss device dtype = Tensor device dtype '[]
data ZerosLike
ZerosLike :: ZerosLike
class Optimizer optim gradients tensors dtype device
step :: Optimizer optim gradients tensors dtype device => LearningRate device dtype -> HList gradients -> HList tensors -> optim -> (HList tensors, optim)
runStep :: forall model optim parameters gradients tensors dtype device. (Parameterized model, parameters ~ Parameters model, HasGrad (HList parameters) (HList gradients), tensors ~ gradients, HMap' ToDependent parameters tensors, Castable (HList gradients) [ATenTensor], Optimizer optim gradients tensors dtype device, HMapM' IO MakeIndependent tensors parameters) => model -> optim -> Loss device dtype -> LearningRate device dtype -> IO (model, optim)
runStep' :: forall model optim parameters gradients tensors dtype device. (Parameterized model, parameters ~ Parameters model, tensors ~ gradients, HMap' ToDependent parameters tensors, Optimizer optim gradients tensors dtype device, HMapM' IO MakeIndependent tensors parameters) => model -> optim -> LearningRate device dtype -> HList gradients -> IO (model, optim)

-- | Dummy state representation for GD Optimizer
data GD
GD :: GD
mkGD :: GD
newtype GDStep device dtype
GDStep :: LearningRate device dtype -> GDStep device dtype

-- | Gradient descent step with a dummy state variable
gd :: forall gradients tensors dtype device. HZipWith (GDStep device dtype) tensors gradients tensors => LearningRate device dtype -> HList gradients -> HList tensors -> GD -> (HList tensors, GD)

-- | State representation for GDM Optimizer
data GDM (momenta :: [Type])
GDM :: Float -> HList momenta -> GDM (momenta :: [Type])
[beta] :: GDM (momenta :: [Type]) -> Float
[momenta] :: GDM (momenta :: [Type]) -> HList momenta
mkGDM :: forall parameters momenta. HMap' ZerosLike parameters momenta => Float -> HList parameters -> GDM momenta
data GDMStep device dtype
GDMStep :: Float -> LearningRate device dtype -> GDMStep device dtype

-- | gradient descent with momentum step
gdm :: forall gradients tensors momenta gdmStep dtype device. (HZipWith3 (GDMStep device dtype) tensors gradients momenta gdmStep, HMap' AFst gdmStep tensors, HMap' ASnd gdmStep momenta) => LearningRate device dtype -> HList gradients -> HList tensors -> GDM momenta -> (HList tensors, GDM momenta)
type AdamIter = Tensor '( 'CPU, 0) 'Int64 '[]

-- | State representation for Adam Optimizer
data Adam (momenta :: [Type])
Adam :: AdamIter -> Float -> Float -> HList momenta -> HList momenta -> Adam (momenta :: [Type])
[iter] :: Adam (momenta :: [Type]) -> AdamIter
[beta1] :: Adam (momenta :: [Type]) -> Float
[beta2] :: Adam (momenta :: [Type]) -> Float
[momenta1] :: Adam (momenta :: [Type]) -> HList momenta
[momenta2] :: Adam (momenta :: [Type]) -> HList momenta
mkAdam :: forall parameters momenta. HMap' ZerosLike parameters momenta => AdamIter -> Float -> Float -> HList parameters -> Adam momenta
newtype AdamMomentum1Update
AdamMomentum1Update :: Float -> AdamMomentum1Update
newtype AdamMomentum2Update
AdamMomentum2Update :: Float -> AdamMomentum2Update
data AdamBiasAdjustment
AdamBiasAdjustment :: AdamIter -> Float -> AdamBiasAdjustment
data AdamParameterUpdate device dtype
AdamParameterUpdate :: Float -> LearningRate device dtype -> AdamParameterUpdate device dtype

-- | Adam step
adam :: forall gradients tensors momenta adamStep dtype device. (HZipWith AdamMomentum1Update momenta gradients momenta, HZipWith AdamMomentum2Update momenta gradients momenta, HMap' AdamBiasAdjustment momenta momenta, HZipWith3 (AdamParameterUpdate device dtype) tensors momenta momenta tensors) => LearningRate device dtype -> HList gradients -> HList tensors -> Adam momenta -> (HList tensors, Adam momenta)
instance (parameter GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, momentum GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, shape GHC.Types.~ Torch.Typed.Tensor.Broadcast '[] shape, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype, Torch.Typed.Aux.StandardFloatingPointDTypeValidation device dtype) => Torch.HList.Apply' (Torch.Typed.Optim.AdamParameterUpdate device dtype) (parameter, momentum, momentum) parameter
instance (Torch.HList.HZipWith Torch.Typed.Optim.AdamMomentum1Update momenta gradients momenta, Torch.HList.HZipWith Torch.Typed.Optim.AdamMomentum2Update momenta gradients momenta, Torch.HList.HMap' Torch.Typed.Optim.AdamBiasAdjustment momenta momenta, Torch.HList.HZipWith3 (Torch.Typed.Optim.AdamParameterUpdate device dtype) tensors momenta momenta tensors) => Torch.Typed.Optim.Optimizer (Torch.Typed.Optim.Adam momenta) gradients tensors dtype device
instance (momentum GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.KnownDType dtype, shape GHC.Types.~ Torch.Typed.Aux.Reverse (Torch.Typed.Aux.Reverse shape), Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype) => Torch.HList.Apply' Torch.Typed.Optim.AdamBiasAdjustment momentum momentum
instance (gradient GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, momentum2 GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, shape GHC.Types.~ Torch.Typed.Tensor.Broadcast shape shape, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype) => Torch.HList.Apply' Torch.Typed.Optim.AdamMomentum2Update (momentum2, gradient) momentum2
instance (gradient GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, momentum1 GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, Torch.Typed.Tensor.KnownDevice device) => Torch.HList.Apply' Torch.Typed.Optim.AdamMomentum1Update (momentum1, gradient) momentum1
instance Torch.HList.HAppendFD momenta momenta (momenta Torch.HList.++ momenta) => Torch.Typed.Parameter.Parameterized (Torch.Typed.Optim.Adam momenta)
instance (parameter GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, gradient GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, momentum GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, shape GHC.Types.~ Torch.Typed.Tensor.Broadcast '[] shape, Torch.Typed.Tensor.KnownDevice device, Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype) => Torch.HList.Apply' (Torch.Typed.Optim.GDMStep device dtype) (parameter, gradient, momentum) (parameter, momentum)
instance (Torch.HList.HZipWith3 (Torch.Typed.Optim.GDMStep device dtype) tensors gradients momenta gdmStep, Torch.HList.HMap' Torch.HList.AFst gdmStep tensors, Torch.HList.HMap' Torch.HList.ASnd gdmStep momenta) => Torch.Typed.Optim.Optimizer (Torch.Typed.Optim.GDM momenta) gradients tensors dtype device
instance Torch.Typed.Parameter.Parameterized (Torch.Typed.Optim.GDM momenta)
instance (parameter GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, gradient GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, shape GHC.Types.~ Torch.Typed.Tensor.Broadcast '[] shape, Torch.Typed.Tensor.BasicArithmeticDTypeIsValid device dtype, Torch.Typed.Tensor.KnownDevice device) => Torch.HList.Apply' (Torch.Typed.Optim.GDStep device dtype) (parameter, gradient) parameter
instance forall k (device :: (Torch.Device.DeviceType, GHC.Types.Nat)) (dtype :: Torch.DType.DType) (tensors :: [k]) (gradients :: [k]). Torch.HList.HZipWith (Torch.Typed.Optim.GDStep device dtype) tensors gradients tensors => Torch.Typed.Optim.Optimizer Torch.Typed.Optim.GD gradients tensors dtype device
instance Torch.Typed.Parameter.Parameterized Torch.Typed.Optim.GD
instance (parameter GHC.Types.~ Torch.Typed.Parameter.Parameter device dtype shape, momentum GHC.Types.~ Torch.Typed.Tensor.Tensor device dtype shape, Torch.Typed.Tensor.TensorOptions shape dtype device) => Torch.HList.Apply' Torch.Typed.Optim.ZerosLike parameter momentum

module Torch.Typed.NN.DataParallel
data ForwardConcurrentlyF
ForwardConcurrentlyF :: ForwardConcurrentlyF
ForwardConcurrentlyStochF :: ForwardConcurrentlyF
forwardConcurrently' :: forall devices' device' device model input output models inputs outputs. ('Just device ~ GetDevice model, 'Just device ~ GetDevice input, HasScatter devices' device input inputs, HasReplicate devices' device model models, HZipWithM Concurrently ForwardConcurrentlyF models inputs outputs, HasGather device' devices' outputs output) => model -> input -> IO output
forwardConcurrentlyStoch' :: forall devices' device' device model input output models inputs outputs. ('Just device ~ GetDevice model, 'Just device ~ GetDevice input, HasScatter devices' device input inputs, HasReplicate devices' device model models, HZipWithM Concurrently ForwardConcurrentlyF models inputs outputs, HasGather device' devices' outputs output) => model -> input -> IO output
forwardConcurrently :: forall models inputs outputs. HZipWithM Concurrently ForwardConcurrentlyF models inputs outputs => HList models -> HList inputs -> Concurrently (HList outputs)
forwardConcurrentlyStoch :: forall models inputs outputs. HZipWithM Concurrently ForwardConcurrentlyF models inputs outputs => HList models -> HList inputs -> Concurrently (HList outputs)
class HasGradConcurrently device' devices parameters losses gradients | device' devices parameters losses -> gradients
gradConcurrently :: HasGradConcurrently device' devices parameters losses gradients => HList parameters -> HList losses -> Concurrently (HList gradients)
data GradConcurrentlyF
GradConcurrentlyF :: GradConcurrentlyF
class ReduceGradients (device' :: (DeviceType, Nat)) (devices :: [(DeviceType, Nat)]) xxs ys | device' devices xxs -> ys
reduceGradients :: ReduceGradients device' devices xxs ys => HList xxs -> HList ys
data SumF
SumF :: SumF
instance GHC.Num.Num y => Torch.HList.Apply' Torch.Typed.NN.DataParallel.SumF (y, y) y
instance forall k (device' :: (Torch.Device.DeviceType, GHC.Types.Nat)) (device :: (Torch.Device.DeviceType, GHC.Types.Nat)) (xs :: [*]) (ys :: [k]) (devices :: [(Torch.Device.DeviceType, GHC.Types.Nat)]) (xxs :: [*]). (Torch.Typed.Device.HasToDevice device' device (Torch.HList.HList xs) (Torch.HList.HList ys), Torch.Typed.NN.DataParallel.ReduceGradients device' devices xxs ys, Torch.HList.HZipWith Torch.Typed.NN.DataParallel.SumF ys ys ys, 1 GHC.TypeNats.<= Torch.HList.ListLength xxs) => Torch.Typed.NN.DataParallel.ReduceGradients device' (device : devices) (Torch.HList.HList xs : xxs) ys
instance forall k1 k2 (parameters :: [k1]) (losses :: [k1]) (gradients' :: [k1]) (device' :: (Torch.Device.DeviceType, GHC.Types.Nat)) (devices :: [(Torch.Device.DeviceType, GHC.Types.Nat)]) (gradients :: [k2]). (Torch.HList.HZipWithM Control.Concurrent.Async.Concurrently Torch.Typed.NN.DataParallel.GradConcurrentlyF parameters losses gradients', Torch.Typed.NN.DataParallel.ReduceGradients device' devices gradients' gradients) => Torch.Typed.NN.DataParallel.HasGradConcurrently device' devices parameters losses gradients
instance forall k (device' :: (Torch.Device.DeviceType, GHC.Types.Nat)) (device :: (Torch.Device.DeviceType, GHC.Types.Nat)) (xs :: [*]) (ys :: [k]). Torch.Typed.Device.HasToDevice device' device (Torch.HList.HList xs) (Torch.HList.HList ys) => Torch.Typed.NN.DataParallel.ReduceGradients device' '[device] '[Torch.HList.HList xs] ys
instance forall k1 k2 (parameters :: [k1]) (gradients :: [k2]) (device :: (Torch.Device.DeviceType, GHC.Types.Nat)) (dtype :: Torch.DType.DType). (Torch.Typed.Autograd.HasGrad (Torch.HList.HList parameters) (Torch.HList.HList gradients), Torch.Internal.Class.Castable (Torch.HList.HList gradients) [Torch.Tensor.ATenTensor]) => Torch.HList.Apply' Torch.Typed.NN.DataParallel.GradConcurrentlyF (Torch.HList.HList parameters, Torch.Typed.Optim.Loss device dtype) (Control.Concurrent.Async.Concurrently (Torch.HList.HList gradients))
instance Torch.NN.HasForward model input output => Torch.HList.Apply' Torch.Typed.NN.DataParallel.ForwardConcurrentlyF (model, input) (Control.Concurrent.Async.Concurrently output)

module Torch.Typed.NN
class HasForward f a b | f a -> b
forward :: HasForward f a b => f -> a -> b
forward :: (HasForward f a b, Generic f, Generic a, Generic b, GHasForward (Rep f) (Rep a) (Rep b)) => f -> a -> b
forwardStoch :: HasForward f a b => f -> a -> IO b
forwardStoch :: (HasForward f a b, Generic f, Generic a, Generic b, GHasForward (Rep f) (Rep a) (Rep b)) => f -> a -> IO b

module Torch.Typed.Optim.CppOptim
class CppOptimizer option
initOptimizer :: forall model tensors. (CppOptimizer option, Parameterized model, HMap' ToDependent (Parameters model) tensors, Castable (HList tensors) [ATenTensor]) => option -> model -> IO (CppOptimizerState option (Parameters model))
unsafeStep :: forall model dev dtype lossShape tensors res. (CppOptimizer option, Parameterized model, HMap' ToDependent (Parameters model) tensors, HMap' ToParameter tensors (Parameters model), Castable (HList tensors) [ATenTensor]) => model -> CppOptimizerState option (Parameters model) -> Tensor dev dtype lossShape -> IO (model, CppOptimizerState option (Parameters model))
data ToParameter
ToParameter :: ToParameter
data CppOptimizerState option (params :: [*])
CppOptimizerState :: option -> CppOptimizerRef -> CppOptimizerState option (params :: [*])
type CppOptimizerRef = ForeignPtr Optimizer
runStep :: (CppOptimizer option, Parameterized model, HMap' ToDependent (Parameters model) tensors, HMap' ToParameter tensors (Parameters model), Castable (HList tensors) [ATenTensor]) => model -> CppOptimizerState option (Parameters model) -> Loss dev dtype -> IO (model, CppOptimizerState option (Parameters model))
data AdagradOptions
AdagradOptions :: Double -> Double -> Double -> Double -> Double -> AdagradOptions
[adagradLr] :: AdagradOptions -> Double
[adagradLrDecay] :: AdagradOptions -> Double
[adagradWeightDecay] :: AdagradOptions -> Double
[adagradInitialAccumulatorValue] :: AdagradOptions -> Double
[adagradEps] :: AdagradOptions -> Double
data AdamOptions
AdamOptions :: Double -> (Double, Double) -> Double -> Double -> Bool -> AdamOptions
[adamLr] :: AdamOptions -> Double
[adamBetas] :: AdamOptions -> (Double, Double)
[adamEps] :: AdamOptions -> Double
[adamWeightDecay] :: AdamOptions -> Double
[adamAmsgrad] :: AdamOptions -> Bool
data AdamwOptions
AdamwOptions :: Double -> (Double, Double) -> Double -> Double -> Bool -> AdamwOptions
[adamwLr] :: AdamwOptions -> Double
[adamwBetas] :: AdamwOptions -> (Double, Double)
[adamwEps] :: AdamwOptions -> Double
[adamwWeightDecay] :: AdamwOptions -> Double
[adamwAmsgrad] :: AdamwOptions -> Bool
data LbfgsOptions
LbfgsOptions :: Double -> Int -> Int -> Double -> Double -> Int -> Maybe String -> LbfgsOptions
[lbfgsLr] :: LbfgsOptions -> Double
[lbfgsMaxIter] :: LbfgsOptions -> Int
[lbfgsMaxEval] :: LbfgsOptions -> Int
[lbfgsToleranceGrad] :: LbfgsOptions -> Double
[lbfgsToleranceChange] :: LbfgsOptions -> Double
[lbfgsHistorySize] :: LbfgsOptions -> Int
[lbfgsLineSearchFn] :: LbfgsOptions -> Maybe String
data RmspropOptions
RmspropOptions :: Double -> Double -> Double -> Double -> Double -> Bool -> RmspropOptions
[rmspropLr] :: RmspropOptions -> Double
[rmspropAlpha] :: RmspropOptions -> Double
[rmspropEps] :: RmspropOptions -> Double
[rmspropWeightDecay] :: RmspropOptions -> Double
[rmspropMomentum] :: RmspropOptions -> Double
[rmspropCentered] :: RmspropOptions -> Bool
data SGDOptions
SGDOptions :: Double -> Double -> Double -> Double -> Bool -> SGDOptions
[sgdLr] :: SGDOptions -> Double
[sgdMomentum] :: SGDOptions -> Double
[sgdDampening] :: SGDOptions -> Double
[sgdWeightDecay] :: SGDOptions -> Double
[sgdNesterov] :: SGDOptions -> Bool
instance Torch.Typed.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.AdamOptions
instance Torch.Typed.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.AdamwOptions
instance Torch.Typed.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.LbfgsOptions
instance Torch.Typed.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.RmspropOptions
instance Torch.Typed.Optim.CppOptim.CppOptimizer Torch.Optim.CppOptim.SGDOptions
instance Torch.HList.Apply' Torch.Typed.Optim.CppOptim.ToParameter (Torch.Typed.Tensor.Tensor dev dtype shape) (Torch.Typed.Parameter.Parameter dev dtype shape)

module Torch.Typed.Vision
data MNIST (m :: Type -> Type) (device :: (DeviceType, Nat)) (batchSize :: Nat)
MNIST :: MnistData -> MNIST (m :: Type -> Type) (device :: (DeviceType, Nat)) (batchSize :: Nat)
[mnistData] :: MNIST (m :: Type -> Type) (device :: (DeviceType, Nat)) (batchSize :: Nat) -> MnistData
data MnistData
MnistData :: ByteString -> ByteString -> MnistData
[images] :: MnistData -> ByteString
[labels] :: MnistData -> ByteString
type Rows = 28
type Cols = 28
type DataDim = Rows * Cols
type ClassDim = 10
getLabels :: forall n. KnownNat n => MnistData -> [Int] -> CPUTensor 'Int64 '[n]
getLabel :: MnistData -> Int -> Int
getImage :: MnistData -> Int -> CPUTensor 'Float '[DataDim]
getImages' :: forall n. KnownNat n => MnistData -> [Int] -> CPUTensor 'Float '[n, DataDim]
getImages :: forall n. KnownNat n => MnistData -> [Int] -> CPUTensor 'Float '[n, DataDim]
length :: MnistData -> Int
decompressFile :: String -> String -> IO ByteString
initMnist :: String -> IO (MnistData, MnistData)
instance (GHC.TypeNats.KnownNat batchSize, Torch.Typed.Tensor.KnownDevice device, GHC.Base.Applicative m) => Torch.Data.Pipeline.Dataset m (Torch.Typed.Vision.MNIST m device batchSize) GHC.Types.Int (Torch.Typed.Tensor.Tensor device 'Torch.DType.Float '[batchSize, 784], Torch.Typed.Tensor.Tensor device 'Torch.DType.Int64 '[batchSize])

module Torch.Typed
class Randomizable spec f | spec -> f
sample :: Randomizable spec f => spec -> IO f
class GParameterized (f :: Type -> Type) where {
    type family GParameters f :: [Type];
}
gFlattenParameters :: forall a. GParameterized f => f a -> HList (GParameters f)
gReplaceParameters :: forall a. GParameterized f => f a -> HList (GParameters f) -> f a
class Parameterized (f :: Type) where {
    type family Parameters f :: [Type];
    type Parameters f = GParameters (Rep f);
}
flattenParameters :: Parameterized f => f -> HList (Parameters f)
flattenParameters :: (Parameterized f, Generic f, GParameterized (Rep f), Parameters f ~ GParameters (Rep f)) => f -> HList (Parameters f)
replaceParameters :: Parameterized f => f -> HList (Parameters f) -> f
replaceParameters :: (Parameterized f, Generic f, GParameterized (Rep f), Parameters f ~ GParameters (Rep f)) => f -> HList (Parameters f) -> f
data MakeIndependent
MakeIndependent :: MakeIndependent
data ToDependent
ToDependent :: ToDependent
newtype Parameter (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat])
UnsafeMkParameter :: IndependentTensor -> Parameter (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat])
untypeParam :: Parameter device dtype shape -> Parameter
toDependent :: forall shape dtype device. Parameter device dtype shape -> Tensor device dtype shape
makeIndependent :: forall shape dtype device. Tensor device dtype shape -> IO (Parameter device dtype shape)
type family ReplaceDType'' (tensor :: t) (dtype :: DType) :: t
type family ReplaceDevice'' (tensor :: t) (device :: (DeviceType, Nat)) :: t
data NamedTensor (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: Shape)
[FromTensor] :: forall device dtype shape' shape. shape ~ ToNats shape' => Tensor device dtype shape -> NamedTensor device dtype shape'
type family FindDim (a :: Size) (shape :: Shape) :: Nat
type family ToShape a :: Shape
type family ToDType a :: DType
data TensorListUnfold
TensorListUnfold :: TensorListUnfold
data TensorListFold
TensorListFold :: TensorListFold

-- | To avoid overlapped instance for (Unnamed t =&gt; Castable t
--   D.ATenTensor)
newtype Wrap a
Wrap :: a -> Wrap a
[unWrap] :: Wrap a -> a
type family Numel (shape :: [Nat]) :: Nat
type family MatMulDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type MatMul shape shape' = CheckMatMul shape shape' (ComputeMatMul (Reverse shape) (Reverse shape'))
type family CheckMatMul (shape :: [Nat]) (shape' :: [Nat]) (result :: Maybe [Nat]) :: [Nat]
type family ComputeMatMul (reversedShape :: [Nat]) (reversedShape' :: [Nat]) :: Maybe [Nat]
type family ComparisonDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type family BasicArithmeticDTypeIsValid (device :: (DeviceType, Nat)) (dtype :: DType) :: Constraint
type Broadcast shape shape' = CheckBroadcast shape shape' (ComputeBroadcast (Reverse shape) (Reverse shape'))
type family CheckBroadcast (shape :: [Nat]) (shape' :: [Nat]) (result :: Maybe [Nat]) :: [Nat]
type family ComputeBroadcast (reversedShape :: [Nat]) (reversedShape' :: [Nat]) :: Maybe [Nat]
data SomeDevice
[SomeDevice] :: forall (device :: (DeviceType, Nat)). KnownDevice device => Proxy device -> SomeDevice
data SomeDType
[SomeDType] :: forall (dtype :: DType). KnownDType dtype => Proxy dtype -> SomeDType
data SomeShape
[SomeShape] :: forall (shape :: [Nat]). KnownShape shape => Proxy shape -> SomeShape
type family All (pred :: a -> Constraint) (l :: [a]) :: Constraint
class TensorOptions (shape :: [Nat]) (dtype :: DType) (device :: (DeviceType, Nat))
optionsRuntimeShape :: TensorOptions shape dtype device => [Int]
optionsRuntimeDType :: TensorOptions shape dtype device => DType
optionsRuntimeDevice :: TensorOptions shape dtype device => Device
type family ComputeItemType (ty :: Type) (shape :: [Nat]) :: Type
type family ComputeHaskellType (dtype :: DType) :: Type
data UnknownShapeTensor device dtype
UnknownShapeTensor :: Tensor device dtype shape -> UnknownShapeTensor device dtype
type CUDATensor deviceIndex = Tensor '( 'CUDA, deviceIndex)
type CPUTensor = Tensor '( 'CPU, 0)
data Tensor (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat])
[UnsafeMkTensor] :: forall device dtype shape. Tensor -> Tensor device dtype shape
type family IsUnnamed t (device :: (DeviceType, Nat)) (dtype :: DType) (shape :: [Nat]) :: Constraint
class Unnamed t where {
    type family UTShape t :: [Nat];
    type family UTDevice t :: (DeviceType, Nat);
    type family UTDType t :: DType;
}
toUnnamed :: forall device dtype shape. (Unnamed t, IsUnnamed t device dtype shape) => t -> Tensor device dtype shape
fromUnnamed :: forall device dtype shape. (Unnamed t, IsUnnamed t device dtype shape) => Tensor device dtype shape -> t
toDynamic :: Unnamed t => t -> Tensor
type family FromNats (shape :: [Nat]) :: Shape
type family FromNat (shape :: Nat) :: Size
type family ToNats (shape :: Shape) :: [Nat]
type family ToNat (shape :: Size) :: Nat
type Shape = [Type -> Type]
type Size = Type -> Type
class KnownDevice (device :: (DeviceType, Nat))
deviceVal :: KnownDevice device => Device
type family ComputeDType (dtype' :: dtype) :: DType
class KnownDType (dtype :: DType)
dtypeVal :: KnownDType dtype => DType
class KnownShape (shape :: [Nat])
shapeVal :: KnownShape shape => [Int]
getFiniteI :: Finite n -> Int
someShape :: [Int] -> SomeShape
someDType :: DType -> SomeDType
someDevice :: Device -> SomeDevice
withTensor :: Tensor -> (forall shape dtype device. KnownShape shape => Tensor device dtype shape -> r) -> r
add :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
sub :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
mul :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
div :: forall shape'' shape shape' dtype dtype' dtype'' device. (dtype'' ~ DTypePromotion dtype dtype', shape'' ~ Broadcast shape shape', BasicArithmeticDTypeIsValid device dtype, BasicArithmeticDTypeIsValid device dtype', BasicArithmeticDTypeIsValid device dtype'') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device dtype'' shape''
gt :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
lt :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
ge :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
le :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
eq :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
ne :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(>.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(<.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(>=.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(<=.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(==.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''
(/=.) :: forall shape'' shape shape' dtype dtype' device. (shape'' ~ Broadcast shape shape', ComparisonDTypeIsValid device dtype, ComparisonDTypeIsValid device dtype') => Tensor device dtype shape -> Tensor device dtype' shape' -> Tensor device 'Bool shape''

-- | matrix multiplication See
--   <a>https://pytorch.org/docs/stable/torch.html#torch.matmul</a>.
matmul :: forall shape'' shape shape' dtype device. (shape'' ~ MatMul shape shape', MatMulDTypeIsValid device dtype) => Tensor device dtype shape -> Tensor device dtype shape' -> Tensor device dtype shape''
select :: forall dim idx shape' shape dtype device. (KnownNat dim, KnownNat idx, InRange shape dim idx, shape' ~ Remove shape dim) => Tensor device dtype shape -> Tensor device dtype shape'
selectIdx :: forall dim n shape' shape dtype device. (KnownNat dim, n ~ Index shape dim, shape' ~ Remove shape dim) => Tensor device dtype shape -> Finite n -> Tensor device dtype shape'

-- | reshape &gt;&gt;&gt; t :: CPUTensor 'D.Int64 '[2,3,4] = fromJust
--   [[[111,112,113,114],[121,122,123,124],[131,132,133,134]],[[211,212,213,214],[221,222,223,224],[231,232,233,234]]]
--   &gt;&gt;&gt; t' = reshape <tt>'[24] t &gt;&gt;&gt; toList . Just $ t'
--   [111,112,113,114,121,122,123,124,131,132,133,134,211,212,213,214,221,222,223,224,231,232,233,234]
--   &gt;&gt;&gt; toList . Just $ reshape </tt>'[2,3,4] t'
--   [[[111,112,113,114],[121,122,123,124],[131,132,133,134]],[[211,212,213,214],[221,222,223,224],[231,232,233,234]]]
reshape :: forall shape' shape dtype device. (KnownShape shape', Numel shape ~ Numel shape') => Tensor device dtype shape -> Tensor device dtype shape'
toSparse :: Tensor device dtype shape -> Tensor device dtype shape
toDense :: Tensor device dtype shape -> Tensor device dtype shape

-- | move tensor to CPU TODO: can this fail?
toCPU :: forall device shape dtype. Tensor device dtype shape -> CPUTensor dtype shape

-- | move tensor to the first CUDA device TODO: what if this fails?
toCUDA :: forall device' device shape dtype. Tensor device dtype shape -> CUDATensor 0 dtype shape

-- | returns tensor dimension uses compile-time information only
dim :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> Int

-- | returns tensor shape as list uses compile-time information only
shape :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> [Int]

-- | returns tensor data type uses compile-time information only
dtype :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> DType

-- | returns tensor device uses compile-time information only
device :: forall device dtype shape t. (TensorOptions shape dtype device, IsUnnamed t device dtype shape) => t -> Device
toInt :: Tensor device dtype shape -> Int
toFloat :: forall device. Tensor device 'Float '[] -> Float
toDouble :: forall device. Tensor device 'Double '[] -> Double
toBool :: forall device. Tensor device 'Bool '[] -> Bool
data Device
Device :: DeviceType -> Int16 -> Device
[deviceType] :: Device -> DeviceType
[deviceIndex] :: Device -> Int16
data DeviceType
CPU :: DeviceType
CUDA :: DeviceType
data DType

-- | Bool
Bool :: DType

-- | Byte
UInt8 :: DType

-- | Char
Int8 :: DType

-- | Short
Int16 :: DType

-- | Int
Int32 :: DType

-- | Long
Int64 :: DType

-- | Half
Half :: DType

-- | Float
Float :: DType

-- | Double
Double :: DType

-- | ComplexHalf
ComplexHalf :: DType

-- | ComplexFloat
ComplexFloat :: DType

-- | ComplexDouble
ComplexDouble :: DType

-- | QInt8
QInt8 :: DType

-- | QUInt8
QUInt8 :: DType

-- | QInt32
QInt32 :: DType

-- | BFloat16
BFloat16 :: DType
class (Castable a (ForeignPtr Scalar)) => Scalar a
data Reduction
ReduceNone :: Reduction
ReduceMean :: Reduction
ReduceSum :: Reduction
data Tri
Upper :: Tri
Lower :: Tri

module Torch.Vision
data MNIST (m :: * -> *)
MNIST :: Int -> MnistData -> MNIST (m :: * -> *)
[batchSize] :: MNIST (m :: * -> *) -> Int
[mnistData] :: MNIST (m :: * -> *) -> MnistData
getLabels' :: Int -> MnistData -> [Int] -> Tensor
getImages' :: Int -> Int -> MnistData -> [Int] -> Tensor
grayScale10 :: [Char]
grayScale70 :: [Char]
dispImage :: Tensor -> IO ()
data PixelFormat
Y8 :: PixelFormat
YF :: PixelFormat
YA8 :: PixelFormat
RGB8 :: PixelFormat
RGBF :: PixelFormat
RGBA8 :: PixelFormat
YCbCr8 :: PixelFormat
CMYK8 :: PixelFormat
CMYK16 :: PixelFormat
RGBA16 :: PixelFormat
RGB16 :: PixelFormat
Y16 :: PixelFormat
YA16 :: PixelFormat
Y32 :: PixelFormat
readImage :: FilePath -> IO (Either String (Tensor, PixelFormat))
readImageAsRGB8 :: FilePath -> IO (Either String Tensor)
readImageAsRGB8WithScaling :: FilePath -> Int -> Int -> Bool -> IO (Either String (Image PixelRGB8, Tensor))
centerCrop :: Int -> Int -> Image PixelRGB8 -> Image PixelRGB8
inline_c_ffi_6989586621679673135 :: Ptr Word8 -> Ptr Word8 -> CInt -> CInt -> CInt -> CInt -> CInt -> IO ()
drawLine :: Int -> Int -> Int -> Int -> (Int, Int, Int) -> Image PixelRGB8 -> IO ()
inline_c_ffi_6989586621679673186 :: Ptr Word8 -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> IO ()
drawRect :: Int -> Int -> Int -> Int -> (Int, Int, Int) -> Image PixelRGB8 -> IO ()
drawString :: String -> Int -> Int -> (Int, Int, Int) -> (Int, Int, Int) -> Image PixelRGB8 -> IO ()
drawChar :: Int -> Int -> Int -> (Int, Int, Int) -> (Int, Int, Int) -> Image PixelRGB8 -> IO ()
inline_c_ffi_6989586621679673265 :: Ptr Word8 -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> IO ()
resizeRGB8 :: Int -> Int -> Bool -> Image PixelRGB8 -> Image PixelRGB8
inline_c_ffi_6989586621679673315 :: Ptr Word8 -> Ptr Word8 -> CInt -> CInt -> CInt -> CInt -> CInt -> CInt -> IO ()
pixelFormat :: DynamicImage -> PixelFormat
fromDynImage :: DynamicImage -> Tensor
inline_c_ffi_6989586621679673357 :: Ptr Word16 -> Ptr Int32 -> CInt -> IO ()
inline_c_ffi_6989586621679673382 :: Ptr Word32 -> Ptr Int64 -> CInt -> IO ()
fromImages :: [Image PixelRGB8] -> IO Tensor
writeImage :: forall p. Pixel p => Int -> Int -> Int -> p -> Tensor -> IO (Image p)
writeBitmap :: FilePath -> Tensor -> IO ()
writePng :: FilePath -> Tensor -> IO ()
hwc2chw :: Tensor -> Tensor
chw2hwc :: Tensor -> Tensor
randomIndexes :: Int -> [Int]
instance GHC.Classes.Eq Torch.Vision.PixelFormat
instance GHC.Show.Show Torch.Vision.PixelFormat
instance GHC.Base.Monad m => Torch.Data.StreamedPipeline.Datastream m GHC.Types.Int (Torch.Vision.MNIST m) (Torch.Tensor.Tensor, Torch.Tensor.Tensor)
instance GHC.Base.Applicative m => Torch.Data.Pipeline.Dataset m (Torch.Vision.MNIST m) GHC.Types.Int (Torch.Tensor.Tensor, Torch.Tensor.Tensor)
