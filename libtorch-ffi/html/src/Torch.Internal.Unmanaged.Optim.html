<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds #-}</span><span>
</span><span id="line-2"></span><span class="hs-pragma">{-# LANGUAGE OverloadedStrings #-}</span><span>
</span><span id="line-3"></span><span class="hs-pragma">{-# LANGUAGE PolyKinds #-}</span><span>
</span><span id="line-4"></span><span class="hs-pragma">{-# LANGUAGE QuasiQuotes #-}</span><span>
</span><span id="line-5"></span><span class="hs-pragma">{-# LANGUAGE RecordWildCards #-}</span><span>
</span><span id="line-6"></span><span class="hs-pragma">{-# LANGUAGE ScopedTypeVariables #-}</span><span>
</span><span id="line-7"></span><span class="hs-pragma">{-# LANGUAGE TemplateHaskell #-}</span><span>
</span><span id="line-8"></span><span>
</span><span id="line-9"></span><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Torch.Internal.Unmanaged.Optim</span><span> </span><span class="hs-keyword">where</span><span>
</span><span id="line-10"></span><span>
</span><span id="line-11"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../file:///nix/store/ppq6k5yv7rcfj913wd07v54z631236px-safe-exceptions-lib-safe-exceptions-0.1.7.1-haddock-doc/share/doc/safe-exceptions/html/src"><span class="hs-identifier">Control.Exception.Safe</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><a href="../file:///nix/store/ppq6k5yv7rcfj913wd07v54z631236px-safe-exceptions-lib-safe-exceptions-0.1.7.1-haddock-doc/share/doc/safe-exceptions/html/src"><span class="hs-identifier">bracket</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-12"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign</span></span><span>
</span><span id="line-13"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign.C.String</span></span><span>
</span><span id="line-14"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign.C.Types</span></span><span>
</span><span id="line-15"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><span class="hs-identifier">Foreign.Ptr</span></span><span>
</span><span id="line-16"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/7jyx4kin0q5papk7lf8kprhr4x899b8n-inline-c-lib-inline-c-0.9.1.3-haddock-doc/share/doc/inline-c/html/src"><span class="hs-identifier">Language.C.Inline.Context</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">C</span></span><span>
</span><span id="line-17"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/ndda0ssi3r5zi3zrw4z7ydby8w0z38lq-inline-c-cpp-lib-inline-c-cpp-0.4.0.2-haddock-doc/share/doc/inline-c-cpp/html/src"><span class="hs-identifier">Language.C.Inline.Cpp</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">C</span></span><span>
</span><span id="line-18"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/ndda0ssi3r5zi3zrw4z7ydby8w0z38lq-inline-c-cpp-lib-inline-c-cpp-0.4.0.2-haddock-doc/share/doc/inline-c-cpp/html/src"><span class="hs-identifier">Language.C.Inline.Cpp.Exceptions</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">C</span></span><span>
</span><span id="line-19"></span><span class="hs-keyword">import</span><span> </span><span class="hs-keyword">qualified</span><span> </span><span class="annot"><a href="../file:///nix/store/7jyx4kin0q5papk7lf8kprhr4x899b8n-inline-c-lib-inline-c-0.9.1.3-haddock-doc/share/doc/inline-c/html/src"><span class="hs-identifier">Language.C.Types</span></a></span><span> </span><span class="hs-keyword">as</span><span> </span><span class="annot"><span class="hs-identifier">C</span></span><span>
</span><span id="line-20"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="Torch.Internal.Type.html"><span class="hs-identifier">Torch.Internal.Type</span></a></span><span>
</span><span id="line-21"></span><span class="hs-keyword">import</span><span> </span><span class="annot"><a href="../../../../libtorch-ffi-helper/html/src"><span class="hs-identifier">Torch.Internal.Unmanaged.Helper</span></a></span><span>
</span><span id="line-22"></span><span>
</span><span id="line-23"></span><span class="hs-identifier">C.context</span><span> </span><span class="hs-operator">$</span><span> </span><span class="hs-identifier">C.cppCtx</span><span> </span><span class="hs-operator">&lt;&gt;</span><span> </span><span class="hs-identifier">mempty</span><span> </span><span class="hs-special">{</span><span class="hs-identifier">C.ctxTypesTable</span><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-identifier">typeTable</span><span class="hs-special">}</span><span>
</span><span id="line-24"></span><span>
</span><span id="line-25"></span><span class="hs-identifier">C.include</span><span> </span><span class="hs-string">&quot;&lt;vector&gt;&quot;</span><span>
</span><span id="line-26"></span><span>
</span><span id="line-27"></span><span class="hs-identifier">C.include</span><span> </span><span class="hs-string">&quot;&lt;torch/types.h&gt;&quot;</span><span>
</span><span id="line-28"></span><span>
</span><span id="line-29"></span><span class="hs-identifier">C.include</span><span> </span><span class="hs-string">&quot;&lt;torch/optim.h&gt;&quot;</span><span>
</span><span id="line-30"></span><span>
</span><span id="line-31"></span><span class="hs-identifier">C.include</span><span> </span><span class="hs-string">&quot;&lt;torch/serialize.h&gt;&quot;</span><span>
</span><span id="line-32"></span><span>
</span><span id="line-33"></span><span class="hs-comment">-- optimizerWithAdam</span><span>
</span><span id="line-34"></span><span class="hs-comment">--   :: CDouble</span><span>
</span><span id="line-35"></span><span class="hs-comment">--   -&gt; CDouble</span><span>
</span><span id="line-36"></span><span class="hs-comment">--   -&gt; CDouble</span><span>
</span><span id="line-37"></span><span class="hs-comment">--   -&gt; CDouble</span><span>
</span><span id="line-38"></span><span class="hs-comment">--   -&gt; CDouble</span><span>
</span><span id="line-39"></span><span class="hs-comment">--   -&gt; CBool</span><span>
</span><span id="line-40"></span><span class="hs-comment">--   -&gt; Ptr TensorList</span><span>
</span><span id="line-41"></span><span class="hs-comment">--   -&gt; (Ptr TensorList -&gt; IO (Ptr Tensor))</span><span>
</span><span id="line-42"></span><span class="hs-comment">--   -&gt; CInt</span><span>
</span><span id="line-43"></span><span class="hs-comment">--   -&gt; IO (Ptr TensorList)</span><span>
</span><span id="line-44"></span><span class="hs-comment">-- optimizerWithAdam adamLr adamBetas0 adamBetas1 adamEps adamWeightDecay adamAmsgrad initParams loss numIter =</span><span>
</span><span id="line-45"></span><span class="hs-comment">--   bracket</span><span>
</span><span id="line-46"></span><span class="hs-comment">--     (callbackHelper loss')</span><span>
</span><span id="line-47"></span><span class="hs-comment">--     freeHaskellFunPtr</span><span>
</span><span id="line-48"></span><span class="hs-comment">--     $ \funcPtr -&gt;</span><span>
</span><span id="line-49"></span><span class="hs-comment">--       [C.throwBlock| std::vector&lt;at::Tensor&gt;* {</span><span>
</span><span id="line-50"></span><span class="hs-comment">--         std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);</span><span>
</span><span id="line-51"></span><span class="hs-comment">--         std::vector&lt;at::Tensor&gt;* params = new std::vector&lt;at::Tensor&gt;();</span><span>
</span><span id="line-52"></span><span class="hs-comment">--         auto tfunc = $(void* (*funcPtr)(void*));</span><span>
</span><span id="line-53"></span><span class="hs-comment">--         for(int i=0;i&lt;init_params-&gt;size();i++){</span><span>
</span><span id="line-54"></span><span class="hs-comment">--           params-&gt;push_back((*init_params)[i].detach().set_requires_grad(true));</span><span>
</span><span id="line-55"></span><span class="hs-comment">--         }</span><span>
</span><span id="line-56"></span><span class="hs-comment">--         auto options = torch::optim::AdamOptions()</span><span>
</span><span id="line-57"></span><span class="hs-comment">--           .lr($(double adamLr))</span><span>
</span><span id="line-58"></span><span class="hs-comment">--           .betas(std::make_tuple($(double adamBetas0),$(double adamBetas1)))</span><span>
</span><span id="line-59"></span><span class="hs-comment">--           .eps($(double adamEps))</span><span>
</span><span id="line-60"></span><span class="hs-comment">--           .weight_decay($(double adamWeightDecay))</span><span>
</span><span id="line-61"></span><span class="hs-comment">--           .amsgrad($(bool adamAmsgrad));</span><span>
</span><span id="line-62"></span><span class="hs-comment">--         torch::optim::Adam optimizer(*params, options);</span><span>
</span><span id="line-63"></span><span class="hs-comment">--         optimizer.zero_grad();</span><span>
</span><span id="line-64"></span><span class="hs-comment">--         typedef at::Tensor* (*Func)(std::vector&lt;at::Tensor&gt;*);</span><span>
</span><span id="line-65"></span><span class="hs-comment">--         auto func = (Func)tfunc;</span><span>
</span><span id="line-66"></span><span class="hs-comment">--         for(int i=0;i&lt;$(int numIter);i++){</span><span>
</span><span id="line-67"></span><span class="hs-comment">--           auto loss = func(params);</span><span>
</span><span id="line-68"></span><span class="hs-comment">--           loss-&gt;backward();</span><span>
</span><span id="line-69"></span><span class="hs-comment">--           optimizer.step();</span><span>
</span><span id="line-70"></span><span class="hs-comment">--         }</span><span>
</span><span id="line-71"></span><span class="hs-comment">--         return params;</span><span>
</span><span id="line-72"></span><span class="hs-comment">--       }|]</span><span>
</span><span id="line-73"></span><span class="hs-comment">--   where</span><span>
</span><span id="line-74"></span><span class="hs-comment">--     loss' :: Ptr () -&gt; IO (Ptr ())</span><span>
</span><span id="line-75"></span><span class="hs-comment">--     loss' params = castPtr &lt;$&gt; loss (castPtr params)</span><span>
</span><span id="line-76"></span><span>
</span><span id="line-77"></span><span>
</span><span id="line-78"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#adagrad"><span class="hs-identifier hs-type">adagrad</span></a></span><span>
</span><span id="line-79"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-80"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-81"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-82"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-83"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-84"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span>
</span><span id="line-85"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-86"></span><span id="adagrad"><span class="annot"><span class="annottext">adagrad :: CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; Ptr TensorList
-&gt; IO (Ptr Optimizer)
</span><a href="Torch.Internal.Unmanaged.Optim.html#adagrad"><span class="hs-identifier hs-var hs-var">adagrad</span></a></span></span><span> </span><span id="local-6989586621684956250"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956250"><span class="hs-identifier hs-var">lr</span></a></span></span><span> </span><span id="local-6989586621684956249"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956249"><span class="hs-identifier hs-var">lr_decay</span></a></span></span><span> </span><span id="local-6989586621684956248"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956248"><span class="hs-identifier hs-var">weight_decay</span></a></span></span><span> </span><span id="local-6989586621684956247"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956247"><span class="hs-identifier hs-var">initial_accumulator_value</span></a></span></span><span> </span><span id="local-6989586621684956246"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956246"><span class="hs-identifier hs-var">eps</span></a></span></span><span> </span><span id="local-6989586621684956245"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956245"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-87"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::AdagradOptions()
      .lr($(double lr))
      .lr_decay($(double lr_decay))
      .weight_decay($(double weight_decay))
      .initial_accumulator_value($(double initial_accumulator_value))
      .eps($(double eps));
    torch::optim::Adagrad* optimizer = new torch::optim::Adagrad(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-103"></span><span>
</span><span id="line-104"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#rmsprop"><span class="hs-identifier hs-type">rmsprop</span></a></span><span>
</span><span id="line-105"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-106"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-107"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-108"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-109"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-110"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CBool</span></span><span>
</span><span id="line-111"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span>
</span><span id="line-112"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-113"></span><span id="rmsprop"><span class="annot"><span class="annottext">rmsprop :: CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CBool
-&gt; Ptr TensorList
-&gt; IO (Ptr Optimizer)
</span><a href="Torch.Internal.Unmanaged.Optim.html#rmsprop"><span class="hs-identifier hs-var hs-var">rmsprop</span></a></span></span><span> </span><span id="local-6989586621684956236"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956236"><span class="hs-identifier hs-var">lr</span></a></span></span><span> </span><span id="local-6989586621684956235"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956235"><span class="hs-identifier hs-var">alpha</span></a></span></span><span> </span><span id="local-6989586621684956234"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956234"><span class="hs-identifier hs-var">eps</span></a></span></span><span> </span><span id="local-6989586621684956233"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956233"><span class="hs-identifier hs-var">weight_decay</span></a></span></span><span> </span><span id="local-6989586621684956232"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956232"><span class="hs-identifier hs-var">momentum</span></a></span></span><span> </span><span id="local-6989586621684956231"><span class="annot"><span class="annottext">CBool
</span><a href="#local-6989586621684956231"><span class="hs-identifier hs-var">centered</span></a></span></span><span> </span><span id="local-6989586621684956230"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956230"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-114"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::RMSpropOptions()
      .lr($(double lr))
      .alpha($(double alpha))
      .eps($(double eps))
      .weight_decay($(double weight_decay))
      .momentum($(double momentum))
      .centered($(bool centered));
    torch::optim::RMSprop* optimizer = new torch::optim::RMSprop(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-131"></span><span>
</span><span id="line-132"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#sgd"><span class="hs-identifier hs-type">sgd</span></a></span><span>
</span><span id="line-133"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-134"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-135"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-136"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-137"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CBool</span></span><span>
</span><span id="line-138"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span>
</span><span id="line-139"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-140"></span><span id="sgd"><span class="annot"><span class="annottext">sgd :: CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CBool
-&gt; Ptr TensorList
-&gt; IO (Ptr Optimizer)
</span><a href="Torch.Internal.Unmanaged.Optim.html#sgd"><span class="hs-identifier hs-var hs-var">sgd</span></a></span></span><span> </span><span id="local-6989586621684956227"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956227"><span class="hs-identifier hs-var">lr</span></a></span></span><span> </span><span id="local-6989586621684956226"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956226"><span class="hs-identifier hs-var">momentum</span></a></span></span><span> </span><span id="local-6989586621684956225"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956225"><span class="hs-identifier hs-var">dampening</span></a></span></span><span> </span><span id="local-6989586621684956224"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956224"><span class="hs-identifier hs-var">weight_decay</span></a></span></span><span> </span><span id="local-6989586621684956223"><span class="annot"><span class="annottext">CBool
</span><a href="#local-6989586621684956223"><span class="hs-identifier hs-var">nesterov</span></a></span></span><span> </span><span id="local-6989586621684956222"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956222"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-141"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::SGDOptions($(double lr))
      .momentum($(double momentum))
      .dampening($(double dampening))
      .weight_decay($(double weight_decay))
      .nesterov($(bool nesterov));
    torch::optim::SGD* optimizer = new torch::optim::SGD(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-156"></span><span>
</span><span id="line-157"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#adam"><span class="hs-identifier hs-type">adam</span></a></span><span>
</span><span id="line-158"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-159"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-160"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-161"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-162"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-163"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CBool</span></span><span>
</span><span id="line-164"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span>
</span><span id="line-165"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-166"></span><span id="adam"><span class="annot"><span class="annottext">adam :: CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CBool
-&gt; Ptr TensorList
-&gt; IO (Ptr Optimizer)
</span><a href="Torch.Internal.Unmanaged.Optim.html#adam"><span class="hs-identifier hs-var hs-var">adam</span></a></span></span><span> </span><span id="local-6989586621684956219"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956219"><span class="hs-identifier hs-var">adamLr</span></a></span></span><span> </span><span id="local-6989586621684956218"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956218"><span class="hs-identifier hs-var">adamBetas0</span></a></span></span><span> </span><span id="local-6989586621684956217"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956217"><span class="hs-identifier hs-var">adamBetas1</span></a></span></span><span> </span><span id="local-6989586621684956216"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956216"><span class="hs-identifier hs-var">adamEps</span></a></span></span><span> </span><span id="local-6989586621684956215"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956215"><span class="hs-identifier hs-var">adamWeightDecay</span></a></span></span><span> </span><span id="local-6989586621684956214"><span class="annot"><span class="annottext">CBool
</span><a href="#local-6989586621684956214"><span class="hs-identifier hs-var">adamAmsgrad</span></a></span></span><span> </span><span id="local-6989586621684956213"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956213"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-167"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::AdamOptions()
      .lr($(double adamLr))
      .betas(std::make_tuple($(double adamBetas0),$(double adamBetas1)))
      .eps($(double adamEps))
      .weight_decay($(double adamWeightDecay))
      .amsgrad($(bool adamAmsgrad));
    torch::optim::Adam* optimizer = new torch::optim::Adam(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-183"></span><span>
</span><span id="line-184"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#adamw"><span class="hs-identifier hs-type">adamw</span></a></span><span>
</span><span id="line-185"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-186"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-187"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-188"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-189"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-190"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CBool</span></span><span>
</span><span id="line-191"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span>
</span><span id="line-192"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-193"></span><span id="adamw"><span class="annot"><span class="annottext">adamw :: CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CDouble
-&gt; CBool
-&gt; Ptr TensorList
-&gt; IO (Ptr Optimizer)
</span><a href="Torch.Internal.Unmanaged.Optim.html#adamw"><span class="hs-identifier hs-var hs-var">adamw</span></a></span></span><span> </span><span id="local-6989586621684956210"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956210"><span class="hs-identifier hs-var">adamLr</span></a></span></span><span> </span><span id="local-6989586621684956209"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956209"><span class="hs-identifier hs-var">adamBetas0</span></a></span></span><span> </span><span id="local-6989586621684956208"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956208"><span class="hs-identifier hs-var">adamBetas1</span></a></span></span><span> </span><span id="local-6989586621684956207"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956207"><span class="hs-identifier hs-var">adamEps</span></a></span></span><span> </span><span id="local-6989586621684956206"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956206"><span class="hs-identifier hs-var">adamWeightDecay</span></a></span></span><span> </span><span id="local-6989586621684956205"><span class="annot"><span class="annottext">CBool
</span><a href="#local-6989586621684956205"><span class="hs-identifier hs-var">adamAmsgrad</span></a></span></span><span> </span><span id="local-6989586621684956204"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956204"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-194"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::AdamWOptions()
      .lr($(double adamLr))
      .betas(std::make_tuple($(double adamBetas0),$(double adamBetas1)))
      .eps($(double adamEps))
      .weight_decay($(double adamWeightDecay))
      .amsgrad($(bool adamAmsgrad));
    torch::optim::AdamW* optimizer = new torch::optim::AdamW(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-210"></span><span>
</span><span id="line-211"></span><span>
</span><span id="line-212"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#lbfgs"><span class="hs-identifier hs-type">lbfgs</span></a></span><span>
</span><span id="line-213"></span><span>  </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-214"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CInt</span></span><span>
</span><span id="line-215"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CInt</span></span><span>
</span><span id="line-216"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-217"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CDouble</span></span><span>
</span><span id="line-218"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">CInt</span></span><span>
</span><span id="line-219"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Maybe</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#StdString"><span class="hs-identifier hs-type">StdString</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-220"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span>
</span><span id="line-221"></span><span>  </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-222"></span><span id="lbfgs"><span class="annot"><span class="annottext">lbfgs :: CDouble
-&gt; CInt
-&gt; CInt
-&gt; CDouble
-&gt; CDouble
-&gt; CInt
-&gt; Maybe (Ptr StdString)
-&gt; Ptr TensorList
-&gt; IO (Ptr Optimizer)
</span><a href="Torch.Internal.Unmanaged.Optim.html#lbfgs"><span class="hs-identifier hs-var hs-var">lbfgs</span></a></span></span><span> </span><span id="local-6989586621684956201"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956201"><span class="hs-identifier hs-var">lr</span></a></span></span><span> </span><span id="local-6989586621684956200"><span class="annot"><span class="annottext">CInt
</span><a href="#local-6989586621684956200"><span class="hs-identifier hs-var">max_iter</span></a></span></span><span> </span><span id="local-6989586621684956199"><span class="annot"><span class="annottext">CInt
</span><a href="#local-6989586621684956199"><span class="hs-identifier hs-var">max_eval</span></a></span></span><span> </span><span id="local-6989586621684956198"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956198"><span class="hs-identifier hs-var">tolerance_grad</span></a></span></span><span> </span><span id="local-6989586621684956197"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956197"><span class="hs-identifier hs-var">tolerance_change</span></a></span></span><span> </span><span id="local-6989586621684956196"><span class="annot"><span class="annottext">CInt
</span><a href="#local-6989586621684956196"><span class="hs-identifier hs-var">history_size</span></a></span></span><span> </span><span class="annot"><span class="annottext">Maybe (Ptr StdString)
</span><span class="hs-identifier hs-var">Nothing</span></span><span> </span><span id="local-6989586621684956195"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956195"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-223"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::LBFGSOptions()
      .lr($(double lr))
      .max_iter($(int max_iter))
      .max_eval($(int max_eval))
      .tolerance_grad($(double tolerance_grad))
      .tolerance_change($(double tolerance_change))
      .history_size($(int history_size));
    torch::optim::LBFGS* optimizer = new torch::optim::LBFGS(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-240"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#lbfgs"><span class="hs-identifier hs-var">lbfgs</span></a></span><span> </span><span id="local-6989586621684956193"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956193"><span class="hs-identifier hs-var">lr</span></a></span></span><span> </span><span id="local-6989586621684956192"><span class="annot"><span class="annottext">CInt
</span><a href="#local-6989586621684956192"><span class="hs-identifier hs-var">max_iter</span></a></span></span><span> </span><span id="local-6989586621684956191"><span class="annot"><span class="annottext">CInt
</span><a href="#local-6989586621684956191"><span class="hs-identifier hs-var">max_eval</span></a></span></span><span> </span><span id="local-6989586621684956190"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956190"><span class="hs-identifier hs-var">tolerance_grad</span></a></span></span><span> </span><span id="local-6989586621684956189"><span class="annot"><span class="annottext">CDouble
</span><a href="#local-6989586621684956189"><span class="hs-identifier hs-var">tolerance_change</span></a></span></span><span> </span><span id="local-6989586621684956188"><span class="annot"><span class="annottext">CInt
</span><a href="#local-6989586621684956188"><span class="hs-identifier hs-var">history_size</span></a></span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Just</span></span><span> </span><span id="local-6989586621684956187"><span class="annot"><span class="annottext">Ptr StdString
</span><a href="#local-6989586621684956187"><span class="hs-identifier hs-var">line_search_fn</span></a></span></span><span class="hs-special">)</span><span> </span><span id="local-6989586621684956186"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956186"><span class="hs-identifier hs-var">initParams</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-241"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| torch::optim::Optimizer* {
    std::vector&lt;at::Tensor&gt;* init_params = $(std::vector&lt;at::Tensor&gt;* initParams);
    std::vector&lt;at::Tensor&gt; params;
    for(int i=0;i&lt;init_params-&gt;size();i++){
      params.push_back((*init_params)[i].detach().set_requires_grad(true));
    }
    auto options = torch::optim::LBFGSOptions()
      .lr($(double lr))
      .max_iter($(int max_iter))
      .max_eval($(int max_eval))
      .tolerance_grad($(double tolerance_grad))
      .tolerance_change($(double tolerance_change))
      .history_size($(int history_size))
      .line_search_fn(*$(std::string* line_search_fn));
    torch::optim::LBFGS* optimizer = new torch::optim::LBFGS(params, options);
    optimizer-&gt;zero_grad();
    return dynamic_cast&lt;torch::optim::Optimizer*&gt;(optimizer);
  }|]</span></span><span>
</span><span id="line-259"></span><span>
</span><span id="line-260"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#getParams"><span class="hs-identifier hs-type">getParams</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span class="hs-special">)</span><span> </span><span>
</span><span id="line-261"></span><span id="getParams"><span class="annot"><span class="annottext">getParams :: Ptr Optimizer -&gt; IO (Ptr TensorList)
</span><a href="Torch.Internal.Unmanaged.Optim.html#getParams"><span class="hs-identifier hs-var hs-var">getParams</span></a></span></span><span> </span><span id="local-6989586621684956183"><span class="annot"><span class="annottext">Ptr Optimizer
</span><a href="#local-6989586621684956183"><span class="hs-identifier hs-var">optimizer</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-262"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| std::vector&lt;at::Tensor&gt;* {
    return new std::vector&lt;at::Tensor&gt;($(torch::optim::Optimizer* optimizer)-&gt;param_groups().at(0).params());
  }|]</span></span><span>
</span><span id="line-265"></span><span>
</span><span id="line-266"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#step"><span class="hs-identifier hs-type">step</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span class="hs-special">)</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-267"></span><span id="step"><span class="annot"><span class="annottext">step :: Ptr Optimizer
-&gt; (Ptr TensorList -&gt; IO (Ptr Tensor)) -&gt; IO (Ptr Tensor)
</span><a href="Torch.Internal.Unmanaged.Optim.html#step"><span class="hs-identifier hs-var hs-var">step</span></a></span></span><span> </span><span id="local-6989586621684956180"><span class="annot"><span class="annottext">Ptr Optimizer
</span><a href="#local-6989586621684956180"><span class="hs-identifier hs-var">optimizer</span></a></span></span><span> </span><span id="local-6989586621684956179"><span class="annot"><span class="annottext">Ptr TensorList -&gt; IO (Ptr Tensor)
</span><a href="#local-6989586621684956179"><span class="hs-identifier hs-var">lossFunc</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-268"></span><span>  </span><span class="annot"><span class="annottext">IO (FunPtr (Ptr () -&gt; IO (Ptr ())))
-&gt; (FunPtr (Ptr () -&gt; IO (Ptr ())) -&gt; IO ())
-&gt; (FunPtr (Ptr () -&gt; IO (Ptr ())) -&gt; IO (Ptr Tensor))
-&gt; IO (Ptr Tensor)
forall (m :: * -&gt; *) a b c.
MonadMask m =&gt;
m a -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) -&gt; m c
</span><a href="../file:///nix/store/ppq6k5yv7rcfj913wd07v54z631236px-safe-exceptions-lib-safe-exceptions-0.1.7.1-haddock-doc/share/doc/safe-exceptions/html/src"><span class="hs-identifier hs-var">bracket</span></a></span><span>
</span><span id="line-269"></span><span>    </span><span class="hs-special">(</span><span class="annot"><span class="annottext">(Ptr () -&gt; IO (Ptr ())) -&gt; IO (FunPtr (Ptr () -&gt; IO (Ptr ())))
</span><a href="../../../../libtorch-ffi-helper/html/src"><span class="hs-identifier hs-var">callbackHelper</span></a></span><span> </span><span class="annot"><span class="annottext">Ptr () -&gt; IO (Ptr ())
</span><a href="#local-6989586621684956177"><span class="hs-identifier hs-var">lossFunc'</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-270"></span><span>    </span><span class="annot"><span class="annottext">FunPtr (Ptr () -&gt; IO (Ptr ())) -&gt; IO ()
forall a. FunPtr a -&gt; IO ()
</span><span class="hs-identifier hs-var">freeHaskellFunPtr</span></span><span>
</span><span id="line-271"></span><span>    </span><span class="annot"><span class="annottext">((FunPtr (Ptr () -&gt; IO (Ptr ())) -&gt; IO (Ptr Tensor))
 -&gt; IO (Ptr Tensor))
-&gt; (FunPtr (Ptr () -&gt; IO (Ptr ())) -&gt; IO (Ptr Tensor))
-&gt; IO (Ptr Tensor)
forall a b. (a -&gt; b) -&gt; a -&gt; b
</span><span class="hs-operator hs-var">$</span></span><span> </span><span class="hs-glyph">\</span><span id="local-6989586621684956175"><span class="annot"><span class="annottext">FunPtr (Ptr () -&gt; IO (Ptr ()))
</span><a href="#local-6989586621684956175"><span class="hs-identifier hs-var">funcPtr</span></a></span></span><span> </span><span class="hs-glyph">-&gt;</span><span>
</span><span id="line-272"></span><span>      </span><span class="annot"><span class="">[C.throwBlock| at::Tensor* {
        auto tfunc = $(void* (*funcPtr)(void*));
        auto optimizer = $(torch::optim::Optimizer* optimizer);
        typedef at::Tensor* (*Func)(std::vector&lt;at::Tensor&gt;*);
        auto func = (Func)tfunc;
        auto v = optimizer-&gt;step([&amp;]{
          optimizer-&gt;zero_grad();
          auto loss = func(&amp;(optimizer-&gt;param_groups().at(0).params()));
          loss-&gt;backward();
          return *loss;
        });
        return new at::Tensor(v);
      }|]</span></span><span>
</span><span id="line-285"></span><span>  </span><span class="hs-keyword">where</span><span>
</span><span id="line-286"></span><span>    </span><span class="annot"><a href="#local-6989586621684956177"><span class="hs-identifier hs-type">lossFunc'</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span class="hs-special">)</span><span>
</span><span id="line-287"></span><span>    </span><span id="local-6989586621684956177"><span class="annot"><span class="annottext">lossFunc' :: Ptr () -&gt; IO (Ptr ())
</span><a href="#local-6989586621684956177"><span class="hs-identifier hs-var hs-var">lossFunc'</span></a></span></span><span> </span><span id="local-6989586621684956173"><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621684956173"><span class="hs-identifier hs-var">params</span></a></span></span><span> </span><span class="hs-glyph">=</span><span> </span><span class="annot"><span class="annottext">Ptr Tensor -&gt; Ptr ()
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">(Ptr Tensor -&gt; Ptr ()) -&gt; IO (Ptr Tensor) -&gt; IO (Ptr ())
forall (f :: * -&gt; *) a b. Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
</span><span class="hs-operator hs-var">&lt;$&gt;</span></span><span> </span><span class="annot"><span class="annottext">Ptr TensorList -&gt; IO (Ptr Tensor)
</span><a href="#local-6989586621684956179"><span class="hs-identifier hs-var">lossFunc</span></a></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="annottext">Ptr () -&gt; Ptr TensorList
forall a b. Ptr a -&gt; Ptr b
</span><span class="hs-identifier hs-var">castPtr</span></span><span> </span><span class="annot"><span class="annottext">Ptr ()
</span><a href="#local-6989586621684956173"><span class="hs-identifier hs-var">params</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-288"></span><span>
</span><span id="line-289"></span><span class="hs-comment">-- After this function is called, params(TensorList) of input is updated.</span><span>
</span><span id="line-290"></span><span class="hs-comment">-- TensorList of output is the same as input's params(TensorList).</span><span>
</span><span id="line-291"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#unsafeStep"><span class="hs-identifier hs-type">unsafeStep</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Tensor"><span class="hs-identifier hs-type">Tensor</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#TensorList"><span class="hs-identifier hs-type">TensorList</span></a></span><span class="hs-special">)</span><span>
</span><span id="line-292"></span><span id="unsafeStep"><span class="annot"><span class="annottext">unsafeStep :: Ptr Optimizer
-&gt; Ptr TensorList -&gt; Ptr Tensor -&gt; IO (Ptr TensorList)
</span><a href="Torch.Internal.Unmanaged.Optim.html#unsafeStep"><span class="hs-identifier hs-var hs-var">unsafeStep</span></a></span></span><span> </span><span id="local-6989586621684956169"><span class="annot"><span class="annottext">Ptr Optimizer
</span><a href="#local-6989586621684956169"><span class="hs-identifier hs-var">optimizer</span></a></span></span><span> </span><span id="local-6989586621684956168"><span class="annot"><span class="annottext">Ptr TensorList
</span><a href="#local-6989586621684956168"><span class="hs-identifier hs-var">params</span></a></span></span><span> </span><span id="local-6989586621684956167"><span class="annot"><span class="annottext">Ptr Tensor
</span><a href="#local-6989586621684956167"><span class="hs-identifier hs-var">loss</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-293"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| std::vector&lt;at::Tensor&gt;* {
    auto optimizer = $(torch::optim::Optimizer* optimizer);
    auto loss = $(at::Tensor* loss);
    optimizer-&gt;param_groups().at(0).params() = *$(std::vector&lt;at::Tensor&gt;* params);
    optimizer-&gt;zero_grad();
    loss-&gt;backward();
    optimizer-&gt;step();
    return new std::vector&lt;at::Tensor&gt;(optimizer-&gt;param_groups().at(0).params());
  }|]</span></span><span>
</span><span id="line-302"></span><span>
</span><span id="line-303"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#save"><span class="hs-identifier hs-type">save</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#StdString"><span class="hs-identifier hs-type">StdString</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-304"></span><span id="save"><span class="annot"><span class="annottext">save :: Ptr Optimizer -&gt; Ptr StdString -&gt; IO ()
</span><a href="Torch.Internal.Unmanaged.Optim.html#save"><span class="hs-identifier hs-var hs-var">save</span></a></span></span><span> </span><span id="local-6989586621684956164"><span class="annot"><span class="annottext">Ptr Optimizer
</span><a href="#local-6989586621684956164"><span class="hs-identifier hs-var">optimizer</span></a></span></span><span> </span><span id="local-6989586621684956163"><span class="annot"><span class="annottext">Ptr StdString
</span><a href="#local-6989586621684956163"><span class="hs-identifier hs-var">filename</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-305"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| void {
    std::ofstream output(*$(std::string* filename));
    torch::save(*$(torch::optim::Optimizer* optimizer),output);
  }|]</span></span><span>
</span><span id="line-309"></span><span>
</span><span id="line-310"></span><span class="annot"><a href="Torch.Internal.Unmanaged.Optim.html#load"><span class="hs-identifier hs-type">load</span></a></span><span> </span><span class="hs-glyph">::</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#Optimizer"><span class="hs-identifier hs-type">Optimizer</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">Ptr</span></span><span> </span><span class="annot"><a href="Torch.Internal.Type.html#StdString"><span class="hs-identifier hs-type">StdString</span></a></span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="annot"><span class="hs-identifier hs-type">IO</span></span><span> </span><span class="hs-special">(</span><span class="hs-special">)</span><span>
</span><span id="line-311"></span><span id="load"><span class="annot"><span class="annottext">load :: Ptr Optimizer -&gt; Ptr StdString -&gt; IO ()
</span><a href="Torch.Internal.Unmanaged.Optim.html#load"><span class="hs-identifier hs-var hs-var">load</span></a></span></span><span> </span><span id="local-6989586621684956160"><span class="annot"><span class="annottext">Ptr Optimizer
</span><a href="#local-6989586621684956160"><span class="hs-identifier hs-var">optimizer</span></a></span></span><span> </span><span id="local-6989586621684956159"><span class="annot"><span class="annottext">Ptr StdString
</span><a href="#local-6989586621684956159"><span class="hs-identifier hs-var">filename</span></a></span></span><span> </span><span class="hs-glyph">=</span><span>
</span><span id="line-312"></span><span>  </span><span class="annot"><span class="">[C.throwBlock| void {
    std::ifstream input(*$(std::string* filename));
    torch::load(*$(torch::optim::Optimizer* optimizer),input);
  }|]</span></span><span>
</span><span id="line-316"></span></pre></body></html>